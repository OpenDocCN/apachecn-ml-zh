# 强化学习

**强化学习** ( **RL** )是机器学习继有监督和无监督学习之后的第三大板块。这些技术近年来在人工智能的应用中获得了很大的吸引力。在强化学习中，要进行顺序决策，而不是一次性决策，这使得很难在少数情况下训练模型。在这一章中，我们将介绍强化学习中使用的各种技术，并提供实例来支持。虽然涵盖所有主题超出了本书的范围，但我们确实涵盖了最重要的基础知识，让读者对这一主题产生足够的热情。本章讨论的主题有:

*   马尔可夫决策过程
*   贝尔曼方程
*   动态规划
*   蒙特卡罗方法
*   时间差异学习
*   强化学习和机器学习综合应用的人工智能最新趋势

# 强化学习导论

强化学习模仿人类的学习方式:通过与环境互动，重复获得较高回报的行动，避免因行动而获得较低或负面回报的冒险行动。

![](assets/6b47f60b-b4fc-4091-8259-2b872f998aa7.png)

# 详细比较有监督、无监督和强化学习

由于机器学习有三个主要部分，让我们从较高的层次来看看主要的区别和相似之处:

*   **监督学习:**在监督学习中，我们有一个训练集，对于每个训练算法，我们都给出了正确的答案。训练示例包含所有正确的答案，训练算法的工作就是复制正确的答案。
*   **无监督学习:**在无监督学习中，我们有一组未标记的数据和一个学习算法。学习算法的工作是用 k-means、PCA 等算法来寻找数据中的结构。
*   **强化学习:**在强化学习中，我们没有目标变量。相反，我们有奖励信号，代理需要自己规划路径，以达到奖励存在的目标。

# 强化学习的特点

*   奖励信号的反馈不是即时的。它被延迟了许多时间步长
*   实现目标需要顺序决策，因此时间在强化问题中起着重要作用(这里数据的 IID 假设不成立)
*   代理的操作会影响其接收的后续数据

在强化学习中，需要一点点监督，但与监督学习相比，监督要少得多。

以下是强化学习问题的几个实际例子:

*   **自主直升机:**自主直升机的目标是通过控制操纵杆、踏板等，改变其滚转、俯仰和偏航来控制其位置。传感器每秒发送 10 次输入，提供直升机位置和方向的精确估计。直升机的工作是接收这种输入，并控制操纵杆相应地移动。当直升机处于这个位置和方位时，很难提供直升机下一步需要做什么的信息，也没有训练集来控制动作。相反，RL 算法给出了不同类型的反馈:当直升机表现良好时，它会给出奖励信号，当直升机做错事情时，它会给出负面奖励。基于这些信号，直升机控制旅程。学习算法的工作是提供奖励函数并自行训练路径。
*   **电脑象棋:**电脑下棋是另一个例子，在游戏的任何阶段，我们都不会事先知道最佳的棋步是什么；所以用监督学习算法下棋是非常困难的。很难说 *X* 是棋盘位置而 *Y* 是这个特定棋盘位置的最佳移动。取而代之的是，每当它赢了一局，我们提供奖励(+1)，每当它输了一局，我们给出负奖励(-1)，我们让算法算出在一段时间内赢得游戏的必要步骤。
*   **训练猫咪:**我们在猫咪做好事的时候给它奖励，每次它做坏事的时候，我们都明确表示这是不好的行为。一段时间后，猫学会做更多的好事，少做坏事。

**强化学习比监督学习难的原因包括:**

*   这不是一次性的决策问题。因此，在监督学习中，一种算法根据给定的属性预测某人是否患有癌症；而在 RL 中，你必须在一段时间内持续采取行动。我们称之为顺序决策。
*   在一盘棋中，我们在赢/输之前走了 60 步。我们不确定哪些是正确的动作，哪些是错误的动作。在第 25 步，我们走错了一步，最终导致我们在第 60 步输掉了比赛。
*   信用分配问题:(正面或负面奖励)多做一件好事，少做一件坏事。
*   在汽车碰撞的例子中，在碰撞前的某个时刻，驾驶员可能会刹车。然而，不是刹车导致了撞车，而是刹车前发生了一些事情，最终导致了撞车。RL 在一段时间内学习模式，这可能包括之前开得太快，没有观察其他道路交通，忽略警告标志，等等。
*   RL 适用于不同的应用，当有长期后果时，用于顺序决策。

# 强化学习基础

在我们深入探讨强化学习的细节之前，我想介绍一些理解 RL 方法的各种细节所必需的基础知识。这些基础知识出现在本章的各个部分，我们将在需要时详细解释:

*   **环境:**这是任何有状态的系统，以及状态间转换的机制。例如，机器人的环境是它操作的景观或设施。
*   **Agent:** 这是一个与环境交互的自动化系统。
*   **状态:**环境或系统的状态是完全描述环境的一组变量或特征。
*   **目标或吸收状态或终结状态:**这是提供比其他任何状态更高折扣累积奖励的状态。高累积奖励可防止最佳策略依赖于训练期间的初始状态。每当一个特工达到目的，我们就要完成一集。
*   **动作:**这定义了状态之间的转换。代理负责执行或至少推荐一个动作。在执行该动作时，代理从环境中收集奖励(或惩罚)。
*   **策略:**这定义了要为环境的任何状态选择和执行的操作。换句话说，政策就是代理人的行为；这是一张从国家到行动的地图。政策可以是确定性的，也可以是随机的。
*   **最佳策略:**这是通过训练生成的策略。它定义了 Q-learning 中的模型，并随着任何新的情节不断更新。
*   **奖励**:这量化了代理人与环境的正面或负面互动。奖励通常是代理人到达每个州后立即获得的收入。
*   **回报或价值函数**:价值函数(也叫回报)是对每个状态未来回报的预测。这些用于评估状态的好/坏，在此基础上，代理将选择/采取行动来选择下一个最佳状态:

![](assets/aac43ed9-9f5f-486b-a832-a8b83b89af67.jpg)

*   **插曲:**这定义了从初始状态达到目标状态所需的步骤数。剧集也被称为试验。
*   **地平线:**这是在奖励最大化中使用的未来步骤或动作的数量。地平线可以是无限的，在这种情况下，为了让政策的价值趋同，未来的回报会打折扣。
*   **探索对开发:** RL 是一种试错学习。目标是找到最佳政策；同时，保持警惕，探索一些未知的政策。一个经典的例子是寻宝:如果我们只是贪婪地去那些地方(探索)，我们就找不到其他可能存在隐藏宝藏的地方(探索)。通过探索未知的状态，通过冒险，即使当眼前的回报很低并且没有失去最大回报时，我们也可能实现更大的目标。换句话说，我们正在逃离局部最优，以便实现全局最优(这就是探索)，而不仅仅是单纯关注眼前的回报(这就是剥削)。这里有几个例子来解释这种差异:
    *   **餐厅选择**:偶尔去逛逛不知名的餐厅，可能会发现比我们经常喜欢的餐厅好很多的餐厅:
        *   **剥削**:去自己喜欢的餐厅
        *   **探索**:尝试新餐厅
    *   **石油钻探示例:**通过探索新的未开发位置，我们可能会获得比仅仅探索同一个地方更有益的新见解:
        *   **开采**:在已知最佳位置钻探石油
        *   **探索**:在新的位置钻孔
*   **状态-值对状态-动作函数:**在动作-值中，Q 表示代理在状态 *S* 下采取动作 *A* 时，以及之后按照某个策略π(a|s)行事(这是在给定状态下采取动作的概率)时，将获得的预期回报(累计折扣奖励)。

在状态值中，该值是座席处于状态 *s* 时根据策略 *π(a|s)* 进行操作的预期回报。更具体地说，状态值是对策略下的操作值的预期:

![](assets/82c099fd-f397-44a9-a6ab-e225f4196949.jpg)

*   **策略内与策略外 TD 控制:**策略外学习者独立于代理的动作学习最优策略的值。Q-learning 是一个偏离政策的学习者。策略学习者学习由代理执行的策略的价值，包括探索步骤。
*   **预测与控制问题:**预测讲的是我做得有多好，基于给定的政策:意思是，如果有人给了我一个政策，我执行了，我会因此得到多少奖励。然而，在控制中，问题是找到最好的策略，这样我就可以获得最大的回报。
*   **预测:**评估给定策略的状态值。

对于统一随机策略，所有状态的价值函数是什么？

*   **控制:**通过寻找最佳策略来优化未来。

什么是所有可能策略的最优价值函数，什么是最优策略？

通常在强化学习中，我们需要先解决预测问题，然后才能解决控制问题，就像我们需要找出所有的策略来找出最佳或最优的策略一样。

*   **RL 代理分类:**RL 代理包括以下一个或多个组件:
    *   **策略:**代理的行为功能(从状态到动作的映射)；策略可以是确定性的，也可以是随机的
    *   **价值函数:**每个状态对每个状态的预期未来奖励的预测有多好
    *   **模型:**代理对环境的表示。一个模型预测环境下一步会做什么:
        *   **跃迁:** p 预测下一个状态(即动力学):

![](assets/855b6f6d-281e-4744-87c3-1370d42f75bf.jpg)

![](assets/ef69378e-ca7f-4093-9c42-bee0cb0ec3b8.jpg)

让我们解释 RL 代理分类法中基于策略和价值组合的各种可能的类别，并用下面的迷宫示例对单个组件进行建模。在下面的迷宫中，你既有起点，也有目标；代理需要尽快达到目标，走一条获得总最大奖励和最小总负奖励的路径。这个问题主要有五种分类方法可以解决:

*   基于价值
*   基于策略
*   演员评论家
*   无模型
*   基于模型

![](assets/09e53731-bc14-4177-b702-a74419bfb9a1.png)

# 类别 1 -基于价值

价值函数看起来确实像图像的右边(未来折扣奖励的总和)，其中每个州都有一些价值。假设，距离目标一步之遥的状态的值为-1；距离目标两步的值为-2。同样，起点的值为-16。如果代理卡在错误的地方，该值可能高达-24。事实上，代理确实会根据达到目标的最佳可能值在网格中移动。例如，代理处于值为-15 的状态。在这里，它可以选择向北或向南移动，所以它选择向北移动是因为奖励很高，为-14，而不是向南移动，值为-16。通过这种方式，代理选择它在网格中的路径，直到它达到目标。

*   **值函数**:在所有状态下只定义值
*   **无策略(隐式)**:不存在独占策略；策略是根据每个州的值来选择的

![](assets/6f2a9deb-42c4-4eac-8b69-88e19b4bd5ea.png)

# 第 2 类-基于政策

下图中的箭头表示座席在任何这些状态下选择的下一个移动方向。例如，代理首先向东移动，然后向北移动，跟随所有箭头，直到达到目标。这也称为从状态到动作的映射。一旦我们有了这个映射，代理只需要读取它并做出相应的行为。

*   **政策**:政策或箭头得到调整，以达到未来可能的最大回报。顾名思义，只有策略被存储和优化，才能实现回报最大化。
*   **无值函数**:状态不存在值。

![](assets/d9647e45-8bfa-4503-80c1-e7426836dbf0.png)

# 第三类-演员兼评论家

在 Actor-criteria 中，我们同时具有策略和价值功能(或者是基于价值和基于策略的组合)。这种方法是两全其美的:

*   政策
*   价值函数

# 第 4 类-无型号

在 RL 中，一个基本的区别是它是基于模型的还是无模型的。在无模型环境中，我们没有明确地对环境建模，或者我们不知道完整环境的全部动态。相反，我们只需直接转到策略或价值函数来获得经验，并弄清楚策略如何影响奖励:

*   策略和/或价值功能
    *   没有模型

# 第 5 类-基于模型

在基于模型的 RL 中，我们首先构建环境的整个动态:

*   策略和/或价值功能
*   模型

在浏览了所有上述类别之后，下面的文氏图显示了一个位置上 RL 代理的整个分类法。如果你拿起任何与强化学习相关的论文，这些方法可以适用于这一领域的任何部分。

![](assets/25d39897-99ba-408b-9aac-89699ddff267.png)

# 顺序决策中的基本范畴

顺序决策中有两种基本类型的问题:

*   **强化学习**(例如自主直升机等):
    *   环境最初是未知的
    *   代理与环境交互，并从环境中获得策略、奖励和价值
    *   代理改进了其策略
*   **策划**(如国际象棋、雅达利游戏等):
    *   环境模型或环境的完整动力学是已知的
    *   代理使用其模型执行计算(没有任何外部交互)
    *   代理改进了其策略
    *   这些类型的问题也被称为推理、搜索、自省等等

虽然前面两个类别可以根据给定的问题联系在一起，但这基本上是两种类型的设置的广泛观点。

# 马尔可夫决策过程和贝尔曼方程

**马尔可夫决策过程** ( **MDP** )正式描述了一个强化学习的环境。其中:

*   环境是完全可观察的
*   当前状态完全表征了过程(这意味着未来状态完全依赖于当前状态，而不是历史状态或值)
*   几乎所有的 RL 问题都可以形式化为多目标规划(例如，最优控制主要处理连续的多目标规划)

**MDP 的中心思想:** MDP 研究一个国家的简单马尔可夫性质；例如，*S<sub>t+1</sub>T5】完全依赖最新状态*S<sub>t</sub>T9】而非任何历史依赖。在下面的等式中，当前状态捕获历史中的所有相关信息，这意味着当前状态是对未来的充分统计:**

![](assets/fff4f0fa-d259-4ab2-aa28-87c1c100eae3.jpg)

这个属性的直观意义可以用自动直升机的例子来解释:下一步是让直升机要么向右、向左、俯仰，要么滚转等等，完全取决于直升机的当前位置，而不是五分钟前的位置。

**MDP 建模:** RL 问题使用 MDP 公式作为五元组来建模世界( *S，A，{P <sub>sa</sub> }，y，R* )

*   *S* -状态集(直升机可能方位集)
*   *A* -一组动作(设置所有可能拉动操纵杆的位置)
*   *P <sub>sa</sub>* -状态转移分布(或状态转移概率分布)提供从一个状态到另一个状态的转移以及马尔可夫过程所需的相应概率:

![](assets/80e7ceac-3a25-4712-9867-e0ac0b6ceeb4.jpg)

![](assets/90b5eefe-105a-4a50-9d06-f178dbef95af.jpg)

*   γ -折扣系数:

![](assets/407e9280-c087-4a72-9084-439aa6063fb7.jpg)

*   R -奖赏函数(将一组状态映射到实数，正数或负数):

![](assets/eca4bf7f-48fb-4520-a2b7-d54e94989bf4.jpg)

回报的计算方法是对未来的回报进行贴现，直到达到最终状态。

**MDP 的贝尔曼方程:**MDP 的数学公式采用贝尔曼方程，求解得到环境的最优策略。贝尔曼方程也被称为动态规划方程，并且是与被称为动态规划的数学优化方法相关联的最优性的必要条件。贝尔曼方程是可以在整个环境中求解的线性方程。然而，求解这些方程的时间复杂度是 *O (n <sup>3</sup> )* ，当环境中的状态数量较大时，这在计算上变得非常昂贵；有时，探索所有的州是不可行的，因为环境本身就很大。在这些情况下，我们需要考虑其他解决问题的方法。

在贝尔曼方程中，价值函数可以分解为两部分:

*   立竿见影的奖励 *R <sub>t+1</sub>* ，来自你最终将与之同归于尽的继承国
*   继承州的折扣值 *yv(S <sub>t+1</sub> )* 从该时间步长开始，您将获得:

![](assets/830ec35d-a609-4a95-82ca-75fb66918d12.jpg)

![](assets/b051d78c-2eea-4d68-965e-d05bb7171a8a.jpg)

![](assets/c32f127f-9949-4095-8134-68ab17463ce6.jpg)

![](assets/c6eaafb3-5304-45c1-a4bd-e282114216ea.jpg)

![](assets/f9fe05f8-37d8-48b4-92ed-f50e7020694e.jpg)

![](assets/048a5882-984e-4c0f-8247-183524445ebb.jpg)

**MDP 的网格世界示例:**机器人导航任务生活在以下类型的网格世界中。一个障碍物显示为单元(2，2)，机器人无法通过它导航。我们希望机器人移动到右上角的单元格(4，3)，当它到达该位置时，机器人将获得+1 的奖励。机器人应该避开牢房(4，2)，因为如果它进入那个牢房，它将获得-1 奖励。

![](assets/3b4cd702-4e69-483c-b1bc-38a87145d44d.png)

机器人可以处于以下任何位置:

*   *11 个状态* -(除了细胞(2，2)，其中我们有一个机器人的障碍)
*   A = {北偏北、南偏南、东偏东、西偏西}

在现实世界中，机器人的动作是嘈杂的，机器人可能无法准确地移动到它被要求移动的地方。例子可能包括它的一些轮子打滑，它的零件连接松散，它有不正确的驱动器，等等。当被要求移动 1 米时，它可能不能准确移动 1 米；相反，它可以移动 90-105 厘米，以此类推。

在简化的网格世界中，机器人的随机动力学可以建模如下。如果我们命令机器人向北走，机器人有 10 %的几率向左拖动，有 10%的几率向右拖动。实际上只有 80%的时间可能是向北的。当机器人从墙上反弹(包括障碍物)并停留在同一位置时，不会发生任何事情:

![](assets/99656f10-d888-4675-a9d1-a59bc214bfcd.png)

这个网格世界示例中的每个状态都由(x，y)坐标表示。假设它处于状态(3，1)，我们要求机器人向北移动，那么状态转移概率矩阵如下:

![](assets/8d5a795d-50a1-4c93-9eba-d4eab69f7c3f.jpg)

![](assets/ef9e0bb5-1996-47e4-bdea-7e8ebae05a9e.jpg)

![](assets/d7fd3afd-ef0b-4b3f-a14b-93fd389bf9f8.jpg)

![](assets/f1a7d98e-b6ec-4485-950f-501bf1ba88c1.jpg)

![](assets/d876b092-88ff-4381-9b19-161f8b0d2d78.jpg)

机器人停留在同一位置的概率为 0。

正如我们所知，所有状态转移概率总和总计为 1:

![](assets/29cec2a1-a861-4f4d-a4de-e1d27f7c1729.jpg)

奖励功能:

![](assets/51cdfe66-82e0-4423-955d-ebaae328f81d.jpg)

![](assets/394bf3e9-f028-401e-98ab-e71a486b9ca0.jpg)

![](assets/99079c6f-6d73-43ce-b513-c345ee270ec5.jpg)

对于所有其他状态，都有小的负奖励值，这意味着它会在绕着电网运行时为机器人的电池或燃料消耗充电，这将创建在达到奖励+1 的目标时不会浪费移动或时间的解决方案，这将鼓励机器人以尽可能少的燃料使用尽快达到目标。

当机器人达到+1 或-1 状态时，世界结束。达到任何一种状态后都不可能有更多的奖励；这些可以称为吸收状态。这些都是零成本吸收状态，机器人永远呆在那里。

MDP 工作模式:

*   在状态 *S <sub>0</sub>*
*   选择*a<sub>0</sub>T3】*
*   到达*S<sub>1</sub>~ P*<sub>*s0*，a0</sub>
*   选择*a<sub>1</sub>T3】*
*   到达*S<sub>2</sub>~ P*T4*S1*、*a1*T9】
*   等等....

过了一段时间，需要所有的奖励和总结才能获得:

![](assets/e97a5b8a-e83f-4d7a-876c-ee6bf9b4ad56.jpg)

![](assets/c3f07521-2279-4ece-95f5-c47f6f016b1f.jpg)

贴现因子模拟了一个经济应用，其中今天赚的一美元比明天赚的一美元更有价值。

机器人需要随时间选择动作(一个 <sub>0</sub> ，一个 <sub>1</sub> ，一个 <sub>2，....</sub>)最大化预期收益:

![](assets/a1b8ba85-d923-4fff-bb79-f3ef6cf881db.jpg)

在此期间，强化学习算法学习一个策略，该策略是每个状态的动作映射，这意味着它是一个推荐的动作，机器人需要根据它存在的状态采取该动作:。

![](assets/a4d4919b-08c7-4792-846b-970e4073eb4c.jpg)

**网格世界的最优策略:**策略从状态映射到动作，也就是说，如果你处于一个特定的状态，你需要采取这个特定的动作。以下策略是使总回报或折扣奖励总和的期望值最大化的最优策略。策略总是查看当前状态，而不是以前的状态，这是马尔可夫属性:

![](assets/3db0d6aa-9ddd-4276-86c1-7d0f056cb1e8.png)

一个棘手的问题是在位置(3，1):最优政策显示向左(西)而不是向北(北)，这可能会有更少的州；然而，我们可能会进入一个更危险的状态。所以，左转可能需要更长的时间，但它会安全到达目的地，而不会陷入负面陷阱。这些类型的东西可以从计算中获得，这对于人类来说并不明显，但是计算机非常擅长提出这些策略:

定义: *V <sup>π</sup> ，V*，π**

*V <sup>π</sup>* =对于任何给定的保单π，价值函数为 *V <sup>π</sup> : S - > R* 使得 *V <sup>π</sup> (S)* 是从状态 S 开始的预期总收益，并执行π

![](assets/bb43a510-6d2a-4074-b978-a313938ff566.jpg)

**网格世界的随机策略:**以下是随机策略及其值函数的示例。这项政策是一项相当糟糕的负面政策。对于任何策略，我们都可以记下该特定策略的价值函数:

![](assets/87b34858-50d5-4721-ae17-06d67ac31eec.png)

![](assets/10e84dc6-cd74-4239-93a5-c50b671bb8c4.jpg)

![](assets/3545c5a5-6e39-4060-bdce-7aa699eda3df.jpg)

![](assets/993ed20c-4285-4b2e-ad2b-94815402a103.jpg)

![](assets/ec76ccfa-8216-48d6-80ec-91b67596b571.jpg)

![](assets/a0e04406-276a-435f-b84f-e3ff7804c543.jpg)

用简单的英语来说，贝尔曼方程说明当前状态的值等于应用于新状态的预期总回报的即时奖励和折扣因子(*S’*)乘以它们对这些状态采取行动(策略)的概率。

贝尔曼方程用于求解封闭形式策略的价值函数，给定固定策略，如何求解价值函数方程。

贝尔曼方程对价值函数施加了一组线性约束。原来，我们通过求解一组线性方程组来求解任意状态 *S* 下的价值函数。

**有网格世界问题的贝尔曼方程示例:**

为单元格 *(3，1)* 选择的策略是向北移动。然而，我们在系统中有随机性，大约 80%的时间它在所述方向上移动，并且 *20%* 的时间它侧向漂移，或者向左(10%)或者向右(10%)。

![](assets/fb881b87-50c3-4122-afea-af7c95f9beab.jpg)

![](assets/94b277b8-982c-4979-be4a-2674b6643af4.png)

![](assets/29c70538-50c2-4817-b49b-202202c88797.jpg)

网格内 MDP 的所有 11 种状态都可以写出类似的方程。我们可以获得以下指标，我们将使用线性方程方法系统从这些指标中求解所有未知值:

*   11 个方程
*   11 个未知的价值函数变量
*   11 项限制

这是用`n`方程求解一个`n`变量问题，对于这个问题，我们可以使用一个方程组很容易地找到精确的解的形式，从而得到由所有状态组成的网格的整个封闭形式的 V (π)的精确解。

# 动态规划

动态规划是一种通过将复杂问题分解为子问题并解决每个子问题来解决它们的顺序方法。一旦它解决了子问题，它就把这些子问题的解决方案放在一起，以解决原来的复杂问题。在强化学习领域，动态规划是一种求解方法，在给定环境的完美模型如马尔可夫决策过程(MDP)的情况下，计算最优策略。

动态编程适用于具有以下两个属性的问题。MDP 实际上满足这两个属性，这使得 DP 非常适合通过求解贝尔曼方程来解决它们:

*   最优子结构
    *   优选原则适用
    *   最优解可以分解为子问题
*   重叠子问题
    *   子问题重复出现多次
    *   解决方案可以缓存和重用
*   幸运的是，MDP 同时满足这两个属性！
    *   贝尔曼方程具有状态值的递归分解
    *   价值函数存储和重用解决方案

然而，经典的动态规划算法在强化学习中的效用有限，这是因为它们假设了一个完美的模型和高计算开销。然而，这仍然很重要，因为它们为理解 RL 领域中的所有方法提供了必要的基础。

# 用动态规划计算最优策略的算法

利用动态规划计算 MDP 最优策略的标准算法如下，我们将在本章后面的部分详细介绍这两种算法:

*   **值迭代算法:**迭代算法，对状态值进行迭代，直到达到最优值；并且随后，最佳值被用来确定最佳策略
*   **策略迭代算法:**一种迭代算法，其中策略评估和策略改进交替使用以达到最优策略

**值迭代算法:**值迭代算法很容易计算，因为它只对状态值进行迭代应用。首先，我们将计算最优值函数 *V** ，然后将这些值插入最优策略方程以确定最优策略。仅给出问题的大小，对于 11 个可能的州，每个州可以有四个政策(北-北、南-南、东-东、西)，这给出了一个总体的 11 个 <sup>4 个</sup>可能的政策。值迭代算法由以下步骤组成:

1.  初始化所有状态的 *V(S) = 0*
2.  对于每个，更新:

![](assets/4a102a3e-7209-4117-b8ec-3eeb2f3c723e.jpg)

3.  通过重复计算步骤 2，我们将最终收敛到所有状态的最优值:

![](assets/a5dc0cdc-dac6-4cd8-8eaf-dddca84832b3.jpg)

在算法的步骤 2 中，有两种更新值的方法

*   **同步更新** -通过执行同步更新(或贝尔曼备份操作符)，我们将执行 RHS 计算，并用以下公式代替 LHS:

![](assets/04c2e370-be36-49b4-a4a5-4a90a3e2a97b.jpg)

*   **异步更新** -一次更新一个状态的值，而不是同时更新所有状态，其中状态将以固定顺序更新(更新状态号 1，然后是 2，以此类推。).在收敛期间，异步更新比同步更新快一点。

**网格世界上值迭代的说明示例:**下图说明了值迭代在网格世界上的应用，本节末尾提供了解决一个实际问题的完整代码。在使用贝尔曼方程将先前的值迭代算法应用于 MDP 之后，我们获得了所有状态的以下最优值 V*(伽马值选择为 *0.99* ):

![](assets/ee152c5d-dc67-47d6-aea3-33f9720d402a.png)

当我们将这些值插入到我们的策略等式中时，我们获得了以下策略网格:

![](assets/64353d0d-c72b-4a29-89c4-a3d3886ffa6a.jpg)

![](assets/312a4378-55a5-4703-9c8b-208c36c9283e.png)

这里，在位置(3，1)处，我们想从数学上证明为什么最优政策建议向左(西)而不是向上(北):

![](assets/d9453760-74dd-4254-8dc1-171416637e42.jpg)

![](assets/d5aee181-ca37-46b6-9083-cb98eebccb50.jpg)

Due to the wall, whenever the robot tries to move towards South (downwards side), it will remain in the same place, hence we assigned the value of the current position 0.71 for probability of 0.1.

同样，对于 north，我们计算总收益如下:

![](assets/fa9282f8-491d-4f08-9fac-98bbab470b13.jpg)

![](assets/064676af-8cfd-4b30-aa9d-2987e08dc77b.jpg)

因此，最好是向西而不是向北移动，因此选择了最佳政策来这样做。

**策略迭代算法:**策略迭代是获得 MDP 最优策略的另一种方式，其中策略评估和策略改进算法被迭代地应用，直到解收敛到最优策略。策略迭代算法由以下步骤组成:

1.  初始化随机策略π
2.  重复执行以下操作，直到收敛
    *   使用线性方程组求解当前策略的贝尔曼方程以获得 V <sup>π</sup> :

![](assets/1ded09d2-4c8f-419d-af55-884c8cdc1809.jpg)

![](assets/f0f42585-97ea-4308-a40b-8ccb300b5965.jpg)

3.  通过重复这些步骤，价值和政策都将收敛到最佳值:

![](assets/4f42f02b-aa29-4c36-8fff-43ec05603743.jpg)

![](assets/cd05b104-befb-4a60-b5b3-ee0d8e0561c4.jpg)

策略迭代倾向于处理较小的问题。如果一个 MDP 有大量的州，政策的反复计算将是昂贵的。因此，大型 MDP 倾向于使用价值迭代，而不是策略迭代。

**如果我们在现实生活中不知道确切的状态转移概率呢例如** *P <sub>s，a</sub>T5**？***

我们需要使用以下简单公式从数据中估计概率:

![](assets/174ea57d-fd33-40b7-9091-3d7279870b58.jpg)

![](assets/dc770b91-1dab-433f-8b18-15e24e9b1a16.jpg)

如果对于某些状态没有可用的数据，这导致 0/0 问题，我们可以从均匀分布中获得默认概率。

# 使用基本 Python 的值和策略迭代算法的网格世界示例

经典的网格世界的例子已经被用来说明用动态规划求解 MDP 贝尔曼方程的价值和政策迭代。在下面的网格中，代理将从网格西南角的(1，1)位置开始，目标是向东北角移动，到达位置(4，3)。一旦达到目标，代理将获得+1 的奖励。在旅程中，它应该避开危险区域(4，2)，因为这将给出奖励-1 的负面惩罚。代理无法从任何方向进入障碍物(2，2)所在的位置。目标区和危险区是终端状态，这意味着代理继续移动，直到它达到这两种状态之一。所有其他州的奖励是-0.02。在这里，任务是为每个状态(总共 11 个状态)的代理确定最优策略(移动方向)，使代理的总回报最大，或者使代理能够尽快达到目标。代理可以朝 4 个方向移动:北、南、东、西。

![](assets/e1d53aff-b610-4d56-b800-8228006e0c45.png)

完整的代码是用带有类实现的 Python 编程语言编写的。进一步阅读，请参考 Python 中的面向对象编程，了解类、对象、构造函数等。

导入`random`包，以生成 N、E、S、W 方向的移动:

```
>>> import random,operator

```

下面的`argmax`函数根据每个状态的值计算给定状态中的最大状态:

```
>>> def argmax(seq, fn):
...     best = seq[0]; best_score = fn(best)
...     for x in seq:
...         x_score = fn(x)
...     if x_score > best_score:
...         best, best_score = x, x_score
...     return best

```

为了在组件级别添加两个向量，使用了以下代码:

```
>>> def vector_add(a, b):
...     return tuple(map(operator.add, a, b))

```

方向提供了增量值，需要添加到代理的现有位置；方位可以应用于 *x* 轴或 *y* 轴:

```
>>> orientations = [(1,0), (0, 1), (-1, 0), (0, -1)]

```

下面的函数用于将代理转向正确的方向，因为我们知道，在每个命令中，代理在大约 80%的时间内都在那个方向上移动，而 10%的时间它会向右移动，10%的时间它会向左移动。：

```
>>> def turn_right(orientation):
...     return orientations[orientations.index(orientation)-1]
>>> def turn_left(orientation):
...     return orientations[(orientations.index(orientation)+1) % len(orientations)]
>>> def isnumber(x):
...     return hasattr(x, '__int__')

```

马尔可夫决策过程在这里被定义为一个类。每个 MDP 由初始位置、状态、转换模型、奖励函数和伽马值定义。

```
>>> class MDP:
... def __init__(self, init_pos, actlist, terminals, transitions={}, states=None, gamma=0.99):
...     if not (0 < gamma <= 1):
...         raise ValueError("MDP should have 0 < gamma <= 1 values")
...     if states:
...         self.states = states
...     else:
...         self.states = set()
...         self.init_pos = init_pos
...         self.actlist = actlist
...         self.terminals = terminals
...         self.transitions = transitions
...         self.gamma = gamma
...         self.reward = {}

```

返回该州的数字奖励:

```
... def R(self, state):
...     return self.reward[state]

```

带有从一个状态和一个动作的转换模型，返回每个状态的(概率、结果-状态)对列表:

```
... def T(self, state, action):
...     if(self.transitions == {}):
...         raise ValueError("Transition model is missing")
...     else:
...         return self.transitions[state][action]

```

可以在特定状态下执行的一组操作:

```
... def actions(self, state):
...     if state in self.terminals:
...         return [None]
...     else:
...         return self.actlist

```

类`GridMDP`是为用每个状态、终端位置、初始位置和伽马值(折扣)的网格值来建模 2D 网格世界而创建的:

```
>>> class GridMDP(MDP):
... def __init__(self, grid, terminals, init_pos=(0, 0), gamma=0.99):

```

以下代码用于反转网格，因为我们希望在底部而不是顶部看到*行 0* :

```
... grid.reverse()

```

以下`__init__`命令是网格类中用于初始化参数的构造函数:

```
... MDP.__init__(self, init_pos, actlist=orientations,
terminals=terminals, gamma=gamma)
... self.grid = grid
... self.rows = len(grid)
... self.cols = len(grid[0])
... for x in range(self.cols):
...     for y in range(self.rows):
...         self.reward[x, y] = grid[y][x]
...         if grid[y][x] is not None:
...             self.states.add((x, y))

```

状态转换向所需方向随机提供 80%，向左右方向随机提供 10%。这是为了模拟机器人在地板上滑动的随机性，等等:

```
... def T(self, state, action):
...     if action is None:
...         return [(0.0, state)]
...     else:
...         return [(0.8, self.go(state, action)),
...                (0.1, self.go(state, turn_right(action))),
...                (0.1, self.go(state, turn_left(action)))]

```

根据状态在有效状态列表中的位置，返回方向上的状态。如果下一个状态不在列表中，就像撞墙一样，那么代理应该保持相同的状态:

```
... def go(self, state, direction):
...     state1 = vector_add(state, direction)
...     return state1 if state1 in self.states else state

```

将(x，y)到 v 的映射转换为[[...，v，...]]网格:

```
... def to_grid(self, mapping):
...     return list(reversed([[mapping.get((x, y), None)
...                         for x in range(self.cols)]
...                         for y in range(self.rows)]))

```

将方向转换为箭头，以获得更好的图形表示:

```
... def to_arrows(self, policy):
...     chars = {(1, 0): '>', (0, 1): '^', (-1, 0): '<', (0, -1):
 'v', None: '.'}
...     return self.to_grid({s: chars[a] for (s, a) in policy.items()})

```

以下代码用于使用值迭代求解 MDP，并返回最佳状态值:

```
>>> def value_iteration(mdp, epsilon=0.001):
...     STSN = {s: 0 for s in mdp.states}
...     R, T, gamma = mdp.R, mdp.T, mdp.gamma
...     while True:
...         STS = STSN.copy()
...         delta = 0
...         for s in mdp.states:
...             STSN[s] = R(s) + gamma * max([sum([p * STS[s1] for 
...             (p, s1) in T(s,a)]) for a in mdp.actions(s)])
...             delta = max(delta, abs(STSN[s] - STS[s]))
...         if delta < epsilon * (1 - gamma) / gamma:
...             return STS

```

给定一个 MDP 和一个效用函数`STS`，确定最佳策略，作为从状态到动作的映射:

```
>>> def best_policy(mdp, STS):
...     pi = {}
...     for s in mdp.states:
...         pi[s] = argmax(mdp.actions(s), lambda a: expected_utility(a, s, STS, mdp))
...     return pi

```

根据 MDP 和 STS，在状态`s`下进行`a`的预期效用:

```
>>> def expected_utility(a, s, STS, mdp):
...     return sum([p * STS[s1] for (p, s1) in mdp.T(s, a)])

```

以下代码用于通过交替执行策略评估和策略改进步骤，使用策略迭代来解决 MDP 问题:

```
>>> def policy_iteration(mdp):
...     STS = {s: 0 for s in mdp.states}
...     pi = {s: random.choice(mdp.actions(s)) for s in mdp.states}
...     while True:
...         STS = policy_evaluation(pi, STS, mdp)
...         unchanged = True
...         for s in mdp.states:
...             a = argmax(mdp.actions(s),lambda a: expected_utility(a, s, STS, mdp))
...             if a != pi[s]:
...                 pi[s] = a
...                 unchanged = False
...         if unchanged:
...             return pi

```

以下代码用于使用近似值(修改后的策略迭代)将更新后的效用映射`U`从 MDP 的每个州返回到其效用:

```
>>> def policy_evaluation(pi, STS, mdp, k=20):
...     R, T, gamma = mdp.R, mdp.T, mdp.gamma
 ..     for i in range(k):
...     for s in mdp.states:
...         STS[s] = R(s) + gamma * sum([p * STS[s1] for (p, s1) in T(s, pi[s])])
...     return STS

>>> def print_table(table, header=None, sep=' ', numfmt='{}'):
...     justs = ['rjust' if isnumber(x) else 'ljust' for x in table[0]]
...     if header:
...         table.insert(0, header)
...     table = [[numfmt.format(x) if isnumber(x) else x for x in row]
...             for row in table]
...     sizes = list(map(lambda seq: max(map(len, seq)),
...                      list(zip(*[map(str, row) for row in table]))))
...     for row in table:
...         print(sep.join(getattr(str(x), j)(size) for (j, size, x)
...             in zip(justs, sizes, row)))

```

下面是一个 4 x 3 网格环境的输入网格，它向代理提出了一个顺序决策问题:

```
>>> sequential_decision_environment = GridMDP([[-0.02, -0.02, -0.02, +1],
...                                           [-0.02, None, -0.02, -1],
...                                           [-0.02, -0.02, -0.02, -0.02]],
...                                           terminals=[(3, 2), (3, 1)])

```

以下代码用于在给定的顺序决策环境中执行值迭代:

```
>>> value_iter = best_policy(sequential_decision_environment,value_iteration (sequential_decision_environment, .01))
>>> print("\n Optimal Policy based on Value Iteration\n")
>>> print_table(sequential_decision_environment.to_arrows(value_iter))

```

![](assets/74da56a1-d723-455b-95e9-d0dc0250d05c.png)

策略迭代的代码是:

```
>>> policy_iter = policy_iteration(sequential_decision_environment)
>>> print("\n Optimal Policy based on Policy Iteration & Evaluation\n")
>>> print_table(sequential_decision_environment.to_arrows(policy_iter))

```

![](assets/392578a0-9fec-43e7-95e8-b9247979d1c4.png)

从前面有两个结果的输出中，我们可以得出结论，值和策略迭代都为代理提供了相同的最优策略，使其能够以最快的方式跨越网格到达目标状态。当问题规模足够大时，在计算上最好选择价值迭代而不是策略迭代，因为在策略迭代中，我们需要在策略评估和策略改进的每次迭代中执行两个步骤。

# 蒙特卡罗方法

使用**蒙特卡罗** ( **MC** )方法，我们将首先计算价值函数并确定最优策略。在这种方法中，我们不假设对环境有完全的了解。MC 只需要经验，经验由状态、动作和来自与环境的实际或模拟交互的奖励的样本序列组成。从实际经验中学习是惊人的，因为它不需要事先了解环境的动态，但仍能达到最佳行为。这与人类或动物如何从实际经验而不是任何数学模型中学习非常相似。令人惊讶的是，在许多情况下，很容易根据期望的概率分布生成经验样本，但以显式形式获得分布是不可行的。

蒙特卡罗方法解决了强化学习问题的基础上平均样本回报的每一集。这意味着我们假设经验被分成几集，无论选择什么动作，所有的集最终都会终止。只有在每集结束后，才会对值进行估计并更改策略。MC 方法是逐集递增的，但不是逐步递增的(这是一种在线学习，我们将在时间差异学习部分介绍)。

蒙特卡罗方法对整个事件中每个状态-动作对的样本和平均回报进行计算。然而，在同一个事件中，在一个阶段采取行动后的回报取决于后来各州采取的行动。因为所有的动作选择都在进行学习，所以从早期状态的角度来看，问题变得不稳定。为了处理这种非平稳性，我们采用了动态规划的策略迭代思想，首先，我们计算一个固定的任意策略的价值函数；后来，我们改进了政策。

# 动态规划与蒙特卡罗方法的比较

动态编程需要对环境或所有可能的转换有完整的了解，而蒙特卡罗方法只在一个事件中对采样的状态-动作轨迹进行工作。DP 只包括一步过渡，而 MC 则一直到剧集结尾到达终端节点。关于 MC 方法的一个重要事实是，每个状态的估计是独立的，这意味着一个状态的估计不建立在任何其他状态的估计之上，就像 DP 的情况一样。

# MC 相对于 DP 方法的主要优势

以下是 MC 相对于 DP 方法的主要优势:

*   就计算费用而言，MC 方法更有吸引力，因为它的优点是估计单个状态的值与状态的数量无关
*   可以生成许多示例剧集，从感兴趣的状态开始，只对这些状态的返回进行平均，而忽略所有其他状态
*   MC 方法具有从实际经验或模拟经验中学习的能力

![](assets/07892bdb-f6c3-4df5-a608-fe572f91d43a.png)

# 蒙特卡罗预测

我们知道，蒙特卡罗方法预测给定策略的状态值函数。任何状态的价值都是从该状态开始的预期回报或预期累积未来折扣奖励。这些值是用 MC 方法估算的，只是为了对访问该州后观察到的回报进行平均。随着观察到越来越多的值，平均值应该会根据大数定律收敛到期望值。事实上，这是适用于所有蒙特卡罗方法的原理。蒙特卡罗策略评估算法包括以下步骤:

1.  初始化:

![](assets/ccddfec7-6c3d-4e1a-be32-a7e2d78da30f.jpg)

2.  永远重复:
    *   使用π生成一集
    *   对于剧集中出现的每个状态 *s* :
        *   第一次出现 *s* 后返回
        *   将 *G* 添加到退货中
        *   平均回报率

# 蒙特卡罗预测对网格世界问题的适用性

下图是为了说明而绘制的。然而，实际上，蒙特卡罗方法不能很容易地用于解决网格世界类型的问题，因为并不是所有的策略都保证终止。如果发现某个策略导致代理保持相同的状态，那么下一集将永远不会结束。像(**State-Action-悬赏-State-Action** ( **SARSA** ，我们将在 TD Learning Control 中的本章后半部分介绍)这样的分步学习方法没有这个问题，因为它们会在剧集中快速了解到这样的策略很差，然后切换到其他东西。

![](assets/689a86dc-8421-4d1f-aea5-b773dd7d80e7.png)

# 用 Python 模拟 21 点蒙特卡罗方法示例

流行的赌场纸牌游戏 21 点的目标是获得数值之和尽可能大而不超过 21 的牌。所有的牌面(国王、王后和杰克)都算 10，一张王牌可以算 1，也可以算 11，这取决于玩家想要的使用方式。只有 ace 有这个灵活性选项。所有其他卡片都是按面值计价的。游戏从发给庄家和玩家的两张牌开始。庄家的一张牌面朝上，另一张面朝下。如果玩家在前两张牌中有一张“自然 21”(一张王牌和一张 10 张牌)，除非庄家也有一张“自然”，否则玩家获胜，在这种情况下，游戏是平局。如果玩家没有自然牌，那么他可以要求额外的牌，一张接一张(命中)，直到他停止(卡住)或超过 21 张(破产)。如果玩家破产，他就输了；如果玩家坚持，那么就轮到庄家了。庄家根据固定策略打或粘，没有选择:庄家通常粘在 17 或更大的任何和上，否则打。如果庄家破产，那么玩家自动获胜。如果他坚持，结果要么是赢，要么是输，要么是平，这取决于庄家或玩家的总和是否接近 21。

![](assets/bb29df47-542e-4a33-a924-f975294c8b80.png)

二十一点问题可以公式化为一个插曲式的有限 MDP，其中二十一点的每一局都是一个插曲。在终端状态下，每一集的赢、输和抽分别给予+1、-1 和 0 的奖励，游戏状态下的剩余奖励给予 0 的值，不打折扣(gamma = 1)。所以终端奖励也是这个游戏的回报。我们从无限副牌中抽牌，这样就不存在可追踪的图案。整个游戏在下面的代码中用 Python 建模。

以下代码片段的灵感来自于张的 RL Python 代码，并在的学生理查德·萨顿的许可下发表在本书中，他是著名的强化:学习:简介 T5 的作者。

导入以下包用于阵列操作和可视化:

```
>>> from __future__ import print_function 
>>> import numpy as np 
>>> import matplotlib.pyplot as plt 
>>> from mpl_toolkits.mplot3d import Axes3D 

```

在每个回合，玩家或庄家可以采取一种可能的行动:要么打，要么站。这是仅有的两种可能的状态:

```
>>> ACTION_HIT = 0 
>>> ACTION_STAND = 1   
>>> actions = [ACTION_HIT, ACTION_STAND] 

```

玩家的策略是用 21 组数值模拟的，因为玩家在超过 21:

```
>>> policyPlayer = np.zeros(22) 

>>> for i in range(12, 20): 
...     policyPlayer[i] = ACTION_HIT 

```

如果玩家得到 20 或 21 的值，他就采取棍上策略，否则他将继续打一副牌来抽一张新卡:

```
>>> policyPlayer[20] = ACTION_STAND 
>>> policyPlayer[21] = ACTION_STAND 

```

玩家目标策略的函数形式:

```
>>> def targetPolicyPlayer(usableAcePlayer, playerSum, dealerCard): 
...     return policyPlayer[playerSum] 

```

玩家行为策略的函数形式:

```
>>> def behaviorPolicyPlayer(usableAcePlayer, playerSum, dealerCard): 
...     if np.random.binomial(1, 0.5) == 1: 
...         return ACTION_STAND 
...     return ACTION_HIT 

```

经销商的固定策略是继续下注，直到值为 17，然后在 17 到 21 之间下注:

```
>>> policyDealer = np.zeros(22) 
>>> for i in range(12, 17): 
...     policyDealer[i] = ACTION_HIT 
>>> for i in range(17, 22): 
...     policyDealer[i] = ACTION_STAND 

```

以下功能用于从一副牌中抽出一张新的替换牌:

```
>>> def getCard(): 
...     card = np.random.randint(1, 14) 
...     card = min(card, 10) 
...     return card 

```

让我们玩游戏吧！

```
>>> def play(policyPlayerFn, initialState=None, initialAction=None): 

```

1.  玩家、玩家轨迹和玩家是否使用 ace 的总和为 11:

```
...     playerSum = 0 
...     playerTrajectory = [] 
...     usableAcePlayer = False 

```

2.  抽卡的经销商状态:

```
...     dealerCard1 = 0 
...     dealerCard2 = 0 
...     usableAceDealer = False 

...     if initialState is None: 

```

3.  生成随机初始状态:

```
...         numOfAce = 0 

```

4.  初始化玩家的卡片:

```
...         while playerSum < 12: 

```

5.  如果玩家的牌总数少于 12 张，则总是打一副抽牌:

```
...             card = getCard() 
...             if card == 1: 
...                 numOfAce += 1 
...                 card = 11 
...                 usableAcePlayer = True 
...             playerSum += card 

```

6.  如果玩家的和大于 21，他必须至少持有一张王牌，但两张王牌也是可以的。在这种情况下，他将使用 ace 作为 1，而不是 11。如果玩家只有一张王牌，那么他就没有可用的王牌了:

```
...         if playerSum > 21: 
...             playerSum -= 10 
...             if numOfAce == 1: 
...                 usableAcePlayer = False 

```

7.  初始化经销商卡:

```
...         dealerCard1 = getCard() 
...         dealerCard2 = getCard() 

...     else: 
...         usableAcePlayer = initialState[0] 
...         playerSum = initialState[1] 
...         dealerCard1 = initialState[2] 
...         dealerCard2 = getCard() 

```

8.  初始化游戏状态:

```
...     state = [usableAcePlayer, playerSum, dealerCard1] 

```

9.  初始化经销商的总和:

```
...     dealerSum = 0 
...     if dealerCard1 == 1 and dealerCard2 != 1: 
...         dealerSum += 11 + dealerCard2 
...         usableAceDealer = True 
...     elif dealerCard1 != 1 and dealerCard2 == 1: 
...         dealerSum += dealerCard1 + 11 
...         usableAceDealer = True 
...     elif dealerCard1 == 1 and dealerCard2 == 1: 
...         dealerSum += 1 + 11 
...         usableAceDealer = True 
...     else: 
...         dealerSum += dealerCard1 + dealerCard2 

```

10.  游戏从这里开始，因为玩家需要从这里开始抽额外的牌:

```
...     while True: 
...         if initialAction is not None: 
...             action = initialAction 
...             initialAction = None 
...         else: 

```

11.  根据玩家的当前总和采取行动:

```
...             action = policyPlayerFn(usableAcePlayer, playerSum, dealerCard1) 

```

12.  跟踪玩家的轨迹进行重要性采样:

```
...         playerTrajectory.append([action, (usableAcePlayer, playerSum, dealerCard1)]) 

...         if action == ACTION_STAND: 
...             break 

```

13.  如果要打出一副牌，则获得一张新牌:

```
...         playerSum += getCard() 

```

14.  如果总和大于 21，玩家在这里崩溃，游戏结束，他得到-1 的奖励。然而，如果他有一张王牌，他可以用它来挽救比赛，否则他会输。

```
...         if playerSum > 21: 
...             if usableAcePlayer == True: 
...                 playerSum -= 10 
...                 usableAcePlayer = False 
...             else: 
...                 return state, -1, playerTrajectory 

```

15.  现在轮到庄家了。他会根据一个总和抽牌:如果他到了 17 岁，他会停下来，否则继续抽牌。如果庄家也有王牌，他可以用它来达到破产的情况，否则他就破产了:

```
...     while True: 
...         action = policyDealer[dealerSum] 
...         if action == ACTION_STAND: 
...             break 
...         dealerSum += getCard() 
...         if dealerSum > 21: 
...             if usableAceDealer == True: 
...                 dealerSum -= 10 
...                 usableAceDealer = False 
...             else: 
...                 return state, 1, playerTrajectory 

```

16.  现在，我们将玩家的总和与庄家的总和进行比较，以决定谁在不破产的情况下获胜:

```
...     if playerSum > dealerSum: 
...         return state, 1, playerTrajectory 
...     elif playerSum == dealerSum: 
...         return state, 0, playerTrajectory 
...     else: 
...         return state, -1, playerTrajectory 

```

以下代码说明了带有*在线策略*的蒙特卡罗示例:

```
>>> def monteCarloOnPolicy(nEpisodes): 
...     statesUsableAce = np.zeros((10, 10)) 
...     statesUsableAceCount = np.ones((10, 10)) 
...     statesNoUsableAce = np.zeros((10, 10)) 
...     statesNoUsableAceCount = np.ones((10, 10)) 
...     for i in range(0, nEpisodes): 
...         state, reward, _ = play(targetPolicyPlayer) 
...         state[1] -= 12 
...         state[2] -= 1 
...         if state[0]: 
...             statesUsableAceCount[state[1], state[2]] += 1 
...             statesUsableAce[state[1], state[2]] += reward 
...         else: 
...             statesNoUsableAceCount[state[1], state[2]] += 1 
...             statesNoUsableAce[state[1], state[2]] += reward 
...     return statesUsableAce / statesUsableAceCount, statesNoUsableAce / statesNoUsableAceCount 

```

下面的代码讨论了带有探索开始的蒙特卡罗，其中每个状态-动作对的所有回报都被累加和平均，而不管观察到它们时实施的是什么策略:

```
>>> def monteCarloES(nEpisodes): 
...     stateActionValues = np.zeros((10, 10, 2, 2)) 
...     stateActionPairCount = np.ones((10, 10, 2, 2)) 

```

行为策略是贪婪的，得到平均收益(s，a)的`argmax`:

```
...     def behaviorPolicy(usableAce, playerSum, dealerCard): 
...         usableAce = int(usableAce) 
...         playerSum -= 12 
...         dealerCard -= 1 
...         return np.argmax(stateActionValues[playerSum, dealerCard, usableAce, :] 
                      / stateActionPairCount[playerSum, dealerCard, usableAce, :]) 

```

播放将持续几集，每集随机初始化状态、动作和状态-动作对的更新值:

```
...     for episode in range(nEpisodes): 
...         if episode % 1000 == 0: 
...             print('episode:', episode) 
...         initialState = [bool(np.random.choice([0, 1])), 
...                        np.random.choice(range(12, 22)), 
...                        np.random.choice(range(1, 11))] 
...         initialAction = np.random.choice(actions) 
...         _, reward, trajectory = play(behaviorPolicy, initialState, initialAction) 
...         for action, (usableAce, playerSum, dealerCard) in trajectory: 
...             usableAce = int(usableAce) 
...             playerSum -= 12 
...             dealerCard -= 1 

```

更新状态-动作对的值:

```
...             stateActionValues[playerSum, dealerCard, usableAce, action] += reward 
...             stateActionPairCount[playerSum, dealerCard, usableAce, action] += 1 
...     return stateActionValues / stateActionPairCount 

```

打印状态值:

```
>>> figureIndex = 0 
>>> def prettyPrint(data, tile, zlabel='reward'): 
...     global figureIndex 
...     fig = plt.figure(figureIndex) 
...     figureIndex += 1 
...     fig.suptitle(tile) 
...     ax = fig.add_subplot(111, projection='3d') 
...     x_axis = [] 
...     y_axis = [] 
...     z_axis = [] 
...     for i in range(12, 22): 
...         for j in range(1, 11): 
...             x_axis.append(i) 
...             y_axis.append(j) 
...             z_axis.append(data[i - 12, j - 1]) 
...     ax.scatter(x_axis, y_axis, z_axis,c='red') 
...     ax.set_xlabel('player sum') 
...     ax.set_ylabel('dealer showing') 
...     ax.set_zlabel(zlabel) 

```

10，000 和 500，000 次迭代有或没有可用 ace 的策略结果:

```
>>> def onPolicy(): 
...     statesUsableAce1, statesNoUsableAce1 = monteCarloOnPolicy(10000) 
...     statesUsableAce2, statesNoUsableAce2 = monteCarloOnPolicy(500000) 
...     prettyPrint(statesUsableAce1, 'Usable Ace & 10000 Episodes') 
...     prettyPrint(statesNoUsableAce1, 'No Usable Ace & 10000 Episodes') 
...     prettyPrint(statesUsableAce2, 'Usable Ace & 500000 Episodes') 
...     prettyPrint(statesNoUsableAce2, 'No Usable Ace & 500000 Episodes') 
...     plt.show() 

```

策略迭代的优化或蒙特卡罗控制:

```
>>> def MC_ES_optimalPolicy(): 
...     stateActionValues = monteCarloES(500000) 
...     stateValueUsableAce = np.zeros((10, 10)) 
...     stateValueNoUsableAce = np.zeros((10, 10)) 
    # get the optimal policy 
...     actionUsableAce = np.zeros((10, 10), dtype='int') 
...     actionNoUsableAce = np.zeros((10, 10), dtype='int') 
...     for i in range(10): 
...         for j in range(10): 
...             stateValueNoUsableAce[i, j] = np.max(stateActionValues[i, j, 0, :]) 
...             stateValueUsableAce[i, j] = np.max(stateActionValues[i, j, 1, :]) 
...             actionNoUsableAce[i, j] = np.argmax(stateActionValues[i, j, 0, :]) 
...             actionUsableAce[i, j] = np.argmax(stateActionValues[i, j, 1, :]) 
...     prettyPrint(stateValueUsableAce, 'Optimal state value with usable Ace') 
...     prettyPrint(stateValueNoUsableAce, 'Optimal state value with no usable Ace') 
...     prettyPrint(actionUsableAce, 'Optimal policy with usable Ace', 'Action (0 Hit, 1 Stick)') 
...     prettyPrint(actionNoUsableAce, 'Optimal policy with no usable Ace', 'Action (0 Hit, 1 Stick)') 
...     plt.show() 

# Run on-policy function 
>>> onPolicy()

```

![](assets/f4ff63ff-0cf0-42bd-8ce1-6c0d0b4c6233.png)

从前面的图表中，我们可以得出结论，一手牌中的可用王牌即使在玩家总和较低的组合中也能给出高得多的奖励，而对于没有可用王牌的玩家来说，如果这些值小于 20，就赢得的奖励而言，这些值是非常不同的。

```
# Run Monte Carlo Control or Explored starts 
>>> MC_ES_optimalPolicy() 

```

![](assets/b13b10f8-dfe6-4741-8edd-4e20d163044b.png)

从最优策略和状态值，我们可以得出这样的结论:如果有一张可用的王牌，我们可以打得比棒还多，而且与手里没有王牌时相比，状态值的奖励要高得多。虽然我们谈论的结果是显而易见的，但我们可以看到手握王牌的影响有多大。

# 时间差异学习

**时间差异** ( **TD** )学习是强化学习的中心和新颖主题。TD 学习是**蒙特卡洛** ( **MC** )和**动态规划** ( **DP** )思想的结合。与蒙特卡罗方法一样，时域方法可以直接从经验中学习，而不需要环境模型。与动态规划类似，TD 方法部分基于其他学习到的估计值更新估计值，而不需要等待最终结果，这与 MC 方法不同，后者仅在达到最终结果后更新估计值。

![](assets/bfcebf0b-e1a9-4295-88ba-e7926fad7066.png)

# 蒙特卡罗方法与时间差分学习的比较

虽然蒙特卡罗方法和时间差分学习有相似之处，但是时间差分学习相对于蒙特卡罗方法有其固有的优势。

| **蒙特卡罗方法** | **时间差学习** |
| MC 必须等到剧集结束后才能知道回归。 | TD 每走一步都可以在线学习，不需要等到剧集结束。 |
| MC 具有高方差和低偏差。 | TD 具有低方差和一些体面的偏见。 |
| MC 没有利用马尔可夫特性。 | 道明利用了马尔可夫特性。 |

# TD 预测

TD 和 MC 都是用经验解决 *z* 预测问题。给定一些策略π，两种方法都更新了它们对在该体验中出现的非终端状态 *S <sub>t</sub>* 的估计*v*v*T6】π。蒙特卡罗方法等到访问后的返回已知，然后使用该返回作为 *V(S <sub>t</sub> )* 的目标。*

![](assets/f386dfe8-db2f-4601-bdd7-3d4738838d0a.jpg)

前面的方法可以称为常数- *α MC* ，其中 MC 必须等到剧集结束后才能确定增量为 *V(S <sub>t</sub> )* (这时才知道 *G <sub>t</sub>* )。

TD 方法只需要等到下一个时间步。在时间 *t+1* 时，他们立即形成一个目标，并使用观察到的奖励*R<sub>t+1</sub>T5】和估计 *V(S <sub>t+1</sub> )* 进行有用的更新。最简单的 TD 方法，称为 *TD(0)* ，是:*

![](assets/411caf6c-087f-4b53-8ff3-2f1840ccee13.jpg)

MC 更新的目标是*G<sub>t</sub>T3，而 TD 更新的目标是*R<sub>t+1</sub>+y V(S<sub>t+1</sub>)*。*

在下图中，对 TD 和 MC 方法进行了比较。正如我们在方程 TD(0)中写的，我们使用真实数据的一个步骤，然后使用下一个状态的价值函数的估计值。同样，我们也可以用两步真实数据来更好地了解现实，并估计第三阶段的价值函数。然而，随着我们增加步骤，最终需要越来越多的数据来执行参数更新，这将花费越多的时间。当我们在每一集中采取无限步直到它接触到更新参数的终点时，TD 就变成了蒙特卡罗方法。

![](assets/83e70f98-893d-4f38-8c7a-ed97cc99b59b.png)

估算 *v* 算法的 TD (0)由以下步骤组成:

1.  初始化:

![](assets/563eb982-a570-46aa-a462-a28daa033907.jpg)

2.  重复(每集):
    *   初始化 *S*
    *   重复(每集的每一步):
        *   π给 S 的
        *   采取行动 A，观察 *R，S'*
        *   ![](assets/aee5ce2f-16de-4ed0-93ca-6fe3996f0899.jpg)
        *   ![](assets/897e09ad-17c0-4c7d-b5e8-315735358394.jpg)
3.  直到 *S* 结束。

# 道明学习的驾驶办公室示例

在这个简单的例子中，你每天从家到办公室，你试图预测早上到达办公室需要多长时间。当你离开家时，你会注意到时间、一周中的哪一天、天气(是否下雨、刮风等等)以及你认为相关的任何其他参数。例如，在周一早上，你正好在早上 8 点离开，你估计需要 40 分钟才能到达办公室。早上 8 点 10 分，你注意到有一个贵宾经过，你需要等到完整的车队已经出发，所以你重新估计从那以后需要 45 分钟，或者总共需要 55 分钟。15 分钟后，你及时完成了旅程的高速公路部分。现在你进入一条绕行道路，你现在把你的总旅行时间减少到 50 分钟。不幸的是，此时，你被困在一堆牛车后面，道路太窄，无法通过。你最终不得不跟随那些牛车，直到你在 8:50 转到你办公室所在的小街上。七分钟后，你到达你的办公室停车场。状态、时间和预测的顺序如下:

![](assets/a39cfb5d-f299-4a49-b3c0-20f892db7bdb.png)

本例中的奖励是旅程中每一段经过的时间，我们使用的是折扣系数(gamma， *v = 1* ，因此每个状态的回报是从该状态到目的地(办公室)的实际时间。每个状态的值是预测的到达时间，这是上表中的第二列，也就是所遇到的每个状态的当前估计值。

![](assets/2aa08c58-759e-4fab-ada1-3766e1a0b11e.png)

在上图中，蒙特卡罗用于绘制事件序列的预测总时间。箭头始终显示常数-α MC 方法推荐的预测变化。这些是每个阶段的估计值和实际回报(57 分钟)之间的误差。在 MC 方法中，学习只发生在完成之后，为此需要等到 57 分钟过去。然而，在现实中，你可以在达到最终结果之前进行估计，并相应地修正你的估计。道明的工作原理是一样的，在每个阶段，它都试图相应地预测和修正估计值。所以，TD 方法是马上学的，不需要等到最后的结果。事实上，这就是人类在现实生活中的预测。由于这些积极的特性，TD 学习被认为是强化学习中的新方法。

# 南亚区域合作联盟政策贸易发展控制

**状态-动作-奖励-状态-动作** ( **SARSA** )是一个策略上的 TD 控制问题，其中策略将使用策略迭代(GPI)进行优化，只有时间 TD 方法用于评估预测的策略。第一步，算法学习一个 SARSA 函数。特别地，对于策略上的方法，我们使用用于学习 v <sub>π的 TD 方法来估计当前行为策略π以及所有状态和动作(a)的 *q <sub>π</sub> (s，a)* 。</sub>现在，我们考虑从状态-动作对到状态-动作对的转换，并学习状态-动作对的值:

![](assets/bcc3db10-d490-4b75-bdd0-c50fa362a1dd.jpg)

该更新在从非终端状态*S<sub>t</sub>T3】的每次转换之后完成。如果 *S <sub>t+1</sub>* 为端子，那么 *Q (S <sub>t+1、</sub> A <sub>t+1</sub> )* 定义为零。这个规则使用五个事件的每一个元素(*S<sub>t</sub>T17】、*A<sub>t</sub>T21】、 *Rt* 、*St<sub>+1</sub>T27】、*A<sub>t+1</sub>T31】，它们构成了从一个状态-动作对到下一个状态-动作对的过渡。这种五重组合产生了该算法的名称 SARSA。*****

与所有策略上的方法一样，我们不断地为行为策略π估计 q <sub>π</sub> ，同时相对于 q <sub>π向贪婪方向改变π。</sub>SARSA 的计算算法如下:

1.  初始化:

![](assets/6a1abf16-fa6e-4785-ab28-182d77b8a4d4.jpg)

2.  重复(每集):
    *   初始值设定项
    *   使用从 Q 派生的策略从 S 中选择 A(例如，ε-贪婪)
    *   重复(每集的每一步):
        *   采取行动 *A* ，观察 *R，S'*
        *   从使用源自 Q 的*策略中选择*A’*(例如ε - greedy)*
        **   ![](assets/e402c6fe-49e2-4815-8114-83581c5a4aa4.jpg)*   ![](assets/aebfab89-a7b7-4bc7-9652-6c1012041866.jpg)*

**   直到 *S* 结束*

 *# 问-学习-脱离策略 TD 控制

q 学习是许多强化学习问题在实际应用中最常用的方法。偏离策略的 TD 控制算法被称为 Q 学习。在这种情况下，学习到的动作值函数 Q 直接逼近最佳动作值函数![](assets/d9799bc2-4a59-4877-9139-3d1b431e447b.png)，而与所遵循的策略无关。这种近似简化了算法的分析，并使早期收敛证明成为可能。该策略仍然有效，因为它决定了访问和更新哪些状态-动作对。然而，正确收敛所需要的只是继续更新所有对。正如我们所知，这是一个最低要求，因为任何保证在一般情况下找到最优行为的方法都必须要求它。收敛的算法如下所示:

1.  初始化:

![](assets/bf8db6c6-b1d4-428b-a634-487b5f06ae29.jpg)

2.  重复(每集):
    *   初始值设定项
    *   重复(每集的每一步):
        *   使用从 Q 派生的策略从 S 中选择 A(例如，ε -贪婪)
        *   采取行动 A，观察 *R，S'*
        *   ![](assets/14c2e5cf-cb2f-4dcf-8681-d996bfa19692.jpg)
        *   ![](assets/a4bfd224-dff9-4d3e-bdee-9d6e47109ceb.jpg)
3.  直到 *S* 结束

# 道明控制有政策和无政策的悬崖行走实例

一个悬崖行走网格世界的例子被用来比较 SARSA 和 Q 学习，以突出政策上(SARSA)和政策下(Q 学习)方法之间的差异。这是一个标准的未折扣的、不连续的任务，有开始和结束目标状态，允许在四个方向(北、西、东、南)移动。奖励-1 用于除标记为*悬崖*的区域之外的所有过渡，踩下该区域将对代理处以奖励-100 的惩罚，并立即将代理送回开始位置。

![](assets/796e60ed-631a-4e7f-81a1-16671e951166.png)

以下代码片段从张的 RL Python 代码中获得灵感，并经强化学习的著名作者 *Richard S. Sutton* 的学生许可，在本书中发布(详情请参见阅读部分):

```
# Cliff-Walking - TD learning - SARSA & Q-learning 
>>> from __future__ import print_function 
>>> import numpy as np 
>>> import matplotlib.pyplot as plt 

# Grid dimensions 
>>> GRID_HEIGHT = 4 
>>> GRID_WIDTH = 12 

# probability for exploration, step size,gamma  
>>> EPSILON = 0.1 
>>> ALPHA = 0.5 
>>> GAMMA = 1 

# all possible actions 
>>> ACTION_UP = 0; ACTION_DOWN = 1;ACTION_LEFT = 2;ACTION_RIGHT = 3 
>>> actions = [ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT] 

# initial state action pair values 
>>> stateActionValues = np.zeros((GRID_HEIGHT, GRID_WIDTH, 4)) 
>>> startState = [3, 0] 
>>> goalState = [3, 11] 

# reward for each action in each state 
>>> actionRewards = np.zeros((GRID_HEIGHT, GRID_WIDTH, 4)) 
>>> actionRewards[:, :, :] = -1.0 
>>> actionRewards[2, 1:11, ACTION_DOWN] = -100.0 
>>> actionRewards[3, 0, ACTION_RIGHT] = -100.0 

# set up destinations for each action in each state 
>>> actionDestination = [] 
>>> for i in range(0, GRID_HEIGHT): 
...     actionDestination.append([]) 
...     for j in range(0, GRID_WIDTH): 
...         destinaion = dict() 
...         destinaion[ACTION_UP] = [max(i - 1, 0), j] 
...         destinaion[ACTION_LEFT] = [i, max(j - 1, 0)] 
...         destinaion[ACTION_RIGHT] = [i, min(j + 1, GRID_WIDTH - 1)] 
...         if i == 2 and 1 <= j <= 10: 
...             destinaion[ACTION_DOWN] = startState 
...         else: 
...             destinaion[ACTION_DOWN] = [min(i + 1, GRID_HEIGHT - 1), j] 
...         actionDestination[-1].append(destinaion) 
>>> actionDestination[3][0][ACTION_RIGHT] = startState 

# choose an action based on epsilon greedy algorithm 
>>> def chooseAction(state, stateActionValues): 
...     if np.random.binomial(1, EPSILON) == 1: 
...         return np.random.choice(actions) 
...     else: 
...         return np.argmax(stateActionValues[state[0], state[1], :]) 

# SARSA update 

>>> def sarsa(stateActionValues, expected=False, stepSize=ALPHA): 
...     currentState = startState 
...     currentAction = chooseAction(currentState, stateActionValues) 
...     rewards = 0.0 
...     while currentState != goalState: 

...         newState = actionDestination[currentState[0]][currentState[1]] [currentAction] 

...         newAction = chooseAction(newState, stateActionValues) 
...         reward = actionRewards[currentState[0], currentState[1], currentAction] 
...         rewards += reward 
...         if not expected: 
...             valueTarget = stateActionValues[newState[0], newState[1], newAction] 
...         else: 
...             valueTarget = 0.0 
...             actionValues = stateActionValues[newState[0], newState[1], :] 
...             bestActions = np.argwhere(actionValues == np.max(actionValues)) 
...             for action in actions: 
...                 if action in bestActions: 

...                     valueTarget += ((1.0 - EPSILON) / len(bestActions) + EPSILON / len(actions)) * stateActionValues[newState[0], newState[1], action] 

...                 else: 
...                     valueTarget += EPSILON / len(actions) * stateActionValues[newState[0], newState[1], action] 
...         valueTarget *= GAMMA 
...         stateActionValues[currentState[0], currentState[1], currentAction] += stepSize * (reward+ valueTarget - stateActionValues[currentState[0], currentState[1], currentAction]) 
...         currentState = newState 
...         currentAction = newAction 
...     return rewards 

# Q-learning update 
>>> def qlearning(stateActionValues, stepSize=ALPHA): 
...     currentState = startState 
...     rewards = 0.0 
...     while currentState != goalState: 
...         currentAction = chooseAction(currentState, stateActionValues) 
...         reward = actionRewards[currentState[0], currentState[1], currentAction] 
...         rewards += reward 
...         newState = actionDestination[currentState[0]][currentState[1]] [currentAction] 
...         stateActionValues[currentState[0], currentState[1], currentAction] += stepSize * (reward + GAMMA * np.max(stateActionValues[newState[0], newState[1], :]) - 
...             stateActionValues[currentState[0], currentState[1], currentAction]) 
...         currentState = newState 
...     return rewards 

# print optimal policy 
>>> def printOptimalPolicy(stateActionValues): 
...     optimalPolicy = [] 
...     for i in range(0, GRID_HEIGHT): 
...         optimalPolicy.append([]) 
...         for j in range(0, GRID_WIDTH): 
...             if [i, j] == goalState: 
...                 optimalPolicy[-1].append('G') 
...                 continue 
...             bestAction = np.argmax(stateActionValues[i, j, :]) 
...             if bestAction == ACTION_UP: 
...                 optimalPolicy[-1].append('U') 
...             elif bestAction == ACTION_DOWN: 
...                 optimalPolicy[-1].append('D') 
...             elif bestAction == ACTION_LEFT: 
...                 optimalPolicy[-1].append('L') 
...             elif bestAction == ACTION_RIGHT: 
...                 optimalPolicy[-1].append('R') 
...     for row in optimalPolicy: 
...         print(row) 

>>> def SARSAnQLPlot(): 
    # averaging the reward sums from 10 successive episodes 
...     averageRange = 10 

    # episodes of each run 
...     nEpisodes = 500 

    # perform 20 independent runs 
...     runs = 20 

...     rewardsSarsa = np.zeros(nEpisodes) 
...     rewardsQlearning = np.zeros(nEpisodes) 
...     for run in range(0, runs): 
...         stateActionValuesSarsa = np.copy(stateActionValues) 
...         stateActionValuesQlearning = np.copy(stateActionValues) 
...         for i in range(0, nEpisodes): 
            # cut off the value by -100 to draw the figure more elegantly 
...             rewardsSarsa[i] += max(sarsa(stateActionValuesSarsa), -100) 
...             rewardsQlearning[i] += max(qlearning(stateActionValuesQlearning), -100) 

    # averaging over independent runs 
...     rewardsSarsa /= runs 
...     rewardsQlearning /= runs 

    # averaging over successive episodes 
...     smoothedRewardsSarsa = np.copy(rewardsSarsa) 
...     smoothedRewardsQlearning = np.copy(rewardsQlearning) 
...     for i in range(averageRange, nEpisodes): 
...         smoothedRewardsSarsa[i] = np.mean(rewardsSarsa[i - averageRange: i + 1]) 
...         smoothedRewardsQlearning[i] = np.mean(rewardsQlearning[i - averageRange: i + 1]) 

    # display optimal policy 
...     print('Sarsa Optimal Policy:') 
...     printOptimalPolicy(stateActionValuesSarsa) 
...     print('Q-learning Optimal Policy:') 
...     printOptimalPolicy(stateActionValuesQlearning) 

    # draw reward curves 
...     plt.figure(1) 
...     plt.plot(smoothedRewardsSarsa, label='Sarsa') 
...     plt.plot(smoothedRewardsQlearning, label='Q-learning') 
...     plt.xlabel('Episodes') 
...     plt.ylabel('Sum of rewards during episode') 
...     plt.legend() 

# Sum of Rewards for SARSA versus Qlearning 
>>> SARSAnQLPlot() 

```

![](assets/a83fb746-3b3d-42de-b21d-2ea0692c4b8f.png)

在最初的过渡之后，Q-learning 学习最优策略的值以沿着最优路径行走，在该路径中，代理沿着悬崖边缘行进。不幸的是，这会导致偶尔因为ε-贪婪的动作选择而掉下悬崖。而另一方面，SARSA 将动作选择考虑在内，并通过网格的上部学习更长、更安全的路径。虽然 Q-learning 学习到了最优策略的价值，但它的在线性能比 SARSA 差，后者学习的是迂回且最安全的策略。即使我们观察下图中显示的以下奖励总和，SARSA 在该集期间的奖励负总和也比 Q-learning 少。

![](assets/2d15986e-4572-45c9-acc3-7e1cd283640d.png)

# 融合机器学习和深度学习的强化学习应用

强化学习与机器学习或深度学习相结合，为近年来的各种前沿问题创造了最先进的人工智能解决方案。代码示例的完整解释超出了本书的范围，但是我们将为您提供这些技术内部的高级视图。以下是该领域最流行和已知的最新趋势，但应用并不仅限于此:

*   汽车控制(自动驾驶汽车)
*   谷歌深度思维阿尔法围棋游戏
*   机器人技术(以足球为例)

# 汽车控制-自动驾驶汽车

自动驾驶汽车是这个行业的新趋势，许多科技巨头现在都在这个领域工作。深度学习技术，如卷积神经网络，用于学习控制动作的 Q 函数，如向前、向后、向左和向右转弯等，通过混合和匹配从可用的动作空间。整个算法被称为一个 **DQN** ( **DeepQ Network** )。这种方法可以用于玩雅达利、赛车等游戏。详见斯坦福大学*四月余**拉斐尔·佩莱斯斯基-史密斯**里什·贝迪*的论文*模拟自主车辆控制深度强化学习*。

![](assets/b93077ab-19ad-4011-9c2d-7fec5b06562b.png)

# 谷歌 DeepMind 的 AlphaGo

**谷歌 DeepMind 的 AlphaGo** 是人工智能领域的一个新轰动，因为许多行业专家曾预测，击败人类玩家需要大约 10 年的时间，但 AlphaGo 战胜人类的事实证明他们错了。围棋的主要复杂性在于其穷尽的搜索空间:假设 *b* 是游戏的广度， *d* 是游戏的深度，这意味着围棋要探索的组合是( *b* ~250， *d* ~150)，而象棋则是( *b* ~35， *d* ~80)。这清楚地表明了围棋在复杂性上的不同。事实上，IBM 深蓝在 1997 年使用蛮力或穷举搜索技术击败了加里·卡斯帕罗夫，这在围棋游戏中是不可能的。

**AlphaGo** 使用价值网络评估董事会头寸，使用政策网络选择移动。神经网络在最先进的蒙特卡罗树搜索程序级别上运行，用于模拟和估计搜索树中每个状态的值。进一步阅读，请参考*大卫·西尔弗等人*的论文*掌握深度神经网络和树搜索围棋*，摘自*谷歌深度思维*。

![](assets/a6e660e5-4d33-4990-b03b-11d2977dbfa6.png)

# 机器人足球

机器人学作为一个强化学习领域，与大多数研究得很好的强化学习标准问题有很大不同。机器人学中的问题通常最好用高维、连续的状态和动作来表示。机器人强化学习中常见的 10-30 维连续动作被认为很大。强化学习在机器人技术上的应用涉及许多挑战，包括考虑真实物理系统的无噪声环境，以及通过真实世界的经验进行学习可能成本高昂。因此，算法或流程需要足够健壮，以完成必要的工作。此外，为指导学习系统的环境生成奖励值和奖励函数将是困难的。

虽然机器人强化学习有多种建模方法，但一种实用的价值函数逼近方法使用多层感知器来学习各种子任务，如学习防御、拦截、位置控制、踢腿、运动速度控制、运球和罚球。更多详情，请参考论文*机器人学中的强化学习:一项调查*，作者为*詹斯·科伯*、*安德鲁·巴格内尔*和*扬·彼得斯*。

![](assets/f2d27db5-25b7-47a9-b3a7-ed3e16e5ecc5.png)

有很多内容需要涵盖，这本书作为强化学习的介绍，而不是详尽的讨论。感兴趣的读者请浏览*进一步阅读*部分的资源。我们希望你会喜欢它！

# 进一步阅读

强化学习有许多经典资源，我们鼓励读者浏览这些资源:

*   萨顿和巴尔托，《强化学习:导论》。*麻省理工学院出版社*，美国马萨诸塞州剑桥，1998 年
*   *RL 课程*由*大卫·西尔弗*从 YouTube:[https://www.youtube.com/watch?v=2pWv7GOvuf0&列表= PL7-jpktc4r 78-wczqn5iqyuhbz8 foxt](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT)
*   *机器学习*(斯坦福) *Andrew NG* 来自 YouTube(讲座 16-20):[https://www.youtube.com/watch?v=UzxYlbK2c7E&列表= pla89 dcfa 6 dace 599](https://www.youtube.com/watch?v=UzxYlbK2c7E&list=PLA89DCFA6ADACE599)
*   *来自*摩根&克莱普尔*出版社的 *Csaba* 强化学习算法*
*   *人工智能:现代方法* 3 <sup>第三版</sup>由*斯图亚特·拉塞尔*和*彼得·诺维格*、*普伦蒂斯霍尔*

# 摘要

在本章中，您学习了各种强化学习技术，如马尔可夫决策过程、贝尔曼方程、动态规划、蒙特卡罗方法、时间差分学习，包括策略内(SARSA)和策略外(Q-learning)，并通过 Python 示例以实用的方式了解其实现。您还学习了 Q-learning 如今在许多实际应用中是如何使用的，因为这种方法是通过与环境交互从反复试验中学习的。

接下来，我们研究了机器学习强化学习的一些其他实际应用，以及用于解决最新问题的深度学习。

最后，*如果你想全职从事强化学习，已经为你提供了进一步的阅读*。我们祝你一切顺利！*