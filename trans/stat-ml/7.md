# 推荐引擎

**推荐引擎** ( **REs** )最著名的用途是向任何想了解机器学习领域的未知人士或新手解释机器学习是什么。一个经典的例子可能是亚马逊如何推荐类似于你已经购买的书籍，你可能也非常喜欢！此外，从经验来看，推荐引擎似乎是大规模机器学习的一个例子，每个人都理解，或者可能已经理解。但是，尽管如此，推荐引擎正在各地使用。例如，你可能认识的人会在脸书或领英上出现，它会通过显示你最有可能喜欢交朋友的人或你可能感兴趣联系的专业人士来推荐你。当然，这些功能推动了他们的业务发展，并且是公司的核心驱动力。

RE 背后的想法是预测人们可能喜欢什么，并揭示项目/产品之间的关系，以帮助发现过程；这样，它就类似于一个搜索引擎。但一个主要的区别是，搜索引擎的工作方式是被动的；它们只在用户请求某样东西时才显示结果——但推荐引擎是主动的——它试图向人们呈现他们不一定搜索过或他们过去可能没有听说过的相关内容。

# 基于内容的过滤

基于内容的方法试图使用项目的内容或属性，以及两个内容片段之间相似性的一些概念，来生成关于给定项目的相似项目。在这种情况下，余弦相似度用于确定提供推荐的最近用户或项目。

例如:如果你买了一本书，那么你很有可能会买那些经常和其他顾客一起去的相关书籍，以此类推。

# 余弦相似性

因为我们将致力于这个概念，所以重申一下基础知识是很好的。余弦相似性是内积空间的两个非零向量之间的相似性度量，用于度量它们之间角度的余弦。*0<sup>0</sup>T3 的余弦为 *1* ，对于任何其他角度都小于 *1* :*

![](assets/25093364-4ced-4841-b436-82b0390359fd.jpg)

这里*A<sub>I</sub>T3*B<sub>I</sub>T7】分别是向量 *A* 和 *B* 的分量:**

![](assets/0b75ef22-83d3-4948-b58d-f3c6f798e6fd.png)

例如:我们假设 *A = [2，1，0，2，0，1，1]* ， *B = [2，1，1，1，1，0，1，1]* 是两个向量，我们想要计算余弦相似度:

![](assets/9c0ebae4-a08e-4d82-a1ed-f3d7929dd760.jpg)

![](assets/15411e2b-2610-48ae-9039-b6b2a8585b19.jpg)
![](assets/8c9e4aba-29a9-4af5-9a19-5f496f10a311.jpg)

![](assets/27dcee21-8a8b-48b2-89ef-fa12567672d4.jpg)

*0.823* 的值表示两个向量之间非常高的相似度，因为最高可能是 *1* 。在计算相似项目或用户时，我们将对他们的评分向量应用余弦相似度，并根据余弦相似度按降序排列，这将根据相似度得分对所有其他项目进行排序，接近我们正在比较的向量。我们将在本章稍后部分讨论的示例中详细了解这一点。

# 协同过滤

协同过滤是一种群策群力的方法，其中许多用户对项目的偏好集合被用来生成用户对他们尚未评价/审阅的项目的估计偏好。它基于相似性的概念。协同过滤是一种方法，在这种方法中，相似的用户及其评分不是由相似的年龄等决定的，而是由用户表现出的相似偏好决定的，例如观看的电影相似、评分相似等。

# 协同过滤相对于基于内容的过滤的优势

与基于内容的过滤相比，协作过滤具有许多优势。其中一些如下:

*   **不要求理解项目内容**:项目的内容不一定能讲出事情的全貌，比如电影类型/流派等等。
*   **无物品冷启动问题**:即使没有某个物品的信息，我们仍然可以预测物品等级，而无需等待用户购买。
*   **捕捉用户兴趣随时间的变化**:只关注内容并不能为用户的视角和偏好提供任何灵活性。
*   **捕捉固有的细微特征**:对于潜在因素模型来说，这是非常真实的。如果大多数用户购买了两个不相关的项目，那么另一个与其他用户有相似兴趣的用户很可能会购买那个不相关的项目。

# 基于交替最小二乘算法的协同过滤矩阵分解

**交替最小二乘** ( **ALS** )是一种求解矩阵分解问题的优化技术。这种技术实现了良好的性能，并且被证明相对容易实现。这些算法是一大类潜在因素模型的成员，它们试图通过相对较少的未观察到的潜在原因/因素来解释大量用户和项目/电影之间观察到的交互。矩阵分解算法将用户项目数据(矩阵维数 *m* x *n* )视为稀疏矩阵，并尝试用两个低维密集矩阵( *X* 和 *Y* 进行重构，其中 *X* 具有维数 *m* x *k* 和 *Y* 具有维数 *k* x *n 【T23*

潜在因素可以解释为一些解释变量，试图解释用户行为背后的原因，因为协同过滤的目的是试图基于用户的行为进行预测，而不是电影或用户的属性，等等:

![](assets/b7157c72-0970-4eef-b7f4-1cbf73a9db25.jpg)

![](assets/c89b64af-5a97-41b2-95fe-6aca1da3a21a.png)

通过将 *X* 和 *Y* 矩阵相乘，我们试图重建原始矩阵 *A* ，通过减少原始可用等级之间的均方根误差从稀疏矩阵 *A* ( *m* x *n* 维度)和通过将*X*(*m*X*k*维度)和*Y*(*m*X*k*维度)但是 *X* 和 *Y* 相乘得到的矩阵填充了 *m* x *n* 尺寸中的所有槽，但是我们将减少 *A* 中仅有的可用额定值之间的误差。通过这样做，空白处的所有其他值将产生合理合理的评级。

但是，在这种情况下，未知值太多。未知值只不过是需要填入 *X* 和 *Y* 矩阵中的值，这样可以尽量接近矩阵 *A* 中的原始评分。为了解决这个问题，最初，从均匀分布在 *0* 到 *1* 之间生成随机值，并乘以 *5* ，为两个 *X* 和 *Y* 矩阵生成 *0* 到 *5* 之间的值。下一步，实际应用 ALS 方法；迭代应用以下两个步骤，直到达到阈值迭代次数:

1.  *X* 值利用 *Y* 值、学习率( *λ* )和原始稀疏矩阵( *A* )进行更新
2.  *Y* 值利用 *X* 值、学习率( *λ* )和原始稀疏矩阵( *A* )进行更新

学习率( *λ* )用于控制收敛速度；较高的值会导致值的快速变化(类似于优化问题)，有时会导致超出最佳值。类似地，低值需要多次迭代来收敛解。

这里提供了 ALS 的数学表示:

![](assets/b07f5a2b-e2d8-427f-b22b-7a3f7da1a188.jpg)

![](assets/ae87bd28-852c-477c-9749-356628ad9ec6.jpg)

![](assets/165f5d2b-7b1b-40ca-8b9b-fb79857050c3.jpg)

![](assets/3f0ec5e1-bde8-4a90-82ed-cf37b15921d5.jpg)

![](assets/d5ea24ff-e912-41f1-9fad-fed7a673b04b.jpg)

目标是最小化两者之间的误差或平方差。因此，它被称为最小二乘法。在简单的机器学习术语中，我们可以称之为回归问题，因为实际和预测之间的误差正在被最小化。实际上，这个方程从来没有通过计算逆来求解。然而，通过从 *X* 计算 *Y* 并从 *Y* 再次计算 *X* 已经实现了等价，并且这样，它将继续，直到所有迭代都达到，并且这是交替部分实际上出现的地方。开始时 *Y* 是人工生成的， *X* 将基于 *Y* 进行优化，后期通过基于 *X* 优化*Y*；通过这样做，最终解决方案开始在迭代次数上收敛到最优。接下来提供了基本的 Python 语法；我们将在下面的例子中使用相同的例子，以说明电影镜头的例子:

![](assets/a9f36a9b-0c46-4950-bced-3eaef91c4d02.jpg)

# 推荐引擎模型的评估

需要计算对任何模型的评估，以确定该模型相对于实际数据有多好，从而可以通过调整超参数等来提高其性能。事实上，整个机器学习算法的准确性是根据其问题类型来衡量的。在分类问题中，需要计算混淆矩阵，而在回归问题中，需要计算均方误差或调整后的 R 平方值。

均方误差是原始稀疏用户项矩阵(也称为 *A* )与两个低维稠密矩阵( *X* 和 *Y* )重构误差的直接度量。这也是在迭代中最小化的目标函数:

![](assets/16021f92-7704-4f49-adaf-b54dc404dc5b.jpg)

![](assets/51d2cd75-fea5-4fa5-a28d-536d0b9ab4d3.jpg)

均方根误差提供了与变量测度的原始维数相等的维数，因此我们可以分析误差分量与原始值的大小。在我们的示例中，我们计算了电影镜头数据的**均方根误差** ( **RMSE** )。

# 基于网格搜索的推荐引擎超参数选择

在任何机器学习算法中，超参数的选择在模型如何概括底层数据方面起着至关重要的作用。同样，在推荐引擎中，我们可以使用以下超参数:

*   **迭代次数**:迭代次数越高，算法收敛越好。实践证明，ALS 在 10 次迭代内收敛，但建议读者尝试各种数值，看看算法是如何工作的。
*   **潜在因素数量**:这些是试图提供人群行为模式背后原因的解释变量。潜在因素越高，模型越好，但是过高的值可能无法提供显著的提升。
*   **学习率**:学习率是一个可调旋钮，用来改变算法的收敛速度。过高的值可能会因高振荡而突然出现而不是收敛，过低的值可能会让算法采取过多的步骤来收敛。

鼓励读者尝试各种组合，看看准确度值和推荐结果如何变化。在后面的部分中，我们尝试了各种值来提供说明。

# 电影镜头数据的推荐引擎应用

著名的电影镜头数据已从教育发展推荐部分下的链接[https://grouplens.org/datasets/movielens/](https://grouplens.org/datasets/movielens/)使用，文件名显示为 [ml-latest-small.zip](http://files.grouplens.org/datasets/movielens/ml-latest-small.zip) ，其中所有需要的文件均以`.csv`格式保存(`ratings.csv`、`movies.csv`、`links.csv`、`tags.csv`)。为了简单起见，我们在以下示例中使用的文件只是分级和电影。尽管如此，我们鼓励读者合并其他文件，以进一步提高准确性！

```py
>>> import os 
""" First change the following directory link to where all input files do exist """ 
>>> os.chdir("D:\\Book writing\\Codes\\Chapter 7\\ml-latest-small\\ml-latest-small") 

>>> import pandas as pd 
>>> import numpy as np 
>>> import matplotlib.pyplot as plt

```

在下面的代码中，收视率数据提供了用户 ID、电影 ID 和收视率值的详细信息，这意味着每个唯一的用户，他/她已经给了多少部电影，收视率是多少！

```py
>>> ratings = pd.read_csv("ratings.csv") 
>>> print (ratings.head()) 

```

![](assets/d6881da5-0993-48b3-9eeb-8dd89fa9f8c7.png)

在电影数据中，为每部电影存储了具有唯一电影标识、电影标题及其流派的详细信息。我们在这一章没有使用体裁；但是，您可以尝试通过将文本拆分并转换为一个热编码向量(将类别映射到数字空间)来将流派添加到数据中，以提高模型的准确性:

```py
>>> movies = pd.read_csv("movies.csv") 
>>> print (movies.head()) 

```

![](assets/65bd9c71-80ce-43be-a878-0bde0435a1c1.png)

在下面的代码中，我们将分级和电影数据结合在一起，以便可以轻松检索标题进行显示:

```py
#Combining movie ratings & movie names 
>>> ratings = pd.merge(ratings[['userId','movieId','rating']], movies[['movieId', 'title']],how='left',left_on ='movieId' ,right_on = 'movieId')

```

下面的代码将数据转换为矩阵形式，其中行是唯一的用户标识，列是唯一的电影标识，矩阵中的值是用户提供的评分。这个矩阵本质上主要是稀疏的，因此我们将 NAN 值替换为 0 来进行计算。代码后面部分的全部计算都基于这个矩阵:

```py
>>> rp = ratings.pivot_table(columns = ['movieId'],index = ['userId'],values = 'rating') 
>>> rp = rp.fillna(0) 

```

熊猫数据帧建立在 NumPy 数组之上，因此建议使用 NumPy 数组代替熊猫数据帧；在计算用户-用户相似性矩阵或项目-项目相似性矩阵时，像这样的小转换节省了巨大的计算开销。

```py
# Converting pandas DataFrame to NumPy for faster execution in loops etc. 
>>> rp_mat = rp.as_matrix() 

```

The main reason behind the improved computational performance of NumPy array compared with pandas is due to the homogeneity of elements in NumPy array. At the same time, this feature does not allow NumPy arrays to carry heterogeneous elements (for example, character, numeric, float, and so on.). Also, if someone is writing `for loops` on NumPy arrays means, they might be doing something wrong, as NumPy is built for manipulating all the elements in a shot, rather than hovering around each element.

样本余弦相似性由以下伪值代码说明。但是，基于内容的过滤方法保持不变:

```py
>>> from scipy.spatial.distance import cosine 
#The cosine of the angle between them is about 0.822\. 
>>> a= np.asarray( [2, 1, 0, 2, 0, 1, 1, 1]) 
>>> b = np.asarray( [2, 1, 1, 1, 1, 0, 1, 1]) 
>>> print (1-cosine(a,b)) 

```

![](assets/ec05c5db-cbd3-4d9b-a31e-647bc265282b.png)

在接下来的部分中，我们已经介绍了以下小节:

*   用户-用户相似性矩阵
*   电影-电影(项目-项目)相似性矩阵
*   使用 ALS 的协同过滤
*   协同过滤中的网格搜索

# 用户-用户相似性矩阵

下面的代码说明了基于完全蛮力计算的用户-用户相似性矩阵计算(在时间复杂度为*的另一个 for 循环中使用一个 for 循环在 <sup>2</sup>* 上)。有许多其他有效的方法来计算相同的内容，但是为了便于读者理解，这里我们提供了一个尽可能简单的方法:

```py
>>> m, n = rp.shape 
# User similarity matrix 
>>> mat_users = np.zeros((m, m)) 

>>> for i in range(m): 
...     for j in range(m): 
...         if i != j: 
...             mat_users[i][j] = (1- cosine(rp_mat[i,:], rp_mat[j,:])) 
...         else: 
...             mat_users[i][j] = 0\. 

>>> pd_users = pd.DataFrame(mat_users,index =rp.index ,columns= rp.index ) 

```

以下自定义函数将任意用户 ID 和要显示的相似用户数作为输入，并根据相似用户的相关余弦相似度分数返回相似用户:

```py
# Finding similar users 
>>> def topn_simusers(uid = 16,n=5): 
...     users = pd_users.loc[uid,:].sort_values(ascending = False) 
...     topn_users = users.iloc[:n,] 
...     topn_users = topn_users.rename('score')     
...     print ("Similar users as user:",uid) 
...     return pd.DataFrame(topn_users) 

>>> print (topn_simusers(uid=17,n=10)) 

```

![](assets/e113b361-49d3-4420-be66-bc8d9b14b582.png)

我们的任务不是仅仅通过观察相似的用户本身来完成的；相反，我们也想看看任何特定用户评价最高的电影是什么。以下功能为任何给定用户及其最喜欢的电影提供该信息:

```py
# Finding most rated movies of a user 
>>> def topn_movieratings(uid = 355,n_ratings=10):     
...     uid_ratings = ratings.loc[ratings['userId']==uid] 
...     uid_ratings = uid_ratings.sort_values(by='rating',ascending = [False]) 
...     print ("Top",n_ratings ,"movie ratings of user:",uid) 
...     return uid_ratings.iloc[:n_ratings,]     

>>> print (topn_movieratings(uid=596,n_ratings=10)) 

```

下面的截图显示了用户`596`评分最高的电影及其标题，这样我们就可以知道该用户对哪些电影的评分最高:

![](assets/cbfa5774-e2d3-4bef-a494-952393b8fdd8.png)

# 电影-电影相似矩阵

代码的前几节讨论了基于内容的用户-用户相似性，而在下一节中，我们将讨论一个纯粹的电影-电影相似性关系矩阵，这样我们将更深入地挖掘每部电影与其他电影的接近程度。

在下面的代码中，时间函数被用于计算电影-电影相似性矩阵。在我的 i7 电脑上花了整整 30 分钟。它可能需要更多的时间在中等计算机上，因此我已经存储了输出结果，并为方便起见进行了回读；我们鼓励读者自己运行并检查:

```py
# Movie similarity matrix 
>>> import time 
>>> start_time = time.time() 
>>> mat_movies = np.zeros((n, n)) 

>>> for i in range(n): 
...     for j in range(n): 
...         if i!=j: 
...             mat_movies[i,j] = (1- cosine(rp_mat[:,i], rp_mat[:,j])) 
...         else: 
...             mat_movies[i,j] = 0\. 
>>> print("--- %s seconds ---" % (time.time() - start_time)) 

>>> pd_movies = pd.DataFrame(mat_movies,index =rp.columns ,columns= rp.columns )

```

以下两行代码是可选的；我更喜欢从磁盘上读回来，而不是重新运行代码并等待 30 分钟:

```py
>>> pd_movies.to_csv('pd_movies.csv',sep=',') 
>>> pd_movies = pd.read_csv("pd_movies.csv",index_col='movieId') 

```

Readers are encouraged to apply `scipy.spatial.distance.cdist` function with `cosine`, as the parameter can speed up the runtime.

以下代码用于根据用户的偏好等级检索最相似的顶级电影`n`数量。这种分析对于了解其他哪些电影与你实际喜欢的电影相似非常重要:

```py
# Finding similar movies 
>>> def topn_simovies(mid = 588,n=15): 
...     mid_ratings = pd_movies.loc[mid,:].sort_values(ascending = False) 
...     topn_movies = pd.DataFrame(mid_ratings.iloc[:n,]) 
...     topn_movies['index1'] = topn_movies.index 
...     topn_movies['index1'] = topn_movies['index1'].astype('int64') 
...     topn_movies = pd.merge(topn_movies,movies[['movieId','title']],how = 'left', left_on ='index1' ,right_on = 'movieId') 
...     print ("Movies similar to movie id:",mid,",",movies['title'][movies['movieId'] == mid].to_string(index=False),",are") 
...     del topn_movies['index1'] 
...     return topn_movies 

>>> print (topn_simovies(mid=589,n=15)) 

```

仔细查看以下结果，与`Terminator 2`相似的电影有`Jurassic Park`、`Terminator, The`、`Braveheart`、`Forrest Gump`、`Speed`等；所有这些电影实际上都属于动作类。结果似乎听起来足够让我从这个分析中选择我的下一部电影来看！基于内容的过滤似乎奏效了！

![](assets/049c03d8-ec0f-4048-8bf2-27a93c4c5b42.png)

# 使用 ALS 的协同过滤

我们已经完成了基于内容的过滤，从下一节开始，我们将讨论使用 ALS 方法的协作过滤:

```py
# Collaborative filtering 
>>> import os 
""" First change the following directory link to where all input files do exist """ 
>>> os.chdir("D:\\Book writing\\Codes\\Chapter 7\\ml-latest-small\\ml-latest-small") 

>>> import pandas as pd 
>>> import numpy as np 
>>> import matplotlib.pyplot as plt 

>>> ratings = pd.read_csv("ratings.csv") 
>>> print (ratings.head()) 

>>> movies = pd.read_csv("movies.csv") 
>>> print (movies.head()) 

>>> rp = ratings.pivot_table(columns = ['movieId'],index = ['userId'],values = 'rating') 
>>> rp = rp.fillna(0) 

>>> A = rp.values 
>>> print ("\nShape of Original Sparse Matrix",A.shape) 

```

![](assets/5d057d3d-4e1b-4126-8da3-dc935aa9586b.png)

与基于内容的过滤相比，初始数据处理步骤保持不变。这里，我们将使用的主要文件是稀疏分级矩阵。

以下`W`矩阵实际上与原始评分矩阵(矩阵`A`)具有相同的维度，但是每当用户对任何电影提供评分时，其值仅为`0`或`1`(任何电影的最低有效评分为 0.5，最高评分为 5)；我们需要这种类型的矩阵来计算误差等等(我们将在后面的代码部分中看到它的应用)，因为这种方式更便于最小化误差:

```py
>>> W = A>0.5 
>>> W[W==True]=1 
>>> W[W==False]=0 
>>> W = W.astype(np.float64,copy=False) 

```

同样，还需要另一个矩阵`W_pred`来提供建议。`W_pred`矩阵的值为`0`或`1`，与`W`矩阵完全相反。这样做的原因是，如果我们将预测的评分矩阵乘以这个`W_pred`矩阵，这将使已经提供的评分的所有值`0`，以便其他未审核/未评分的值可以很容易地按降序排列，并向从未评分/看过这些电影的用户建议前 5 或前 10 部电影。如果您仔细观察，这里我们也为所有对角线元素分配了零，因为我们不应该向用户推荐与最有可能的电影相同的电影，这是明智的:

```py
>>> W_pred = A<0.5 
>>> W_pred[W_pred==True]=1 
>>> W_pred[W_pred==False]=0 
>>> W_pred = W_pred.astype(np.float64,copy=False) 
>>> np.fill_diagonal(W_pred,val=0) 

```

超参数在以下代码中用样本值初始化，迭代次数设置为`200`，潜在因子数量设置为`100`，学习率为`0.1`:

```py
# Parameters 
>>> m,n = A.shape 
>>> n_iterations = 200 
>>> n_factors = 100 
>>> lmbda = 0.1  

```

`X`和`Y`值从均匀分布【0-1】的随机数开始，乘以`5`，在 0 和 5 之间转换。`X`和`Y`的维数分别为( *m* x *k* )和( *k* x *n* )，因为我们将从一个随机值开始，并针对每次迭代逐步优化:

```py
>>> X = 5 * np.random.rand(m,n_factors) 
>>> Y = 5* np.random.rand(n_factors,n) 

```

RMSE 值通过以下公式计算。这里，我们与`W`矩阵相乘，以便在误差计算中仅考虑评级指标；虽然矩阵`np.dot(X, Y)`在整个矩阵中都有值，但是我们不应该考虑它们，因为误差度量只需要为可用的等级计算:

```py
>>> def get_error(A, X, Y, W):
... return np.sqrt(np.sum((W * (A - np.dot(X, Y)))**2)/np.sum(W))

```

以下步骤是整个 ALS 方法中最关键的部分。最初，这里我们是基于给定的`Y`优化`X`，然后是基于给定的`X`优化`Y`；我们将重复这个过程，直到完成所有迭代次数。每 10 次迭代后，我们打印以查看 RMSE 值如何随着迭代次数的变化而变化:

```py
>>> errors = [] 
>>> for itr in range(n_iterations): 
...     X = np.linalg.solve(np.dot(Y,Y.T)+ lmbda * np.eye(n_factors),np.dot(Y,A.T)).T 
...     Y = np.linalg.solve(np.dot(X.T,X)+ lmbda * np.eye(n_factors),np.dot(X.T,A)) 
...     if itr%10 == 0: 
...         print(itr," iterations completed","RMSError value is:",get_error(A,X,Y,W)) 
...     errors.append(get_error(A,X,Y,W)) 

```

![](assets/429012cc-b950-4b0a-88ac-0f5dc39dc77e.png)

从前面的结果来看，很明显，误差值实际上随着迭代次数的增加而减少，这实际上是算法按预期执行。以下代码用于在图表上绘制相同的错误:

```py
>>> print ("RMSError of rated movies: ",get_error(A,X,Y,W)) 
>>> plt.plot(errors); 
>>> plt.ylim([0, 3.5]); 
>>> plt.xlabel("Number of Iterations");plt.ylabel("RMSE") 
>>> plt.title("No.of Iterations vs. RMSE") 
>>> plt.show()

```

![](assets/fd635735-5f50-463a-9632-9915b6e350ef.png)

一旦迭代次数结束，我们将获得更新的 *X* 和 *Y* 矩阵，这些矩阵将用于创建整个预测评级矩阵，该矩阵可以从简单的点积获得，如下所示:

```py
>>> A_hat = np.dot(X,Y) 

```

在计算出预测矩阵(`A_hat`)后，下一个也是最后一个任务就是利用它向用户推荐最相关的电影。在下面的代码中，我们根据任何特定用户提供的电影评论模式或评分向他们推荐电影:

```py
>>> def print_recommovies(uid=315,n_movies=15,pred_mat = A_hat,wpred_mat = W_pred ): 
...     pred_recos = pred_mat*wpred_mat 
...     pd_predrecos = pd.DataFrame(pred_recos,index =rp.index ,columns= rp.columns ) 
...     pred_ratings = pd_predrecos.loc[uid,:].sort_values(ascending = False) 
...     pred_topratings = pred_ratings[:n_movies,] 
...     pred_topratings = pred_topratings.rename('pred_ratings')   
...     pred_topratings = pd.DataFrame(pred_topratings) 
...     pred_topratings['index1'] = pred_topratings.index 
...     pred_topratings['index1'] = pred_topratings['index1'].astype('int64') 
...     pred_topratings = pd.merge(pred_topratings,movies[['movieId','title']],how = 'left',left_on ='index1' ,right_on = 'movieId') 
...     del pred_topratings['index1']     
...     print ("\nTop",n_movies,"movies predicted for the user:",uid," based on collaborative filtering\n") 
...     return pred_topratings 

>>> predmtrx = print_recommovies(uid=355,n_movies=10,pred_mat=A_hat,wpred_mat=W_pred) 
>>> print (predmtrx)

```

![](assets/6937fe84-009c-4220-9ff4-06364fac06ba.png)

从前面的推荐可以看出，电影用户 355 最可能喜欢的是`Goodfellas`，其次是`Princess Bride`、`There's Something About Mary`等等。嗯，这些推荐需要用户自己判断！

# 协同过滤中的网格搜索

正如我们前面提到的，我们需要调整参数，以便了解我们将在哪里获得最佳的机器学习模型。在任何机器学习模型中，调整参数都是一种事实上的标准。在下面的代码中，我们尝试了迭代次数、潜在因素和学习率的各种组合。整个代码或多或少会保持不变，但我们总是保持一个标签，记录我们见过的最少的错误；如果出现的任何新错误少于现有错误，我们会相应地打印组合:

```py
# Grid Search on Collaborative Filtering 
>>> niters = [20,50,100,200] 
>>> factors = [30,50,70,100] 
>>> lambdas = [0.001,0.01,0.05,0.1] 

>>> init_error = float("inf") 

>>> print("\n\nGrid Search results of ALS Matrix Factorization:\n") 
>>> for niter in niters: 
...     for facts in factors: 
...         for lmbd in lambdas: 

...             X = 5 * np.random.rand(m,facts) 
...             Y = 5* np.random.rand(facts,n) 

...             for itr in range(niter): 
...                 X = np.linalg.solve(np.dot(Y,Y.T)+ lmbd * np.eye(facts), np.dot(Y,A.T)).T 
...                 Y = np.linalg.solve(np.dot(X.T,X)+ lmbd * np.eye(facts), np.dot(X.T,A)) 

...             error = get_error(A,X,Y,W) 

...             if error<init_error: 
...                 print ("No.of iters",niter,"No.of Factors",facts,"Lambda",lmbd, "RMSE",error) 
...                 init_error = error 

```

![](assets/7d96907d-4c8a-4040-9bf8-0bcee35e49da.png)

从网格搜索中获得的最佳可能 RMSE 值是`1.695345`，它小于从基本方法中获得的 RMSE 值，即`1.6961`。因此，在实现任何算法之前执行网格搜索总是明智的。

在 R 代码中，`recommenderlab`包已经被用来解决协同过滤问题，因为这个包有很多特性和功能可以玩。但是基本的基于内容的过滤算法是建立在第一原则之上的:

The following R code may take about 30 minutes to run (of course runtime depends on the system configuration though!).

推荐引擎(基于内容和协作过滤)的代码如下:

```py
setwd("D:\\Book writing\\Codes\\Chapter 7\\ml-latest-small\\ml-latest-small")
ratings = read.csv("ratings.csv") 
movies = read.csv("movies.csv") 

ratings = ratings[,!names(ratings) %in% c("timestamp")]

library(reshape2) 

# Creating Pivot table 
ratings_mat = acast(ratings,userId~movieId)
ratings_mat[is.na(ratings_mat)] =0 

# Content-based filtering 
library(lsa)
a = c(2, 1, 0, 2, 0, 1, 1, 1) 
b = c(2, 1, 1, 1, 1, 0, 1, 1) 
print (paste("Cosine similarity between A and B is",round(cosine(a,b), 4))) 

m = nrow(ratings_mat);n = ncol(ratings_mat) 

# User similarity 
matrix mat_users = matrix(nrow = m, ncol = m) 
for (i in 1:m){ 
 for (j in 1:m){ 
 if (i != j){ 
 mat_users[i,j] = cosine(ratings_mat[i,],ratings_mat[j,])
 } else { 
 mat_users[i,j] = 0.0 
 } 
 }
} 
colnames(mat_users) = rownames(ratings_mat); 
rownames(mat_users) = rownames(ratings_mat) 
df_users = as.data.frame(mat_users) 

# Finding similar users 
topn_simusers <- function(uid=16,n=5){ 
 sorted_df = sort(df_users[uid,],decreasing = TRUE)[1:n]
 print(paste("Similar users as user:",uid)) 
 return(sorted_df) 
} 
print(topn_simusers(uid = 17,n=10)) 

# Finding most rated movies of a user 
library(sqldf) 

ratings_withmovie = sqldf(" select a.*,b.title from ratings as a left join movies as b on a.movieId = b.movieId") 

# Finding most rated movies of a user 
topn_movieratings <- function(uid=355,n_ratings=10){ 
 uid_ratings = ratings_withmovie[ratings_withmovie$userId==uid,]
 sorted_uidrtng = uid_ratings[order(-uid_ratings$rating),]
 return(head(sorted_uidrtng,n_ratings)) 
} 
print( topn_movieratings(uid = 596,n=10)) 

# Movies similarity matrix 
mat_movies = matrix(nrow = n, ncol = n) 
for (i in 1:n){ 
 for (j in 1:n){ 
 if (i != j){ 
 mat_movies[i,j] = cosine(ratings_mat[,i],ratings_mat[,j]) 
 } else { 
 mat_movies[i,j] = 0.0 
 } 
 } 
} 
colnames(mat_movies) = colnames(ratings_mat); 
rownames(mat_movies) = colnames(ratings_mat) 
df_movies = as.data.frame(mat_movies)

write.csv(df_movies,"df_movies.csv") 

df_movies = read.csv("df_movies.csv") 
rownames(df_movies) = df_movies$X 
colnames(df_movies) = c("aaa",df_movies$X) 
df_movies = subset(df_movies, select=-c(aaa)) 

# Finding similar movies 
topn_simovies <- function(mid=588,n_movies=5){ 
 sorted_df = sort(df_movies[mid,],decreasing = TRUE)[1:n_movies]
 sorted_df_t = as.data.frame(t(sorted_df)) 
 colnames(sorted_df_t) = c("score") 
 sorted_df_t$movieId = rownames(sorted_df_t)

 print(paste("Similar",n_movies, "movies as compared to the movie",mid,"are :")) 
 sorted_df_t_wmovie = sqldf(" select a.*,b.title from sorted_df_t as a left join movies as b on a.movieId = b.movieId")
 return(sorted_df_t_wmovie) 
} 
print(topn_simovies(mid = 589,n_movies=15)) 

# Collaborative filtering 
ratings = read.csv("ratings.csv") 
movies = read.csv("movies.csv") 

library(sqldf) 
library(reshape2) 
library(recommenderlab) 

ratings_v2 = ratings[,-c(4)]
ratings_mat = acast(ratings_v2,userId~movieId) 
ratings_mat2 = as(ratings_mat, "realRatingMatrix")

getRatingMatrix(ratings_mat2)

#Plotting user-item complete matrix 
image(ratings_mat2, main = "Raw Ratings")

# Fitting ALS method on Data
rec=Recommender(ratings_mat2[1:nrow(ratings_mat2)],method="UBCF", param=list(normalize = "Z-score",method="Cosine",nn=5, minRating=1))
rec_2=Recommender(ratings_mat2[1:nrow(ratings_mat2)],method="POPULAR")

print(rec) 
print(rec_2) 

names(getModel(rec)) 
getModel(rec)$nn 

# Create predictions for all the users 
recom_pred = predict(rec,ratings_mat2[1:nrow(ratings_mat2)], type="ratings") 

# Putting predictions into list 
rec_list<-as(recom_pred,"list") 
head(summary(rec_list)) 

print_recommendations <- function(uid=586,top_nmovies=10){ 
 recoms_list = rec_list[[uid]] 
 sorted_df = as.data.frame(sort(recoms_list,decreasing = TRUE)[1:top_nmovies]) 
 colnames(sorted_df) = c("score") 
 sorted_df$movieId = rownames(sorted_df) 
 print(paste("Movies recommended for the user",uid,"are follows:"))
 sorted_df_t_wmovie = sqldf(" select a.*,b.title from sorted_df as a left join movies as b on a.movieId = b.movieId")
 return(sorted_df_t_wmovie) 
} 
print(print_recommendations(uid = 580,top_nmovies = 15))

```

# 摘要

在本章中，您已经了解了向用户推荐电影的基于内容和协作过滤技术，或者通过使用余弦相似度考虑其他用户，或者通过考虑电影评分进行矩阵分解计算。在计算上，基于内容的过滤计算速度更快，但只考虑一个维度，要么是其他用户，要么是其他类似的电影。而在协同过滤中，推荐是通过同时考虑用户和电影维度来提供的。所有的 Python 实现都是从第一原则开始的，因为我们没有一个足够好的包来实现它，而且了解基础知识也很好。在 R 编程中，我们使用`recommenderlab`包应用协同过滤。最后，展示了一个关于如何在推荐引擎中调整超参数的网格搜索示例。

在下一章中，我们将介绍无监督学习的细节，更准确地说，是聚类和主成分分析模型。