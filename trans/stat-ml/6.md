# 支持向量机和神经网络

在本章中，我们将同时介绍支持向量机和神经网络，它们的计算复杂度较高，需要相对大量的计算资源，但在大多数情况下，与其他机器学习方法相比，它们确实提供了明显更好的结果。

一个**支持向量机** ( **SVM** )可以想象成一个曲面，它最大化在多维空间中表示的各种类型的数据点之间的边界，也称为超平面，它在每个子区域中创建最均匀的点。

支持向量机可以用于任何类型的数据，但是对于相对于观测值具有非常高维度的数据类型具有特殊的额外优势，例如:

*   文本分类，其中语言具有单词向量的维度
*   为了通过正确标记色谱图来控制 DNA 测序的质量

# 支持向量机工作原理

支持向量机根据其工作原理主要分为三类:

*   最大边距分类器
*   支持向量分类器
*   支持向量机

# 最大余量分类器

人们通常用最大裕度分类器来推广支持向量机。然而，与最大裕度分类器相比，支持向量机有更多的优点，我们将在本章中介绍。画出无限的超平面来对同一组数据进行分类是可行的，但问题是，哪一个才是理想的超平面？最大边缘分类器提供了一个答案:具有最大分离宽度边缘的超平面。

**超平面:**在继续之前，让我们快速回顾一下什么是超平面。在 *n 维*空间中，超平面是 n-1 维的平坦仿射子空间。这意味着，在二维空间中，超平面是一条将二维空间分成两半的直线。超平面由以下等式定义:

![](assets/dc5ea386-e02f-4d2c-820a-fe0b77c95619.jpg)

位于超平面上的点必须遵循上面的等式。然而，上面和下面也有区域。这意味着观测值可以落在两个区域中的任何一个，也称为类区域:

![](assets/745876b0-e515-4ca8-887f-12e1ed2d4070.jpg)

![](assets/f015222f-d50d-4cb7-8237-8331007f8ce6.jpg)

最大裕度分类器的数学表示如下，这是一个优化问题:

![](assets/51532f4a-fc51-45e1-a843-f236c56f158a.jpg)

![](assets/705b4c75-a831-492e-ac87-33117f0d01c5.jpg)

![](assets/01984b5c-6306-4283-a78c-05a3fc7c4f26.jpg)

![](assets/221b0d6c-1d4e-441b-8db1-a73ba75390dc.jpg)

**约束 2** 通过取系数与 x 个变量的乘积，最终与类变量指标的乘积，确保观测值位于超平面的正确侧。

In non-separable cases, the maximum margin classifier will not have a separating hyperplane, which is also known as no feasible solution. This issue will be solved with support vector classifiers, which we will be covering in the next section.

在下图中，我们可以画出无限个独立的超平面来分隔两个类(蓝色和红色)。然而，最大裕度分类器试图在两个类别之间拟合最宽的平板(最大化正和负超平面之间的裕度)，并且接触正和负超平面的观测值被称为支持向量:

Classifier performance purely depends on the support vectors and any changes to observation values which are not support vectors (or observations that do not touch hyperplanes) do not impact any change in the performance of the Maximum Margin Classifier, as only extreme points are considered in the algorithm.

![](assets/1a927995-167f-432c-8fd8-43761b1335c7.png)

# 支持向量分类器

支持向量分类器是最大裕度分类器的扩展版本，其中对于不可分离的情况允许一些违规，以便创建最佳拟合，即使在阈值限制内有微小误差。事实上，在现实生活场景中，我们几乎找不到任何具有纯可分类的数据；大多数类在重叠类中有一些或更多的观察。

支持向量分类器的数学表示如下，对约束稍加修正以适应误差项:

![](assets/d51b149e-6345-4272-9425-7df2882bad0e.jpg)

![](assets/506c4e44-6f42-406c-ac42-3c0b47ba8b99.jpg)

![](assets/7dbe84f5-1926-4d9b-946f-f4e5849d4673.jpg)

![](assets/9bcc3499-3d6c-47d2-950f-da53b6199beb.jpg)

![](assets/45c879a2-ae9f-4dc7-aaaf-192b702efda7.jpg)

在约束 4 中， *C* 值是非负调谐参数，以适应模型中更多或更少的总体误差。具有较高的 *C* 值将导致更健壮的模型，而较低的值由于较少违反误差项而创建灵活的模型。在实践中， *C* 值将是所有机器学习模型中常见的一个调整参数。

更改 *C* 值对边距的影响如下图所示；使用高值的 *C* ，模型将更加宽容，并且在左图中也有违规(错误)的空间，而使用较低值的 *C* ，没有接受违规的范围导致边距宽度减小。 *C* 是支持向量分类器中的一个调整参数:

![](assets/8e5ead41-4af2-4cb6-8399-bceaca80fb43.png)

# 支持向量机

当决策边界是非线性的并且无论代价函数是什么都不能与支持向量分类器分离时，使用支持向量机！下图解释了一维和二维的非线性可分情况:

![](assets/138bc690-b0c0-40a7-b474-1576ab7e8ab4.png)

很明显，无论成本值是多少，我们都无法使用支持向量分类器进行分类。因此，我们需要使用另一种处理数据的方式，称为内核技巧，使用内核函数来处理非线性可分离的数据。

下图中，在将数据从一维数据转换为二维数据时，应用了一个 2 次多项式核。通过这样做，数据在更高维度上变得可线性分离。在左图中，不同的类(红色和蓝色)仅绘制在 *X <sub>1</sub>* 上，而在应用了 2 度之后，我们现在有了 2 个维度， *X <sub>1</sub>* 和 *X <sup>2</sup> <sub>1</sub>* (原始维度和新维度)。多项式核的次数是一个调整参数；从业者需要用不同的值来调整它们，以检查模型在哪些地方可能有更高的精度:

![](assets/194bec38-6788-43e0-9f25-0f5b2b600969.png)

然而，在 2 维的情况下，核技巧如下所述应用于 2 次多项式核。在将数据投影到更高维度之后，似乎已经使用线性平面成功地对观测进行了分类:

![](assets/609ef1c5-fca1-4799-bf07-feb50c8b30e9.png)

# 内核函数

核函数是在给定原始特征向量的情况下，返回与其对应的映射特征向量的点积相同的值的函数。核函数不显式地将特征向量映射到更高维空间，也不计算映射向量的点积。内核通过一系列不同的运算产生相同的值，这些运算通常可以更有效地计算。

使用核函数的主要原因是消除了从给定的基本向量空间导出高维向量空间的计算要求，从而可以在更高维度上线性分离观测值。为什么有人需要像这样，导出的向量空间会随着维数的增加呈指数级增长，并且它会变得几乎难以继续计算，即使你有 30 左右的可变大小。以下示例显示了变量的大小是如何增长的。

**例**:当我们有 *x* 和 *y* 两个变量时，用一个多项式次数核，另外需要计算 x <sup>2</sup> ，y <sup>2</sup> ，以及 *xy* 维度。反之，如果我们有三个变量 x、y 和 z，那么我们需要计算 *x <sup>2</sup>* 、 *y <sup>2</sup>* 、 *z <sup>2</sup>* 、 *xy* 、 *yz* 、 *xz* 和 *xyz* 向量空间。到这个时候，你会意识到多一个维度的增加会创造出如此多的组合。因此，需要注意降低其计算复杂性；这就是核仁创造奇迹的地方。内核在下面的等式中有更正式的定义:

![](assets/efccbd50-ac85-4eb7-8b9e-2b07b1d637a3.jpg)

**多项式核**:多项式核使用普遍，尤其是 2 次。事实上，支持向量机的发明者*弗拉迪米尔·N·瓦普尼克*利用 2 度核对手写数字进行分类。多项式核由以下等式给出:

![](assets/806081cb-7d8b-49d8-a219-73c757048233.jpg)

**径向基函数(RBF) /高斯核**:对于需要非线性模型的问题，RBF 核是很好的首选。作为映射特征空间中的超平面的决策边界类似于作为原始空间中的超球面的决策边界。高斯核产生的特征空间可以有无限多个维度，否则这是不可能的。径向基函数核由以下等式表示:

![](assets/d8f7c961-aba2-4d04-a727-69981c630ae7.jpg)

这通常简化为以下等式:

![](assets/d1659f8b-e375-45da-81d1-3e4cc077a9dd.jpg)

当使用支持向量机时，缩放特征是可取的，但是当使用径向基函数核时，这是非常重要的。当γ![](assets/ae8e6d6e-490e-4396-9543-e6caa1957ddc.png)值很小时，它会在更高的维度上给你一个尖的凸起；较大的值会产生更柔和、更宽的凸起。一个小的伽玛会给你低偏差和高方差的解；另一方面，高伽马会给你高偏差和低方差的解决方案，这就是你如何使用径向基函数核控制模型的拟合:

![](assets/5bb737fc-2cbe-4d01-9c0a-d6c2b3ae1084.png)

# 带有字母识别数据示例的 SVM 多标签分类器

字母识别数据已从 UCI 机器学习存储库中使用，用于使用 SVM 分类器进行说明。下载数据的链接在这里:[https://archive.ics.uci.edu/ml/datasets/letter+recognition](https://archive.ics.uci.edu/ml/datasets/letter+recognition)。任务是将大量黑白矩形像素显示器中的每一个识别为英文字母表中的 26 个大写字母之一(从 A 到 Z；总共 26 个类)，基于整数中的一些特征，例如 x-box(框的水平位置)、y-box(框的垂直位置)、框的宽度、框的高度等等:

```py
>>> import os 
""" First change the following directory link to where all input files do exist """ 
>>> os.chdir("D:\\Book writing\\Codes\\Chapter 6") 

>>> import pandas as pd 
>>> letterdata = pd.read_csv("letterdata.csv") 
>>> print (letterdata.head()) 

```

![](assets/968bd8ba-5fd1-4b5f-8973-3142f3df4cda.png)

以下代码用于从`x`变量中移除目标变量，同时为了方便起见创建新的`y`变量:

```py
>>> x_vars = letterdata.drop(['letter'],axis=1) 
>>> y_var = letterdata["letter"] 

```

由于 scikit-learn 不直接支持字符，我们需要将它们转换为数字映射。在这里，我们用字典做到了这一点:

```py
>>> y_var = y_var.replace({'A':1,'B':2,'C':3,'D':4,'E':5,'F':6,'G':7, 'H':8,'I':9, 'J':10,'K':11,'L':12,'M':13,'N':14,'O':15,'P':16,'Q':17,'R':18,'S':19,'T':20,'U':21, 'V':22, 'W':23,'X':24,'Y':25,'Z':26}) 

>>> from sklearn.metrics import accuracy_score,classification_report 
>>> from sklearn.model_selection import train_test_split 
>>> x_train,x_test,y_train,y_test = train_test_split(x_vars,y_var,train_size = 0.7,random_state=42) 

# Linear Classifier 
>>> from sklearn.svm import SVC 

```

# 最大裕度分类器-线性核

以下代码显示了成本值为 1.0 的线性分类器(也称为最大利润分类器):

```py
>>> svm_fit = SVC(kernel='linear',C=1.0,random_state=43) 
>>> svm_fit.fit(x_train,y_train) 

>>> print ("\nSVM Linear Classifier - Train Confusion Matrix\n\n",pd.crosstab(y_train, svm_fit.predict(x_train),rownames = ["Actuall"],colnames = ["Predicted"]) )      
>>> print ("\nSVM Linear Classifier - Train accuracy:",round(accuracy_score(y_train, svm_fit.predict(x_train)),3)) 
>>> print ("\nSVM Linear Classifier - Train Classification Report\n", classification_report(y_train,svm_fit.predict(x_train))) 

```

![](assets/50971089-56e4-46b7-8054-61181961f7f6.png)

以下代码用于打印测试精度值:

```py
>>> print ("\n\nSVM Linear Classifier - Test Confusion Matrix\n\n",pd.crosstab(y_test, svm_fit.predict(x_test),rownames = ["Actuall"],colnames = ["Predicted"]))       
>>> print ("\nSVM Linear Classifier - Test accuracy:",round(accuracy_score( y_test,svm_fit.predict(x_test)),3)) 
>>> print ("\nSVM Linear Classifier - Test Classification Report\n", classification_report(y_test,svm_fit.predict(x_test)))

```

![](assets/7a0e953e-b735-41d5-837a-8ed14b07e132.png)

从上面的结果中，我们可以看到线性分类器的测试准确率为 85%，就准确率而言，这是一个不错的值。让我们也来探索一下多项式核。

# 多项式核

在下面的代码中使用了一个 2 次多项式核来检查是否有可能提高精度。成本值相对于线性分类器保持恒定，以便确定非线性核的影响:

```py
#Polynomial Kernel
>>> svm_poly_fit = SVC(kernel='poly',C=1.0,degree=2)
>>> svm_poly_fit.fit(x_train,y_train)
>>> print ("\nSVM Polynomial Kernel Classifier - Train Confusion Matrix\n\n",pd.crosstab(y_train,svm_poly_fit.predict(x_train),rownames = ["Actuall"],colnames = ["Predicted"]) )
>>> print ("\nSVM Polynomial Kernel Classifier - Train accuracy:",round(accuracy_score( y_train,svm_poly_fit.predict(x_train)),3))
>>> print ("\nSVM Polynomial Kernel Classifier - Train Classification Report\n", classification_report(y_train,svm_poly_fit.predict(x_train)))

```

![](assets/c0cfe244-3086-4dbb-ab6f-e74bc2a1b82f.png)

```py
>>> print ("\n\nSVM Polynomial Kernel Classifier - Test Confusion Matrix\n\n", pd.crosstab(y_test,svm_poly_fit.predict(x_test),rownames = ["Actuall"],colnames = ["Predicted"]))
>>> print ("\nSVM Polynomial Kernel Classifier - Test accuracy:",round(accuracy_score( y_test,svm_poly_fit.predict(x_test)),3))
>>> print ("\nSVM Polynomial Kernel Classifier - Test Classification Report\n", classification_report(y_test,svm_poly_fit.predict(x_test)))

```

![](assets/fbad3f32-da84-4410-bd85-e59560ee5afa.png)

多项式核产生了 95.4%的测试精度，与线性分类器的 85%的测试精度相比，这是一个巨大的改进。通过提高一度，我们实现了 10%的精度提升。

# 径向基函数核

在最后的实验中，使用径向基函数核来确定测试精度。这里，对于相应的其他核，成本值保持恒定，但是伽马值被选择为 0.1 以适合模型:

```py
#RBF Kernel 
>>> svm_rbf_fit = SVC(kernel='rbf',C=1.0, gamma=0.1) 
>>> svm_rbf_fit.fit(x_train,y_train) 
>>> print ("\nSVM RBF Kernel Classifier - Train Confusion Matrix\n\n",pd.crosstab( y_train,svm_rbf_fit.predict(x_train),rownames = ["Actuall"],colnames = ["Predicted"]))      
>>> print ("\nSVM RBF Kernel Classifier - Train accuracy:",round(accuracy_score( y_train, svm_rbf_fit.predict(x_train)),3)) 
>>> print ("\nSVM RBF Kernel Classifier - Train Classification Report\n", classification_report(y_train,svm_rbf_fit.predict(x_train))) 

```

![](assets/98decf11-2342-48eb-a760-661ae7b5562e.png)

```py
>>> print ("\n\nSVM RBF Kernel Classifier - Test Confusion Matrix\n\n", pd.crosstab(y_test,svm_rbf_fit.predict(x_test),rownames = ["Actuall"],colnames = ["Predicted"]))       
>>> print ("\nSVM RBF Kernel Classifier - Test accuracy:",round( accuracy_score( y_test,svm_rbf_fit.predict(x_test)),3)) 
>>> print ("\nSVM RBF Kernel Classifier - Test Classification Report\n", classification_report(y_test,svm_rbf_fit.predict(x_test))) 

```

![](assets/abed3a46-2d80-474f-9de3-2d64e9495fc4.png)

从径向基函数核得到的测试精度为 96.9%，略好于多项式核的 95.4%。然而，通过使用网格搜索仔细调整参数，可以进一步提高测试精度。

网格搜索已经通过使用径向基函数核改变成本和伽马值来执行。以下代码描述了详细信息:

```py
# Grid Search - RBF Kernel 
>>> from sklearn.pipeline import Pipeline 
>>> from sklearn.model_selection import train_test_split,GridSearchCV 

>>> pipeline = Pipeline([('clf',SVC(kernel='rbf',C=1,gamma=0.1 ))]) 

>>> parameters = {'clf__C':(0.1,0.3,1,3,10,30), 
              'clf__gamma':(0.001,0.01,0.1,0.3,1)} 

>>> grid_search_rbf = GridSearchCV(pipeline,parameters,n_jobs=-1,cv=5, verbose=1, scoring='accuracy') 
>>> grid_search_rbf.fit(x_train,y_train) 

>>> print ('RBF Kernel Grid Search Best Training score: %0.3f' % grid_search_rbf.best_score_) 
>>> print ('RBF Kernel Grid Search Best parameters set:') 
>>> best_parameters = grid_search_rbf.best_estimator_.get_params() 

>>> for param_name in sorted(parameters.keys()): 
...     print ('\t%s: %r' % (param_name, best_parameters[param_name])) 

```

![](assets/a83b819e-d8f2-48ce-849d-925554f35caa.png)

```py
>>> predictions = grid_search_rbf.predict(x_test) 
>>> print ("RBF Kernel Grid Search - Testing accuracy:",round(accuracy_score(y_test, predictions),4)) 
>>> print ("\nRBF Kernel Grid Search - Test Classification Report",classification_report( y_test, predictions)) 
>>> print ("\n\nRBF Kernel Grid Search- Test Confusion Matrix\n\n",pd.crosstab(y_test, predictions,rownames = ["Actuall"],colnames = ["Predicted"]))       

```

![](assets/c6b8717c-92f0-4c07-9179-8707edb112bc.png)

通过观察上述结果，我们可以得出结论，获得的最佳测试精度为 97.15%，这是比任何其他分类器获得的值都高的值。因此，我们可以得出结论，径向基函数核产生最好的结果可能！

SVM 分类器的下列代码:

```py
# SVM Classifier   
# First change the following   directory link to where all the input files do exist   
setwd("D:\\Book   writing\\Codes\\Chapter 6")   

letter_data = read.csv("letterdata.csv")   
set.seed(123)   
numrow = nrow(letter_data)   
trnind = sample(1:numrow,size =   as.integer(0.7*numrow))   
train_data =   letter_data[trnind,]   
test_data = letter_data[-trnind,]   

library(e1071)   

accrcy <- function(matrx){    
  return(   sum(diag(matrx)/sum(matrx)))}   
precsn <- function(matrx){   
  return(diag(matrx) /   rowSums(matrx)) }   
recll <- function(matrx){   
  return(diag(matrx) /   colSums(matrx)) }   

# SVM - Linear Kernel   
svm_fit = svm(letter~.,data = train_data,kernel="linear",cost=1.0,   scale = TRUE)   
tr_y_pred = predict(svm_fit,   train_data)   
ts_y_pred =   predict(svm_fit,test_data)   
tr_y_act =   train_data$letter;ts_y_act = test_data$letter   

tr_tble =   table(tr_y_act,tr_y_pred)   
print(paste("Train   Confusion Matrix"))   
print(tr_tble)   
tr_acc = accrcy(tr_tble)   
print(paste("SVM Linear   Kernel Train accuracy:",round(tr_acc,4)))   

tr_prec = precsn(tr_tble)   
print(paste("SVM Linear   Kernel Train Precision:"))   
print(tr_prec)   

tr_rcl = recll(tr_tble)   
print(paste("SVM Linear Kernel   Train Recall:"))   
print(tr_rcl)   

ts_tble =   table(ts_y_act,ts_y_pred)   
print(paste("Test   Confusion Matrix"))   
print(ts_tble)   

ts_acc = accrcy(ts_tble)   
print(paste("SVM Linear   Kernel Test accuracy:",round(ts_acc,4)))   

ts_prec = precsn(ts_tble)   
print(paste("SVM Linear   Kernel Test Precision:"))   
print(ts_prec)   

ts_rcl = recll(ts_tble)   
print(paste("SVM Linear   Kernel Test Recall:"))   
print(ts_rcl)   

# SVM - Polynomial Kernel   
svm_poly_fit =   svm(letter~.,data = train_data,kernel="poly",cost=1.0,degree = 2    ,scale = TRUE)   

tr_y_pred =   predict(svm_poly_fit, train_data)   
ts_y_pred =   predict(svm_poly_fit,test_data)   

tr_y_act =   train_data$letter;ts_y_act = test_data$letter   

tr_tble =   table(tr_y_act,tr_y_pred)   
print(paste("Train   Confusion Matrix"))   
print(tr_tble)   
tr_acc = accrcy(tr_tble)   
print(paste("SVM   Polynomial Kernel Train accuracy:",round(tr_acc,4)))   

tr_prec = precsn(tr_tble)   
print(paste("SVM   Polynomial Kernel Train Precision:"))   
print(tr_prec)   

tr_rcl = recll(tr_tble)   
print(paste("SVM   Polynomial Kernel Train Recall:"))   
print(tr_rcl)   

ts_tble =   table(ts_y_act,ts_y_pred)   
print(paste("Test   Confusion Matrix"))   
print(ts_tble)   

ts_acc = accrcy(ts_tble)   
print(paste("SVM   Polynomial Kernel Test accuracy:",round(ts_acc,4)))   

ts_prec = precsn(ts_tble)   
print(paste("SVM   Polynomial Kernel Test Precision:"))   
print(ts_prec)   

ts_rcl = recll(ts_tble)   
print(paste("SVM   Polynomial Kernel Test Recall:"))   
print(ts_rcl)   

# SVM - RBF Kernel   
svm_rbf_fit = svm(letter~.,data   = train_data,kernel="radial",cost=1.0,gamma = 0.2  ,scale = TRUE)   

tr_y_pred =   predict(svm_rbf_fit, train_data)   
ts_y_pred =   predict(svm_rbf_fit,test_data)   

tr_y_act =   train_data$letter;ts_y_act = test_data$letter   

tr_tble =   table(tr_y_act,tr_y_pred)   
print(paste("Train   Confusion Matrix"))   
print(tr_tble)   
tr_acc = accrcy(tr_tble)   
print(paste("SVM RBF   Kernel Train accuracy:",round(tr_acc,4)))   

tr_prec = precsn(tr_tble)   
print(paste("SVM RBF   Kernel Train Precision:"))   
print(tr_prec)   
 tr_rcl = recll(tr_tble)   
print(paste("SVM RBF   Kernel Train Recall:"))   
print(tr_rcl)   

ts_tble =   table(ts_y_act,ts_y_pred)   
print(paste("Test   Confusion Matrix"))   
print(ts_tble)   

ts_acc = accrcy(ts_tble)   
print(paste("SVM RBF   Kernel Test accuracy:",round(ts_acc,4)))   

ts_prec = precsn(ts_tble)   
print(paste("SVM RBF   Kernel Test Precision:"))   
print(ts_prec)   

ts_rcl = recll(ts_tble)   
print(paste("SVM RBF   Kernel Test Recall:"))   
print(ts_rcl)   

# Grid search - RBF Kernel   
library(e1071)   
svm_rbf_grid =   tune(svm,letter~.,data = train_data,kernel="radial",scale=TRUE,ranges   = list(   
  cost = c(0.1,0.3,1,3,10,30),   
  gamma =   c(0.001,0.01,0.1,0.3,1) ),   
  tunecontrol =   tune.control(cross = 5))   

print(paste("Best   parameter from Grid Search"))   
print(summary(svm_rbf_grid))   

best_model =   svm_rbf_grid$best.model   
tr_y_pred = predict(best_model,data   = train_data,type = "response")   
ts_y_pred =   predict(best_model,newdata = test_data,type = "response")   
tr_y_act =   train_data$letter;ts_y_act = test_data$letter   

tr_tble =   table(tr_y_act,tr_y_pred)   
print(paste("Train   Confusion Matrix"))   
print(tr_tble)   
tr_acc = accrcy(tr_tble)   
print(paste("SVM RBF   Kernel Train accuracy:",round(tr_acc,4)))   

tr_prec = precsn(tr_tble)   
print(paste("SVM RBF   Kernel Train Precision:"))   
print(tr_prec)   

tr_rcl = recll(tr_tble)   
print(paste("SVM RBF   Kernel Train Recall:"))   
print(tr_rcl)   

ts_tble =   table(ts_y_act,ts_y_pred)   
print(paste("Test   Confusion Matrix"))   
print(ts_tble) 
ts_acc = accrcy(ts_tble)   
print(paste("SVM RBF   Kernel Test accuracy:",round(ts_acc,4)))   

ts_prec = precsn(ts_tble)   
print(paste("SVM RBF   Kernel Test Precision:"))   
print(ts_prec)   

ts_rcl = recll(ts_tble)   
print(paste("SVM RBF   Kernel Test Recall:"))   
print(ts_rcl)   

```

# 人工神经网络

**人工神经网络** ( **神经网络** s)使用从生物大脑的复制品中得到的模型来模拟一组输入信号和输出信号之间的关系，生物大脑对来自其感觉输入的刺激做出反应。人脑由大约 900 亿个神经元组成，它们之间有大约 1 万亿个连接；人工神经网络方法试图使用互连的人工神经元(或节点)来建模问题，以解决机器学习问题。

众所周知，人工神经网络从生物神经元中获得灵感。我们将花一些时间了解生物神经元是如何工作的。细胞的树突通过生化过程接收输入信号，从而根据脉冲的相对重要性对其进行加权。当细胞体开始积累传入的信号时，就达到了一个阈值，在这个阈值下，细胞会激发，然后输出信号通过电化学过程沿着轴突传递。在轴突末端，电信号再次被处理为化学信号，传递给邻近的神经元，这些神经元将成为树突，传递给其他神经元。

类似的工作原理松散地用于构建人工神经网络，其中每个神经元都有一组输入，每个输入都被赋予特定的权重。神经元根据这些加权输入计算函数。线性神经元采用加权输入的线性组合，并对聚合和应用激活函数(sigmoid、tanh、relu 等)。详情如下图所示。

网络将输入的加权和馈入逻辑函数(在`sigmoid`函数的情况下)。逻辑函数根据设定的阈值返回一个介于 0 和 1 之间的值；例如，这里我们将阈值设置为 0.7。任何大于 0.7 的累积信号给出 1 的信号，反之亦然；任何小于 0.7 的累积信号都会返回 0:

![](assets/e725247b-be9d-4604-af2f-4b56e83885a9.png)

Neural network models are being considered as universal approximators, which means by using a neural network methodology, we can solve any type of problems with the fine-tuned architecture. Hence, studying neural networks is a branch of study and special care is needed. In fact, deep learning is a branch of machine learning, where every problem is being modeled with artificial neural networks.

具有 n 个输入树突的典型人工神经元可以由以下公式表示。 *w* 权重允许 *x* 的每个 *n* 输入对输入信号的总和贡献或多或少的量。累加值被传递到激活函数 *f(x)* ，结果信号 *y(x)* 是输出轴:

![](assets/034aac78-91a7-4fb3-8220-b7878f647e5a.jpg)

选择构建神经网络所需的参数如下:

*   **激活函数:**选择激活函数在将信号聚集成输出信号以传播到网络的其他神经元中起着主要作用。
*   **网络架构或拓扑:**这代表所需的层数和每层的神经元数量。更多的层和神经元将创建高度非线性的决策边界，而如果我们减少架构，模型将更不灵活，更健壮。
*   **训练优化算法:**优化算法的选择也起着至关重要的作用，以便快速准确地收敛到最佳的最优解，我们将在本章后面的章节中详细介绍。
*   **神经网络的应用:**近年来，神经网络(深度学习的一个分支)在人工智能的应用方面获得了巨大的关注，在语音、文本、视觉等许多领域都是如此。我们将在本章后面的部分介绍深度学习。一些著名的应用如下:
    *   **图像和视频:**识别图像中的物体或分类它是狗还是猫
    *   **文本处理(NLP):** 基于深度学习的聊天机器人等等
    *   **语音:**识别语音
    *   **结构化数据处理:**构建功能强大的模型，以获得非线性决策边界

# 激活功能

激活功能是人工神经元处理信息并通过网络传递信息的机制。激活函数取一个数字，并对其执行某种固定的数学函数映射。有许多不同类型的激活函数。最受欢迎的有:

*   乙状结肠的
*   双曲正切
*   Relu
*   线性的

**Sigmoid 函数:** Sigmoid 的数学形式为*σ(x)= 1/(1+e-x)*。它接受一个实数值，并将其压缩到 0 到 1 之间的范围内。Sigmoid 是一个流行的选择，这使得计算导数变得容易，并且易于解释。

**Tanh 函数:** Tanh 将实数挤压到 *[-1，1]* 的范围内。输出以零为中心。实际上，tanh 非线性总是优于 sigmoid 非线性。此外，还可以证明 tanh 是缩放的 sigmoid 神经元*tanh(x)= 2σ(2x)1*。

**整流线性单元(ReLU)功能:**最近几年 ReLU 变得非常流行。它计算函数 *f(x) = max (0，x)* 。激活只是零阈值。

**线性函数:**线性激活函数用于线性回归问题，由于使用的函数为 *f(x) = x* ，因此它总是提供导数为 1。

**Relu** 由于其更好的收敛特性，现在被广泛用于代替 **Sigmoid** 或 **Tanh** 。

下图描述了所有激活功能。线性激活函数用于线性回归情况，而所有其他激活函数用于分类问题:

![](assets/05c34345-1438-480c-b4b7-2ad721933a71.png)

# 正向传播和反向传播

在下面的例子中，前向传播和反向传播用两个隐藏层深度神经网络来说明，其中除了输入和输出层之外，两个层每个都有三个神经元。输入层中神经元的数量基于 x(独立)变量的数量，而输出层中神经元的数量由模型需要预测的类的数量决定。

为了方便起见，我们只显示了每层中的一个神经元；然而，读者可以尝试在同一层内创建其他神经元。权重和偏差是从一些随机数开始的，因此在向前和向后传递中，这些都可以更新，以便将误差最小化。

在前向传播过程中，要素被输入到网络，并通过后续层进行馈送，以产生输出激活。如果我们在隐藏层 1 中看到，获得的激活是偏置权重 1 和输入值加权组合的组合；如果总值超过阈值，它将触发到下一层，否则信号对于下一层值将是 0。偏差值是控制触发点所必需的。在某些情况下，加权组合信号较低；在这些情况下，偏差将补偿调整聚集值的额外金额，这可以触发下一个级别。完整的等式可以在下图中看到:

![](assets/4fe4e519-6ede-487c-9188-c2f29e495265.png)

一旦在*隐藏层 1* ( **隐藏层 1** 、**隐藏层 2** 和**隐藏层 3** 神经元)中计算了所有神经元，则需要以类似的方式从第一层的隐藏神经元的输出中计算下一层神经元，并加上偏差(偏差权重 4)。下图描述了*层 2* 中显示的隐藏神经元 4:

![](assets/801c3b0d-4d97-4231-90b3-3a23c3e50084.png)

在最后一层(也称为输出层)，通过采用权重和从隐藏层 2 获得的输出的加权组合，以相同的方式从从隐藏层 2 获得的输出计算输出。一旦我们从模型中获得输出，就需要与实际值进行比较，并且我们需要在整个网络中反向传播误差，以便校正整个神经网络的权重:

![](assets/4c4afc95-7422-4788-a421-28444110d421.png)

在下图中，我们取了输出值的导数，然后乘以该值，得到误差分量，该误差分量是通过将实际值与模型输出求差得到的:

![](assets/75f2fc89-195d-4980-ae7c-d6f15fa723b1.png)

以类似的方式，我们也将从第二个隐藏层反向传播错误。在下图中，从第二个隐藏层中的隐藏 4 神经元计算误差:

![](assets/a000990e-0fed-4f11-ba6c-ced0ee9ff7b8.png)

在下图中，基于从第 2 层中的所有神经元获得的误差，为第 1 层中的隐藏 1 神经元计算误差:

![](assets/06b0590e-3750-4dc5-a352-8f44388eba82.png)

一旦隐藏层 1 中的所有神经元都被更新，输入和隐藏层之间的权重也需要更新，因为我们不能更新输入变量的任何内容。在下图中，我们将更新输入和隐藏层 1 中的神经元的权重，因为层 1 中的神经元仅利用输入的权重:

![](assets/8d3863d0-eca1-43f5-8eea-b96807bd5542.png)

最后，在下图中，第 2 层神经元正在正向传播过程中更新:

![](assets/c95f97d7-934a-42fb-ab8d-5f3aa22614ef.png)

我们还没有展示下一次迭代，在这次迭代中，输出层的神经元被错误更新，反向传播再次开始。以类似的方式，更新所有的权重，直到解决方案收敛或达到迭代次数。

# 神经网络优化

各种技术已被用于优化神经网络的权重:

*   **随机梯度下降** ( **SGD** )
*   动力
*   **内斯特罗夫加速梯度** ( **NAG** )
*   自适应梯度(阿达格勒)
*   Adadelta
*   RMSprop
*   **自适应矩估计** ( **亚当**)
*   **有限记忆布赖登-弗莱彻-戈德法布-尚诺** ( **L-BFGS** )

在实践中， **Adam** 是一个不错的默认选择；我们将在这一部分介绍它的工作方法。如果您负担不起完整的批量更新，那么试试 L-BFGS:

![](assets/7ab1f5bf-055a-4866-95e7-c9abe2d9a84b.png)

# 随机梯度下降

梯度下降是一种最小化目标函数 *J(* θ *)* 的方法，该目标函数由模型参数θ ε R <sup>d</sup> 参数化，方法是相对于参数在目标函数梯度的相反方向上更新参数。学习率决定了达到最小值所采取的步骤的大小:

*   批量梯度下降(每次迭代中使用的所有训练观察)
*   每次迭代一次观察
*   小批量梯度下降(每次迭代约 50 个训练观测值):

![](assets/c2f04560-876b-47e8-a194-c42e04351a37.png)

在下面的图像中，仔细观察了 2D 投影，其中比较了全批次和随机梯度下降随批次大小 *1* 的收敛特性。如果我们看到，由于考虑了所有的观察，整批更新更加平滑。然而，由于每次更新使用 *1* 观察的原因，SGD 具有摆动收敛特性:

![](assets/c3f8b670-3161-4102-b836-0c4956a572a7.png)

# 动力

SGD 很难在一个维度比另一个维度更陡峭的曲面曲线上导航；在这些场景中，SGD 在峡谷的斜坡上振荡，同时只能沿着底部犹豫不决地向局部最优方向前进。

当利用动量时，我们把球推下山。球滚下坡时积累动量，途中变得越来越快，直至停止(由于空气阻力等)；类似地，动量项对于梯度指向相同方向的维度增加，而对于梯度改变方向的维度减少更新。因此，我们获得了更快的收敛和更少的振荡:

![](assets/dffb5656-9053-4673-a623-eb735b62d79a.png)

# nestereov 加速梯度- nag

如果一个球滚下山坡，盲目地沿着一个斜坡，这是非常不令人满意的，它应该知道它要去哪里，这样它就知道在山坡再次向上倾斜之前要减速。NAG 是一种赋予动量项这种先见之明的方法。

动量首先计算当前梯度(蓝色小向量)，然后在更新的累积梯度方向(蓝色大向量)上进行大跳跃，而 NAG 首先在前一个累积梯度方向(棕色向量)上进行大跳跃，测量梯度，然后进行校正(绿色向量)。这种提前更新可以防止球跑得太快，从而提高响应速度和性能:

![](assets/018fe398-cc15-42e6-a45e-68bdbdf37fee.png)

# 阿达格勒

Adagrad 是一种基于梯度的优化算法，它使差分学习速率适应参数，对不频繁的参数执行较大的更新，对频繁的参数执行较小的更新。

Adagrad 大大提高了 SGD 的鲁棒性，并将其用于大规模神经网络的训练。Adagrad 的主要优势之一是它消除了手动调整学习速率的需要。大多数实现使用默认值 0.01，并保持该值。

Adagrad 的主要弱点是它在分母中的平方梯度的累积:由于每个相加的项都是正的，所以累积的总和在训练过程中不断增长。这反过来会导致学习速率缩小，最终变得非常小，此时算法不再能够获取额外的知识。以下算法旨在解决这一缺陷。

# Adadelta

Adadelta 是 Adagrad 的扩展，旨在降低其激进的单调递减的学习速率。不是累积所有过去的平方梯度，Adadelta 将累积的过去梯度的窗口限制为固定大小 *w* (不是低效存储 *w* 先前的平方梯度，梯度的总和被递归地定义为所有过去的平方梯度的衰减平均值)。

# RMSprop

**RMSprop** 和 **Adadelta** 都是在同一时间独立开发的，用于解决 Adagrad 的学习速率急剧下降的问题(RMSprop 还将学习速率除以平方梯度的指数衰减平均值)。

# 自适应矩估计-亚当

Adam 是另一种为每个参数计算自适应学习速率的方法。除了像 Adadelta 和 RMSprop 那样存储过去平方梯度的指数衰减平均值之外，Adam 还保持过去梯度的指数衰减平均值，类似于动量。

当你有疑问的时候，就用亚当！

# 有限记忆 broyden-Fletcher-goldfarb-shanno-L-BFGS 优化算法

L-BFGS 是 BFGS 的有限内存，它属于近似 BFGS 算法的拟牛顿方法家族，该算法利用有限的计算机内存。BFGS 目前被认为是最有效的，也是迄今为止最流行的，准牛顿更新公式。

L-BFGS 方法最好用下图来解释，其中迭代从随机点( *xt* )开始，在该点计算二阶导数或 hessian 矩阵，这是原始函数的二次近似；在计算二次函数后，它一步计算最小值，在计算函数值最小的新点( *xt+1* )后，较早的点将成为下一次迭代的起点。

在第二次迭代中，将在新的点( *xt+1* )和一步计算的另一个最小值( *xt+2* )处进行另一次二次近似。这样，L-BFGS 以更快的方式收敛到解，并且即使在非凸函数上也是有效的(在 R 代码中，我们使用了 nnet 包，其中 L-BFGS 被用于优化目的):

![](assets/c2ebf168-e43c-462e-b128-ed48c9394b6a.png)

# 神经网络中的缺失

丢弃是神经网络中的正则化，以避免数据的过拟合。典型地，初始层中的脱落率为 0.2(80%的神经元始终随机存在)，中间层中的脱落率为 0.5。理解辍学概念的一个直观方法是办公室团队，其中一些团队成员擅长与客户沟通，尽管他们不擅长技术细节，而一些成员擅长技术知识，但没有足够好的沟通技巧。假设有些成员离开办公室，然后其他成员试图代替其他人完成工作。这样，善于沟通的团队成员也会同样学到技术细节；其他一些擅长技术知识的团队成员也学习与客户沟通。这样，所有团队成员将变得足够独立和健壮，可以执行所有类型的工作，这对办公室有好处(假设团队经理会给办公室所有团队成员足够的假期！):

![](assets/f99148fa-3053-49a6-8a95-90861e6c0e77.png)

# 基于 scikit-learn 的人工神经网络分类器在手写数字识别中的应用

以 scikit-learn 数据集的手写数字为例说明了一个人工神经网络分类器示例，其中手写数字是从 0 到 9 创建的，它们各自的 64 个特征(8×8 矩阵)的像素强度在 0 到 255 之间，因为可以表示任何黑白(或灰度)图像。对于彩色图像，RGB(红色、绿色和蓝色)通道将用于表示所有颜色:

```py
# Neural Networks - Classifying hand-written digits  
>>> import pandas as pd 
>>> from sklearn.datasets import load_digits 
>>> from sklearn.cross_validation import train_test_split 
>>> from sklearn.pipeline import Pipeline 
>>> from sklearn.preprocessing import StandardScaler 

>>> from sklearn.neural_network import MLPClassifier 
>>> digits = load_digits() 
>>> X = digits.data 
>>> y = digits.target 

# Checking dimensions 
>>> print (X.shape) 
>>> print (y.shape) 

# Plotting first digit 
>>> import matplotlib.pyplot as plt  
>>> plt.matshow(digits.images[0])  
>>> plt.show() 

```

![](assets/f5b62c21-5187-4493-9999-9aa1108b2359.png)

```py
>>> from sklearn.model_selection import train_test_split 
>>> x_vars_stdscle = StandardScaler().fit_transform(X) 
>>> x_train,x_test,y_train,y_test = train_test_split(x_vars_stdscle,y,train_size = 0.7,random_state=42) 

# Grid Search - Neural Network  
>>> from sklearn.pipeline import Pipeline 
>>> from sklearn.model_selection import train_test_split,GridSearchCV 
>>> from sklearn.metrics import accuracy_score,classification_report 

```

MLP 分类器已经被用于第一和第二隐藏层中连续的 100 和 50 的隐藏层。亚当优化器已经被用于减少错误。所有学习率为 0.0001 的神经元都使用了 Relu 激活函数。最后，初始时迭代总数为 300:

```py
>>> pipeline = Pipeline([('mlp',MLPClassifier(hidden_layer_sizes= (100,50,), activation='relu',solver='adam',alpha=0.0001,max_iter=300 ))]) 

```

上述参数仅用于启动分类器，而下面的代码描述了使用管道函数的网格搜索。学习率和最大迭代次数用于寻找最佳组合。但是，我们鼓励读者添加其他功能，并在可以获得更好结果的地方进行测试:

```py
>>> parameters = {'mlp__alpha':(0.001,0.01,0.1,0.3,0.5,1.0), 'mlp__max_iter':(100,200,300)} 

```

使用了五重交叉验证的网格搜索，使用了默认的核心数量和评分作为准确性。尽管如此，您可以将其更改为 10 倍交叉验证等等，以了解准确性如何随着交叉验证指标的变化而变化:

```py
>>> grid_search_nn = GridSearchCV(pipeline,parameters,n_jobs=-1,cv=5,verbose=1, scoring='accuracy') 
>>> grid_search_nn.fit(x_train,y_train) 

>>> print ('\n\nNeural Network Best Training score: %0.3f' % grid_search_nn.best_score_) 
>>> print ('\nNeural Network Best parameters set:') 
best_parameters = grid_search_nn.best_estimator_.get_params() 
>>> for param_name in sorted(parameters.keys()): 
...     print ('\t%s: %r' % (param_name, best_parameters[param_name])) 

```

获得最大精度值的最佳参数为 96.3%，α为 0.001，最大迭代次数为 200:

![](assets/83b95738-07ee-4638-8c57-8d0fa727998f.png)

```py
>>> predictions_train = grid_search_nn.predict(x_train) 
>>> predictions_test = grid_search_nn.predict(x_test) 
>>> print ("\nNeural Network Training accuracy:",round(accuracy_score(y_train, predictions_train),4)) 
>>> print ("\nNeural Network Complete report of Training data\n",classification_report(y_train, predictions_train)) 
>>> print ("\n\nNeural Network Grid Search- Train Confusion Matrix\n\n",pd.crosstab(y_train, predictions_train,rownames = ["Actuall"],colnames = ["Predicted"]))       

```

![](assets/c0a89520-cfb2-4ab5-84a4-27d43bef7795.png)

```py
>>> print ("\n\nNeural Network Testing accuracy:",round(accuracy_score(y_test, predictions_test),4)) 
>>> print ("\nNeural Network Complete report of Testing data\n",classification_report( y_test, predictions_test)) 
>>> print ("\n\nNeural Network Grid Search- Test Confusion Matrix\n\n",pd.crosstab(y_test, predictions_test,rownames = ["Actuall"],colnames = ["Predicted"])) 

```

![](assets/2560aec7-8fe9-4e53-8862-4bc496fd7b2b.png)

上图显示，获得的最佳测试精度为 97.59%，这是以特别好的精度预测数字。

在下面的 R 代码中，使用了`nnet`包，其中 L-BFGS 被用作优化器。`nnet`包中的神经元数量有一个最大值为 13 的限制。因此，我们无法检查神经元数量超过 13 的结果:

人工神经网络分类器的 R 码示例:

```py
# Artificial Neural Networks   
setwd("D:\\Book   writing\\Codes\\Chapter 6")   
digits_data = read.csv("digitsdata.csv")   
remove_cols = c("target")   
x_data =   digits_data[,!(names(digits_data) %in% remove_cols)]   
y_data = digits_data[,c("target")]   
normalize <- function(x)   {return((x - min(x)) / (max(x) - min(x)))}   

data_norm <-   as.data.frame(lapply(x_data, normalize))   
data_norm <-   replace(data_norm, is.na(data_norm), 0.0)   
data_norm_v2 =   data.frame(as.factor(y_data),data_norm)   
names(data_norm_v2)[1] = "target"   

set.seed(123)   
numrow = nrow(data_norm_v2)   
trnind = sample(1:numrow,size =   as.integer(0.7*numrow))   
train_data =   data_norm_v2[trnind,]   
test_data = data_norm_v2[-trnind,]   
f <- as.formula(paste("target   ~",    
paste(names(train_data)[!names(train_data)   %in% "target"], collapse = " + ")))   

library(nnet)   
accuracy <-   function(mat){return(sum(diag(mat)) / sum(mat))}   

nnet_fit =   nnet(f,train_data,size=c(9),maxit=200)   
y_pred =   predict(nnet_fit,newdata = test_data,type = "class")   
tble =   table(test_data$target,y_pred)   
print(accuracy(tble))   

#Plotting nnet from the github   packages   
require(RCurl)   
root.url<-'https://gist.githubusercontent.com/fawda123'   
raw.fun<-paste(root.url,  '5086859/raw/cc1544804d5027d82b70e74b83b3941cd2184354/nnet_plot_fun.r',   
  sep='/')   
script<-getURL(raw.fun,   ssl.verifypeer = FALSE)   
eval(parse(text = script))   
rm('script','raw.fun')   
# Ploting the neural net   
plot(nnet_fit)   

# Grid Search - ANN   
neurons =   c(1,2,3,4,5,6,7,8,9,10,11,12,13)   
iters =   c(200,300,400,500,600,700,800,900)   

initacc = 0   
for(itr in iters){   
  for(nd in neurons){   
    nnet_fit =   nnet(f,train_data,size=c(nd),maxit=itr,trace=FALSE)   
    y_pred =   predict(nnet_fit,newdata = test_data,type = "class")   
    tble =   table(test_data$target,y_pred)   
    acc = accuracy(tble)   

    if (acc>initacc){   
      print(paste("Neurons",nd,"Iterations",itr,"Test   accuracy",acc))   
      initacc = acc   
    }   

  }   
} 

```

# 深度学习入门

深度学习是一类机器学习算法，它利用神经网络来建立模型，以解决结构化和非结构化数据集(如图像、视频、自然语言处理、语音处理等)上的监督和非监督问题:

![](assets/a72a537c-1422-45b0-a892-a48d1f2be7c1.png)

深度神经网络/深度体系结构由输入和输出层之间的多个隐藏单元层组成。每层都与后续层完全连接。一层中每个人工神经元的输出是下一层中每个人工神经元的输出输入:

![](assets/6a68ae88-30a5-4d16-9024-e352a539052b.png)

随着越来越多的隐藏层被添加到神经网络中，越来越复杂的决策边界被创建来分类不同的类别。在下图中可以看到复杂决策边界的示例:

![](assets/0c725e22-f258-4560-89c1-7ef3818afabf.png)

# 解决方法

反向传播用于通过计算输出单元处的网络误差来求解深层，并通过层反向传播以更新权重来减少误差项。

**设计深度神经网络的经验法则:**虽然设计神经网络没有硬性的规则，但以下规则将提供一些指导:

*   所有隐藏层每层都应该有相同数量的神经元
*   通常，两个隐藏层足以解决大多数问题
*   在每一层之后对所有输入变量使用缩放/批量归一化(均值 0，方差 1)提高了收敛效率
*   除了使用动量和损失之外，每次迭代后步长的减小改善了收敛性

# 深度学习软件

深度学习软件在最近几年已经发展了许多倍。在这一章中，我们使用 Keras 来开发一个模型，因为 Keras 模型对于新手来说很容易理解和原型化新概念。然而，许多其他软件也存在，并被世界各地的许多从业者使用:

*   **antao:**蒙特利尔大学开发的基于 Python 的深度学习库
*   **TensorFlow:** 谷歌的深度学习库运行在 Python/C++之上
*   **Keras / Lasagne:** 轻量级包装，位于 antao/TensorFlow 之上，支持更快的模型原型制作
*   **Torch:** 基于 Lua 的深度学习库，广泛支持机器学习算法
*   **Caffe:** 深度学习库主要用于图片处理

TensorFlow 最近在深度学习社区中获得了发展势头，因为它得到了谷歌的支持，并且使用 TensorBoard 具有良好的可视化功能:

![](assets/81f12d42-73f5-405f-82fa-23f2a38bae9b.png)

# 基于 Keras 的深度神经网络分类器在手写数字中的应用

我们使用与之前使用 scikit-learn 训练模型时相同的数据，以便在 scikit-learn 和深度学习软件 Keras 之间进行苹果对苹果的比较。因此，数据加载步骤保持不变:

```py
>>> import numpy as np
>> import pandas as pd
>>> import matplotlib.pyplot as plt
>>> from sklearn.datasets import load_digits
>>> from sklearn.model_selection import train_test_split
>>> from sklearn.preprocessing import StandardScaler
>>> from sklearn.metrics import accuracy_score,classification_report

```

从这里开始，我们将使用 Keras 库模块。尽管选择了各种优化器；我们将在我们的模型中使用`Adam`。我们也鼓励读者尝试其他优化方法:

```py
>>> from keras.models import Sequential
>>> from keras.layers.core import Dense, Dropout, Activation
>>> from keras.optimizers import Adadelta,Adam,RMSprop
>>> from keras.utils import np_utils

```

通过运行前面的代码，如果我们在一个中央处理器上运行 Keras，我们将获得以下消息:

![](assets/bb35e0f1-21eb-4a84-9689-f1d01110d0cc.png)

但是，如果我们在 GPU 上运行它，则会出现以下代码。事实上，我的个人电脑上安装了一个 GPU(型号: *NVIDIA GTX 1060* )，内存容量为 6 GB RAM。对于大多数应用程序来说，6 GB 对于初学者和爱好者来说已经足够了，而对于深度学习的研究人员来说，可能需要 12gb；当然取决于工作的性质:

![](assets/e77698cd-eb1c-435a-ad16-1a60802a7358.png)

In order to change the mode from CPU to GPU and vice versa from GPU to CPU, one needs to update the `Theano.rc` text file saved in the user folder. The following figure provides the various values to be configured if we are using GPU in the `Theano.rc` file. For CPU, only the `[global]` option is needed. Replace the device with CPU in it and delete the rest (`[nvcc]` and `[lib]`), as the latter is used for GPU settings only!

![](assets/a527391f-40f9-4fc7-a3f3-9809e737f543.png)

以下代码从 scikit-learn 数据集加载数字数据。快速检查数据形状的一段代码，因为数据本身嵌入在 numpy 数组中，因此我们不需要将其更改为任何其他格式，因为深度学习模型在 numpy 数组上接受训练:

```py
>>> digits = load_digits() 
>>> X = digits.data 
>>> y = digits.target 
>>> print (X.shape) 
>>> print (y.shape) 
>>> print ("\nPrinting first digit") 
>>> plt.matshow(digits.images[0])  
>>> plt.show()

```

前面的代码以矩阵形式打印第一位数字。下面的数字看起来像一个 *0* :

![](assets/2ad95c22-a5bc-4029-af74-a1f04dd9c213.png)

我们正在使用以下代码执行数据标准化，以贬低系列，然后使用标准差将所有 64 个维度放在一个相似的尺度中。虽然在这种情况下，它不是很严格，因为所有的值都在 0 到 255 之间，但是通过这样做，我们稍微降低了计算要求:

```py
>>> x_vars_stdscle = StandardScaler().fit_transform(X) 

```

代码的以下部分基于 70-30 的分割将数据分割为训练和测试:

```py
>>> x_train,x_test,y_train,y_test = train_test_split(x_vars_stdscle,y,train_size = 0.7,random_state=42) 

```

调整超参数在调整深度学习模型中起着至关重要的作用(当然，这也适用于任何机器学习模型！).我们用`nb_classes`作为 10，是因为位数的范围是 0-9；`batch_size`为 128，这意味着对于每个批次，我们利用 128 个观察值来更新权重；最后，我们使用`nb_epochs`作为 200，这意味着模型需要训练的纪元数量是 200(同样，我们可以想象模型从头到尾将被更新 200 次):

```py
# Defining hyper parameters 
>>> np.random.seed(1337)  
>>> nb_classes = 10 
>>> batch_size = 128 
>>> nb_epochs = 200 

```

下面的代码实际上基于`nb_classes`值为多类值创建了 n 维向量。在这里，我们将使用 softmax 分类器对所有训练观察进行训练，得到的维数为 10:

```py
>>> Y_train = np_utils.to_categorical(y_train, nb_classes) 

```

核心模型构建代码看起来像乐高积木，如下所示。在这里，我们将模型初始化为顺序的，而不是并行的，等等:

```py
#Deep Layer Model building in Keras 
>>> model = Sequential() 

```

在第一层，我们使用 100 个输入形状为 64 列的神经元(因为 X 中的列数为 64)，随后是 relu 激活函数，drop 值为 0.5(我们随机选择了 drop；鼓励读者尝试不同的价值观，看看结果如何变化):

```py
>>> model.add(Dense(100,input_shape= (64,))) 
>>> model.add(Activation('relu')) 
>>> model.add(Dropout(0.5))

```

在第二层，我们使用了 50 个神经元(为了比较使用 scikit-learn 方法获得的结果，我们使用了类似的体系结构):

```py
>>> model.add(Dense(50)) 
>>> model.add(Activation('relu')) 
>>> model.add(Dropout(0.5)) 

```

在输出层，类的数量需要与 softmax 分类器一起使用:

```py
>>> model.add(Dense(nb_classes)) 
>>> model.add(Activation('softmax')) 

```

这里我们用`categorical_crossentropy`编译，因为输出是多类；然而，如果我们想使用二进制类，我们需要使用`binary_crossentropy`来代替:

```py
>>> model.compile(loss='categorical_crossentropy', optimizer='adam')

```

该模型正在以下步骤中使用所有给定的批次大小和时期数量进行训练:

```py
#Model training
>>> model.fit(x_train, Y_train, batch_size=batch_size, nb_epoch=nb_epochs,verbose=1)

```

在这里，我们只是用损失值来表示时代的开始和结束阶段。据我们观察，在 200 次迭代中，损失值从 2.6925 降到了 0.0611:

![](assets/3c411067-6bff-4ceb-86fa-7b853ab57e8b.png)

```py
#Model Prediction
>>> y_train_predclass = model.predict_classes(x_train,batch_size=batch_size)
>>> y_test_predclass = model.predict_classes(x_test,batch_size=batch_size)
>>> print ("\n\nDeep Neural Network - Train accuracy:"),
(round(accuracy_score(y_train,y_train_predclass),3))
>>> print ("\nDeep Neural Network - Train Classification Report")
>>> print classification_report(y_train,y_train_predclass)
>>> print ("\nDeep Neural Network - Train Confusion Matrix\n")
>>> print (pd.crosstab(y_train,y_train_predclass,rownames = ["Actuall"],colnames = ["Predicted"]) )

```

![](assets/6fcc43eb-52cb-4ea0-a764-09072e55d98c.png)

根据之前的训练结果，我们对训练数据有 100%的准确率:

```py
>>> print ("\nDeep Neural Network - Test accuracy:"),(round(accuracy_score(y_test, y_test_predclass),3))
>>> print ("\nDeep Neural Network - Test Classification Report")
>>> print (classification_report(y_test,y_test_predclass))
>>> print ("\nDeep Neural Network - Test Confusion Matrix\n")
>>> print (pd.crosstab(y_test,y_test_predclass,rownames = ["Actuall"],colnames = ["Predicted"]) )

```

![](assets/55abd54a-90ac-4199-a18f-cc4915c55d87.png)

然而，真正的评估是对测试数据进行的。在这里，我们获得了 97.6%的准确率，这与 sci kit-learn 97.78%的准确率相似。因此，已经证明我们已经成功地在深度学习软件中复制了结果；然而，在 Keras 中，我们可以做比 scikit-learn 中好得多的事情(例如卷积神经网络、递归神经网络、自动编码器等等，这些在本质上都非常先进)。

# 摘要

在这一章中，你已经学习了计算量最大的方法，支持向量机和神经网络。支持向量机在维数非常高的数据上表现非常好，而其他方法在这种情况下无法工作。利用核函数，支持向量机可以达到很高的测试精度；在本章中，我们已经详细介绍了内核实际上是如何工作的。近年来，神经网络已经非常流行，用于解决各种问题；在这里，我们涵盖了使用 scikit-learn 和 Keras 构建神经网络模型所需的所有深度学习基础知识。此外，对 scikit-learn 和 Keras 模型的结果进行了比较，以显示苹果对苹果的比较。利用深度学习，可以解决许多新一代人工智能问题，无论是文本、语音、图像、视频等等。事实上，深度学习本身已经完全成为一个独立的领域。

在下一章中，我们将研究使用基于内容和协作过滤方法的推荐引擎，这是第一个向任何希望理解机器学习的新手解释的经典机器学习示例。