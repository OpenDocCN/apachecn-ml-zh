# 视频字幕应用

随着视频制作的速度以指数级的速度增长，视频已经成为一种重要的传播媒介。然而，由于缺乏适当的字幕，更多的观众仍然无法观看视频。

视频字幕是翻译视频以生成有意义的内容摘要的艺术，在计算机视觉和机器学习领域是一项具有挑战性的任务。传统的视频字幕方法并没有产生多少成功的故事。然而，随着最近人工智能在深度学习帮助下的发展，视频字幕最近获得了大量的关注。卷积神经网络和递归神经网络的能力使得构建端到端的企业级视频字幕系统成为可能。卷积神经网络对视频中的图像帧进行处理，提取重要特征，再由递归神经网络依次处理，生成有意义的视频摘要。视频字幕系统的一些重要应用如下:

*   工业工厂安全措施的自动监控
*   基于通过视频字幕获得的内容对视频进行聚类
*   银行、医院和其他公共场所更好的安全系统
*   在网站中搜索视频以获得更好的用户体验

通过深度学习构建智能视频字幕系统主要需要两种类型的数据:视频和文本字幕，它们是训练端到端系统的标签。

作为本章的一部分，我们将讨论以下内容:

*   讨论美国有线电视新闻网和 LSTM 在视频字幕中的作用
*   探索序列到序列视频字幕系统的体系结构
*   利用 s *序列到序列—视频到文本*架构构建视频字幕系统

在下一节中，我们将介绍卷积神经网络和 LSTM 版本的递归神经网络如何用于构建端到端视频字幕系统。

# 技术要求

你将需要具备 Python 3、TensorFlow、Keras 和 OpenCV 的基本知识。

本章代码文件可在 GitHub:
[https://GitHub . com/PacktPublishing/Intelligent-Projects-use-Python/tree/master/chapter 05](https://github.com/PacktPublishing/Intelligent-Projects-using-Python/tree/master/Chapter05)上找到

查看以下视频，查看正在运行的代码:
[http://bit.ly/2BeXK1c](http://bit.ly/2BeXK1c)

# 视频字幕中的 CNNs 和 LSTMs

视频减去音频可以被认为是以顺序方式排列的图像的集合。可以使用在特定图像分类问题上训练的卷积神经网络，例如 **ImageNet** ，从这些图像中提取重要特征。预训练网络的最后一个完全连接层的激活可以用于从视频的顺序采样图像中导出特征。从视频中顺序采样图像的频率取决于视频中内容的类型，并且可以通过训练进行优化。

下图所示(*图 5.1* )是用于从视频中提取特征的预训练神经网络:

![](assets/079370cc-0b47-4578-bf5e-3fceaaba687f.png)

Figure 5.1: Video image feature extraction using pre-trained neural networks

从上图中我们可以看到，视频中顺序采样的图像通过预先训练的卷积神经网络，最后一个完全连接层中 **`4,096`** 单元的激活作为输出。如果将时间 *t* 的视频图像表示为*x<sub>t</sub>T8】，最后一个全连接层的输出表示为*f<sub>t</sub>∈R<sup>4096</sup>*，则*f<sub>t =</sub>f<sub>w</sub>(x<sub>t</sub>)*<sub xmlns:epub="http://www.idpf.org/2007/ops">。</sub>这里， *W* 表示卷积神经网络直到最后一个完全连接层的权重。*

这些系列的输出特征为 *f <sub>1、</sub> f <sub>2、。。。。。</sub> f <sub>t。。</sub> f <sub>N</sub>* 可以作为输入馈送到递归神经网络，该神经网络学习基于输入特征生成文本标题，如下图所示(*图 5.2* ):

![](assets/c4301074-d0a6-499d-97fb-04b39f20d188.png)

Figure 5.2: LSTM in the processing of the sequential input features from CNN

从上图中我们可以看到，生成的特征 *f <sub>1、</sub> f <sub>2、。。。。。</sub> f <sub>t。。来自预训练卷积神经网络的</sub> f <sub>N</sub>* 由 LSTM 依次处理，以产生文本输出 *o <sub>1、</sub> o <sub>2、。。。。。</sub> o <sub>t。。</sub> o <sub>N、</sub>* 是给定视频的文字说明。例如，上图中视频的标题可能是*一名戴着黄色头盔的男子正在工作*:

*o1，o2，。。。。。ot。。。oN = {“A”，“人”“在”“A”“黄色”“头盔”“正在”“工作”}*

现在我们已经很好地了解了视频字幕在深度学习框架中的工作原理，接下来让我们在下一节讨论一个更高级的视频字幕网络，称为*序列到序列视频字幕*。在本章中，我们将使用相同的网络架构来构建*视频字幕系统。*

# 一种序列到序列视频字幕系统

序列到序列的体系结构基于一篇名为**序列到序列—视频到文本**的论文，作者是 Subhashini Venugopalan、Marcus Rohrbach、Jeff Donahue、Raymond Mooney、Trevor Darrell 和 Kate Saenko。论文可以在[https://arxiv.org/pdf/1505.00487.pdf](https://arxiv.org/pdf/1505.00487.pdf)找到。

在下图(*图 5.3* )中，说明了一个基于前一篇论文的*序列到序列视频字幕*神经网络架构:

![](assets/6dac330d-1814-4543-8312-a3b863240d00.png)

Figure 5.3: Sequence-to-sequence video-captioning network architecture

序列到序列模型像以前一样通过预先训练的卷积神经网络处理视频图像帧，最后一个完全连接的层的输出激活被作为特征馈送到随后的 LSTMs。如果我们在时间步长 *t* 将预训练卷积神经网络的最后一个完全连接层的输出激活表示为 *f <sub>t</sub> ∈ R <sup>4096</sup>* ，那么我们将具有来自视频的 *N* 图像帧的 *N* 这样的特征向量。这些 *N* 特征向量 *f1，f2，.。。。。f <sub>t</sub> 。。。f <sub>N</sub>* 依次馈入 LSTMs，生成文字字幕。

有两个背靠背的 LSTMs，LSTMs 中的序列数是来自视频的图像帧数和字幕词汇表中文本字幕的最大长度之和。如果网络在视频的 *N* 个图像帧上训练，并且词汇表中的最大文本字幕长度为 *M* ，则 LSTMs 在 *(N+M)* 个时间步长上训练。在 *N* 时间步骤中，第一 LSTM 处理特征向量 *f1、f2、.。。。。f <sub>t</sub> 。。。f <sub>N</sub>* 依次，其产生的隐藏状态馈入第二 LSTM。在这些 *N* 时间步长中，第二 LSTM 不需要文本输出目标。如果我们将第一个 LSTM 在时间步长 *t* 处的隐藏状态表示为 *h <sub>t</sub>* ，则第一个 *N* 时间步长对第二个 LSTM 的输入为 *h <sub>t</sub>* 。请注意，从 N+1 时间步长到第一个 LSTM 的输入是零填充的，因此输入对隐藏状态*h<sub>t</sub>T31】t>N*没有影响。请注意，这并不能保证 t > N 的隐藏状态*h<sub>t</sub>T37】总是会相同。其实我们可以选择将*h<sub>T</sub>T41】作为*h<sub>T</sub>T45】给第二个 LSTM 进行任意时间步长 *t > N.****

从 *(N+1)* 时间步开始，第二个 LSTM 需要一个文本输出目标。在任何时间步 *t > N* 输入的是*h<sub>t</sub>，w<sub>t-1</sub>*，其中*h<sub>t</sub>T13】是第一个 LSTM 在时间步*t**w<sub>t-1</sub>T19】是在时间步 *(t-1)的文字说明词***

在 *(N+1)* 时间步，输入到第二个 LSTM 的单词*w<sub>N</sub>T7】是由`<bos>`表示的句子的开始。一旦产生句尾符号`<eos>`，网络被训练停止产生字幕词。总而言之，这两个 LSTMs 的设置方式是，一旦处理完所有视频图像帧特征 *![](assets/8ad769e2-57c1-4581-98e1-7c7f70a12413.png)* ，它们就开始产生文本字幕词。*

处理时间步长 *t > N* 的第二个 LSTM 输入的另一种方法是只馈送*【w<sub>T-1</sub>*而不是*【h<sub>T</sub>，w<sub>T-1</sub>*，并在时间步长 *T* 即*【h<sub>T</sub>，c*传递第一个 LSTM 的隐藏和单元状态这样一个视频字幕网络的架构可以说明如下(见*图 5.4* ):

![](assets/f2805f4e-f512-4272-8988-4c3b2b73a400.png)

Figure 5.4: An alternate architecture for sequence to sequence model

预先训练的卷积神经网络通常具有共同的结构，例如`VGG16`、`VGG19`、`ResNet`，并且在 ImageNet 上预先训练。然而，我们可以基于从我们正在为其构建视频字幕系统的领域中的视频提取的图像来重新训练这些架构。我们还可以选择一个全新的 CNN 架构，并在特定领域的视频图像上对其进行训练。

到目前为止，我们已经涵盖了使用本节中说明的序列到序列架构开发视频字幕系统的所有技术先决条件。请注意，本节中建议的替代架构设计是为了鼓励读者尝试几种设计，看看哪种设计最适合给定的问题和数据集。

从下一部分开始，我们将致力于构建智能视频字幕系统。

# 视频字幕系统的数据

我们通过在`MSVD dataset`上训练模型来构建视频字幕系统，T0 是一个来自微软的字幕视频库。所需数据可从以下链接下载:[http://www . cs . utexas . edu/users/ml/clamp/video description/YouTube eclipse . tar](http://www.cs.utexas.edu/users/ml/clamp/videoDescription/YouTubeClips.tar)***。*** 视频的文字说明可通过以下链接获得:[https://github . com/jazzsaxsifa/video _ to _ sequence/files/387979/video _ corps . CSV . zip](https://github.com/jazzsaxmafia/video_to_sequence/files/387979/video_corpus.csv.zip)。

`MSVD dataset`里有大概`1,938`的视频。我们将使用这些来训练*序列到序列视频字幕系统*。还要注意的是，我们将在*图 5.3* 所示的序列对序列模型上构建模型。然而，建议读者尝试在*图 5.4* 中展示的架构上训练一个模型，并看看它的运行情况。

# 处理视频图像以创建 CNN 功能

一旦我们从指定位置下载了数据，下一个任务就是处理视频图像帧，从预先训练的卷积神经网络的最后一个完全连接的层中提取特征。我们使用在 ImageNet 上预先训练的`VGG16`卷积神经网络。我们将激活从`VGG16`的最后一个完全连接的层中取出。由于`VGG16`的最后一个完全连通层有`4096`个单位，我们的特征向量*f<sub>t</sub>T8】对于每个时间步长 *t* 是一个`4096`，维度向量即*f<sub>t</sub>∈R<sup>4096</sup>*。*

在通过`VGG16`处理视频中的图像之前，需要从视频中进行采样。我们从视频中取样图像，使得每个视频都有`80`帧。在处理来自`VGG16`的`80`图像帧之后，每个视频将具有`80`特征向量 *f1、f2、。。。。。f <sub>t</sub> 。。。f <sub>80</sub>* 。这些特征将被馈送到 LSTMs 以生成文本序列。我们在喀拉斯使用预先训练的`VGG16`模型。我们创建一个`VideoCaptioningPreProcessing`类，首先通过函数`video_to_frames`从每个视频中提取`80`视频帧作为图像，然后这些视频帧由函数`extract_feats_pretrained_cnn`中预先训练的`VGG16`卷积神经网络处理。

`extract_feats_pretrained_cnn`的输出是每个视频帧的维度`4096`的 CNN 特征。因为我们处理的是每个视频的`80`帧，所以每个视频都会有`80`这样的`4096`维度向量。

`video_to_frames`功能可以编码如下:

```py
    def video_to_frames(self,video):

        with open(os.devnull, "w") as ffmpeg_log:
            if os.path.exists(self.temp_dest):
                print(" cleanup: " + self.temp_dest + "/")
                shutil.rmtree(self.temp_dest)
            os.makedirs(self.temp_dest)
            video_to_frames_cmd = ["ffmpeg",'-y','-i', video, 
                                       '-vf', "scale=400:300", 
                                       '-qscale:v', "2", 
                                       '{0}/%06d.jpg'.format(self.temp_dest)]
            subprocess.call(video_to_frames_cmd,
                            stdout=ffmpeg_log, stderr=ffmpeg_log)

```

从前面的代码中，我们可以看到在`video_to_frames`功能中，`ffmpeg`工具用于将视频转换为 JPEG 格式的图像帧。为图像帧指定给`ffmpeg`的尺寸是`300 x 400`。关于`ffmpeg`工具的更多信息，请参考以下链接:[https://www.ffmpeg.org/](https://www.ffmpeg.org/)。

在`extract_feats_pretrained_cnnfunction`中已经建立了从最后一个完全连接的层中提取特征的预先训练的 CNN 模型。该函数的代码如下:

```py
# Extract the features from the pre-trained CNN 
    def extract_feats_pretrained_cnn(self):

        model = self.model_cnn_load()
        print('Model loaded')

        if not os.path.isdir(self.feat_dir):
            os.mkdir(self.feat_dir)
        #print("save video feats to %s" % (self.dir_feat))
        video_list = glob.glob(os.path.join(self.video_dest, '*.avi'))
        #print video_list 

        for video in tqdm(video_list):

            video_id = video.split("/")[-1].split(".")[0]
            print(f'Processing video {video}')

            #self.dest = 'cnn_feat' + '_' + video_id
            self.video_to_frames(video)

            image_list = 
            sorted(glob.glob(os.path.join(self.temp_dest, '*.jpg')))
            samples = np.round(np.linspace(
                0, len(image_list) - 1,self.frames_step))
            image_list = [image_list[int(sample)] for sample in samples]
            images = 
            np.zeros((len(image_list),self.img_dim,self.img_dim,
                     self.channels))
            for i in range(len(image_list)):
                img = self.load_image(image_list[i])
                images[i] = img
            images = np.array(images)
            fc_feats = model.predict(images,batch_size=self.batch_cnn)
            img_feats = np.array(fc_feats)
            outfile = os.path.join(self.feat_dir, video_id + '.npy')
            np.save(outfile, img_feats)
            # cleanup
            shutil.rmtree(self.temp_dest)
```

我们首先使用`model_cnn_load`函数加载预先训练好的 CNN 模型，然后根据指定给`ffmpeg.`的采样频率，使用`video_to_frames`函数为每个视频提取几个视频帧作为图像。我们没有处理通过`ffmpeg`创建的视频中的所有图像帧，而是使用`np.linspace`函数拍摄了`80`等间距的图像帧。使用`load_image`功能将`ffmpeg`生成的图像调整到`224 x 224`的空间维度。最后，这些调整大小的图像通过预先训练的 VGG16 卷积神经网络(CNN)，并且在输出层之前的最后一个完全连接层的输出被提取作为特征。这些提取的特征向量存储在`numpy`阵列中，并在下一阶段由 LSTM 网络处理以产生视频字幕。本节中定义的功能`model_cnn_load`定义如下:

```py
   def model_cnn_load(self):
         model = VGG16(weights = "imagenet", include_top=True,input_shape = 
                                  (self.img_dim,self.img_dim,self.channels))
         out = model.layers[-2].output
         model_final = Model(input=model.input,output=out)
         return model_final
```

从前面的代码可以看出，我们正在加载一个在 ImageNet 上预先训练的`VGG16`卷积神经网络，并且我们正在提取第二个最后一层(索引为`-2`)的输出作为我们的维度特征向量`4096`。

在输入美国有线电视新闻网之前，处理原始`ffmpeg`图像的图像读取和调整大小功能`load_image`定义如下:

```py
    def load_image(self,path):
        img = cv2.imread(path)
        img = cv2.resize(img,(self.img_dim,self.img_dim))
        return img 
```

预处理脚本可以通过调用以下命令来运行:

```py
 python VideoCaptioningPreProcessing.py process_main --video_dest '/media/santanu/9eb9b6dc-b380-486e-b4fd-c424a325b976/Video Captioning/data/' --feat_dir '/media/santanu/9eb9b6dc-b380-486e-b4fd-c424a325b976/Video Captioning/features/' --temp_dest '/media/santanu/9eb9b6dc-b380-486e-b4fd-c424a325b976/Video Captioning/temp/' --img_dim 224 --channels 3 --batch_size=128 --frames_step 80
```

该预处理步骤的输出是作为扩展的 numpy 数组对象`npy`写入的维度`4096`的`80`特征向量。每个视频都有自己的存储在`feat_dir`中的`numpy`数组对象。预处理步骤持续约 28 分钟，从日志中我们可以看到如下内容:

```py
Processing video /media/santanu/9eb9b6dc-b380-486e-b4fd-c424a325b976/Video Captioning/data/jmoT2we_rqo_0_5.avi
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋| 1967/1970 [27:57<00:02, 1.09it/s]Processing video /media/santanu/9eb9b6dc-b380-486e-b4fd-c424a325b976/Video Captioning/data/NKtfKR4GNjU_0_20.avi
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊| 1968/1970 [27:58<00:02, 1.11s/it]Processing video /media/santanu/9eb9b6dc-b380-486e-b4fd-c424a325b976/Video Captioning/data/4cgzdXlJksU_83_90.avi
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 1969/1970 [27:59<00:01, 1.08s/it]Processing video /media/santanu/9eb9b6dc-b380-486e-b4fd-c424a325b976/Video Captioning/data/0IDJG0q9j_k_1_24.avi
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1970/1970 [28:00<00:00, 1.06s/it]
28.045 min: VideoCaptioningPreProcessing
```

在下一节中，我们将讨论视频标签字幕的预处理。

# 处理视频的标记字幕

`corpus.csv`文件包含文字说明形式的视频描述(见*图 5.5* )。下面的截图显示了数据片段。我们可以删除一些`[VideoID,Start,End]`组合记录，并将其作为测试文件，供以后评估:

![](assets/a78d11a7-39d3-4e85-9bc8-768d81b7ee8c.png)

Figure 5.5: A snapshot of the format of the captions file

`VideoID`、`Start`、`End`列组合形成视频名称，格式如下:`VideoID_Start_End.avi`。基于视频名称，卷积神经网络`VGG16`的特征被存储为`VideoID_Start_End.npy`。在下面的代码块中说明了处理视频的文本标题和创建从`VGG16`到视频图像特征的路径交叉引用的功能:

```py
def get_clean_caption_data(self,text_path,feat_path):
        text_data = pd.read_csv(text_path, sep=',')
        text_data = text_data[text_data['Language'] == 'English']
        text_data['video_path'] =
        text_data.apply(lambda row: 
         row['VideoID']+'_'+str(int(row['Start']))+'_'+str(int(row['End']))+'.npy',    
         axis=1)
        text_data['video_path'] = 
        text_data['video_path'].map(lambda x: os.path.join(feat_path, x))
        text_data = 
        text_data[text_data['video_path'].map(lambda x: os.path.exists(x))]
        text_data = 
        text_data[text_data['Description'].map(lambda x: isinstance(x, str))]

        unique_filenames = sorted(text_data['video_path'].unique())
        data =
        text_data[text_data['video_path'].map(lambda x: x in unique_filenames)]
        return data
```

在定义的`get_data`函数中，我们从`video_corpus.csv`文件中移除所有非英文的标题。完成后，我们通过首先构建视频名称(作为`VideoID`、`Start`和`End`要素的串联)并为其添加要素目录名称前缀来形成到视频要素的链接。然后，我们删除所有不指向特征目录中任何实际视频特征向量或具有无效非文本描述的视频语料库文件记录。

数据如下图所示(*图 5.6* ):

![](assets/c08fc9df-4002-49ca-80f3-f9118e0055b1.png)

Figure 5.6: Caption data after preprocessing

# 构建训练和测试数据集

一旦我们训练好模型，我们就想评估模型的运行情况。我们可以根据测试集中的视频内容来验证为测试数据集生成的标题。可以使用以下功能创建列车测试集数据集。我们可以在训练期间创建测试数据集，并在模型训练完成后将其用于评估:

```py
   def train_test_split(self,data,test_frac=0.2):
        indices = np.arange(len(data))
        np.random.shuffle(indices)
        train_indices_rec = int((1 - test_frac)*len(data))
        indices_train = indices[:train_indices_rec]
        indices_test = indices[train_indices_rec:]
        data_train, data_test = 
        data.iloc[indices_train],data.iloc[indices_test]
        data_train.reset_index(inplace=True)
        data_test.reset_index(inplace=True)
        return data_train,data_test
```

一般保留 20%的数据用于评估是一种公平的做法。

# 构建模型

在本节中，将说明核心的模型构建练习。我们首先在文本标题的词汇表中为单词定义一个嵌入层，然后是两个 LSTMs。权重`self.encode_W`和`self.encode_b`用于从卷积神经网络中降低特征*f<sub>t</sub>T6】的维数。对于第二个 LSTM (LSTM 2)，任何时候的另一个输入步骤 *t > N* 是前一个单词 *w <sub>t-1</sub>* ，以及来自第一个 LSTM (LSTM 1)的输出 *h <sub>t</sub>* 。用于*w*<sub xmlns:epub="http://www.idpf.org/2007/ops">*t-1*</sub>的单词嵌入被馈送到 LSTM 2，而不是原始的一个热编码向量。对于第一个 N `(self.video_lstm_step)`，LSTM 1 处理来自 CNN 的输入特征 *f <sub>t</sub>* ，输出隐藏状态 *<sup>h <sub>t</sub></sup> (输出 1)* 馈给 LSTM 2。在这个编码阶段，LSTM 2 不接收任何字*w<sub>t-1</sub>T36】作为输入。**

从 *(N+1)* 时间步长，我们进入解码阶段，其中，与来自 LSTM 1 的*<sup>h<sub>t</sub>T7】(output 1)</sup>*一起，前一个时间步长字嵌入向量 *w <sub>t-1</sub>* 被馈送到 LSTM 2。在该阶段，没有输入到 LSTM 1，因为所有的特征 *f <sub>t</sub>* 在时间步骤 *N* 用尽。解码阶段的时间步数由`self.caption_lstm_step`决定。

现在，如果我们用一个函数 *f2、*来表示 LSTM 2 的活动，那么 *f <sub>2</sub> (h <sub>t、</sub>w<sub>t-1</sub>)= h<sub>2t</sub>*，其中 *h <sub>2t</sub>* 是 LSTM 2 在时间步长 *t* 的隐藏状态。通过 *softmax* 函数将时间 *t* 处的隐藏状态 *h <sub>2t</sub>* 转换为输出单词上的概率分布，并且具有最高概率的一个被选择作为下一个单词![](assets/a18e98b6-e400-4784-9452-c7ab8bc9a7cc.png):

![](assets/09b71b6c-5a27-49ae-965b-98e8810fbdb9.png) = ![](assets/9ae080a8-cf15-45f5-ad50-b98244404366.png)

![](assets/aa71833b-2882-4ace-abe6-d6dad3254469.png)

这些权重 *W <sub>ho</sub>* 和 *b、*在以下代码块中定义为`self.word_emb_W`和`self.word_emb_b`。更多详细信息，请参考`build_model`功能。为了便于解释，构建功能被分解为 3 个部分。构建模型有 3 个主要单元

*   **定义阶段**:定义变量、字幕词的嵌入层和序列到序列模型的两个 LSTMs。
*   **编码阶段**:在这个阶段我们通过 LSTM1 的时间步长传递视频帧图像特征，并将每个时间步长的隐藏状态传递到 LSTM 2 上。该活动一直进行到时间步长 *N* ，其中 *N* 是每个视频的采样视频帧图像的数量。
*   **解码阶段**:在解码阶段，LSTM 2 开始生成文本字幕。关于时间步长，解码阶段从步长 *N +1* 开始。从 LSTM 2 的每个时间步长生成的字连同 LSTM 1 的隐藏状态一起作为输入被馈送到下一个状态。

# 模型变量的定义

视频字幕模型的变量和其他相关定义可定义如下:

```py
 Defining the weights associated with the Network
        with tf.device('/cpu:0'):
            self.word_emb = 
            tf.Variable(tf.random_uniform([self.n_words, self.dim_hidden],
                        -0.1, 0.1), name='word_emb')

        self.lstm1 = 
        tf.nn.rnn_cell.BasicLSTMCell(self.dim_hidden, state_is_tuple=False)
        self.lstm2 = 
        tf.nn.rnn_cell.BasicLSTMCell(self.dim_hidden, state_is_tuple=False)
        self.encode_W = 
        tf.Variable( tf.random_uniform([self.dim_image,self.dim_hidden],
                    -0.1, 0.1), name='encode_W')
        self.encode_b = 
        tf.Variable( tf.zeros([self.dim_hidden]), name='encode_b')

        self.word_emb_W =
        tf.Variable(tf.random_uniform([self.dim_hidden,self.n_words], 
        -0.1,0.1), name='word_emb_W')
        self.word_emb_b = 
        tf.Variable(tf.zeros([self.n_words]), name='word_emb_b')

        # Placeholders 
        video = 
       tf.placeholder(tf.float32, [self.batch_size, 
       self.video_lstm_step, self.dim_image])
        video_mask = 
        tf.placeholder(tf.float32, [self.batch_size, self.video_lstm_step])

        caption = 
        tf.placeholder(tf.int32, [self.batch_size, self.caption_lstm_step+1])
        caption_mask = 
        tf.placeholder(tf.float32, [self.batch_size, self.caption_lstm_step+1])

        video_flat = tf.reshape(video, [-1, self.dim_image])
        image_emb = tf.nn.xw_plus_b( video_flat, self.encode_W,self.encode_b )
        image_emb = 
        tf.reshape(image_emb, [self.batch_size, self.lstm_steps, self.dim_hidden])

        state1 = tf.zeros([self.batch_size, self.lstm1.state_size])
        state2 = tf.zeros([self.batch_size, self.lstm2.state_size])
        padding = tf.zeros([self.batch_size, self.dim_hidden])
```

所有相关变量以及占位符都是由前面的代码定义的。

# 编码阶段

在编码阶段，我们通过 LSTM 1 的时间步长顺序处理每个视频图像帧特征(来自 CNN 最后一层)。视频图像帧的尺寸为`4096.`在将那些高维视频帧特征向量馈送到 LSTM 1 之前，它们被缩小到较小的尺寸`512.`

LSTM 1 处理视频帧图像，并且在每个时间步骤将隐藏状态传递给 LSTM 2，并且该过程持续到时间步骤 *N* ( `self.video_lstm_step`)。编码器的代码如下:

```py
probs = []
        loss = 0.0

        # Encoding Stage 
        for i in range(0, self.video_lstm_step):
            if i > 0:
                tf.get_variable_scope().reuse_variables()

            with tf.variable_scope("LSTM1"):
                output1, state1 = self.lstm1(image_emb[:,i,:], state1)

            with tf.variable_scope("LSTM2"):
                output2, state2 = self.lstm2(tf.concat([padding, output1],1), state2)

```

# 解码阶段

在解码阶段，产生视频字幕的字。LSTM 1 号没有其他输入。然而，LSTM 1 向前滚动，并且产生的隐藏状态被馈送到 LSTM 2 时间步长，如前所述。每一步 LSTM 2 的另一个输入是字幕中前一个单词的嵌入向量。因此，在每一步，LSTM 2 产生一个新的字幕词，其条件是在先前时间步中预测的词以及在那个时间步来自 LSTM 1 的隐藏状态。解码器的代码如下:

```py
# Decoding Stage to generate Captions 
        for i in range(0, self.caption_lstm_step):

            with tf.device("/cpu:0"):
                current_embed = tf.nn.embedding_lookup(self.word_emb, caption[:, i])

            tf.get_variable_scope().reuse_variables()

            with tf.variable_scope("LSTM1"):
                output1, state1 = self.lstm1(padding, state1)

            with tf.variable_scope("LSTM2"):
                output2, state2 = 
                 self.lstm2(tf.concat([current_embed, output1],1), state2)
```

# 为每个小批量建立损失

优化的损失是关于在 LSTM 2 的每个时间步长从整个字幕词语料库中预测正确词的分类交叉熵损失。对于批次中的所有数据点，在解码阶段的每个步骤中累积相同的数据。解码阶段与损失累积相关的代码如下:

```py
            labels = tf.expand_dims(caption[:, i+1], 1)
            indices = tf.expand_dims(tf.range(0, self.batch_size, 1), 1)
            concated = tf.concat([indices, labels],1)
            onehot_labels = 
            tf.sparse_to_dense(concated, tf.stack
                              ([self.batch_size,self.n_words]), 1.0, 0.0)

            logit_words = 
            tf.nn.xw_plus_b(output2, self.word_emb_W, self.word_emb_b)
        # Computing the loss 
            cross_entropy =   
            tf.nn.softmax_cross_entropy_with_logits(logits=logit_words,
            labels=onehot_labels)
            cross_entropy = 
            cross_entropy * caption_mask[:,i]
            probs.append(logit_words)

            current_loss = tf.reduce_sum(cross_entropy)/self.batch_size
            loss = loss + current_loss
```

损耗可以通过任何合理的梯度下降优化器进行优化，例如 Adam、RMSprop 等。我们将选择`Adam`进行实验，因为它在大多数深度学习优化中表现良好。我们可以使用 Adam 优化器定义训练操作，如下所示:

```py
with tf.variable_scope(tf.get_variable_scope(),reuse=tf.AUTO_REUSE):
    train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(loss) 
```

# 为标题创建词汇

在本节中，我们为视频标题创建词汇。我们创建了一些额外的单词，如下所示:

```py
eos => End of Sentence
bos => Beginning of Sentence 
pad => When there is no word to feed,required by the LSTM 2 in the initial N time steps
unk => A substitute for a word that is not included in the vocabulary

```

LSTM 2，其中一个字是一个输入，将需要这四个额外的符号。对于 *(N+1)* 时间步长，当我们开始生成字幕时，我们馈入上一个时间步长 *w <sub>t-1</sub>* 的单词。对于要生成的第一个单词，没有有效的前一时间步单词，因此我们输入虚拟单词`<bos>`，表示句子的开始。同样，当我们到达最后一个时间步时， *w <sub>t-1</sub>* 是字幕的最后一个字。我们训练模型输出最后一个单词`<eos>`，表示句子的结尾。当遇到句尾时，LSTM 2 停止发出任何进一步的单词。

用一个例子来说明，就拿*天气好*这句话来说吧。以下是从时间步长 *(N+1)* 开始的 LSTM 2 的输入和输出标签:

| **时间步长** | **输入** | **输出** |
| *N+1* | `<bos>`、*h<sub>N+1</sub>T4】* | 这 |
| *N+2* | 第、*h<sub>N+2</sub>T3】* | 天气 |
| *N+3* | 天气，*h<sub>N+3</sub>T3】* | 是 |
| *N+4* | 是，*h<sub>N+4</sub>T3】* | 美丽的 |
| *N+5* | 漂亮，*h<sub>N+5</sub>T3】* | `<eos>` |

创建词汇的`create_word_dict`功能详细说明如下:

```py
    def create_word_dict(self,sentence_iterator, word_count_threshold=5):

        word_counts = {}
        sent_cnt = 0

        for sent in sentence_iterator:
            sent_cnt += 1
            for w in sent.lower().split(' '):
               word_counts[w] = word_counts.get(w, 0) + 1
        vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]

        idx2word = {}
        idx2word[0] = '<pad>'
        idx2word[1] = '<bos>'
        idx2word[2] = '<eos>'
        idx2word[3] = '<unk>'

        word2idx = {}
        word2idx['<pad>'] = 0
        word2idx['<bos>'] = 1
        word2idx['<eos>'] = 2
        word2idx['<unk>'] = 3

        for idx, w in enumerate(vocab):
            word2idx[w] = idx+4
            idx2word[idx+4] = w

        word_counts['<pad>'] = sent_cnt
        word_counts['<bos>'] = sent_cnt
        word_counts['<eos>'] = sent_cnt
        word_counts['<unk>'] = sent_cnt

        return word2idx,idx2word

```

# 训练模型

在这一节中，我们将所有部分放在一起，构建训练视频字幕模型的功能。

首先，我们创建单词词汇词典，结合来自训练和测试数据集的视频字幕。完成后，我们调用`build_model`功能来创建视频字幕网络，将两个 LSTMs 结合起来。对于每个有特定*开始*和*结束的视频，*有多个输出视频字幕。在每一批中，从可用的多个视频字幕中随机选择具有特定*开始*和结束的视频的输出视频字幕。LSTM 2 的输入文本标题被调整为在时间步长 *(N+1)* 具有作为`<bos>`的起始单词，而输出文本标题的结束单词被调整为具有作为`<eos>`的最终文本标签。每个时间步长上分类交叉熵损失的总和被作为特定视频的总交叉熵损失。在每个时间步骤中，我们计算整个单词词汇表的分类交叉熵损失，可以表示如下:

![](assets/21a19e2b-fef3-4156-b1b2-8812e5a6b329.png)

这里，![](assets/6642a024-56d0-41c5-88a4-2f2bfe5820a2.png)是在时间步长 *t* 时实际目标词的一个热编码向量，![](assets/3463c520-4df0-40bc-902b-27d871ed8803.png)是来自模型的预测概率向量。

在训练期间捕获每个时期的损失，以了解损失减少的本质。这里需要注意的另一件重要的事情是，我们正在使用张量流的`tf.train.saver`函数保存训练好的模型，以便我们可以恢复模型来进行推理。

`train`功能的详细代码如下图，供参考:

```py
     def train(self):
        data = self.get_data(self.train_text_path,self.train_feat_path)
        self.train_data,self.test_data = self.train_test_split(data,test_frac=0.2)
        self.train_data.to_csv(f'{self.path_prj}/train.csv',index=False)
        self.test_data.to_csv(f'{self.path_prj}/test.csv',index=False)

        print(f'Processed train file written to {self.path_prj}/train_corpus.csv')
        print(f'Processed test file written to {self.path_prj}/test_corpus.csv')

        train_captions = self.train_data['Description'].values
        test_captions = self.test_data['Description'].values

        captions_list = list(train_captions) 
        captions = np.asarray(captions_list, dtype=np.object)

        captions = list(map(lambda x: x.replace('.', ''), captions))
        captions = list(map(lambda x: x.replace(',', ''), captions))
        captions = list(map(lambda x: x.replace('"', ''), captions))
        captions = list(map(lambda x: x.replace('\n', ''), captions))
        captions = list(map(lambda x: x.replace('?', ''), captions))
        captions = list(map(lambda x: x.replace('!', ''), captions))
        captions = list(map(lambda x: x.replace('\\', ''), captions))
        captions = list(map(lambda x: x.replace('/', ''), captions))

        self.word2idx,self.idx2word = self.create_word_dict(captions, 
                                      word_count_threshold=0)

        np.save(self.path_prj/ "word2idx",self.word2idx)
        np.save(self.path_prj/ "idx2word" ,self.idx2word)
        self.n_words = len(self.word2idx)

        tf_loss, tf_video,tf_video_mask,tf_caption,tf_caption_mask, tf_probs,train_op= 
        self.build_model()
        sess = tf.InteractiveSession()

        saver = tf.train.Saver(max_to_keep=100, write_version=1)
        tf.global_variables_initializer().run()

        loss_out = open('loss.txt', 'w')
        val_loss = []

        for epoch in range(0,self.epochs):
            val_loss_epoch = []

            index = np.arange(len(self.train_data))

            self.train_data.reset_index()
            np.random.shuffle(index)
            self.train_data = self.train_data.loc[index]

            current_train_data = 
            self.train_data.groupby(['video_path']).first().reset_index()

            for start, end in zip(
                    range(0, len(current_train_data),self.batch_size),
                    range(self.batch_size,len(current_train_data),self.batch_size)):

                start_time = time.time()

                current_batch = current_train_data[start:end]
                current_videos = current_batch['video_path'].values

                current_feats = np.zeros((self.batch_size, 
                                self.video_lstm_step,self.dim_image))
                current_feats_vals = list(map(lambda vid: np.load(vid),current_videos))
                current_feats_vals = np.array(current_feats_vals) 

                current_video_masks = np.zeros((self.batch_size,self.video_lstm_step))

                for ind,feat in enumerate(current_feats_vals):
                    current_feats[ind][:len(current_feats_vals[ind])] = feat
                    current_video_masks[ind][:len(current_feats_vals[ind])] = 1

                current_captions = current_batch['Description'].values
                current_captions = list(map(lambda x: '<bos> ' + x, current_captions))
                current_captions = list(map(lambda x: x.replace('.', ''), 
                                    current_captions))
                current_captions = list(map(lambda x: x.replace(',', ''), 
                                   current_captions))
                current_captions = list(map(lambda x: x.replace('"', ''), 
                                   current_captions))
                current_captions = list(map(lambda x: x.replace('\n', ''), 
                                   current_captions))
                current_captions = list(map(lambda x: x.replace('?', ''), 
                                   current_captions))
                current_captions = list(map(lambda x: x.replace('!', ''), 
                                   current_captions))
                current_captions = list(map(lambda x: x.replace('\\', ''), 
                                   current_captions))
                current_captions = list(map(lambda x: x.replace('/', ''), 
                                   current_captions))

                for idx, each_cap in enumerate(current_captions):
                    word = each_cap.lower().split(' ')
                    if len(word) < self.caption_lstm_step:
                        current_captions[idx] = current_captions[idx] + ' <eos>'
                    else:
                        new_word = ''
                        for i in range(self.caption_lstm_step-1):
                            new_word = new_word + word[i] + ' '
                        current_captions[idx] = new_word + '<eos>'

                current_caption_ind = []
                for cap in current_captions:
                    current_word_ind = []
                    for word in cap.lower().split(' '):
                        if word in self.word2idx:
                            current_word_ind.append(self.word2idx[word])
                        else:
                            current_word_ind.append(self.word2idx['<unk>'])
                    current_caption_ind.append(current_word_ind)

                current_caption_matrix = 
                sequence.pad_sequences(current_caption_ind, padding='post', 
                                       maxlen=self.caption_lstm_step)
                current_caption_matrix = 
                np.hstack( [current_caption_matrix, 
                          np.zeros([len(current_caption_matrix), 1] ) ] ).astype(int)
                current_caption_masks =
                np.zeros( (current_caption_matrix.shape[0], 
                           current_caption_matrix.shape[1]) )
                nonzeros = 
                np.array( list(map(lambda x: (x != 0).sum() + 1, 
                         current_caption_matrix ) ))

                for ind, row in enumerate(current_caption_masks):
                    row[:nonzeros[ind]] = 1

                probs_val = sess.run(tf_probs, feed_dict={
                    tf_video:current_feats,
                    tf_caption: current_caption_matrix
                    })

                _, loss_val = sess.run(
                        [train_op, tf_loss],
                        feed_dict={
                            tf_video: current_feats,
                            tf_video_mask : current_video_masks,
                            tf_caption: current_caption_matrix,
                            tf_caption_mask: current_caption_masks
                            })
                val_loss_epoch.append(loss_val)

                print('Batch starting index: ', start, " Epoch: ", epoch, " loss: ", 
                loss_val, ' Elapsed time: ', str((time.time() - start_time)))
                loss_out.write('epoch ' + str(epoch) + ' loss ' + str(loss_val) + '\n')

            # draw loss curve every epoch
            val_loss.append(np.mean(val_loss_epoch))
            plt_save_dir = self.path_prj / "loss_imgs"
            plt_save_img_name = str(epoch) + '.png'
            plt.plot(range(len(val_loss)),val_loss, color='g')
            plt.grid(True)
            plt.savefig(os.path.join(plt_save_dir, plt_save_img_name))

            if np.mod(epoch,9) == 0:
                print ("Epoch ", epoch, " is done. Saving the model ...")
                saver.save(sess, os.path.join(self.path_prj, 'model'), global_step=epoch)

        loss_out.close()

```

从前面的代码中我们可以看到，我们通过基于`batch_size.`随机选择一组视频来创建每个批次

对于每个视频，标签都是随机选择的，因为同一个视频已经由多个标记者进行了标记。对于每个选定的标题，我们清理标题文本，并将其中的单词转换为它们的单词索引。字幕的目标移动了 1 个时间步长，因为在每一步中，我们根据字幕中的前一个单词来预测单词。针对指定数量的时期训练模型，并且以指定的时期间隔(此处为`9`)检查模型。

# 培训结果

可以使用以下命令训练模型:

```py
python Video_seq2seq.py process_main --path_prj '/media/santanu/9eb9b6dc-b380-486e-b4fd-c424a325b976/Video Captioning/' --caption_file video_corpus.csv --feat_dir features --cnn_feat_dim 4096 --h_dim 512 --batch_size 32 --lstm_steps 80 --video_steps=80 --out_steps 20 --learning_rate 1e-4--epochs=100 
```

| **参数** | **值** |
| `Optimizer` | `Adam` |
| `learning rate` | `1e-4` |
| `Batch size` | `32` |
| `Epochs` | `100` |
| `cnn_feat_dim` | `4096` |
| `lstm_steps` | `80` |
| `out_steps` | `20` |
| `h_dim` | `512` |

培训的输出日志如下:

```py
Batch starting index: 1728 Epoch: 99 loss: 17.723186 Elapsed time: 0.21822428703308105
Batch starting index: 1760 Epoch: 99 loss: 19.556421 Elapsed time: 0.2106935977935791
Batch starting index: 1792 Epoch: 99 loss: 21.919321 Elapsed time: 0.2206578254699707
Batch starting index: 1824 Epoch: 99 loss: 15.057275 Elapsed time: 0.21275663375854492
Batch starting index: 1856 Epoch: 99 loss: 19.633915 Elapsed time: 0.21492290496826172
Batch starting index: 1888 Epoch: 99 loss: 13.986136 Elapsed time: 0.21542596817016602
Batch starting index: 1920 Epoch: 99 loss: 14.300303 Elapsed time: 0.21855640411376953
Epoch 99 is done. Saving the model ...
24.343 min: Video Captioning

```

正如我们所看到的，使用 GeForce Zotac 1070 GPU 在 100 个时代训练该模型需要大约 24 分钟。

每个时期的训练损失减少如下所示(*图 5.7* ):

![](assets/350a0f8a-c04a-409d-835e-4a0c1448b630.png)

Figure 5.7 Loss profile during training

从上图(*图 5.7* )可以看出，在最初的几个历元中，损耗降低幅度较高，然后在历元 **80** 前后逐渐降低。在下一节中，我们将说明该模型如何为看不见的视频生成字幕。

# 用看不见的测试视频推断

出于推理的目的，我们构建了一个生成器函数`build_generator`，复制`build_model`的逻辑来定义所有模型变量和加载模型并在其上运行推理所需的 TensorFlow 操作:

```py
    def build_generator(self):
        with tf.device('/cpu:0'):
            self.word_emb = 
            tf.Variable(tf.random_uniform([self.n_words, self.dim_hidden],
                        -0.1, 0.1), name='word_emb')

        self.lstm1 =
        tf.nn.rnn_cell.BasicLSTMCell(self.dim_hidden, state_is_tuple=False)
        self.lstm2 = 
        tf.nn.rnn_cell.BasicLSTMCell(self.dim_hidden, state_is_tuple=False)

        self.encode_W = 
        tf.Variable(tf.random_uniform([self.dim_image,self.dim_hidden], 
                    -0.1, 0.1), name='encode_W')
        self.encode_b = 
        tf.Variable(tf.zeros([self.dim_hidden]), name='encode_b')

        self.word_emb_W = 
        tf.Variable(tf.random_uniform([self.dim_hidden,self.n_words],
                     -0.1,0.1), name='word_emb_W')
        self.word_emb_b = 
         tf.Variable(tf.zeros([self.n_words]), name='word_emb_b')
        video = 
        tf.placeholder(tf.float32, [1, self.video_lstm_step, self.dim_image])
        video_mask = 
         tf.placeholder(tf.float32, [1, self.video_lstm_step])

        video_flat = tf.reshape(video, [-1, self.dim_image])
        image_emb = tf.nn.xw_plus_b(video_flat, self.encode_W, self.encode_b)
        image_emb = tf.reshape(image_emb, [1, self.video_lstm_step, self.dim_hidden])

        state1 = tf.zeros([1, self.lstm1.state_size])
        state2 = tf.zeros([1, self.lstm2.state_size])
        padding = tf.zeros([1, self.dim_hidden])

        generated_words = []

        probs = []
        embeds = []

        for i in range(0, self.video_lstm_step):
            if i > 0:
                tf.get_variable_scope().reuse_variables()

            with tf.variable_scope("LSTM1"):
                output1, state1 = self.lstm1(image_emb[:, i, :], state1)

            with tf.variable_scope("LSTM2"):
                output2, state2 = 
                self.lstm2(tf.concat([padding, output1],1), state2)

        for i in range(0, self.caption_lstm_step):
            tf.get_variable_scope().reuse_variables()

            if i == 0:
                with tf.device('/cpu:0'):
                    current_embed = 
                    tf.nn.embedding_lookup(self.word_emb, tf.ones([1], dtype=tf.int64))

            with tf.variable_scope("LSTM1"):
                output1, state1 = self.lstm1(padding, state1)

            with tf.variable_scope("LSTM2"):
                output2, state2 = 
                self.lstm2(tf.concat([current_embed, output1],1), state2)

            logit_words = 
            tf.nn.xw_plus_b( output2, self.word_emb_W, self.word_emb_b)
            max_prob_index = tf.argmax(logit_words, 1)[0]
            generated_words.append(max_prob_index)
            probs.append(logit_words)

            with tf.device("/cpu:0"):
                current_embed =
                tf.nn.embedding_lookup(self.word_emb, max_prob_index)
                current_embed = tf.expand_dims(current_embed, 0)

            embeds.append(current_embed)

        return video, video_mask, generated_words, probs, embeds
```

# 推理功能

在推理过程中，我们调用`build_generator`来定义模型和推理所需的其他张量流操作，然后我们使用`tf.train.Saver.restoreutility`从训练好的模型中加载保存的权重。一旦模型被加载并准备好为每个测试视频进行推断，我们提取其相应的视频帧图像预处理特征(来自 CNN)，并将其传递给模型进行推断:

```py
   def inference(self):
        self.test_data = self.get_test_data(self.test_text_path,self.test_feat_path)
        test_videos = self.test_data['video_path'].unique()

        self.idx2word = 
        pd.Series(np.load(self.path_prj / "idx2word.npy").tolist())

        self.n_words = len(self.idx2word)
        video_tf, video_mask_tf, caption_tf, probs_tf, last_embed_tf =           
        self.build_generator()

        sess = tf.InteractiveSession()

        saver = tf.train.Saver()
        saver.restore(sess,self.model_path)

        f = open(f'{self.path_prj}/video_captioning_results.txt', 'w')
        for idx, video_feat_path in enumerate(test_videos):
            video_feat = np.load(video_feat_path)[None,...]
            if video_feat.shape[1] == self.frame_step:
                video_mask = np.ones((video_feat.shape[0], video_feat.shape[1]))
            else:
                continue

            gen_word_idx = 
            sess.run(caption_tf, feed_dict={video_tf:video_feat, 
                     video_mask_tf:video_mask})
            gen_words = self.idx2word[gen_word_idx]

            punct = np.argmax(np.array(gen_words) == '<eos>') + 1
            gen_words = gen_words[:punct]

            gen_sent = ' '.join(gen_words)
            gen_sent = gen_sent.replace('<bos> ', '')
            gen_sent = gen_sent.replace(' <eos>', '')
            print(f'Video path {video_feat_path} : Generated Caption {gen_sent}')
            print(gen_sent,'\n')
            f.write(video_feat_path + '\n')
            f.write(gen_sent + '\n\n')

```

可以通过调用以下命令来运行推理:

```py
python Video_seq2seq.py process_main --path_prj '/media/santanu/9eb9b6dc-b380-486e-b4fd-c424a325b976/Video Captioning/' --caption_file '/media/santanu/9eb9b6dc-b380-486e-b4fd-c424a325b976/Video Captioning/test.csv' --feat_dir features --mode inference --model_path '/media/santanu/9eb9b6dc-b380-486e-b4fd-c424a325b976/Video Captioning/model-99'
```

# 评估结果

评估结果很有希望。测试集`0lh_UWF9ZP4_82_87.avi`和`8MVo7fje_oE_139_144.avi`的两个视频的推理结果如下:

在下面的截图中，我们举例说明了对视频`video0lh_` `UWF9ZP4_82_87.avi`的推断结果:

![](assets/654f2dd8-315e-4313-a3cd-5720be8ac811.png)

Inference on video 0lh_UWF9ZP4_82_87.avi using the trained model

在下面的截图中，我们说明了对另一个`video8MVo7fje_oE_139_144.avi`的推断结果:

![](assets/4b44212d-a2f4-4aa3-8db7-9f3147a53025.png)

Inference on a video/8MVo7fje_oE_139_144.avi using the trained model

从前面的截图*、*可以看出，训练好的模型很好的为提供的测试视频想出了一个好的标题。

这个项目的代码可以在 GitHub 位置[找到。`VideoCaptioningPreProcessing.py`模块可用于预处理视频并创建卷积神经网络特征，而`Video_seq2seq.py`模块可用于训练端到端视频字幕系统并对其进行推理。](https://github.com/PacktPublishing/Python-Artificial-Intelligence-Projects/tree/master/Chapter05)

# 摘要

现在，我们激动人心的视频字幕项目已经结束。您应该能够使用 TensorFlow 和 Keras 构建自己的视频字幕系统。您还应该能够使用本章中解释的技术诀窍来开发其他高级模型，包括卷积神经网络和递归神经网络。下一章将使用受限的玻尔兹曼机器构建一个智能推荐系统。期待您的参与！