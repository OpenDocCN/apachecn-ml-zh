# 二、聚类基础

在这一章中，我们将介绍聚类分析的基本概念，将注意力集中在我们的主要原则上，这些原则是许多算法共有的，也是可以用来评估方法性能的最重要的技术。

特别是，我们将讨论:

*   聚类和距离函数介绍
*   K-均值和 K-均值++
*   评估指标
*   **K-最近邻居** ( **KNN** )
*   **矢量量化** ( **VQ** )

# 技术要求

本章中的代码要求:

*   Python 3.5+(蟒蛇分布:[https://www.anaconda.com/distribution/](https://www.anaconda.com/distribution/)T2 强烈推荐)
*   库:
    *   SciPy 0.19+
    *   NumPy 1.10+
    *   学习 0.20+
    *   熊猫 0.22+
    *   Matplotlib 2.0+
    *   seaborn 0.9+

数据集可以通过 UCI 获得。该 CSV 文件可从[https://archive . ics . UCI . edu/ml/机器学习-数据库/乳腺癌-威斯康星/wdbc.data](https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data) 下载，除了添加加载阶段会出现的列名外，不需要任何预处理。

GitHub 存储库中提供了以下示例:

[https://github . com/PacktPublishing/HandsOn-Unsupervised-Learning-with-Python/chapter 02](https://github.com/PacktPublishing/HandsOn-Unsupervised-Learning-with-Python/tree/master/Chapter02)。

# 聚类介绍

正如我们在[第 1 章](01.html)、*无监督学习入门*中所解释的，聚类分析的主要目标是根据相似性度量或接近性标准对数据集的元素进行分组。在本章的第一部分，我们将重点关注前一种方法，而在第二部分和下一章，我们将分析利用数据集其他几何特征的更通用的方法。

我们来看一个数据生成过程 *p <sub class="calibre20">数据</sub> (x)* 并从中抽取 *N* 个样本:

![](img/165e5351-6d27-4a0d-be73-bf40c3d8df31.png)

可以假设 *p <sub class="calibre20">数据</sub> (x)* 的概率空间可划分为包含 *K* (对于 *K=1，2，...*)区域，以便 *p <sub class="calibre20">数据</sub>(x；k)* 表示样本属于一个聚类的概率 *k* 。这样，我们就说明了当 *p <sub class="calibre20">数据</sub> (x)* 确定时，每个可能的聚类结构都已经存在。可以对更接近 *p <sub class="calibre20">数据</sub> (x)* 的聚类概率分布做进一步的假设(正如我们将在[第 5 章](05.html)、*软聚类和高斯混合模型*中看到的)。然而，当我们试图将概率空间(和相应的样本)分成内聚组时，我们可以假设两种可能的策略:

*   **硬聚类**:在这种情况下，每个样本 *x <sub class="calibre20">p</sub> ∈ X* 被分配到一个聚类*k<sub class="calibre20">I</sub>T9】和*k<sub class="calibre20">I</sub>∨k<sub class="calibre20">j</sub>=∅*为 *i ≠ j* 。我们将要讨论的大多数算法都属于这一类。在这种情况下，问题可以表示为一个参数化函数，该函数为每个输入样本分配一个聚类:*

![](img/6776042f-6aa6-4679-ad10-88e5a63433bd.png)

*   **软聚类**:通常细分为**概率**和**模糊**聚类，这种方法确定了属于预定聚类的每个样本 *x <sub class="calibre20">p</sub>* *∈ X* 的概率 *p(x)* 。因此，如果有 *K* 簇，我们有一个概率向量*p(x)=【p<sub class="calibre20">1</sub>(x)，p <sub class="calibre20">2</sub> (x)、...，p<sub class="calibre20">k</sub>(x)】*，其中 *p <sub class="calibre20">i</sub> (x)* 代表被分配到集群 *i* 的概率。在这种情况下，聚类并不脱节，通常，样本将属于具有相当于概率的*隶属度*的所有聚类(这是模糊聚类特有的概念)。

出于我们的目的，在这一章中，我们简单地假设数据集 *X* 是从一个数据生成过程中绘制的，给定一个度量函数，该数据生成过程的空间可被分割成彼此分离的紧凑区域。事实上，我们的主要目标是找到满足**最大内聚**和**最大分离**双重属性的 *K* 团簇。在讨论 K-means 算法时，这个概念会更加清晰。然而，可以将集群想象成密度远远高于在分隔两个或多个集群的空间中可观察到的密度的斑点，如下图所示:

![](img/20693fb2-5b9c-4697-9ef8-71e1169f77c9.png)

Bidimensional clustering structure obeying the rule of maximum cohesion and maximum separation. N<sub class="calibre26">k</sub> represents the number of samples belonging to the cluster k while N<sub class="calibre26">out</sub>(r) is the number of samples that are outside the balls centered at each cluster center with a maximum radius r

在上图中，考虑到样本离中心的最大距离，我们假设大多数样本将被其中一个球捕获。然而，由于我们不想对球的生长施加任何限制(也就是说，它可以包含任何数量的样本)，因此最好不要考虑半径，而是通过对小的子区域(整个空间的子区域)进行采样并收集它们的密度来评估分离区域。

在理想情况下，集群跨越密度为 *D* 的一些子区域，而分离区域的特征是密度为 *d < < D* 。关于几何性质的讨论可能变得极其复杂，而且在许多情况下，这是极其理论化的。此后，我们只考虑属于不同簇的最近点之间的距离。如果该值远小于所有聚类的样本与其聚类中心之间的最大距离，我们可以确定分离是有效的，并且很容易区分聚类和分离区域。相反，当使用距离度量时(例如，在 K-means 中)，我们需要考虑的另一个重要要求是聚类的**凸性**。如果 *∀ x <sub class="calibre20">1</sub> ，x <sub class="calibre20">2</sub> ∈ C* ，并且属于连接*x<sub class="calibre20">1</sub>t17】和*x<sub class="calibre20">2</sub>t21】的线段的所有点都属于 *C* ，则通用集合 *C* 是凸的。在下图中，对凸簇和非凸(凹)簇进行了比较:**

![](img/948fc509-81ad-4ec3-9012-8d8867d41ab3.png)

Example of a convex cluster (left) and a concave one (right)

遗憾的是，由于距离函数的对称性，像 K-means 这样的算法无法管理非凸聚类。在我们的探索中，我们将展示这个限制以及其他方法如何克服它。

# 距离函数

即使聚类的一般定义通常基于**相似度**的概念，也很容易使用其逆，该逆由**距离函数**(相异度度量)表示。最常见的选择是**欧氏距离**，但是在选择它之前，需要考虑它的性质以及它们在高维空间中的行为。让我们首先引入**闵可夫斯基距离**作为欧几里得距离的推广。如果样品为*x<sub class="calibre20">I</sub>∈ℜ<sup class="calibre27">n</sup>t13】，则定义为:*

![](img/db15a1eb-e8cb-40c2-bbcc-222fe193e9bf.png)

对于 *p=1* ，我们得到**曼哈顿**(或**城市街区**)距离，而 *p=2* 对应标准欧氏距离。我们要理解 *d <sub class="calibre20">p</sub>* 在 *p → ∞* 时的行为。假设我们在二维空间中工作，有一个中心为 *x <sub class="calibre20">c</sub> =(0，0)* 的簇和一个采样点 *x=(5，3)* ，距离 *d <sub class="calibre20">p</sub> (x <sub class="calibre20">c</sub> ，x)* 相对于 *p* 的不同值为:

![](img/a7e8b29a-189f-44d6-b5b1-1f03e08ebd2d.png)

很明显(也很简单的证明)如果*| x<sub class="calibre20">1</sub><sup class="calibre27">j</sup>-x<sub class="calibre20">2</sub>T7】j|*是最大的分量绝对差，*p→∩*， *d <sub class="calibre20">p</sub> (x <sub class="calibre20">c</sub> ，x)→| x<sub class="calibre20">1</sub>T19】j-x 这意味着，如果我们将相似性(或不相似性)视为所有组件差异的结果，我们需要为 p 选择一个小值(例如， *p=1* 或 *2* )。另一方面，如果仅根据成分之间的最大绝对差异就必须认为两个样品不同，则 *p* 的较高值是合适的。一般来说，这种选择非常依赖于上下文，不能轻易概括。出于我们的目的，我们通常只考虑欧几里得距离，这在大多数情况下是合理的。另一方面，当 *N → ∞* 时，选择较大的 *p* 值有重要的后果。让我们从一个例子开始。对于 *p* 和 *N* 的不同值，我们要测量 *1N* 矢量(属于 *ℜ <sup class="calibre27">N</sup>* 的矢量，所有分量等于 *1* )与原点之间的距离(使用对数标尺压缩 *y* 轴)，这可以通过以下方式完成:*

```py
import numpy as np

from scipy.spatial.distance import cdist

distances = np.zeros(shape=(8, 100))

for i in range(1, distances.shape[0] + 1):
    for j in range(1, distances.shape[1] + 1):
        distances[i - 1, j - 1] = np.log(cdist(np.zeros(shape=(1, j)), np.ones(shape=(1, j)), 
                                               metric='minkowski', p=i)[0][0])
```

距离如下图所示:

![](img/31cf72fa-f8a9-4bd4-9a61-f9193e1096c3.png)

Minkowski distances (log-scale) for different values of p and N

第一个结果是，如果我们为 *N* 选取一个值，当 *p → ∞* 时，距离收缩并饱和。这是闵可夫斯基距离结构的正常结果，但有另一个因素，敏锐的读者可能已经注意到了。让我们想象一下将 *1N* 矢量的一个分量设置为 *0.0* 。这相当于从 *N* 维超立方体的一个顶点移动到另一个顶点。距离会怎样？嗯，用一个例子很容易证明，当 *p → ∞* 时，两个距离收敛到同一个值。特别是，阿格沃尔、欣内堡和凯米(在*关于高维空间中距离度量的惊人行为，阿格沃尔 C. C .，欣内堡 a .，凯米 D. A .，ICDT* 2001)证明了一个重要的结果。

假设我们有一个分布 *M* 二元样本*x<sub class="calibre20">I</sub>∑(0，1) <sup class="calibre27">d</sup>* 。如果我们使用闵可夫斯基度量，我们可以计算从 *p(x)* 采样的两个点与原点之间的最大(*D<sub class="calibre20">max</sub><sup class="calibre27">p</sup>*)和最小(*D<sub class="calibre20">min</sub><sup class="calibre27">p</sup>*)距离(一般来说，这个距离可以通过分析计算，但也可以使用迭代程序继续采样，直到 *D <sub class="calibre20">作者证明了以下不等式成立:</sub>*

![](img/251ae7ce-ea13-446d-a7ea-8c372fbc9cf0.png)

在之前的公式中，*C<sub class="calibre20">p</sub>T3】是一个依赖于 *p* 的常数。当 *p → ∞* 时，期望值的极限*E【D<sub class="calibre20">max</sub><sup class="calibre27">p</sup>-D<sub class="calibre20">min</sub><sup class="calibre27">p</sup>】*被捕获在边界*k<sub class="calibre20">1</sub>C<sub class="calibre20">p</sub>D<sup class="calibre27">1/p-1/2</sup>*和 *(M-1)之间作为术语*d<sup class="calibre27">1/p-1/2</sup><sub class="calibre20">→0</sub>*当 *p > 2* 和 *d → ∞* 时，最大和最小距离之差的期望值收敛到 *0* 。这意味着，独立于样本，当维度足够高并且 *p > 2* 时，使用闵可夫斯基距离几乎不可能区分两个样本。当我们在距离函数上寻找相似性时，这个定理警告我们当 *d > > 1* 时 *p* 选择大值。当 *d > > 1* (即使 *p=1* 将是最佳选择)时，欧几里德度量的常见选择也是相当可靠的，因为它对组件的重量影响最小(可以假设它们具有相同的重量)，并保证在高维空间中的可区分性。相反，高维空间中的 *p > > 2* 对于最大分量保持不变而所有其他分量都被修改的所有样本产生不可区分的距离(例如，如果 *x=(5，0) → (5，a)* 其中 *|* *a| < 5* ，如下例所示:**

```py
import numpy as np

from scipy.spatial.distance import cdist

distances = []

for i in range(1, 2500, 10):
    d = cdist(np.array([[0, 0]]), np.array([[5, float(i/500)]]), metric='minkowski', p=15)[0][0]
    distances.append(d)

print('Avg(distances) = {}'.format(np.mean(distances)))
print('Std(distances) = {}'.format(np.std(distances)))
```

输出如下:

```py
Avg(distances) = 5.0168687736484765
Std(distances) = 0.042885311128215066
```

因此，对于`p = 15`，所有样品 *(5，x)* (对于*x≈0.002，5.0)* 与原点的距离，其平均值约为`5.0`，标准偏差约为`0.04`。当`p`变大时，`Avg(distances) = 5.0`和`Std(distances) = 0.04`。

在这一点上，我们可以开始讨论最常见和广泛采用的聚类算法之一:K-means。

# k 均值

**K-means** 是最大分离最大内聚力原则的最简单实现。让我们假设我们有一个数据集*x∈ℜ<sup class="calibre27">m×n</sup>t5】(即 *M N* 维样本)，我们希望将其拆分为 *K* 个聚类和一组 *K* **质心**对应于分配给每个聚类的样本的平均值 *K <sub class="calibre20">j</sub>* :*

![](img/02e9a6e4-5321-44f8-b545-b0e6cd0bf18f.png)

集合 *M* 和质心有一个指示迭代步骤的附加索引(上标)。从最初的猜测 *M <sup class="calibre27">(0)</sup>* 开始，K-means 试图最小化称为**惯性**的目标函数(即，分配给一个聚类的样本之间的总平均聚类内距离 *K <sub class="calibre20">j</sub>* 及其质心 *μ <sub class="calibre20">j</sub>* ):

![](img/56a960f6-4a96-4e97-883e-17277d02904c.png)

很容易理解 *S(t)* 不能认为是绝对测度，因为它的值受样本方差的影响很大。然而， *S(t+1) < S(t)* 意味着质心正在向最佳位置移动，在该位置，分配给聚类的点与相应质心的距离尽可能最小。因此，迭代过程(也称为**劳氏算法**)从用随机值初始化 *M <sup class="calibre27">(0)</sup>* 开始。下一步是将每个样本 *x <sub class="calibre20">i</sub> ∈ X* 分配给质心距离 *x <sub class="calibre20">i</sub>* 最小的聚类:

![](img/79466b97-a3cf-45e3-9f64-df1346e7efb2.png)

一旦完成所有作业，新质心将作为算术平均值重新计算:

![](img/2654403d-f595-446f-a602-b22518cc792f.png)

重复该过程，直到质心停止变化(这也意味着序列 *S(0) >* *S(1) >...> S(t <sub class="calibre20">结束</sub> )* )。读者应该马上明白，计算时间受最初猜测的影响很大。如果 *M <sup class="calibre27">(0)</sup>* 非常接近 *M <sup class="calibre27">(t <sub class="calibre28">端</sub> )</sup>* ，几次迭代就能找到最优配置。反之，当 *M <sup class="calibre27">(0)</sup>* 纯随机时，低效初始选择的概率接近于 *1* (也就是说，每一个初始均匀随机选择在计算复杂度上几乎是等价的)。

# K-means++

找到最优初始构型相当于最小化惯性；然而，Arthur 和 Vassilvitskii(在 *K-means++:小心播种的优势，Arthur D，Vassilvitskii S，第十八届年度 ACM-SIAM 离散算法研讨会论文集，* 2007)提出了一种替代的初始化方法(称为 **K-means++** ，该方法可以通过选择更高概率接近最终质心的初始质心来显著提高收敛速度。完整的证明相当复杂，可以在前面提到的论文中找到。在这种情况下，我们直接提供最终结果和一些重要的结果。

让我们考虑函数*D()*定义为:

![](img/7a396ec7-a6d1-4776-8bfb-cb03028f76a7.png)

*D()*代表样品 *x ∈ X* 和已经选择的质心之间的最短距离。一旦计算出函数，就可以确定概率分布 *G(x)* ，如下所示:

![](img/c5c5330a-28ea-4610-b18e-e25a45a3d399.png)

第一个质心 *μ <sub class="calibre20">1</sub>* 是从均匀分布中采样的。此时，可以计算所有样品的*D()**X∈X*以及分布 *G(x)* 。很简单，如果我们从 *G(x)* 采样，在密集区域选择值的概率比均匀采样或在分离区域选取质心的概率大得多。因此，我们继续从 *G(x)* 中取样 *μ <sub class="calibre20">2</sub>* 。重复该过程，直到所有 *K* 质心都已确定。当然，由于这是一种概率方法，我们不能保证最终的配置是最佳的。但是，K-means++的使用是*O(log K)-竞争*。事实上，如果 *S <sub class="calibre20">opt</sub>* 是 *S* 的理论最优值，那么作者证明了以下不等式成立:

![](img/3ad9f4df-5b14-4473-8e60-6c6a94113d0d.png)

由于 *S* 减少了一个更好的选择，前面的公式为期望值*E【S】*设定了一个上限，大致与 *log K* 成正比。例如对于 *K=10* 、*E【S】![](img/2a4a8dc7-9511-42b7-9190-213e1d376b85.png)19.88![](img/8e1db6f7-d663-4345-a87e-7f79fab73475.png)S<sub class="calibre20">opt</sub>*和*E【S】![](img/2a4a8dc7-9511-42b7-9190-213e1d376b85.png)12.87![](img/8e1db6f7-d663-4345-a87e-7f79fab73475.png)S<sub class="calibre20">opt</sub>*对于 *K=3* 。这一结果揭示了两个重要因素。第一个是 *K* 不是特别大的时候 K-means++表现更好，第二个，可能也是最重要的一个，是单个 K-means++初始化不能足以获得最优配置。因此，常见的实现(例如 scikit-learn)会执行可变数量的初始化，并选择初始惯性最小的一个。

# 威斯康星乳腺癌数据集分析

在本章中，我们使用众所周知的**乳腺癌威斯康星数据集**进行聚类分析。最初，提出数据集是为了训练分类器；然而，它对于非平凡的聚类分析非常有帮助。它包含 569 条记录，由 32 个属性组成(包括诊断和识别号)。所有属性都与肿瘤的生物学和形态学属性严格相关，但我们的目标是考虑到基本事实(良性或恶性)和数据集的统计属性来验证一般假设。在继续之前，澄清一些要点很重要。数据集是高维的，聚类是非凸的(因此我们不能期望完美的分割)。此外，我们的目标不是使用聚类算法来获得分类器的结果；因此，基本事实只能作为潜在群体的一般标志来考虑。这样一个例子的目的是展示如何执行一个简短的初步分析，选择最佳数量的集群，并验证最终结果。

下载后(如技术要求部分所述)，CSV 文件必须放在我们一般称为`<data_folder>`的文件夹中。第一步是加载数据集，并通过熊猫`DataFrame`展示的函数`describe()`执行全局统计分析，如下所示:

```py
import numpy as np
import pandas as pd

bc_dataset_path = '<data_path>\wdbc.data'

bc_dataset_columns = ['id','diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean', 
 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 
 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',
 'radius_se','texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 
 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se', 
 'fractal_dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst', 
 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst',
 'concave points_worst', 'symmetry_worst', 'fractal_dimension_worst']

df = pd.read_csv(bc_dataset_path, index_col=0, names=bc_dataset_columns).fillna(0.0)
print(df.describe())
```

我强烈建议使用 Jupyter Notebook(在这种情况下，命令必须只有`df.describe()`)，其中所有的命令都会产生内联输出。出于实际原因，在下面的截图中，显示了表格输出的第一部分(包含八个属性):

![](img/d74fa1bc-0ff5-4cf0-b446-501b8ecb7340.png)

Statistical report of the first eight attributes of the dataset

当然，我邀请读者检查所有属性的值，即使我们只关注一个子集。特别是，我们需要观察前八个属性之间存在的不同尺度。标准偏差范围从 0.01 到 350，这意味着许多向量可能仅仅因为一个或两个属性而极其相似。另一方面，使用方差缩放对值进行归一化将赋予所有属性相同的责任(例如，`area_mean`在`143.5`和`2501`之间有界，而`smoothness_mean`在`0.05`和`0.16`之间有界)。迫使它们具有相同的方差会影响因子的生物学影响，并且由于我们没有任何具体的指示，我们没有*授权*做出这样的选择)。显然，一些属性在聚类过程中将具有更高的权重，我们接受它们的主要影响作为与上下文相关的条件。

现在我们从`perimeter_mean`、`area_mean`、`smoothness_mean`、`concavity_mean`、`symmetry_mean`的配对剧情开始初步分析。该图显示在下面的截图中:

![](img/547f2d24-2fa1-442a-9fae-48d8f2101df3.png)

Pair-plot of perimeter mean, area mean, smoothness mean, concavity mean, and symmetry mean

该图将每个非对角线属性绘制为所有其他属性的函数，而对角线图表示每个属性分成两个部分的分布(在这种情况下，这就是诊断)。因此，第二个非对角线图(左上角)是`perimeter_mean`作为`area_mean`的函数的图，以此类推。快速分析突出了一些有趣的元素:

*   `area_mean`和`perimeter_mean`有明显的相关性，决定了明显的分离。当`area_mean`大于约 1000 时，显然周长也增加，诊断从良性突然切换到恶性。因此，这两个属性是最终结果的决定因素，其中一个可能是多余的。
*   其他图(例如，`perimeter_mean` / `area_mean`对`smoothness_mean`、`area_mean`对`symmetry_mean`、`concavity_mean`对`smoothness_mean`、`concavity_mean`对`symmetry_mean`)具有水平间隔(将轴反转为垂直)。这意味着，对于自变量( *x* 轴)假设的几乎所有值，有一个阈值将另一个变量的值分为两组(良性和恶性)。
*   一些图(例如，`perimeter_mean` / `area_mean`对`concavity_mean` / `concavity_mean`对`symmetry_mean`)显示了稍微负的倾斜对角线间隔。这意味着，当自变量很小时，对于因变量的几乎所有值，诊断保持不变，而另一方面，当自变量变得越来越大时，诊断成比例地切换到相反的值。例如，对于小的`perimeter_mean`值，`concavity_mean`可以在不影响诊断的情况下达到其最大值，(这是良性的)，而`perimeter_mean > 150`总是独立于`concavity_mean`产生恶性诊断。

当然，我们不能轻易地从分割分析中得出结论(因为我们需要考虑所有的交互)，但是为了给每个聚类提供一个语义标签，这个活动将是有帮助的。在这一点上，通过**t-分布式随机邻域嵌入** ( **t-SNE** )变换在二维平面上可视化数据集(没有非结构属性)是有帮助的(更多细节，请查看*使用 t-SNE 可视化数据，范德马滕 l，辛顿 g，机器学习研究杂志 9，* 2008)。这可以通过以下方式实现:

```py
import pandas as pd

from sklearn.manifold import TSNE

cdf = df.drop(['diagnosis'], axis=1)

tsne = TSNE(n_components=2, perplexity=10, random_state=1000)
data_tsne = tsne.fit_transform(cdf)

df_tsne = pd.DataFrame(data_tsne, columns=['x', 'y'], index=cdf.index)
dff = pd.concat([df, df_tsne], axis=1)
```

下面的截图显示了结果图:

![](img/76f8af1e-fb26-4c7c-b513-3c22f2e7a99b.png)

Bidimensional t-SNE plot of the Breast Cancer Wisconsin dataset

图是高度非线性的(别忘了这是从 *ℜ <sup class="calibre27">30</sup>* 到 *ℜ <sup class="calibre27">2</sup>* 的投影)，但大部分恶性样本都在半平面 *y < 0* 内。不幸的是，也有中等百分比的良性样本在这个区域，因此我们不期望使用 *K=2* 的完美分离(在这种情况下，很难理解真实的几何形状，但是 t-SNE 保证二维分布与原始高维分布具有最小的库尔巴克-莱布勒散度)。现在让我们使用 *K=2* 执行初始聚类。我们将使用`n_clusters=2`和`max_iter=1000`创建`KMeans` scikit-learn 类的实例(只要有可能，`random_state`将始终设置为等于`1000`)。

其余参数为默认参数(K-means++初始化，10 次尝试)，如下所示:

```py
import pandas as pd

from sklearn.cluster import KMeans

km = KMeans(n_clusters=2, max_iter=1000, random_state=1000)
Y_pred = km.fit_predict(cdf)

df_km = pd.DataFrame(Y_pred, columns=['prediction'], index=cdf.index)
kmdff = pd.concat([dff, df_km], axis=1)
```

下面的截图显示了结果图:

![](img/95b0ef98-33df-43c6-9048-2646a247fc7e.png)

K-means clustering (with K=2) of the Breast Cancer Wisconsin dataset

不出意外的话，对于 *y < -20* 来说，结果还是比较准确的，但是算法并不能将边界点( *y ≈ 0* )也纳入到主要恶性聚类中。这主要是由于原始集合的非凸性，用 K-means 很难解决这个问题。而且在投影中，大部分带有 *y* *≈ 0* 的恶性样本与良性样本混在一起，因此基于接近度的其他方法误差概率也较高。正确分离这些样本的唯一机会来自原始分布。事实上，如果属于同一类别的点可以被 *ℜ <sup class="calibre27">30</sup>* 中不相交的球捕获，K-means 也可以成功。不幸的是，在这种情况下，混合集看起来非常有凝聚力，因此我们不能期望在没有转换的情况下提高性能。然而，出于我们的目的，该结果允许我们应用主要的评估指标，然后从 *K=2* 移动到更大的值。借助 *K > 2* ，我们将分析一些集群，将它们的结构与配对图进行比较。

# 评估指标

在本节中，我们将分析一些常用的方法，这些方法可以用来评估聚类算法的性能，并帮助找到最佳的聚类数。

# 最小化惯性

K-means 和类似算法的最大缺点之一是对聚类数量的明确要求。有时，这部分信息是由外部约束强加的(例如，在乳腺癌的例子中，只有两种可能的诊断)，但在许多情况下(当需要探索性分析时)，数据科学家必须检查不同的配置并对其进行评估。评估 K-means 性能和选择适当数量簇的最简单方法是基于不同最终惯性的比较。

让我们从下面这个简单的例子开始，它基于 12 个非常紧凑的高斯斑点，这些斑点是用 scikit-learn 函数`make_blobs()`生成的:

```py
from sklearn.datasets import make_blobs

X, Y = make_blobs(n_samples=2000, n_features=2, centers=12, 
                  cluster_std=0.05, center_box=[-5, 5], random_state=100)
```

这些斑点如下图所示:

![](img/322cf165-b1a9-4232-af3b-dd6df21c53a4.png)

Dataset made up of 12 disjoint bidimensional blobs

现在让我们计算 *K ∈ [2，20]，*的惯性(在训练的`KMeans`模型中作为实例变量`inertia_`可用)，如下所示:

```py
from sklearn.cluster import KMeans

inertias = []

for i in range(2, 21):
    km = KMeans(n_clusters=i, max_iter=1000, random_state=1000)
    km.fit(X)
    inertias.append(km.inertia_)
```

结果图如下:

![](img/ff8ae401-b30f-43e8-bb3d-51a4b94b3531.png)

Inertia as a function of the number of clusters

前面的情节显示了一种常见的行为。当团簇的数量很少时，密度成比例地低，因此内聚力低，因此惯性高。集群数量的增加迫使模型创建更具凝聚力的群体，惯性开始突然减小。如果我们继续这个过程和 *M > > K* ，我们将观察到一个非常缓慢的接近值，该值对应于一个配置，其中 *K=M* (每个样本是一个聚类)。一般的启发式规则(当没有外部约束时)是选择对应于将高变化区域与几乎平坦的区域分开的点的聚类数。通过这种方式，我们确信所有集群都达到了最大的凝聚力，没有内部分裂。当然，在这种情况下，如果我们选择 *K=15* ，九个斑点将被分配给不同的簇，而另外三个将被分成两部分。显然，当我们分裂一个高密度区域时，惯性仍然很低，但是最大分离的原则不再被遵循。

我们现在可以用威斯康星乳腺癌数据集重复实验，其 K ∈ [2，50] 、如下:

```py
from sklearn.cluster import KMeans

inertias = []

for i in range(2, 51):
    km = KMeans(n_clusters=i, max_iter=1000, random_state=1000)
    km.fit(cdf)
    inertias.append(km.inertia_)
```

下面的截图显示了结果图:

![](img/aafbed3f-3a72-4178-9011-37c00df8ce23.png)

Inertia as a function of the number of clusters for the Breast Cancer Wisconsin dataset

在这种情况下，基本事实表明，我们应该根据诊断结果分成两组。然而，该图显示了急剧下降，在 *K=8* 处结束，并以较低的斜率继续，直到约 *K=40* 。在初步分析期间，我们已经看到二维投影由许多共享同一诊断的孤立斑点组成。因此，我们可以决定采用例如 *K=8* 并分析对应于每个聚类的特征。由于这不是一个分类任务，基本事实可以作为主要参考，但是正确的探索性分析可以尝试理解子结构的组成，以便为技术人员(例如，医生)提供进一步的细节。

现在，让我们在乳腺癌威斯康星数据集上对八个聚类进行 K 均值聚类，以描述两个样本组的结构，如下所示:

```py
import pandas as pd

from sklearn.cluster import KMeans

km = KMeans(n_clusters=8, max_iter=1000, random_state=1000)
Y_pred = km.fit_predict(cdf)

df_km = pd.DataFrame(Y_pred, columns=['prediction'], index=cdf.index)
kmdff = pd.concat([dff, df_km], axis=1)
```

下面的截图显示了结果图:

![](img/66165fe7-cca1-420c-b2c8-71ffbfe2d2d8.png)

K-means clustering (with K=8) result for the Breast Cancer Wisconsin dataset

现在让我们考虑位于地块底部的子聚类( *-25 < x < 30* 和 *-60 < y < -40* ，如下所示:

```py
sdff = dff[(dff.x > -25.0) & (dff.x < 30.0) & (dff.y > -60.0) & (dff.y < -40.0)]
print(sdff[['perimeter_mean', 'area_mean', 'smoothness_mean', 
            'concavity_mean', 'symmetry_mean']].describe())
```

下表显示了易于打印的统计表:

![](img/18fc95e3-1810-4f73-b667-60fd6b38760c.png)

Statistical description of a malignant cluster

从基本事实来看，我们知道所有这些样本都是恶性的，但我们可以尝试确定一个规律。比率`area_mean` / `perimeter_mean`约为`9.23`，相对标准差与平均值相比非常小。这意味着这些样本代表了非常窄范围内的扩展肿瘤。而且`concavity_mean`和`symmetry_mean`都大于整体数值。因此(没有科学合理分析的假设)，我们可以得出结论，分配给这些簇的样本代表已经到达晚期的非常坏的肿瘤。

为了与良性样本进行比较，现在让我们考虑由 *x > -10* 和 *20 < y < 50* 划定的区域，如下所示:

```py
sdff = dff[(dff.x > -10.0) & (dff.y > 20.0) & (dff.y < 50.0)]
print(sdff[['perimeter_mean', 'area_mean', 'smoothness_mean',
            'concavity_mean', 'symmetry_mean']].describe())
```

结果如下图所示:

![](img/587be3b8-98fa-4865-bda2-dc81fb1ffc50.png)

Statistical description of a benign cluster

这种情况下`area_mean` / `perimeter_mean`的比值约为`4.89`，但`area_mean`的标准差较大(确实其最大值约为`410`)。`concavity_mean`相对于前一个非常小(即使标准偏差大致相同)，而`symmetry_mean`几乎相等。从这个简要的分析，我们可以推断出`symmetry_mean`不是判别特征，而`concavity_mean`小于或等于`0.04`的比`area_mean` / `perimeter_mean`小于`5.42`(考虑最大值)应该保证一个良性的结果。由于`concavity_mean`可以达到非常大的最大值(大于与恶性样本相关的最大值)，因此有必要考虑其他特征，以决定其值是否应被视为警报。然而，我们可以得出结论，认为属于这些聚类的所有样本都是良性的，错误概率可以忽略不计。我想重复一遍，与其说这是真正的分析，不如说这是一种练习，在这种情况下，数据科学家的主要任务是收集能够支持结论的上下文信息。即使存在基本事实，这个验证过程也总是强制性的，因为潜在原因的复杂性会导致完全错误的陈述和规则。

# 轮廓分数

在不了解基本事实的情况下，评估聚类算法性能的最常见方法是**轮廓得分**。它提供了每个样本的索引和全局图形表示，显示了集群的内部一致性和分离程度。为了计算分数，我们需要引入两个辅助措施。第一个是样本的平均聚类内距离*x<sub class="calibre20">I</sub>∈K<sub class="calibre20">j</sub>T7】假设基数为 *|K <sub class="calibre20">j</sub> | = n(j)* :*

![](img/6d06560c-6407-440e-b835-f40070b91824.png)

对于 K 均值，距离假设为欧氏距离，但没有具体的限制。当然，*d()*必须与聚类过程中使用的距离函数相同。

给定一个样本 *x <sub class="calibre20">i</sub> ∈ K <sub class="calibre20">j</sub>* ，让我们将最近的集群表示为 *K <sub class="calibre20">c</sub>* 。这样，我们还可以定义最小最近聚类距离(作为平均最近聚类距离):

![](img/f69d0af8-8bc1-4c93-96b4-966cb0576f90.png)

通过这两个度量，我们可以定义 *x <sub class="calibre20">i</sub> ∈ X* 的轮廓分数:

![](img/711bbc3f-b474-41f6-a54f-93d5fe852a59.png)

分数*s(∞(-1，1)* 。当*s(→-1*时，表示*b(<)T33】a(*)，因此样本 *x <sub class="calibre20">i</sub> ∈ K <sub class="calibre20">j</sub>* 比分配给 *K <sub class="calibre20">j</sub>* 的其他样本更接近最近的聚类 *K* <sub class="calibre20">*c*</sub> 。这种情况表明分配错误。反之，当*s()→1*、*b()>>a()*时，那么样本 *x <sub class="calibre20">i</sub>* 与其*邻居*(属于同一个聚类)的距离比分配给最近聚类的任何其他点都要近得多。显然，这是一个最佳条件，也是微调算法时要使用的参考。然而，由于该指数不是全局的，引入侧影图是有帮助的，侧影图显示了每个样本获得的分数，按聚类分组并按降序排序。

让我们考虑 *K={2，4，6，8}* 乳腺癌威斯康星数据集的轮廓图(完整代码包含在存储库中):

![](img/525f0950-ddf2-4279-9c1a-7000ce3a1a1f.png)

Silhouette plots for the Breast Cancer Wisconsin dataset

第一个图显示了 *K=2* 的*自然*聚类。第一个轮廓非常清晰，表明平均簇间距离具有较大的方差。此外，一个集群的任务比另一个集群多得多(即使它不太清晰)。从数据集描述中，我们知道这两个类别是不平衡的(357 个良性对 212 个恶性)，因此不对称是部分合理的。然而，一般来说，当数据集平衡时，良好的轮廓图的特征是具有接近 1.0 的圆形轮廓的同质聚类。事实上，当形状类似于长雪茄时，这意味着簇内距离非常接近它们的平均值(高凝聚力)，并且相邻簇之间有明显的分离。对于 *K=2* ，我们有合理的分数，因为第一个聚类达到了 0.6，而第二个聚类有一个对应于 0.8 左右的峰值。然而，在后一种情况下，大多数样品的特征是*s()>0.75*，而在前一种情况下，大约一半的样品低于 0.5。这种分析表明，越大的聚类越均匀，K-means 分配样本越容易(即在测度方面，*x<sub class="calibre20">I</sub>∈K<sub class="calibre20">2</sub>*的方差越小，在高维空间中，代表 *K <sub class="calibre20">2</sub>* 的球比代表 *K <sub class="calibre20">1</sub>* 的球更均匀)。

其他图显示了类似的情况，因为已经检测到一个非常紧密的集群和一些尖锐的集群。这意味着有一个非常一致的宽度差异。然而，增加 *K* ，我们获得稍微更均匀的聚类，因为分配的样本数量趋于相似。具有**()>0.75*的非常圆的(几乎是矩形的)聚类的存在证实了数据集包含至少一组非常内聚的样本，其相对于分配给其他聚类的任何其他点的距离非常接近。我们知道恶性类(即使它的基数较大)更紧凑，而良性类则分布在一个宽得多的子空间上；因此，我们可以假设对于所有的 *K* ，最圆的聚类由恶性样本组成，并且所有其他的可以根据它们的锐度来区分。例如，对于 *K=8* ，第三个聚类很可能对应于第一个图中第二个聚类的中心部分，而较小的聚类包含属于良性子集的孤立区域的样本。*

 *如果不知道地面真相，就要同时考虑 *K=2* 和 *K=8* (甚至更大)。事实上，在第一种情况下，我们可能会丢失许多细粒度的信息，但我们正在确定一个强大的细分(假设由于问题的性质，一个集群不是非常有凝聚力)。另一方面，在 *K > 8* 的情况下，集群明显更小，具有适度更高的凝聚力，并且它们代表具有一些共同特征的子群。正如我们在上一节中所讨论的，最终的选择取决于许多因素，这些工具只能提供一个大致的指示。此外，当聚类是非凸的或者它们的方差不是均匀分布在所有特征中时，K-means 将总是产生次优的性能，因为得到的聚类将包含大的空白空间。如果没有特定的方向，最佳聚类数与包含均匀(宽度大致相同)圆形图的图相关联。如果形状对于任何 *K* 值都保持尖锐，这意味着几何图形与对称度量不完全兼容(例如，簇非常拉伸)，应该考虑其他方法。

# 完整性分数

这项措施(以及从现在开始讨论的所有其他措施)是基于对基本事实的了解。在介绍索引之前，定义一些常见的值是有帮助的。如果我们用 *Y <sub class="calibre20">true</sub>* 表示包含真实赋值的集合，用 *Y <sub class="calibre20">pred</sub>* 表示预测集合(都包含 *M* 值和 *K* 聚类)，我们可以估计以下概率:

![](img/f96440cf-9941-469f-8e59-c4db459c0097.png)

在前面的公式中， *n <sub class="calibre20">真/pred</sub> (k)* 代表属于聚类 *k ∈ K* 的真/预测样本数。此时，我们可以计算出 *Y <sub class="calibre20">true</sub>* 和 *Y <sub class="calibre20">pred</sub>* 的熵:

![](img/bb659233-4d18-40b0-a974-eb645ad18ab2.png)

考虑到熵的定义，*H()*通过均匀分布最大化，而均匀分布又对应于每个分配的最大不确定性。出于我们的目的，也有必要引入条件熵(在已知另一个分布的情况下，表示该分布的不确定性)*Y<sub class="calibre20">true</sub>T5】给定*Y<sub class="calibre20">pred</sub>T9】反过来:**

![](img/290ffc3c-7bad-4dc1-8eee-f8e63be58f26.png)

函数 *n(i，j)* 在第一种情况下表示分配给*K<sub class="calibre20">j</sub>T7】的真标签 *i* 的样本数，在第二种情况下表示分配给 *K <sub class="calibre20">i</sub>* 的真标签 *j* 的样本数。*

完整性分数定义为:

![](img/b8423040-7648-4750-b1cf-f050766763e9.png)

很容易理解当*H(Y<sub class="calibre20">pred</sub>| Y<sub class="calibre20">true</sub>)→0*时，*Y<sub class="calibre20">true</sub>T9】的知识减少了预测的不确定性，因此， *c → 1* 。这相当于说所有具有相同真实标签的样本都被分配到同一个聚类。反之，当*H(Y<sub class="calibre20">pred</sub>| Y<sub class="calibre20">true</sub>**)→H(Y<sub class="calibre20">pred</sub>)*时，则意味着地面真实不提供任何降低预测不确定性的信息， *c → 0* 。*

当然，一个好的集群的特征是 *c → 1* 。在乳腺癌威斯康星数据集的情况下，**完整性分数**，使用 scikit-learn 函数`completenss_score()`(也适用于文本标签)和 *K=2* (与地面真实相关的唯一配置)计算如下:

```py
import pandas as pd

from sklearn.cluster import KMeans
from sklearn.metrics import completeness_score

km = KMeans(n_clusters=2, max_iter=1000, random_state=1000)
Y_pred = km.fit_predict(cdf)

df_km = pd.DataFrame(Y_pred, columns=['prediction'], index=cdf.index)
kmdff = pd.concat([dff, df_km], axis=1)

print('Completeness: {}'.format(completeness_score(kmdff['diagnosis'], kmdff['prediction'])))
```

上一个片段的输出如下:

```py
Completeness: 0.5168089972809706
```

这个结果证实，对于 *K=2* ，K-means 不能完美地分离聚类，因为正如我们所看到的，有一些恶性样本被错误地分配给包含绝大多数良性样本的聚类。然而，由于 *c* 不是非常小，我们可以确定两个类别的大部分样本已经被分配到不同的聚类。请读者使用其他方法(在[第 3 章](03.html)、*高级聚类*中讨论)检查该值，并提供不同结果的简要说明。

# 同质性得分

**同质性分数**与前一个是互补的，它基于一个聚类必须只包含具有相同真实标签的样本的假设。其定义为:

![](img/a0058b86-18c3-459c-bf62-7fc1c35afc87.png)

类似于完备性得分，当 *H(Y <sub class="calibre20">真</sub> |Y <sub class="calibre20">pred</sub> ) → H(Y <sub class="calibre20">真</sub> )* 时，意味着赋值对条件熵没有影响，因此聚类后不确定性没有减少(例如，每个聚类包含属于所有类的样本) *h → 0* 。相反，当*H(Y<sub class="calibre20">true</sub>| Y<sub class="calibre20">pred</sub>**)→0*、 *h → 1* 时，因为预测的知识减少了关于真实赋值的不确定性，并且聚类几乎只包含具有相同标签的样本。重要的是要记住，仅仅这个分数是不够的，因为它不能保证一个聚类包含所有具有相同真实标签的样本 *x <sub class="calibre20">i</sub> ∈ X* 。这就是为什么同质性分数总是与完整性分数一起评估。

对于威斯康星乳腺癌数据集和 *K=2* ，我们获得以下结果:

```py
from sklearn.metrics import homogeneity_score

print('Homogeneity: {}'.format(homogeneity_score(kmdff['diagnosis'], kmdff['prediction'])))
```

相应的输出如下:

```py
Homogeneity: 0.42229071246999117
```

这个值(特别是对于 *K=2* )证实了我们最初的分析。至少有一个聚类(良性样本占大多数的聚类)不是完全同质的，因为它包含属于两个类别的样本。然而，由于该值不是很接近 *0* ，我们可以确定作业是部分正确的。考虑到这两个值， *h* 和 *c* ，我们可以推断 K-means 的表现不是非常好(可能是因为非凸性)，但同时，它能够正确地分离所有最近聚类距离高于特定阈值的样本。不言而喻，有了基础事实的知识，我们不能轻易接受 K-means，我们应该寻找另一种能够同时产生 *h* 和 *c → 1* 的算法。

# 使用 V-测度在同质性和完整性之间进行权衡

熟悉监督学习的读者应该知道 F-score(或 F-measure)的概念，它是精度和召回率的调和平均值。在给定基本事实的情况下，在评估聚类结果时也可以采用同样的权衡。

事实上，在许多情况下，有一个单一的衡量标准来考虑同质性和完整性是有帮助的。使用 **V 测量**(或 V 评分)可以很容易地获得这样的结果，其定义为:

![](img/2842e1c8-983a-44b0-a84b-3e44faca51e7.png)

对于威斯康星乳腺癌数据集，V 度量如下:

```py
from sklearn.metrics import v_measure_score

print('V-Score: {}'.format(v_measure_score(kmdff['diagnosis'], kmdff['prediction'])))
```

上一个片段的输出如下:

```py
V-Score: 0.46479332792160793
```

不出所料，V-Score 是一个平均指标，在这种情况下，会受到较低同质性的负面影响。当然，这个指数并没有提供任何不同的信息，因此它只对综合单一值的完整性和同质性有帮助。然而，通过一些简单但繁琐的数学运算，就有可能证明 V 测度也是对称的(即*V(Y<sub class="calibre20">pred</sub>| V<sub class="calibre20">true</sub>)= V(Y<sub class="calibre20">true</sub>| Y<sub class="calibre20">pred</sub>)*)；因此，给定两个独立的赋值 *Y <sub class="calibre20">1</sub>* 和 *Y <sub class="calibre20">2</sub>* ，*V(Y<sub class="calibre20">1</sub>| Y<sub class="calibre20">2</sub>)*就是它们之间一致的一种度量。这样的场景并不是非常常见，因为其他措施可以达到更好的效果。然而，这样的分数可以用于例如检查两种算法(可能基于不同的策略)是否倾向于产生相同的分配或者它们是否不一致。在后一种情况下，即使地面真相未知，数据科学家也可以理解一种策略肯定不如另一种策略有效，并开始探索过程以找出最佳聚类算法。

# 调整后的互信息得分

该评分的主要目标是评估 *Y <sub class="calibre20">true</sub>* 和 *Y <sub class="calibre20">pred</sub>* 之间的一致程度，而不考虑排列。这样的目标可以采用**互信息** ( **MI** )的信息论概念来衡量；在我们的例子中，它被定义为:

![](img/208136bf-137d-46b0-a63c-99b1a72464e5.png)

功能与之前定义的相同。当 *MI → 0* 、 *n(i，j)**→n<sub class="calibre20">true</sub>(I)n<sub class="calibre20">pred</sub>(j)*，其项分别与 *p(i，j)* 和*p<sub class="calibre20">true</sub>(I)p<sub class="calibre20">pred</sub>(j)*成正比。因此，这个条件相当于说 *Y <sub class="calibre20">真</sub>* 和 *Y <sub class="calibre20">pred</sub>* 在统计学上是独立的，不存在一致。另一方面，通过一些简单的操作，我们可以将 MI 重写为:

![](img/a621ff93-b1e0-4663-93fe-7dac3169b096.png)

因此，作为*H(Y<sub class="calibre20">pred</sub>| Y<sub class="calibre20">true</sub>)≤H(Y<sub class="calibre20">pred</sub>)*，当对地面真相的认识减少了对*Y<sub class="calibre20">pred</sub>T11】的不确定性时，那么<sub xmlns:epub="http://www.idpf.org/2007/ops" class="calibre20">T14】H(Y<sub class="calibre20">pred</sub>| Y<sub class="calibre20">true</sub></sub>*T20】)→0 和就我们的目的而言，最好考虑一个规范化的版本(在 *0* 和 *1* 之间)，该版本也是根据机会进行调整的(也就是说，考虑到真正的赋值是由于机会的可能性)。 **AMI 评分**，其完整推导非小事，超出本书范围，定义为:

![](img/c7a9d2b5-b5bd-4071-9558-e0b615a77c60.png)

当 *Y <sub class="calibre20">真</sub>* 和 *Y <sub class="calibre20">pred</sub>* 完全一致(也存在排列)时，该值等于 *0* ，等于 *1* 。对于威斯康星乳腺癌数据集和 *K=2* ，我们获得以下结果:

```py
from sklearn.metrics import adjusted_mutual_info_score

print('Adj. Mutual info: {}'.format(adjusted_mutual_info_score(kmdff['diagnosis'], kmdff['prediction'])))
```

输出如下:

```py
Adj. Mutual info: 0.42151741598216214
```

该协议是温和的，与其他措施兼容。假设存在排列和机会分配的可能性， *Y <sub class="calibre20">true</sub>* 和 *Y <sub class="calibre20">pred</sub>* 共享中等水平的信息，因为正如我们已经讨论过的，K-means 能够正确分配重叠概率可以忽略的所有样本，同时它倾向于考虑两个聚类之间边界上的良性许多恶性样本(相反，它不会对良性样本进行错误分配)。在没有任何进一步指示的情况下，该索引还建议检查可以管理非凸聚类的其他聚类算法，因为共享信息的缺乏主要是由于无法使用标准球捕捉复杂的几何形状(特别是在重叠更显著的子空间中)。

# 调整后的兰德分数

**调整后的兰德分数**是真实标签分布和预测分布之间差异的度量。为了计算它，有必要定义如下数量:

*   **a** :代表具有相同真实标签 *(y <sub class="calibre20">i</sub> 、y <sub class="calibre20">j</sub> )的样本对数量( *x <sub class="calibre20">i</sub> ，x<sub class="calibre20">j</sub>T7】):y<sub class="calibre20">I</sub>= y<sub class="calibre20">j</sub>T17】并分配到同一聚类*K<sub class="calibre20">c</sub>T21】***
*   **b** :代表不同真标签 *(y* *<sub class="calibre20">i</sub> 、y <sub class="calibre20">j</sub> )的样本对数量(*x**T5】I、x<sub class="calibre20">j</sub>*):y<sub class="calibre20">I</sub>≠y<sub class="calibre20">j</sub>T21】并分配到不同的聚类 *K**

 *如果有 *M* 个值，则二进制组合的总数是使用具有 *k=2* 的二项式系数获得的，因此，差异的初始度量是:

![](img/d14f12e6-41ca-4082-a8db-74afaa1370a5.png)

显然，该值可以由 *a* 或 *b* 控制。在这两种情况下，较高的分数表明作业符合基本事实。然而， *a* 和 *b* 都有可能被随机分配所偏向。这就是为什么引入了调整后的兰德分数。更新后的公式为:

![](img/08edb6be-5dee-4a22-81fd-3b6dc49ee1a3.png)

该值在 *-1* 和 *1* 之间。当 *R <sub class="calibre20">A</sub> → -1* 时，A 和 b 都很小，绝大多数作业都是错的。另一方面，当 *R <sub class="calibre20">A</sub>* *→ 1* 时，预测分布非常接近地面真相。对于威斯康星乳腺癌数据集和 *K=2* ，我们获得以下信息:

```py
from sklearn.metrics import adjusted_rand_score

print('Adj. Rand score: {}'.format(adjusted_rand_score(kmdff['diagnosis'], kmdff['prediction'])))
```

上一个片段的输出如下:

```py
Adj. Rand index: 0.49142453622455523
```

该结果优于其他指标，因为该值大于 *-1* (负极端)。它证实了分布之间的差异不是很明显，这主要是由于样本子集有限。这个分数非常可靠，也可以用作评估聚类算法性能的单一指标。接近 0.5 的值证实了 K-means 不太可能是最优解，但与此同时，数据集具有几乎可以被对称球完全捕获的几何形状，除了一些重叠概率很高的非凸区域。

# 应急矩阵

一个非常简单而强大的工具是**权变矩阵** *C <sub class="calibre20">m</sub>* ，它可以在地面真相已知的情况下显示聚类算法的性能。如果有 *m* 类，*c<sub class="calibre20">m</sub>∈ℜ<sup class="calibre27">m×m</sup>*并且每个元素 *C <sub class="calibre20">m</sub> (i，j)* 代表已经分配给聚类 *j* 的带有 *Y <sub class="calibre20">true</sub> = i* 的样本数量。因此，一个完美的列联矩阵是对角的，而所有其他单元中元素的存在表明了一个聚类错误。

在我们的案例中，我们获得了以下信息:

```py
from sklearn.metrics.cluster import contingency_matrix

cm = contingency_matrix(kmdff['diagnosis'].apply(lambda x: 0 if x == 'B' else 1), kmdff['prediction'])
```

前面片段的输出可以被可视化为热图(变量`cm`是一个(2 × 2)矩阵):

![](img/a5efa5bc-14b5-44e3-beb0-10c360696630.png)

Graphical representation of the contingency matrix

这个结果表明，几乎所有良性样本都被正确聚类，而中等百分比的恶性样本被错误地分配到第一个聚类。我们已经确认使用这一其他指标，但是，类似于分类任务中的混淆矩阵，应急矩阵允许立即可视化哪些类最难分离，帮助数据科学家寻找更有效的解决方案。

# k 近邻

**K-最近邻** ( **K** NN)是属于一个叫做**基于实例的学习**的类别的方法。在这种情况下，没有参数化模型，而是为了加快特定查询的速度而对样本进行了重新排列。在最简单的情况下(也称为蛮力搜索)，假设我们有一个数据集 *X* 包含 *M* 个样本*x<sub class="calibre20">I</sub>∈ℜ<sup class="calibre27">n</sup>t15】。给定距离函数 *d(x <sub class="calibre20">i</sub> ，x <sub class="calibre20">j</sub> )* ，可以将试样 *x <sub class="calibre20">i</sub>* 的半径邻域定义为:*

![](img/539ec0a2-ed36-460b-818d-07dd652d1fc2.png)

集合 *ν(x <sub class="calibre20">i</sub> )* 是一个以*x<sub class="calibre20">I</sub>T7】为中心的球，包括所有距离小于或等于 *R* 的样品。或者，也可以只计算最上面的 *k* 最近邻，也就是距离*x<sub class="calibre20">I</sub>T17】更近的 *k* 样本(一般来说，这个集合是*ν(x<sub class="calibre20">I</sub>**)*的子集，但是 *k* 很大的时候也会出现相反的情况)。这个过程很简单，但不幸的是，从计算的角度来看太昂贵了。事实上，对于每个查询，都需要计算*M<sup class="calibre27">2</sup>**N*维距离(即假设每距离 *N* 次运算，复杂度为 *O(NM <sup class="calibre27">2</sup> )* )，这一条件让蛮力方法遭受了维度的诅咒。比如*N = 2**M = 1，000* ，复杂度为 *O(2 ![](img/04b172e2-2609-4bcc-afae-e7798788553c.png) 10 <sup class="calibre27"> 6 </sup> )* ，但是 *N=1，000* 和 *M=10，000* 就变成了 *O(10 <sup class="calibre27"> 11 </sup> )* 。例如，如果每个操作需要 1 纳秒，那么查询将需要 100 秒，这在许多实际情况下超出了可容忍的限度。此外，对于 64 位浮点值，成对距离矩阵每次计算大约需要 764 兆字节，考虑到任务的性质，这也是一个过高的要求。**

由于这些原因，KNN 的具体实现仅在 *M* 非常小时使用强力搜索，并且在所有其他情况下依赖稍微复杂的结构。第一种替代方法基于 **kd 树**，这是二叉树到多维数据集的自然扩展。

在下图中，部分 kd 树由 *3* 维向量组成:

![](img/3d1185cb-7e88-4c5f-aa2a-076e095f611a.png)

Example of kd-tree with 3-dimensional vectors

kd 树的构造非常简单。给定一个根样本(*一个 <sub class="calibre20">1</sub> ，一个 <sub class="calibre20">2</sub> ，...，a <sub class="calibre20">N</sub>* ，考虑第一个特征进行第一次拆分操作，使左分支包含(*b<sub class="calibre20">1</sub>T21】a<sub class="calibre20">1</sub>，...、*等)和右边的一个(*c<sub class="calibre20">1</sub>T22】a<sub class="calibre20">1</sub>，...，*等等)。该过程继续第二个特征、第三个特征，以此类推，直到第一个特征，以此类推，直到到达叶节点(分配给叶的样本数是一个需要调整的超参数。在 scikit-learn 中，该参数称为`leaf_size`，默认值为 30 个样本)。

当维数 *N* 不是特别大的时候，计算复杂度就变成了 *O(N log M)* ，比蛮力搜索好了不少。比如*N = 1000**M = 10000*，计算复杂度就变成了*O(4000)<<O(10<sup class="calibre27">11</sup>)*。遗憾的是，当 *N* 大的时候，一个 kd-tree 查询就变成了 *O(NM)* ，所以，考虑到前面的例子， *O(10 <sup class="calibre27">7</sup> )* ，比蛮力搜索要好，但对于实时查询来说，有时候还是太贵了。

KNN 常用的第二种数据结构是**球树**。在这种情况下，根节点由一个 *R <sub class="calibre20">0</sub>* 球表示，该球被精确定义为样本的邻域:

![](img/7b152ca0-7fd8-4993-bbe9-26de5ca22350.png)

选择第一个球以捕获所有样本。此时，其他更小的球嵌套在*β<sub class="calibre20">R0</sub>T3】中，确保每个样品始终属于一个球。在下图中，有一个简单球树的示意图:*

![](img/05e3457f-cea4-4480-9261-6cc1010f7823.png)

Example of a simple ball-tree

由于每个球完全由其中心 *c <sub class="calibre20">j</sub>* 决定，所以使用测试样本*x<sub class="calibre20">I</sub>T7】的查询需要计算距离 *d(x <sub class="calibre20">i</sub> ，c <sub class="calibre20">j</sub> )* 。因此，从底部(最小的球所在的位置)开始，执行完整的扫描。如果没有球包含样本，级别会增加，直到到达根节点(请记住，样本可以属于单个球)。由于球的属性(也就是说，给定中心和半径，可以用一次距离计算来检查样本的成员资格)，计算复杂度现在总是 *O(N log M)* 。一旦确定了正确的球，样本的邻居 *x* <sub xmlns:epub="http://www.idpf.org/2007/ops" class="calibre20">*i*</sub> 需要计算有限数量的成对距离(该值小于叶子大小，因此与数据集的维度相比，通常可以忽略不计)。*

当然，这些结构是在培训阶段构建的，在生产阶段不会修改。这意味着仔细选择最小半径或分配给叶节点的样本数。事实上，由于查询通常需要多个邻居 *k* ，因此只有当 *k < |ν(x <sub class="calibre20">i</sub> )|* 时，才能达到最优性。换句话说，我们希望在包含*x<sub class="calibre20">I</sub>T9】的同一子结构中找到所有邻居。每当*k>|ν(x<sub class="calibre20">I</sub>**)|*时，算法也必须检查相邻结构并合并结果。当然，当叶子尺寸太大时(与样本总数 *M* 相比)，这些树的优势就消失了，因为需要计算太多的成对距离才能回答一个查询。考虑到软件的生产使用，必须做出正确的叶片尺寸选择。*

例如，如果推荐系统需要一个具有 100 个邻居的初始查询和几个(例如，5 个)具有 10 个邻居的后续查询，那么等于 10 的叶子大小将优化细化阶段，但是它对第一个查询有负面影响。相反，选择等于 100 的叶大小将减慢所有 10 个邻居的查询。折衷可能是 25，这减少了第一个查询的负担，但对细化查询的成对距离的计算有适度的负面影响。

我们现在可以基于 Olivetti 人脸数据集(由 scikit-learn 直接提供)分析一个简短的示例。它由 400 幅 64 × 64 灰度图像组成，代表不同人的肖像。让我们从如下加载数据集开始:

```py
from sklearn.datasets import fetch_olivetti_faces

faces = fetch_olivetti_faces()
X = faces['data']
```

变量`X`包含数据集的展平版本(400 个 4，096 维实例已经在 0 和 1 之间规范化)。此时，我们可以训练一个`NearestNeighbor`模型，假设默认查询有 10 个样本(参数`n_neighbors`)和等于 20 的半径(参数`radius`)。我们保留默认的`leaf_size (30)`并使用`p=2`(欧几里德距离)明确设置闵可夫斯基度量。该算法基于球树，但我邀请读者测试不同的度量和 kd 树。我们现在可以创建一个`NearestNeighbors`实例并继续训练模型:

```py
from sklearn.neighbors import NearestNeighbors

knn = NearestNeighbors(n_neighbors=10, metric='minkowski', p=2, radius=20.0, algorithm='ball_tree')
knn.fit(X)
```

一旦模型被训练，使用一个有噪声的测试面来寻找 10 个最近的邻居，如下所示:

```py
import numpy as np

i = 20
test_face = X[i] + np.random.normal(0.0, 0.1, size=(X[0].shape[0]))
```

测试面绘制在下面的截图中:

![](img/de8295ff-3225-4772-b868-e85d1a98137c.png)

Noisy test face

可以使用仅提供测试样本的方法`kneighbors()`来执行具有默认邻居数量的查询(在邻居数量不同的情况下，必须调用函数来提供参数`n_neighbors`)。该函数，如果参数`return_distance=True`，返回一个包含`distances, neighbors`的元组，如下所示:

```py
distances, neighbors = knn.kneighbors(test_face.reshape(1, -1))
```

查询结果如下图所示:

![](img/6acb69db-60e8-4ca1-92fd-98f40f831451.png)

Nearest neighbors of the test sample with their relative distances

第一个样本总是测试样本(在这种情况下，它被去噪，因此它的距离不是零)。正如可以看到的，即使距离是累积函数，第二个和第四个样本指的是同一个人，而其他样本共享不同的解剖元素。当然，欧几里德距离不是衡量图像之间差异的最合适方式，但这个例子在一定程度上证实了，当图片相当相似时，全局距离也可以为我们提供一个有价值的工具，可以用来寻找相似的样本。

现在让我们使用设置`radius=100`的方法执行半径查询，如下所示:

```py
import numpy as np

distances, neighbors = knn.radius_neighbors(test_face.reshape(1, -1), radius=100.0)
sd, sd_arg = np.sort(distances[0]), np.argsort(distances[0])
```

包含前 20 个邻居的结果如下图所示:

![](img/4dea69de-b863-49e9-aecc-64441e7b9ccd.png)

First 50 neighbors using a radius query

有趣的是，注意到距离不会很快发散(第二个样本有`d=8.91`和第五个`d=10.26`)。这主要是由于两个因素:第一个因素是样本之间的全局相似性(在几何元素和色调方面)，第二个因素很可能与欧氏距离对 4，096 维向量的影响有关。正如在讨论聚类基础时所解释的，高维样本可能缺乏可区分性(特别是当 *p > > 1* 时)。在这种情况下，图片不同部分的平均效果会产生与分类系统完全不兼容的结果。特别是，深度学习模型倾向于通过使用卷积网络来避免这个*陷阱*，卷积网络可以学习检测不同级别的特定特征。我建议用不同的度量重复这个例子，并观察 *p* 对 radius 查询的样本所显示的实际差异的影响。

# 矢量量化

**矢量量化** ( **VQ** )是一种利用无监督学习来对样本*x<sub class="calibre20">I</sub>∈ℜ<sup class="calibre27">n</sup>T9】(为简单起见，我们假设多维样本被展平)或整个数据集 *X* 执行有损压缩的方法。主要思想是找到一个带有多个条目的码本*Q*C*C<T23】N*并将每个元素与一个条目 *q <sub class="calibre20">i</sub> ∈ Q* 相关联。在单个样本的情况下，每个条目将表示一组或多组特征(例如，它可以是平均值)，因此，该过程可以描述为变换 *T* ，其一般表示为:*

![](img/cfb86f7a-2dce-4c03-9d28-1886d7c91fea.png)

码本定义为 *Q = (q <sub class="calibre20">1</sub> ，q <sub class="calibre20">2</sub> ，...，q <sub class="calibre20">C</sub> )* 。因此，给定由一组特征集合(例如，一组两个连续元素)组成的合成数据集，VQ 关联单个码本条目:

![](img/c661aadf-8aae-416e-97bc-b331de4a0af8.png)

由于输入样本是使用固定值的组合来表示的,*汇总整个组的*,该过程被定义为量化。类似地，如果输入是数据集 *X* ，转换就像任何标准的聚类过程一样对样本组进行操作。主要区别在于目的:使用 VQ 以质心表示每个聚类，从而减少数据集的方差。这个过程是不可逆的。一旦执行转换，就不可能重建原始聚类(唯一可行的过程是基于从具有相同原始均值和协方差的分布中采样，但重建显然是近似的)。

让我们从展示一个非常简单的高斯数据集开始，如下所示:

```py
import numpy as np

nb_samples = 1000
data = np.random.normal(0.0, 1.5, size=(nb_samples, 2))

n_vectors = 16
qv = np.random.normal(0.0, 1.5, size=(n_vectors, 2))
```

我们的目标是用 16 个向量来表示数据集。在下面的屏幕截图中，图表显示了初始配置的示例:

![](img/e4083e41-2003-4fa7-b7f3-2f89d53ac1b8.png)

Initial configuration of the vectors for the VQ example

当我们使用随机数时，同一代码的后续执行会产生不同的初始配置。该过程迭代所有样本，选择最近的量化矢量，并将其距离减少固定量`delta=0.05`，如下所示:

```py
import numpy as np

from scipy.spatial.distance import cdist

delta = 0.05
n_iterations = 1000

for i in range(n_iterations):
    for p in data:
        distances = cdist(qv, np.expand_dims(p, axis=0))
        qvi = np.argmin(distances)
        alpha = p - qv[qvi]
        qv[qvi] += (delta * alpha)

distances = cdist(data, qv)
Y_qv = np.argmin(distances, axis=1)
```

代替固定的 for 循环，也可以使用 while 循环来检查量化向量是否已经达到它们的稳定状态(比较在时间 *t* 和 *t+1* 计算的向量的范数)。过程结束时的结果如下图所示:

![](img/3ca0903d-ae6a-4d50-876d-2b402aff5517.png)

Final configuration of the quantization vectors (left). Influence area of each quantization vector (right)

正如预期的那样，量化矢量已经达到最终配置，其中每个矢量代表数据集的一小部分(如右图所示)。此时，给定一个点，最近的向量将表示它。有趣的是，注意到全局方差没有受到影响，但是，选择任何子集，内部方差反而显著减少。向量的相对位置反映了数据集的密度，因为一个区域中*更多的样本会吸引更多的向量*。这样，通过构建距离矩阵，有可能获得粗略的密度估计(例如，当向量与其近邻的平均距离较高时，这意味着底层区域的密度较低)。我们将在[第六章](06.html)、*异常检测*中更详细地讨论这个话题。

现在让我们考虑一个例子，其中一个样本代表一只浣熊的图片。由于过程可能非常长，第一步是加载示例 RGB 图像(由 SciPy 提供)，并将其大小调整为 192 × 256，如下所示:

```py
from scipy.misc import face
from skimage.transform import resize

picture = resize(face(gray=False), output_shape=(192, 256), mode='reflect')
```

原始图片(已经在[0，1]范围内归一化)如下图所示:

![](img/7f746df8-c3e8-4ee9-9cea-be5f76238cb2.png)

Sample RGB picture for VQ example

我们希望使用 2 × 2 正方形区域(由包含 2 × 2 × 3 特征的展平向量表示)计算的 24 个向量来执行 VQ。然而，我们将使用 K-means 算法来寻找质心，而不是从头开始执行这个过程。第一步是收集所有正方形区域，如下所示:

```py
import numpy as np

square_fragment_size = 2
n_fragments = int(picture.shape[0] * picture.shape[1] / (square_fragment_size**2))

fragments = np.zeros(shape=(n_fragments, square_fragment_size**2 * picture.shape[2]))
idx = 0

for i in range(0, picture.shape[0], square_fragment_size):
    for j in range(0, picture.shape[1], square_fragment_size):
        fragments[idx] = picture[i:i + square_fragment_size, 
                                 j:j + square_fragment_size, :].flatten()
        idx += 1
```

此时，可以用 24 个量化向量执行 K 均值聚类，如下所示:

```py
from sklearn.cluster import KMeans

n_qvectors = 24

km = KMeans(n_clusters=n_qvectors, random_state=1000)
km.fit(fragments)

qvs = km.predict(fragments)
```

在训练结束时，变量`qvs`将包含与每个正方形区域相关联的质心索引(可通过实例变量`cluster_centers_`获得)。

现在可以使用质心构建量化图像，如下所示:

```py
import numpy as np

qv_picture = np.zeros(shape=(192, 256, 3))
idx = 0

for i in range(0, 192, square_fragment_size):
    for j in range(0, 256, square_fragment_size):
        qv_picture[i:i + square_fragment_size,
                   j:j + square_fragment_size, :] = \
            km.cluster_centers_[qvs[idx]].\
                reshape((square_fragment_size, square_fragment_size, 3))
        idx += 1
```

量化图像如下图所示:

![](img/d3128ff0-f632-4a71-bd39-655c31928ff5.png)

Picture quantized with 24 vectors

结果显然是原始图像的有损压缩版本。每个组可以用指向码本中条目的索引(例如，在我们的例子中，它可以是 8 位整数)来表示(`km.cluster_centers_`)。因此，如果最初有 192 × 256 × 3 = 1，474，560 个 8 位值，量化后我们有 12，288 个 8 位索引(2 × 2 × 3 块的数量)加上 24 个 12 维量化矢量。为了了解 VQ 对图像的影响，为原始图像和处理后的图像绘制 RGB 直方图会很有帮助，如下图所示:

![](img/b8a8810a-c3f7-45af-9bf5-c52d7da2abd8.png)

RGB histogram of the original image (top) and quantized version (bottom) For readers who are not familiar with histograms, we can briefly describe them as having a dataset *X* and a fixed number of bins. Each bin is assigned to a range (starting from *min(X)* and ending in *max(X)*) and each range (*a, b*) is associated with the number of samples such that *a ≤ x < b*. The resulting plot is proportional to an approximation of the actual probability distribution that generated *X*. In our case, on the x-axis, there are all possible values for each pixel per channel (8-bit), while the y-axis represents the estimated frequency (*Nx / Total number of pixels*).

可以看到，量化减少了信息量，但直方图倾向于再现原始直方图。增加量化向量的数量具有减少近似的效果，产生差异不太明显的直方图。对这个话题的完整分析超出了本书的范围；然而，我邀请读者用其他图像和不同数量的量化向量来测试这个过程。还可以将原始图像的(共)方差(或者，熵)与量化版本进行比较，并找到保留 80%方差的阈值。例如，仅考虑红色通道，用频率计数近似每个值(0 ÷ 255)的概率，我们得到以下结果:

```py
import numpy as np

hist_original, _ = np.histogram(picture[:, :, 0].flatten() * 255.0, bins=256)
hist_q, _ = np.histogram(qv_picture[:, :, 0].flatten() * 255.0, bins=256)

p_original = hist_original / np.sum(hist_original)
H_original = -np.sum(p_original * np.log2(p_original + 1e-8))

p_q = hist_q / np.sum(hist_q)
H_q = -np.sum(p_q * np.log2(p_q + 1e-8))

print('Original entropy: {0:.3f} bits - Quantized entropy: {1:.3f} bits'.format(H_original, H_q))
```

上一个片段的输出如下:

```py
Original entropy: 7.726 bits - Quantized entropy: 5.752 bits
```

由于信息量与熵成正比，我们现在已经确认，24 个量化向量(具有 2 × 2 个正方形块)能够解释大约 74%的红色通道的原始熵(即使三个通道不是独立的，也可以通过对三个熵求和来获得总熵的粗略近似值)。这种方法可以有效地用于寻找压缩强度和最终结果质量之间的折衷。

# 摘要

在这一章中，我们解释了聚类分析的基本概念，从相似性的概念和如何衡量它开始。我们讨论了 K-means 算法及其称为 K-means++的优化变体，并分析了乳腺癌威斯康星数据集。然后，我们讨论了最重要的评估指标(有或没有基本事实的知识)，我们了解了哪些因素会影响绩效。接下来的两个主题是 KNN 和 VQ，前者是一种非常著名的算法，可用于在给定查询向量的情况下查找最相似的样本，后者是一种利用聚类算法来查找样本(例如，图像)或数据集的有损表示的技术。

在下一章中，我们将介绍一些最重要的高级聚类算法，展示它们如何轻松解决非凸问题。

# 问题

1.  如果两个样本的 Minkowski 距离( *p=5* )等于 10，你能说说它们的曼哈顿距离吗？
2.  对 K 均值收敛速度产生负面影响的主要因素是数据集的维数。这是正确的吗？
3.  能够对 K 均值的性能产生积极影响的最重要因素之一是聚类的凸性。这是正确的吗？
4.  聚类应用程序的同质性得分等于 0.99。这是什么意思？
5.  调整后的兰德分数等于-0.5 是什么意思？
6.  考虑到前面的问题，不同数量的簇能产生更好的分数吗？
7.  基于 KNN 的应用程序平均每分钟需要 100 个基于 5-NN 的查询。每分钟执行 2 个 50-NN 查询(每个查询需要 4 秒钟，叶大小=25)，之后立即执行 2 秒钟的阻塞任务。假设没有其他延迟，一片叶子大小=50，每分钟可以执行多少次*基本*查询？
8.  球树结构不适合管理高维数据，因为它受到维度的诅咒。这是正确的吗？

9.  数据集是通过从 3 个二维高斯分布中采样 1000 个样本获得的: *N([-1.0，0.0]，diag[0.8，0.2])* ， *N([0.0，5.0]，diag[0.1，0.1])* ，以及 *N([-0.8，0.0]* ， *diag[0.6，0.3])* 。最有可能的集群数量是多少？
10.  能否采用 VQ 压缩一个文本文件(例如，构建一个在*【0.0，1.0】*范围内统一映射的 10000 个单词的字典，将文本拆分为标记，并将其转换为一系列浮点数)？

# 进一步阅读

*   *关于高维空间中距离度量的* *的惊人行为**阿加尔瓦尔 c .**欣内堡 a .**凯米 d . a .**ICDT*，2001
*   *K**-意指++:* *小心播种的优势***Arthur d**Vassilvitskii s .**第十八届年度 ACM-SIAM 离散算法研讨会论文集*，2007*
**   *使用 t-SNE* 、*范德马滕*、*辛顿*、*机器学习研究杂志 9* ，2008*   *两个线性不可分集合的鲁棒线性规划判别*、*贝内特·k·p .*、*莽草酸*、*优化方法和软件 1* ，1992*   1995 年 7 月至 8 月，通过线性规划进行乳腺癌诊断和预后、*漫画有限公司*、*西大街*、*沃尔伯格西大街*、*运筹学*，43(4)，第 570-577 页*   *V-Measure:一种基于条件熵的外部聚类评估方法*，*罗森伯格 A.* ，*赫希伯格 J.* ，*2007 年自然语言处理和计算自然语言学习中的经验方法联合会议论文集*，2007***