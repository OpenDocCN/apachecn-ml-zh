# 无监督学习入门

在本章中，我们将介绍基本的机器学习概念，假设您有一些统计学习和概率论的基本知识。您将了解到机器学习技术的使用以及逻辑过程，这提高了我们对数据集的性质和属性的了解。整个过程的目的是建立能够支持业务决策的描述性和预测性模型。

无监督学习旨在为数据探索、挖掘和生成提供工具。在这本书里，你将通过具体的例子和分析来探索不同的场景，你将学习如何应用基本的和更复杂的算法来解决特定的问题。

在这一介绍性章节中，我们将讨论:

*   为什么我们需要机器学习？
*   描述性、诊断性、预测性和规定性分析
*   机器学习的类型
*   我们为什么使用 Python？

# 技术要求

本章中的代码要求:

*   Python 3.5+(蟒蛇分布:[https://www.anaconda.com/distribution/](https://www.anaconda.com/distribution/)强烈推荐)
*   库:
    *   SciPy 0.19+
    *   NumPy 1.10+
    *   学习 0.19+
    *   熊猫 0.22+
    *   Matplotlib 2.0+
    *   seaborn 0.9+

示例可在 GitHub 资源库中获得:[https://GitHub . com/packktpublishing/HandsOn-Unsupervised-Learning-with-Python/tree/master/chapter 01](https://github.com/PacktPublishing/HandsOn-Unsupervised-Learning-with-Python/tree/master/Chapter01)。

# 为什么我们需要机器学习？

数据无处不在。此时此刻，成千上万的系统正在收集构成特定服务历史的记录，以及日志、用户交互和许多其他依赖于上下文的元素。仅在十年前，大多数公司甚至无法有效管理 1%的数据。由于这个原因，数据库被定期删除，只有重要的数据被保留在永久存储服务器中。

相反，如今几乎每个公司都可以利用可扩展的云基础架构来应对不断增长的传入数据量。Apache Hadoop 或 Apache Spark 等工具允许数据科学家和工程师实施涉及海量数据的复杂管道。至此，所有的障碍都被拆除，民主化进程已经到位。然而，这些大型数据集的实际价值是什么？从业务角度来看，只有当信息能够帮助做出正确的决策、减少不确定性并提供更好的上下文洞察力时，信息才是有价值的。这意味着，如果没有正确的工具和知识，大量数据对公司来说只是一种成本，需要加以限制才能提高利润率。

机器学习是计算机科学(特别是人工智能)的一个大分支，旨在通过利用现有数据集来实现现实的**描述性**和**预测性**模型。由于这本书致力于实用的无监督解决方案，我们将只关注通过寻找隐藏的原因和关系来描述上下文的算法。然而，即使只是从理论的角度来看，展示机器学习问题之间的主要差异也是有帮助的。只有对目标的完全认知(不仅限于技术层面)才能对最初的问题——为什么我们需要机器学习——给出合理的答案。

我们可以先说人类有非凡的认知能力，启发了很多系统，但当元素数量显著增加时，他们就缺乏分析能力。例如，如果你是一名老师，第一次和他/她的班级见面，你可以在浏览整个小组后，计算出女生比例的粗略估计。通常，即使估计是由两个或两个以上的个人进行的，估计也可能是准确的，并接近实际计数。然而，如果我们用聚集在一个院子里的一所学校的全部人口重复这个实验，性别的区别将不明显。这是因为所有的学生在课堂上都清晰可见；然而，在院子里区分性别受到某些因素的限制(例如，高个子可以隐藏矮个子)。去掉类比，我们可以说，大量的数据通常承载着大量的信息。为了提取和分类信息，有必要采取自动化的方法。

在进入下一部分之前，让我们讨论最初由 Gartner 定义的描述性、诊断性、预测性和规定性分析的概念。然而，在这种情况下，我们希望专注于我们正在分析的系统(例如，通用上下文)，以便获得对其行为越来越多的控制。

下图显示了整个过程:

![](assets/b580415b-871e-47dc-ae45-3ef713972486.png)

Descriptive, diagnostic, predictive, and prescriptive flow

# 描述性分析

在几乎所有数据科学场景中，首先要解决的问题是理解其本质。我们需要知道系统是如何工作的，或者数据集描述了什么。没有这种分析，我们的知识就太有限，无法做出任何假设或假设。例如，我们可以观察一个城市几年的平均温度图表。如果我们不能描述发现相关性、季节性和趋势的时间序列，任何其他问题仍然没有解决。在我们的特定环境中，如果我们没有发现对象组之间的相似性，我们就不能试图找到一种方法来总结它们的共同特征。数据科学家必须为每个特定的问题使用特定的工具，但是，在这个阶段结束时，所有可能的(和有用的)问题都必须得到回答。

此外，由于这个过程必须具有明确的业务价值，因此让不同的利益相关者参与进来以收集他们的知识并将其转化为一种通用语言是很重要的。例如，当处理医疗保健数据时，医生可能会谈论遗传因素，但出于我们的目的，最好说一些样本之间存在相关性，因此我们没有完全授权将它们视为统计上独立的元素。一般来说，描述性分析的结果是一个总结，包含了所有必要的度量评估和结论，以限定上下文，并减少不确定性。在温度图的例子中，数据科学家应该能够回答自相关、峰值的周期性、潜在异常值的数量以及趋势的存在。

# 诊断分析

到目前为止，我们一直在处理输出数据，这些数据是在特定的底层流程生成之后观察到的。描述了系统之后，自然的问题涉及到原因。温度取决于许多气象和地理因素，这些因素可以很容易观察到，也可以完全隐藏起来。时间序列中的季节性明显受到一年中的时间段的影响，但是异常值呢？

例如，我们在一个被称为冬季的地区发现了一个高峰。我们如何证明这一点？在一个简单的方法中，这可以被认为是一个噪声异常值，可以被过滤掉。但是，如果已经观察到，并且措施背后有一个基本事实(例如，各方都同意这不是错误)，我们应该假设存在一个**隐藏的**(或**潜在的**)原因。

这可能令人惊讶，但大多数更复杂的场景都有大量难以分析的潜在原因(有时称为**因素**)。总的来说，这是一个不错的条件，但是，正如我们将要讨论的，将它们包含在模型中以通过数据集了解它们的影响是很重要的。

另一方面，决定放弃所有未知元素意味着降低模型的预测能力，同时准确性成比例下降。因此，诊断分析的首要目标不一定是找出所有原因，而是列出可观察和可测量的要素(称为**因素**)，以及所有潜在的潜在要素(通常将其总结为单个全局要素)。

在某种程度上，诊断分析通常类似于逆向工程过程，因为我们可以轻松监控结果，但更难发现潜在原因和可观察结果之间的现有关系。因此，这种分析通常是概率性的，有助于发现某个确定的原因带来特定影响的概率。这样，也更容易排除非影响元素，并确定最初被排除的关系。然而，这个过程需要对统计学习方法有更深入的了解，除了几个例子，比如高斯混合，这本书不会讨论。

# 预测分析

一旦收集了总体描述性知识，并且对潜在原因的认识令人满意，就有可能创建预测模型。这些模型的目标是根据模型本身的历史和结构来推断未来的结果。在许多情况下，这个阶段与下一个阶段一起分析，因为我们很少对系统的*自由演化*感兴趣(例如，下个月温度将如何变化)，而是对我们影响输出的方式感兴趣。

也就是说，让我们只关注预测，考虑应该考虑的最重要的因素。首先要考虑的是过程的性质。对于确定性过程，我们不需要机器学习，除非它们的复杂性如此之高，以至于我们不得不将它们视为黑盒。我们将要讨论的绝大多数例子都是关于随机过程的，其中不确定性是无法消除的。例如，我们知道一天中的温度可以被建模为依赖于先前观察的条件概率(例如，高斯)。因此，预测并不是要把系统变成一个确定性的系统，这是不可能的，而是要减少分布的方差，这样，只有在很短的温度范围内，概率才会很高。另一方面，由于我们知道许多潜在因素在幕后起作用，我们永远不能接受基于尖峰分布的模型(例如，概率为 1 的单一结果)，因为这种选择会对最终精度产生可怕的负面影响。

如果我们的模型参数化了受学习过程影响的变量(例如高斯的均值和协方差矩阵)，我们的目标是在所谓的**偏差-方差权衡**中找到最佳平衡。由于这一章是介绍性的，我们不是用数学公式来形式化概念，而是需要一个实用的定义(进一步的细节可以在*博纳科尔索 g .**掌握机器学习算法，Packt，2018* 中找到)。

定义统计预测模型的常用术语是**估计量。**因此，估计量的**偏差是不正确假设和学习程序的可测量影响。换句话说，如果一个过程的平均值是 5.0，而我们的估计值的平均值是 3.0，我们可以说这个模型是有偏差的。考虑到前面的例子，如果观测值和预测值之间的误差的期望值不为零，我们就使用有偏估计器。重要的是要理解，我们并不是说每一个单独的估计都必须有零误差，但是在收集足够的样本并计算平均值的同时，它的值应该非常接近零(只有在无限个样本的情况下，它才能为零)。只要它大于零，就意味着我们的模型不能正确预测训练值。显而易见，我们正在寻找**无偏估计量**，平均而言，它能产生准确的预测。**

另一方面，估计器的**方差是在不属于训练集的样本存在的情况下鲁棒性的度量。在这一节的开始，我们说我们的过程通常是随机的。这意味着任何数据集都必须被视为来自特定的数据生成过程 *p <sub class="calibre20">数据</sub>T5。如果我们有足够多的代表性元素 *x <sub class="calibre20">i</sub> ∈ X* ，我们可以假设使用有限的数据集 *X* 训练一个分类器会得到一个能够对所有潜在样本进行分类的模型，这些样本可以从 *p <sub class="calibre20">数据</sub>* 中提取。***

例如，如果我们需要建模一个人脸分类器，它的上下文仅限于肖像(不允许进一步的人脸姿态)，我们可以收集不同个人的大量肖像。我们唯一关心的是不排除现实生活中可能存在的类别。假设我们有 10，000 张不同年龄和性别的个人照片，但我们没有任何带帽子的肖像。当系统生产时，我们接到客户的电话，说系统对许多图片进行了错误分类。经过分析，我们发现它们总是代表戴帽子的人。很明显，我们的模型不对错误负责，因为它是用只代表数据生成过程的一个区域的样本训练的。因此，为了解决这个问题，我们收集其他样本并重复训练过程。然而，现在我们决定使用一个更复杂的模型，期望它会工作得更好。不幸的是，我们观察到更差的验证精度(例如，在训练阶段没有使用的子集上的精度)，以及更高的训练精度。这里发生了什么？

当一个估计器学会了对训练集进行完美的分类，但是它对从未见过的样本的能力很差时，我们说它是**过度训练**，并且它的方差对于特定的任务来说太高(相反，**欠训练**模型有很大的偏差，所有的预测都非常不准确)。直觉上，模型对训练数据了解太多，已经失去了泛化能力。为了更好地理解这个概念，让我们看看高斯数据生成过程，如下图所示:

![](assets/ea94f42c-877d-4645-af58-f554aac0c9cb.png)

Original data generating process (solid line) and sampled data histogram

如果训练集没有以完全一致的方式进行采样，或者它是部分不平衡的(一些类比其他类具有更少的样本)，或者如果模型易于过度拟合，则结果可以用不准确的分布来表示，如下所示:

![](assets/4cf3f8d7-a098-4d74-a620-8bf091a7cde0.png)

Learned distribution

在这种情况下，模型被迫学习训练集的细节，直到它从分布中排除了许多潜在的样本。结果不再是高斯分布，而是双峰分布，其中一些概率错误地很低。当然，测试和验证集是从训练集没有覆盖的小区域中采样的(因为训练数据和验证数据之间没有重叠)，因此模型在提供完全不正确的结果时会失败。

换句话说，我们可以说方差太高，因为模型已经学会处理太多的细节，在合理的阈值上增加了不同分类的可能性范围。例如，肖像分类器可以了解到，戴蓝色眼镜的人在 30-40 岁的年龄范围内总是男性(这是一种不现实的情况，因为细节水平通常很低，但是，这有助于理解问题的本质)。

我们可以总结说，一个好的预测模型必须具有非常低的偏差和成比例的低方差。不幸的是，通常不可能有效地最小化这两种措施，所以必须接受一种折衷。

一个具有良好泛化能力的系统可能会有更高的偏差，因为它无法捕捉所有的细节。相反，高方差允许非常小的偏差，但是模型的能力几乎局限于训练集。在这本书里，我们不打算谈论分类器，但是你应该完全理解这些概念，以便始终意识到你在项目工作中可能遇到的不同行为。

# 规定性分析

这样做的主要目的是回答我如何影响系统的输出？为了避免混淆，最好将这一概念翻译成纯机器学习语言，因此问题可能是，获得特定输出需要哪些输入值？

如前一节所述，这个阶段通常与预测分析合并在一起，因为模型通常用于这两项任务。然而，在某些特定情况下，预测仅限于*零输入*演化(例如在温度示例中)，并且在规定阶段必须分析更复杂的模型。主要原因在于能够控制导致特定输出的所有原因。

有时，在没有必要的时候，它们只是被肤浅地分析。当原因不可控制时(例如，气象事件)，或者当包含全局潜在参数集更简单时，都可能发生这种情况。后一种选择在机器学习中非常常见，并且已经开发了许多算法来有效地处理潜在因素的存在(例如，电磁或奇异值分解推荐系统)。出于这个原因，我们没有关注这个特定的方面(这在系统理论中是极其重要的)，同时，我们隐含地假设我们的模型提供了研究由不同输入产生的许多可能输出的能力。

例如，在深度学习中，可以创建反向模型，生成输入空间的显著图，强制特定的输出类。考虑肖像分类器的例子，我们可能有兴趣发现哪些视觉元素影响类的输出。诊断分析通常是无效的，因为原因极其复杂，并且它们的水平太低(例如，轮廓的形状)。因此，逆模型可以通过显示不同几何区域的影响来帮助解决规定性问题。然而，完整的说明性分析超出了本书的范围，在许多情况下，这是不必要的，因此我们在接下来的章节中不考虑这样的步骤。现在我们来分析一下不同类型的机器学习算法。

# 机器学习算法的类型

在这一点上，我们可以简要介绍不同类型的机器学习，重点介绍它们的主要特点和差异。在接下来的章节中，我们将讨论非正式的定义，然后是更正式的定义。如果你不熟悉讨论中涉及的数学概念，可以跳过细节。然而，研究所有未知的理论元素是非常明智的，因为它们是理解下一章分析的概念的基础。

# 监督学习算法

在有监督的场景中，模型的任务是找到样本的正确标签，假设训练集的存在被正确标记，以及将估计值与正确值进行比较的可能性。术语**监督的**来源于外部*教学代理*的想法，该代理在每次预测后提供精确和即时的反馈。该模型可以使用这样的反馈作为误差的度量，并因此执行减少误差所需的校正。

更正式地说，如果我们假设一个数据生成过程，![](assets/ea0ccb2d-9b21-4eb1-b9c5-4bf2675f5c7c.png)数据集获得如下:

![](assets/c7e60603-2d54-4e9f-90b6-acbbb6fc56e5.png)

如前所述，所有样本必须是从数据生成过程中均匀采样的**独立且同分布的** ( **IID** )值。特别是所有的类都必须代表实际的分布(例如如果 *p(y=0) = 0.4* 和 *p(y=1) = 0.6* ，比例应该是 40%或者 60%)。然而，为了避免偏差，当类与类之间的差异不是很大时，一个合理的选择是完全均匀抽样，并且对于 *y=1，2，...，M* 。

通用分类器![](assets/91b8220c-e8df-436d-98be-2c519efcb45d.png)可以通过两种方式建模:

*   输出预测类的参数化函数
*   输出每个输入样本的类别概率的参数化概率分布

在第一种情况下，我们有:

![](assets/b9ecd7f8-cd99-4a23-873d-d2f72c6ac488.png)

考虑到整个数据集 *X* ，可以计算一个全局成本函数 *L* :

![](assets/dc48de69-6f80-43e3-97fc-a4f7d40387fd.png)

由于 *L* 仅依赖于参数向量(*x<sub class="calibre20">I</sub>T5】和*y<sub class="calibre20">I</sub>T9】是常数)，所以一般算法必须找到最小化成本函数的最优参数向量。例如，在**回归**问题中(其中标签是连续的)，误差度量可以是实际值和预测值之间的平方误差:**

![](assets/ee38c0ea-fb0c-48b1-abd7-30b4dd44be28.png)

这样的代价函数可以用不同的方式优化(特定算法特有的)，但是一个非常常见的策略(尤其是在深度学习中)是使用 S **到 S**(**SGD**)算法。它由以下两个步骤的迭代组成:

*   用一小批样本 *x <sub class="calibre20">i</sub> ∈ X* 计算梯度 *∇L* (相对于参数向量)
*   更新权重并在梯度 *-∇L* 的相反方向上移动参数(记住梯度总是指向最大值)

相反，当分类器是概率性的时，它应该被表示为参数化的条件概率分布:

![](assets/51b5190b-ed7a-4b3c-b116-c7cd47c406b7.png)

换句话说，分类器现在将输出给定输入向量的标签 *y* 的概率。现在的目标是找到最佳参数集，它将获得:

![](assets/58889db1-589d-45cd-b629-d434fb8549e3.png)

在前面的公式中，我们已经将 *p <sub class="calibre20">数据</sub>* 表示为条件分布。可以使用概率距离度量来获得优化，例如**库尔巴克-莱布勒散度 *D <sub class="calibre20">KL</sub>*** (仅当两个分布相同时，该距离度量总是非负的*D<sub class="calibre20">KL</sub>**≥**0*和*D<sub class="calibre20">KL</sub>*】T22 =*0*):

![](assets/604286e5-d4cf-4d2f-953c-00a6f76a49b7.png)

通过一些简单的操作，我们获得:

![](assets/ca76916e-73c4-443f-9d41-78f2b0ff0ff7.png)

因此，得到的成本函数对应于 *p* 和 *p <sub class="calibre20">数据</sub>* 之间的交叉熵差，直到一个恒定值(数据生成过程的熵)。因此，现在的训练策略是基于使用单热编码来表示标签(例如，如果有两个标签 *0 → (0，1)* 和 *1* *→ (1，0)* ，因此所有元素的总和必须始终等于 *1* )并使用内在概率输出(例如在逻辑回归中)或 softmax 滤波器，这将 *M* 值转换为概率分布。

在这两种情况下，很明显*隐藏教师*的存在提供了一致的误差度量，允许模型相应地校正参数。特别是第二种方法对我们的目的非常有帮助，因此如果不太了解的话，我建议进一步研究(所有主要定义也可以在*博纳科尔索 g .【机器学习算法】第二版，Packt，2018* 中找到)。

我们现在可以讨论监督学习的一个非常基本的例子，一个可以用来预测简单时间序列演化的线性回归模型。

# 监督你好世界！

在这个例子中，我们想展示如何用二维数据进行简单的线性回归。特别是，让我们假设我们有一个包含 100 个样本的自定义数据集，如下所示:

```
import numpy as np
import pandas as pd

T = np.expand_dims(np.linspace(0.0, 10.0, num=100), axis=1)
X = (T * np.random.uniform(1.0, 1.5, size=(100, 1))) + np.random.normal(0.0, 3.5, size=(100, 1))
df = pd.DataFrame(np.concatenate([T, X], axis=1), columns=['t', 'x'])
```

We have also created a pandas `DataFrame` because it's easier to create plots using the seaborn library ([https://seaborn.pydata.org](https://seaborn.pydata.org)). In the book, the code for the plots (using Matplotlib or seaborn) is normally omitted, but it's always present in the repository.

我们希望以合成的方式表示数据集，如下所示:

![](assets/b608917f-7446-460e-bc2c-f1de0b611761.png)

该任务可以使用线性回归算法来执行，如下所示:

```
from sklearn.linear_model import LinearRegression

lr = LinearRegression()
lr.fit(T, X)

print('x(t) = {0:.3f}t + {1:.3f}'.format(lr.coef_[0][0], lr.intercept_[0]))
```

最后一个命令的输出如下:

```
x(t) = 1.169t + 0.628
```

我们还可以通过绘制数据集和回归线来获得视觉确认，如下图所示:

![](assets/9b5a9033-bbee-4167-8d67-bb17822d7e1a.png)

Dataset and regression line

在这个例子中，回归算法最小化了平方误差成本函数，试图减少预测值和实际值之间的差异。由于对称分布，高斯(零均值)噪声的存在对斜率的影响最小。

# 无监督学习算法

在无监督的情况下，很容易想象，没有隐藏的老师，因此主要目标不能与最小化关于地面事实的预测误差相关。事实上，在这种情况下，相同的基本真理概念有着稍微不同的含义。事实上，当使用分类器时，我们希望训练样本有一个空错误(这意味着除了真正的类之外，其他类永远不会被认为是正确的)。

相反，在无监督的问题中，我们希望模型在没有任何正式指示的情况下学习一些信息。这个条件意味着，唯一可以学习的元素是包含在样本本身中的元素。因此，无监督算法通常旨在发现样本之间的相似性和模式，或者在给定一组向量的情况下再现输入分布。现在我们来分析一些最常见的无监督模型类别。

# 聚类分析

**聚类分析**(通常只称为**聚类**)是一个任务的例子，在这个任务中，我们想要找出大样本集中的共同特征。在这种情况下，我们总是假设存在数据生成过程![](assets/a5fd4b73-81fb-4a6a-b653-25f369c85104.png)，并且我们将数据集 *X* 定义为:

![](assets/6a44d736-7aea-47d8-b3b7-5f48d10e36e5.png)

聚类算法基于隐含的假设，即样本可以根据它们的相似性进行分组。特别地，给定两个向量，相似性函数被定义为度量函数的倒数或倒数。例如，如果我们在欧几里得空间中工作，我们有:

![](assets/a5e03fb9-969b-4595-aaaa-2c2777cbc5db.png)

在前面的公式中，引入了常数 *ε* 来避免被零除。很明显 *d(a，c) < d(a，b)s(a，c) > s(a，b)* 。因此，给定每个聚类的代表![](assets/fba84550-7fae-4228-ad4f-02618cc35ca1.png)，我们可以考虑以下规则来创建分配的向量集:

![](assets/57dee268-a1df-4c52-a95e-fa6fa1a320fb.png)

换句话说，一个集群包含所有那些与代表的距离与所有其他代表相比最小的元素。这意味着，与所有代表相比，一个聚类包含的样本与代表的相似度最大。此外，在分配之后，样本获得*权利*以与同一聚类的其他成员共享其特征。

事实上，聚类分析最重要的应用之一是试图增加被认为相似的样本的同质性。例如，推荐引擎可以基于用户向量的聚类(包含关于他们的兴趣和购买的产品的信息)。一旦定义了组，属于同一集群的所有元素都被认为是相似的，因此我们被隐式授权*共享差异*。如果用户 *A* 购买了产品 *P* 并给予肯定评价，我们可以向没有购买的用户 *B* 建议该商品，反之亦然。这个过程看起来可以是任意的，但是当元素的数量很大，并且特征向量包含许多有区别的元素(例如，等级)时，它会非常有效。

# 生成模型

另一种无监督的方法是基于**生成模型**。这个概念与我们已经讨论过的监督算法没有太大区别，但是，在这种情况下，数据生成过程不包含任何标签。因此，目标是对参数化分布进行建模并优化参数，从而使候选分布和数据生成过程之间的距离最小化:

![](assets/eef6988b-07e0-4944-8189-24758eb9798d.png)

该过程通常基于库尔巴克-莱布勒散度或其他类似的度量:

![](assets/e2d7b236-ba9f-4a7f-8914-a0a34d24aac5.png)

训练阶段结束，我们假设 *L → 0* ，那么 *p ≈ p <sub class="calibre20">数据</sub>T5。通过这种方式，我们没有将分析局限于可能样本的子集，而是局限于整个分布。使用生成模型允许您绘制新的样本，这些样本可能与为训练过程选择的样本非常不同，但它们总是属于相同的分布。因此，它们(可能)总是可以接受的。*

例如，**生成对抗网络** ( **GAN** )是一种特定的深度学习模型，它能够学习图像集的分布，产生与训练样本几乎无法区分的新样本(从视觉语义的角度来看)。由于无监督学习是本书的主要主题，因此我们在此介绍中不会进一步详述 GAN。所有这些概念都将在接下来的章节中进行广泛的讨论(用实际的例子)。

# 关联规则

我们考虑的最后一种无监督方法是基于**关联规则**的发现，它在数据挖掘领域极其重要。一个常见的场景是由产品子集组成的商业交易集合。目标是找出产品之间最重要的关联(例如购买*P<sub class="calibre20">I</sub>T5】和*P<sub class="calibre20">j</sub>T9】的概率为 70%)。特定的算法可以高效地挖掘整个数据库，突出显示出于战略和物流目的可以考虑的所有关系。例如，网上商店可以采用这种方法来促销那些经常与其他商品一起购买的商品。此外，预测方法允许通过建议所有那些由于其他项目的销售增加而很可能售罄的产品来简化供应过程。**

在这一点上，向读者介绍一个无监督学习的实际例子是有帮助的。不需要特别的先决条件，但最好有概率论的基础知识。

# 无人监管的 hello world！

由于这本书完全致力于无监督算法，我决定不把一个简单的聚类分析展示为 hello world！例子，而是一个相当基本的生成模型。让我们假设我们正在监控每小时到达地铁站的列车数量，因为我们需要确定车站所需的安全代理数量。特别是，我们被要求每列火车至少有一名代理人，每当人数较少时，我们将支付罚款。

此外，更容易在每个小时开始时发送一个组，而不是逐个控制代理。因为问题很简单，我们也知道好的分布是泊松分布，用 *μ* 参数化，也是均值。从理论上，我们知道，在独立性的主要假设下，这样的分布可以有效地模拟在固定时间框架内发生的事件的随机数。在一般情况下，生成模型基于参数化分布(例如，具有神经网络)，并且没有对其族做出具体假设。只有在某些特定的情况下(例如，高斯混合)，选择具有特定属性的分布才是合理的，并且在不损失严格性的情况下，我们可以将此示例视为这样的场景。

泊松分布的概率质量函数为:

![](assets/5e23a070-77de-4ec1-aea3-2e2e0d937c01.png)

该分布描述了在预定间隔内观察到 *k* 事件的概率。在我们的例子中，间隔总是一个小时，我们热衷于估计观察超过 10 列火车的概率。如何获得 *μ* 的正确数字？

最常见的策略叫做**最大似然估计** ( **MLE** )。它收集一组观察值，并找到 *μ* 的值，该值最大化了所有点都由我们的分布生成的概率。

假设我们已经收集了 *N* 个观测值(每个观测值是一小时内到达的次数) *μ* 相对于所有样本的**似然度**是所有样本(为简单起见，假设为 IID)在使用 *μ* 计算的概率分布下的联合概率:

![](assets/f6dfcfb2-07d7-4e1c-b2c1-ec884ce245f3.png)

当我们处理乘积和指数时，计算**对数似然**是一个常见的规则:

![](assets/245700f9-7ebf-4b2c-8738-815a9780be8d.png)

一旦计算出对数似然性，就可以将相对于 *μ* 的导数设置为 0，以便找到最佳值。在这种情况下，我们省略了证明(这很容易获得)，直接得出 *μ* 的最大似然估计:

![](assets/bf118ff0-c5b5-45b8-8f26-88f41e0c8868.png)

我们很幸运！最大似然估计只是到达时间的平均值。这意味着，如果我们用平均值 *μ* 观察到 *N* 个值，那么最有可能产生它们的泊松分布将 *μ* 作为特征系数。因此，从这种分布中提取的任何其他样本将与观察到的数据集*兼容*。

我们现在可以从第一次模拟开始。假设我们在一个工作日的下午早些时候收集了 25 个观察结果，如下所示:

```
import numpy as np

obs = np.array([7, 11, 9, 9, 8, 11, 9, 9, 8, 7, 11, 8, 9, 9, 11, 7, 10, 9, 10, 9, 7, 8, 9, 10, 13])
mu = np.mean(obs)

print('mu = {}'.format(mu))
```

最后一个命令的输出如下:

```
mu = 9.12
```

因此，我们平均每小时有 9 列火车到达。直方图如下图所示:

![](assets/d3601318-982b-423d-9fd1-60bac72c0c66.png)

Histogram of the initial distribution

为了计算所请求的概率，我们需要使用**累积分布函数** ( **CDF** )，该函数在 SciPy(在`scipy.stats`包中)中实现。特别是，由于我们感兴趣的是观察到比固定值多的列车的概率，所以有必要使用**生存函数** ( **SF** )，对应于 *1-CDF* ，如下:

```
from scipy.stats import poisson

print('P(more than 8 trains) = {}'.format(poisson.sf(8, mu)))
print('P(more than 9 trains) = {}'.format(poisson.sf(9, mu)))
print('P(more than 10 trains) = {}'.format(poisson.sf(10, mu)))
print('P(more than 11 trains) = {}'.format(poisson.sf(11, mu)))
```

前面片段的输出如下:

```
P(more than 8 trains) = 0.5600494497386543
P(more than 9 trains) = 0.42839824517059516
P(more than 10 trains) = 0.30833234660452563
P(more than 11 trains) = 0.20878680161156604
```

不出所料，观察 10 列以上的概率很低(30%)，派 10 个特工似乎不太合理。然而，由于我们的模型是自适应的，我们可以继续收集观察结果(例如，在清晨)，如下所示:

```
new_obs = np.array([13, 14, 11, 10, 11, 13, 13, 9, 11, 14, 12, 11, 12, 14, 8, 13, 10, 14, 12, 13, 10, 9, 14, 13, 11, 14, 13, 14])

obs = np.concatenate([obs, new_obs])
mu = np.mean(obs)

print('mu = {}'.format(mu))
```

*μ* 的新值如下:

```
mu = 10.641509433962264
```

现在平均差不多每小时 11 趟火车。假设我们已经收集了足够的样本(考虑到所有潜在的事故)，我们可以重新估计概率，如下所示:

```
print('P(more than 8 trains) = {}'.format(poisson.sf(8, mu)))
print('P(more than 9 trains) = {}'.format(poisson.sf(9, mu)))
print('P(more than 10 trains) = {}'.format(poisson.sf(10, mu)))
print('P(more than 11 trains) = {}'.format(poisson.sf(11, mu)))
```

输出如下:

```
P(more than 8 trains) = 0.7346243910180037
P(more than 9 trains) = 0.6193541369812121
P(more than 10 trains) = 0.49668918740243756
P(more than 11 trains) = 0.3780218948425254
```

有了新的数据集，观察到 9 列以上的概率约为 62%(这证实了我们最初的选择)，但现在观察到 10 列以上的概率约为 50%。由于我们不想冒险支付罚款(这比代理的成本高)，所以最好总是派出一组 10 名代理。为了得到进一步的确认，我们决定从分布中抽取 2，000 个值，如下所示:

```
syn = poisson.rvs(mu, size=2000)
```

相应的直方图如下图所示:

![](assets/2153751f-478c-4469-9d35-d23adfac4ba5.png)

Histogram of 2000 points sampled from the final Poisson distribution

该图确认了 10 之后的一个峰值(非常接近 11)和从 *k = 13* 开始的快速衰减，这已经使用有限的数据集被发现(比较直方图的形状以进一步确认)。然而，在这种情况下，我们生成的潜在样本无法出现在我们的观察集中。最大似然估计保证概率分布与数据一致，并保证新样本得到相应的加权。这个例子显然非常简单，它的目标只是展示一个生成模型的动态。

我们将在本书的下几章讨论许多其他更复杂的模型和例子。许多算法共有的一项重要技术在于不选择预定义的分布(这意味着先验知识)，而是使用灵活的参数模型(例如，神经网络)来找出最优分布。只有当潜在的随机过程具有很高的置信度时，预定义先验的选择才是合理的。在所有其他情况下，最好避免任何假设，只依靠数据来找到数据生成过程的最合适的近似值。

# 半监督学习算法

半监督场景可以被认为是标准的监督场景，其利用了属于无监督学习技术的一些特征。事实上，当很容易获得大的未标记数据集但标记成本很高时，就会出现一个非常常见的问题。因此，合理的做法是只标记一小部分样本，并将标记传播到所有与标记样本的距离低于预定阈值的未标记样本。如果数据集是从单个数据生成过程中提取的，并且标记样本是均匀分布的，则半监督算法可以获得与监督算法相当的精度。在这本书里，我们不讨论这些算法；然而，简要介绍两个非常重要的模型是有帮助的:

*   标签传播
*   半监督支持向量机

第一种称为**标签传播**，其目标是将少数样本的标签传播到更大的群体中。这个目标是通过考虑一个图来实现的，其中每个顶点代表一个样本，每个边都使用距离函数进行加权。通过一个迭代过程，所有标记的样本将把它们的标记值的一小部分发送给它们的所有邻居，并且重复这个过程，直到标记停止改变。这个系统有一个稳定点(也就是说，一个不能再进化的配置)，算法可以通过有限的迭代次数很容易地达到它。

标签传播在所有那些可以根据相似性度量来标记一些样本的环境中非常有用。例如，一家在线商店可能有大量客户，但只有 10%的人透露了他们的性别。如果特征向量足够丰富，能够代表男性和女性用户的共同行为，那么就有可能采用标签传播算法来猜测没有透露的客户的性别。当然，重要的是要记住，所有的作业都是基于相似样本具有相同标签的假设。这在许多情况下可能是正确的，但是当特征向量的复杂性增加时，这也可能是误导。

另一个重要的半监督算法家族是基于标准 **SVM** (简称**支持向量机**)对包含未标记样本的数据集的扩展。在这种情况下，我们不想传播现有的标签，而是传播分类标准。换句话说，我们希望使用已标记的数据集训练分类器，并将判别规则扩展到未标记的样本。

与只能评估未标记样本的标准过程相反，半监督 SVM 使用它们来校正分离超平面。假设总是基于相似性:如果 *A* 有标签 *1* ，而未标签样本 *B* 有 *d(A，B)* < ε(其中 *ε* 是预定义的阈值)，那么合理的假设是 *B* 的标签也是 *1* 。这样，即使只有一个子集被手动标记，分类器也可以在整个数据集上获得高精度。类似于标签传播，只有当数据集的结构不是非常复杂时，特别是当相似性假设成立时，这些类型的模型才是可靠的(不幸的是，在某些情况下，找到合适的距离度量非常困难，因此许多相似的样本确实不相似，反之亦然)。

# 强化学习算法

强化学习场景可以被认为是一个有监督的场景，其中隐藏的老师在模型的每个决策之后只提供近似的反馈。更正式地说，强化学习的特征是代理和环境之间的持续交互。前者负责做出决策(行动)，最终确定以增加回报，而后者为每一项行动提供反馈。反馈通常被认为是一种奖励，它的价值可以是积极的(行动已经成功)，也可以是消极的(行动不应该重复)。当代理分析环境(状态)的不同配置时，每个奖励都必须被视为绑定到元组(动作、状态)。因此，最终目标是找到一种策略(一种在每个州都建议最佳行动的策略)，使预期的总回报最大化。

强化学习的一个非常经典的例子是学习如何玩游戏的代理。在一集里，代理测试所有遇到的状态下的动作并收集奖励。一种算法修正了策略，以减少非积极行为(即那些奖励为积极的行为)的可能性，并增加在剧集结束时可获得的预期总奖励。

强化学习有很多有趣的应用，不仅限于游戏。例如，推荐系统可以根据用户提供的二元反馈(例如，拇指向上或向下)来校正建议。强化学习和监督学习的主要区别在于环境提供的信息。事实上，在有监督的情况下，校正通常与它成比例，而在强化学习中，必须考虑一系列动作和未来的奖励来分析它。因此，修正通常基于对预期报酬的估计，其效果受后续行动价值的影响。例如，一个监督模型没有记忆，因此它的校正是立即的，而一个强化学习代理必须考虑一集的部分展开，以便决定一个动作实际上是否是负面的。

强化学习是机器学习的一个迷人的分支。遗憾的是，本主题超出了本作品的范围，因此我们不再详细讨论(您可以在*用 Python 进行强化学习的实践，Ravichandiran S，Packt Publishing，* 2018 和*掌握机器学习算法，Bonaccorso G，Packt Publishing，* 2018 中找到进一步的细节)。

我们现在可以简要解释为什么选择 Python 作为探索无监督学习世界的主要语言。

# 为什么 Python 用于数据科学和机器学习？

在继续更多的技术讨论之前，我认为解释选择 Python 作为本书的编程语言是有帮助的。在过去的十年里，数据科学和机器学习领域的研究呈指数级增长，有数千篇有价值的论文和几十个完整的工具。特别是，由于其高效、优雅和紧凑，Python 被许多研究人员和程序员选择来创建一个完整的科学生态系统，该生态系统已经免费发布。

如今，像 scikit-learn、SciPy、NumPy、Matplotlib、pandas 和许多其他软件包代表了数百个生产就绪系统的主干，它们的使用量一直在增长。此外，复杂的深度学习应用程序，如 Anano、TensorFlow 和 PyTorch，允许每个 Python 用户创建和训练复杂的模型，而没有任何速度限制。事实上，需要注意的是 Python 不再是脚本语言了。它支持几十种特定的任务(例如，网络框架和图形)，并且可以与用 C 或 C++编写的本机代码接口。

由于这些原因，Python 几乎是任何数据科学项目中的最佳选择，由于它的特性，所有不同背景的程序员都可以在短时间内轻松学会有效使用它。其他免费解决方案也是可用的(例如，R、Java 或 Scala)，然而，在 R 的情况下，有统计和数学函数的完整覆盖，但它缺乏构建完整应用程序所必需的支持框架。相反，Java 和 Scala 有一个完整的生产就绪库生态系统，但是，特别是，Java 不像 Python 那样紧凑和易于使用。此外，对本机代码的支持要复杂得多，并且大多数库都完全依赖于 JVM(从而导致性能损失)。

Scala 在大数据全景图中获得了重要的地位，这得益于它的功能属性以及 Apache Spark(可用于利用大数据执行机器学习任务)等框架的存在。然而，考虑到所有的利弊，Python 仍然是最佳选择，这就是为什么它被选为这本书的原因。

# 摘要

在本章中，我们讨论了使用机器学习模型的主要原因，以及如何分析数据集以描述其特征、列举特定行为背后的原因、预测未来行为并影响它。

我们还探讨了有监督、无监督、半监督和强化学习之间的区别，重点是前两种模型。我们还使用了两个简单的例子来理解监督和非监督方法。

在下一章中，我们将介绍聚类分析的基本概念，重点讨论一些非常著名的算法，如 k-means 和**K-近邻** ( **KNN** )以及最重要的评估指标。

# 问题

1.  当监督学习不适用时，无监督学习是最常见的选择。正确吗？
2.  你公司的首席执行官要求你找出决定消极销售趋势的因素。需要进行什么样的分析？

3.  给定独立样本数据集和候选数据生成过程(例如，高斯分布)，通过对所有样本的概率求和来获得似然性。是正确的吗？
4.  在哪个假设下，可能性可以被计算为单一概率的乘积？
5.  假设我们有一个包含一些未知数字特征(例如年龄、分数等)的学生数据集。你想把男生和女生分开，所以你决定把数据集分成两组。不幸的是，这两个集群都有大约 50%的男生和 50%的女生。你如何解释这个结果？
6.  考虑前面的例子，但是重复实验并分成五组。你期望在他们每个人身上找到什么？(列举一些合理的可能性。)
7.  你聚集了一家在线商店的顾客。给定一个新样本，你能做出什么样的预测？

# 进一步阅读

*   *机器学习算法第二版*，*博纳科索格*，*帕克特出版*，2018
*   *Python 强化学习实操**ravichanduran s .**Packt**出版*，2018
*   *与 NumPy 和熊猫的实践数据分析*，*米勒 C.* ，*帕克特出版*，2018