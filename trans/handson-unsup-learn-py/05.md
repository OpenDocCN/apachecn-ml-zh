# 软聚类和高斯混合模型

在本章中，我们将讨论软聚类的概念，它允许我们获得数据集的每个样本相对于定义的聚类配置的隶属度。也就是说，考虑从 0%到 100%的范围，我们想知道*x<sub class="calibre20">I</sub>T3 在多大程度上属于一个集群。极值为 0，表示 *x <sub class="calibre20">i</sub>* 完全在集群的域外，为 1 (100%)，表示 *x <sub class="calibre20">i</sub>* 完全分配给单个集群。所有中间值都意味着两个或多个不同簇的部分区域。因此，与硬聚类相反，在这里，我们感兴趣的不是确定一个固定的赋值，而是一个具有概率分布(或概率本身)相同属性的向量。这种方法允许对边界样本进行更好的控制，并帮助我们找到生成过程的合适近似，数据集应该从该生成过程中提取。*

特别是，我们将讨论以下主题:

*   模糊 C 均值
*   高斯混合
*   AIC 和 BIC 作为绩效指标
*   贝叶斯高斯混合(简介)
*   生成(半监督)高斯混合

# 技术要求

本章将介绍的代码需要以下内容:

*   Python 3.5+ ( [Anaconda 发行版](https://www.anaconda.com/download/)强烈推荐)
*   以下库:
    *   SciPy 0.19+
    *   NumPy 1.10+
    *   学习 0.20+
    *   Scikit-fuzzy 0.2
    *   熊猫 0.22+
    *   Matplotlib 2.0+
    *   seaborn 0.9+

这些例子可以在 GitHub 资源库[中找到，网址为 https://GitHub . com/packktpublishing/HandsOn-Unsupervised-Learning-with-Python/tree/master/chapter 05](https://github.com/PacktPublishing/HandsOn-Unsupervised-Learning-with-Python/tree/master/Chapter05)。

# 软聚类

在[第 4 章](04.html)、*行动中的层次聚类*中分析的所有算法都属于硬聚类方法家族。这意味着一个给定的样本总是被分配给一个单独的聚类。另一方面，软聚类的目的是将每个样本， *x <sub class="calibre20">i</sub>* 与一个向量相关联，一般表示 *x <sub class="calibre20">i</sub>* 属于每个聚类的概率:

![](assets/48b059fe-49bc-493e-bcfb-7be6d9ac7576.png)

或者，输出可以解释为隶属向量:

![](assets/98a661ea-5050-4611-a22e-ffac81d0c972.png)

在形式上，这两个版本没有区别，但通常情况下，当算法不是明确基于概率分布时，采用后者。然而，出于我们的目的，我们总是将 *c(x <sub class="calibre20">i</sub> )* 与概率联系起来。这样，读者就会被激励去思考用来获取数据集的数据生成过程。一个明显的例子是将这些向量解释为与构成数据生成过程近似值的特定贡献相关的概率， *p <sub class="calibre20">data</sub>* 。例如，采用概率混合，我们可以决定近似 *p <sub class="calibre20">数据</sub>* 如下:

![](assets/5f60fae7-4542-4bef-9be1-9337d97bd7be.png)

因此，该过程被分解成(独立的)分量的加权和，输出是每一个分量的 *x <sub class="calibre20">i</sub>* 的概率。当然，我们通常期望每个样本都有一个主导成分，但是通过这种方法，我们对所有边界点都有很大的了解，这些边界点会受到小的扰动，可以分配给不同的聚类。由于这个原因，当输出可以被馈送到另一个可以利用整个概率向量的模型(例如，神经网络)中时，软聚类非常有帮助。例如，推荐者可以首先使用软聚类算法来分割用户，然后处理向量，以便基于显式反馈找到更复杂的关系。一个常见的场景是通过回答诸如“这个结果与你相关吗？”或者“你想看到更多像这样的结果吗？”因为答案是由用户直接提供的，所以它们可以被用在监督或强化学习模型中，该模型的输入基于软自动分割(例如，基于购买历史或详细的页面视图)。通过这种方式，可以通过改变原始分配的效果来轻松管理边缘用户(由于不同集群提供的大量贡献，这可能完全不重要)，同时可以稍微修改对其他具有强成员资格(例如，概率接近 1)的用户的推荐，以提高他们的回报。

我们现在可以开始讨论 Fuzzy c-means，这是一种非常灵活的算法，它将针对 k-means 讨论的概念扩展到软聚类场景。

# 模糊 c 均值

我们将提出的第一个算法是基于软分配的 k-means 的变体。 **Fuzzy c-means** 这个名字来源于一个模糊集的概念，它是经典二进制集(即在这种情况下，一个样本可以属于一个单一的聚类)基于代表整个集合不同区域的不同子集的叠加而扩展到集合。例如，基于一些用户年龄的集合可以具有与三个不同(并且部分重叠)年龄范围相关联的程度`young`、`adult`和`senior`:18-35 岁、28-60 岁和> 50 岁。因此，例如，一个 30 岁的用户在不同程度上既是`young`又是`adult`(考虑到界限，确实是一个边缘用户)。关于这类集合和所有相关运算的更多细节，我建议阅读《概念和模糊逻辑》一书，麻省理工学院出版社，2011 年。出于我们的目的，我们可以将包含 *m* 个样本的数据集 *X* 划分为 *k* 个重叠的聚类，这样每个样本总是与每个聚类相关联，根据隶属度，*w<sub class="calibre20">ij</sub>T18】(该值绑定在 0 和 1 之间)。如果 *w <sub class="calibre20">ij</sub> = 0* ，则表示 *x <sub class="calibre20">i</sub>* 完全在集群*C<sub class="calibre20">j</sub>T30】之外，反之，则*w<sub class="calibre20">ij</sub>*T35】= 1*表示对集群*C<sub class="calibre20">j</sub>T40】的硬分配。所有中间值代表部分成员资格。当然，由于显而易见的原因，样本的所有隶属度之和必须归一化为 1(类似于概率分布)。这样，一个样本总是属于所有聚类的并集，而将一个聚类分成两个或更多的子聚类总是会产生一个连贯的结果，就成员而言。**

该算法基于广义惯性的优化，*S<sub class="calibre20">f</sub>T3:*

![](assets/022da733-107b-4523-a724-b2bdc04c1431.png)

在之前的公式中，*μ<sub class="calibre20">j</sub>T3】是星团*C<sub class="calibre20">j</sub>T7】的质心，而 *m (m > 1)* 是重加权指数系数。当 *m ≈ 1* 时，重量不受影响。数值越大，如*w<sub class="calibre20">ij</sub>∑(0，1)* ，其重要性成比例降低。可以选择这样的系数来比较不同值的结果和期望的模糊程度。事实上，在每次迭代之后(完全等同于 k 均值)，使用以下公式更新权重:**

![](assets/d1acd98b-e3aa-42e7-9bc8-4a9a58ecf622.png)

如果 *x <sub class="calibre20">i</sub>* 接近质心， *μ <sub class="calibre20">j</sub>* ，则和变得接近 0，权重递增(当然，为了避免数值不稳定，在分母上加了一个小常数，所以永远不能等于 0)。当 *m > > 1* 时，指数变得接近 0，总和中的所有项都趋于 1。这意味着对特定集群的偏好减弱，并且 *w <sub class="calibre20">ij</sub> ≈ 1/k* 对应于均匀分布。因此，更大的 *m* 意味着更平坦的划分，在不同的分配之间没有明显的差异(除非样本非常接近质心)，而当 *m ≈ 1* 时，单个主导权重将几乎等于 1，而其他权重将接近 0(即分配是困难的)。

质心以类似于 k 均值的方式更新(换句话说，目标是最大化分离和内部凝聚力):

![](assets/3c071b20-0d4e-406c-aac8-b19c295a59fa.png)

重复该过程，直到质心和权重变得稳定。收敛后，可以用一个特定的度量来评估结果，称为归一化的**邓恩分割系数**，定义如下:

![](assets/e01641d3-1ade-4661-8e50-03a1a1e55628.png)

这样的系数被限制在 0 和 1 之间。当 *P <sub class="calibre20">C</sub> ≈ 0* 时，表示 *w <sub class="calibre20">C</sub>* *≈ 1/k* ，暗示分布平坦，模糊程度高。另一方面，当 *P <sub class="calibre20">C</sub> ≈ 1* 时，则 *w <sub class="calibre20">C</sub> ≈ 1* ，表示几乎是硬性作业。所有其他值都与模糊程度成正比。因此，给定一项任务，数据科学家可以根据期望的结果立即评估算法的执行情况。在某些情况下，硬分配更可取，因此， *P <sub class="calibre20">C</sub>* 可以被认为是在切换到例如标准 k 均值之前要执行的检查。其实当 *P <sub class="calibre20">C</sub> ≈ 1* (而且这样的结果是预期的结果)的时候，再用 Fuzzy c-means 就没有意义了。相反，小于 1 的值(例如， *P <sub class="calibre20">C</sub> = 0.5* )告知我们，由于存在许多边界样本，可能会有非常不稳定的硬性分配。

现在，让我们将模糊 c 均值算法应用于 scikit-learn 提供的简化 MNIST 数据集。该算法由 Scikit-Fuzzy 库提供，它实现了所有最重要的模糊逻辑模型。第一步是加载和标准化样本，如下所示:

```py
from sklearn.datasets import load_digits

digits = load_digits()
X = digits['data'] / 255.0
Y = digits['target']
```

`X`数组包含 1，797 个展平样本， *x ∈ ℜ <sup class="calibre27">64</sup>* ，对应灰度 *8 × 8* 图像(其值在 0 和 1 之间归一化)。我们要分析不同 *m* 系数(1.05 到 1.5 之间的 5 个统一值)的行为，并检查样本的权重(在我们的例子中，我们将使用`X[0]`)。因此，我们调用 Scikit-Fuzzy `cmeans`函数，设置`c=10`(簇的数量)和两个收敛参数，`error=1e-6`和`maxiter=20000`。而且，出于重现性的原因，我们也会随机设置一个标准`seed=1000`。输入数组应包含列形式的样本；因此，我们需要转置它，如下所示:

```py
from skfuzzy.cluster import cmeans

Ws = []
pcs = []

for m in np.linspace(1.05, 1.5, 5):
    fc, W, _, _, _, _, pc = cmeans(X.T, c=10, m=m, error=1e-6, maxiter=20000, seed=1000)
    Ws.append(W)
    pcs.append(pc)
```

前面的片段执行不同类型的聚类，并将相应的权重矩阵`W`和划分系数`pc`附加到两个列表中。在分析特定配置之前，显示测试样本(代表数字 0)的最终重量(对应于每个数字)会很有帮助:

![](assets/3db4c002-6803-4b02-9312-638929fe1d3e.png)

Weights (in an inverse logarithmic scale) for the sample X[0], corresponding to different m values

由于极值往往差异很大，我们选择了使用对数反比标度(即 *-log(w <sub class="calibre20">0j</sub> )* 而不是 *w <sub class="calibre20">0</sub> <sub class="calibre20">j</sub>* )。当 *m = 1.05* 、 *P <sub class="calibre20">C</sub>* 约为 0.96，所有重量(除了与 *C <sub class="calibre20">2</sub>* 对应的重量)都很小时(记住如果 *-log(w) = 30* ，那么 *w = e <sup class="calibre27">-30</sup>* )。这样的配置清楚地显示了具有主导成分的非常硬的聚类( *C <sub class="calibre20">2</sub>* )。上图后续三个地块继续呈现优势，但是，当 *m* 增加(而*P<sub class="calibre20">C</sub>T35】减少)时，优势成分和次优势成分的差异越来越小。这种效果证实了模糊度的增加，对于 *m > 1.38* 达到最大值。事实上，当 *m = 1.5* 时，即使 *P <sub class="calibre20">C</sub> ≈ 0.1* ，所有的权重几乎是相同的，测试样本也不容易被分配到优势聚类。正如我们之前所讨论的，我们现在知道像 k-means 这样的算法可以很容易地找到硬划分，因为平均来说，对应于不同数字的样本彼此之间有很大的不同，并且欧几里德距离足以将它们分配到正确的质心。在这个例子中，我们希望保持适度的模糊性；因此，我们选择了 *m = 1.2* (对应*P**<sub class="calibre20">C</sub>**≈0.73*):*

```py
fc, W, _, _, _, _, pc = cmeans(X.T, c=10, m=1.2, error=1e-6, maxiter=20000, seed=1000)
Mu = fc.reshape((10, 8, 8)) 
```

`Mu`数组包含质心，如下图所示:

![](assets/613fe7ab-800d-4d83-ba8f-f92841044c5b.png)

Centroids corresponding to m = 1.2 and P<sub class="calibre26">C</sub> ≈ 0.73

可以看到，所有不同的数字都被选中，不出所料，第三组(由*C<sub class="calibre20">2</sub>T5】表示)对应数字 0。现在我们来检查一下`X[0]`对应的权重(同样， *W* 是换位的，所以存储在`W[:, 0]`中):*

```py
print(W[:, 0])
```

输出如下:

```py
[2.68474857e-05 9.14566391e-06 9.99579876e-01 7.56684450e-06
 1.52365944e-05 7.26653414e-06 3.66562441e-05 2.09198951e-05
 2.52320741e-04 4.41638611e-05]
```

即使作业不是特别辛苦，集群*C*<sub class="calibre20">T5【2】T6</sub>的统治力还是显而易见的。第二个潜在赋值是 *C <sub class="calibre20">8</sub>* ，对应数字 9(比例约为 4000)。这样的结果与数字的形状绝对一致，而且考虑到最大重量和第二个重量之间的差异，很明显大多数样本几乎不会被赋值(也就是说，就像在 k 均值中一样)，即使有 *P <sub class="calibre20">C</sub> ≈ 0.75* 。为了检查硬分配(使用权重矩阵上的`argmax`函数获得)的性能，并考虑到我们知道基本事实，我们可以使用`adjusted_rand_score`，如下所示:

```py
from sklearn.metrics import adjusted_rand_score

Y_pred = np.argmax(W.T, axis=1)

print(adjusted_rand_score(Y, Y_pred))
```

上一个片段的输出如下:

```py
0.6574291419247339
```

这样的值证实了大多数样本已经被成功硬分配。作为补充练习，让我们找出权重标准差最小的样本:

```py
im = np.argmin(np.std(W.T, axis=1))

print(im)
print(Y[im])
print(W[:, im])
```

输出如下:

```py
414
8
[0.09956437 0.05777962 0.19350572 0.01874303 0.15952518 0.04650815
 0.05909216 0.12910096 0.17526108 0.06091973]
```

样本*X【414】*代表一个数字(8)，如下图截图所示:

![](assets/a35690c9-1b3e-4f02-a28d-69b704e008ec.png)

Plot of the sample, X[414], corresponding to the weight vector with the smallest standard deviation

在这种情况下，有三个优势集群: *C <sub class="calibre20">8</sub>* 、 *C <sub class="calibre20">4</sub>* 、 *C <sub class="calibre20">7</sub>* (按降序排列)。不幸的是，它们都不对应数字 8，数字 8 与*C<sub class="calibre20">5</sub>T15 相关联。不难理解，这样的错误主要是由于数字下半部分的畸形，这产生了更类似于 9 的结果(这样的错误分类也可能发生在人类身上)。然而，低标准偏差和缺乏明显的主导成分应该告诉我们，决策不容易做出，样本具有属于三个主要类别的特征。一个更复杂的监督模型可以很容易地避免这个错误，但是考虑到我们正在执行一个无监督的分析，并且我们仅仅使用基础事实来进行评估，结果并不是那么负面。我建议你用其他 *m* 值来检验结果，并尝试找出一些潜在的组成规则(即 8 个数字的大部分被柔和地赋给 *C <sub class="calibre20">i</sub>* 和 *C <sub class="calibre20">j</sub>* ，这样我们就可以假设对应的质心编码了部分共有的特征，例如被所有 8 个和 9 个数字共享)。*

我们现在可以讨论高斯混合的概念，这是一种非常广泛使用的建模数据集分布的方法，其特征是由低密度区域包围的密集斑点。

# 高斯混合

**高斯混合**是最著名的软聚类方法之一，有几十种具体的应用。它可以被认为是 k-means 之父，因为它的工作方式非常相似；但是，与该算法相反，给定样本 *x <sub class="calibre20">i</sub> ∈ X* 和 *k* 聚类(其被表示为高斯分布)，它提供概率向量*【p(X<sub class="calibre20">I</sub>**∈C<sub class="calibre20">1</sub>)，...，p(x<sub class="calibre20">I</sub>∈C<sub class="calibre20">k</sub>)】*。

更一般地说，如果数据集 *X* 是从数据生成过程 *p <sub class="calibre20">数据</sub>T5】中采样的，则高斯混合模型基于以下假设:*

![](assets/d446dbad-43ac-4ad0-9b81-8ebf92d53811.png)

换句话说，数据生成过程由多元高斯分布的加权和来近似。这种分布的概率密度函数如下:

![](assets/6806b1ff-28b0-47ef-ae41-6844313862ac.png)

每个多元高斯的每个分量的影响取决于协方差矩阵的结构。下图显示了二元高斯分布的三种主要可能性(结果可以很容易地扩展到 *n* 维空间):

![](assets/37882b8c-c2b1-48c3-a901-106fb8c48cb5.png)

Full covariance matrix (left); diagonal covariance (center); circular/spherical covariance (right)

从现在开始，我们将一直考虑全协方差矩阵的情况，这允许实现最大的表达能力。很容易理解，当这样的分布是完全对称的(即协方差矩阵是圆形/球形)时，伪聚类的形状与 k 均值相同(当然，在高斯混合中，聚类没有边界，但在固定数量的标准差后，总是有可能剪切高斯)。相反，当协方差矩阵不是对角的或具有不同的方差时，影响不再对称(例如，在二元的情况下，一个分量可能比另一个分量显示更大的方差)。在这两种情况下，高斯混合允许我们计算实际概率，而不是测量样本之间的距离， *x* *<sub class="calibre20">i</sub>* ，和均值向量， *μ* *<sub class="calibre20">j</sub>* (如 k 均值)。下图显示了一个单变量混合的示例:

![](assets/45e96e53-863e-4c70-995b-c33388eb96ea.png)

Example of a univariate Gaussian mixture

在这种情况下，每个样本在每个高斯下总是有一个非空概率，其影响由其均值和协方差矩阵决定。例如，对应于 *x* 位置 2.5 的点既可以属于中心高斯，也可以属于右侧高斯(而左侧高斯的影响最小)。正如本章开头所解释的，任何软聚类算法都可以通过选择影响最大的组件(`argmax`)而转变为硬聚类算法。

您将立即理解，在这种特定情况下，通过对角协方差矩阵，`argmax`提供了一条附加信息(被 k 均值完全丢弃)，可以在进一步的处理步骤中使用(也就是说，推荐器应用程序可以提取所有聚类的主要特征，并根据相对概率重新加权它们)。

# 高斯混合模型的电磁算法

完整的算法(在*掌握机器学习算法*中有充分描述，作者:Bonaccorso G .，Packt Publishing，2018)比 k-means 稍微复杂一点，需要更深层次的数学知识。由于这本书的范围更实用，我们只讨论主要步骤，没有提供正式证据。

让我们首先考虑一个数据集， *X* ，包含 *n* 个样本:

![](assets/7e635458-7104-4a4d-8e60-1d4f853b7c27.png)

给定 *k* 分布，我们需要找到权重，*w<sub class="calibre20">j</sub>T5】，以及每个高斯 *(μ <sub class="calibre20">j</sub> ，σ<sub class="calibre20">j</sub>)*的参数，条件如下:*

![](assets/628467df-ca43-4e80-b210-fdf3b7a1078f.png)

最后一个条件对于保持与概率定律的一致性是必要的。如果我们将所有参数分组为一个集合， *θ <sub class="calibre20">j</sub> = (w <sub class="calibre20">j</sub> 、μ <sub class="calibre20">j</sub> 、σ<sub class="calibre20">j</sub>)*，我们可以定义样本*x<sub class="calibre20">I</sub>T13】在*j*T16】th 高斯下的概率，如下:*

![](assets/869cbdeb-320e-442d-baca-6dc1344300b0.png)

类似的，我们可以引入一个伯努利分布，*z<sub class="calibre20">I</sub>T3】j= p(j | x<sub class="calibre20">I</sub>，θ<sub class="calibre20">j</sub>)∞B(p)*，这是*j*T12】th 高斯生成样本*x<sub class="calibre20">I</sub>T17】的概率。换句话说，给定一个样本， *x <sub class="calibre20">i</sub> ，z <sub class="calibre20">ij</sub>* 将等于 1，概率 *p(j|x <sub class="calibre20">i</sub> ，θ <sub class="calibre20">j</sub>* *)* ，否则为 0。*

此时，我们可以计算整个数据集的联合对数似然，如下所示:

![](assets/e3538331-6360-40d2-a9ad-3e1208b7bc2e.png)

在前面的公式中，我们利用了指数指标表示法，它依赖于这样一个事实，即*z<sub class="calibre20">ij</sub>T3 只能是 0 或 1。因此，当 *z <sub class="calibre20">ij</sub> = 0* 时，意味着样品 *x <sub class="calibre20">i</sub>* 还没有由 *j* <sup xmlns:epub="http://www.idpf.org/2007/ops" class="calibre27">th</sup> 高斯生成，产品中对应的项变为 1(即 *x <sup class="calibre27">0</sup> = 1* )。反之，当 *z <sub class="calibre20">ij</sub> = 1* 时，该项等于*x<sub class="calibre20">I</sub>T27】和*j*T30】th 高斯的联合概率。因此，联合对数似然是模型生成整个数据集的联合概率，假设每个 *x <sub class="calibre20">i</sub> ∈ X* 是**独立同分布** ( **IID** )。要解决的问题是**最大似然估计** ( **MLE** )，或者换句话说，寻找最大化*L(θ；x，Z)* 。但是变量 *z <sub class="calibre20">ij</sub>* 并没有被观察到(或者潜伏)，所以不可能直接最大化可能性，因为我们不知道它们的值。**

解决这一问题的最有效方法是采用 em 算法(由 Dempster A. P，Laird N. M .和 Rubin D. B .提出，通过 EM 算法从不完整数据中获得最大似然，皇家统计学会杂志，系列 B. 39 (1)，1977)。完整的解释超出了本书的范围，但我们想提供主要步骤。首先要做的是使用概率链规则，以便将前面的表达式转换为条件概率的总和(这可以很容易地管理):

![](assets/b9e28a78-408d-4541-bfa3-64f553fc6e23.png)

这两种可能性现在很简单。术语 *p(x <sub class="calibre20">i</sub> |j，θ <sub class="calibre20">j</sub> )* 是*j*T12】th 高斯下的*x<sub class="calibre20">I</sub>T9】的概率，而 *p(j|θ <sub class="calibre20">j</sub> )* 只是*j*T20】th 高斯下的概率，相当于权重，*为了消除潜在的变量，EM 算法以迭代的方式进行，由两个步骤组成。第一个(称为**期望步骤**，或 **E 步骤**)是计算没有潜在变量的可能性的代理。如果我们将整个参数集表示为 *θ* ，并且在迭代 *t* 时计算的同一个参数集表示为*θ<sub class="calibre20">t</sub>T37】，我们可以计算出以下函数:*

![](assets/bf126480-b54e-43a6-a949-5fe06cc0748f.png)

*Q(* *θ|θ <sub class="calibre20">t</sub> )* 是关于变量 *z <sub class="calibre20">ij</sub>* 的联合对数似然的期望值，并被调节到数据集 *X* 和迭代时的参数集 *t* 。这种操作的效果是消除潜在的变量(这些变量被求和或积分出来)，并产生实际对数似然的近似值。不难想象，第二步(称为**最大化-步**，或**M-步**)的目标是最大化 *Q(θ|θ <sub class="calibre20">t</sub> )* ，生成一个新的参数集， *θ <sub class="calibre20">t+1</sub>* 。重复该过程，直到参数变得稳定，并且有可能证明最终参数集对应于最大似然估计。跳过所有中间步骤，假设最佳参数设置为 *θ <sub class="calibre20">f</sub>* ，最终结果如下:

![](assets/9472adaf-1521-4725-b506-a53281bbfba7.png)

为了清楚起见，概率 *p(j|x <sub class="calibre20">i</sub> ，θ <sub class="calibre20">f</sub> )* 可以通过使用贝叶斯定理来计算:

![](assets/008bf270-f862-4f9e-83a3-3f7edc62cbc0.png)

比例性可以通过归一化所有项来消除，使得它们的和等于 1(满足概率分布的要求)。

现在，让我们考虑一个使用 scikit-learn 的实际例子。由于目标纯粹是说教，我们使用了一个易于可视化的二维数据集:

```py
from sklearn.datasets import make_blobs

nb_samples = 300
nb_centers = 2

X, Y = make_blobs(n_samples=nb_samples, n_features=2, center_box=[-1, 1], centers=nb_centers, cluster_std=[1.0, 0.6], random_state=1000)
```

数据集是从两个具有不同标准差(1.0 和 0.6)的高斯分布中采样生成的，如下图所示:

![](assets/db575108-46aa-404e-9944-7ad79dde1310.png)

Dataset for the Gaussian mixture example

我们的目标是使用高斯混合模型和 k 均值，并比较最终结果。正如我们预期的两个组成部分，数据生成过程的近似如下:

![](assets/bd8f3fa7-5dbc-4a8f-801a-51074045b51c.png)

我们现在可以用`n_components=2`训练一个`GaussianMixture`实例。默认的协方差类型是完整的，但是可以通过设置`covariance_type`参数来更改该选项。允许的值是`full`、`diag`、`spherical`和`tied`(这迫使算法对所有高斯函数使用共享的单个协方差矩阵):

```py
from sklearn.mixture import GaussianMixture

gm = GaussianMixture(n_components=2, random_state=1000)
gm.fit(X)
Y_pred = gm.fit_predict(X)

print('Means: \n{}'.format(gm.means_))
print('Covariance matrices: \n{}'.format(gm.covariances_))
print('Weights: \n{}'.format(gm.weights_))
```

上一个片段的输出如下:

```py
Means: 
[[-0.02171304 -1.03295837]
 [ 0.97121896 -0.01679101]]

Covariance matrices: 
[[[ 0.86794212 -0.18290731]
  [-0.18290731  1.06858097]]

 [[ 0.44075382  0.02378036]
  [ 0.02378036  0.37802115]]]

Weights: 
[0.39683899 0.60316101]
```

因此，最大似然估计产生了两个分量，其中稍占优势的一个分量(即 *w <sub class="calibre20">2</sub> = 0.6* )。为了知道高斯轴的方向，我们需要计算协方差矩阵的归一化特征向量(这个概念将在[第 7 章](07.html)、*降维和分量分析*中充分解释):

```py
import numpy as np

c1 = gm.covariances_[0]
c2 = gm.covariances_[1]

w1, v1 = np.linalg.eigh(c1)
w2, v2 = np.linalg.eigh(c2)

nv1 = v1 / np.linalg.norm(v1)
nv2 = v2 / np.linalg.norm(v2)

print('Eigenvalues 1: \n{}'.format(w1))
print('Eigenvectors 1: \n{}'.format(nv1))

print('Eigenvalues 2: \n{}'.format(w2))
print('Eigenvectors 2: \n{}'.format(nv2))
```

输出如下:

```py
Eigenvalues 1: 
[0.75964929 1.17687379]
Eigenvectors 1: 
[[-0.608459   -0.36024664]
 [-0.36024664  0.608459  ]]

Eigenvalues 2: 
[0.37002567 0.4487493 ]
Eigenvectors 2: 
[[ 0.22534853 -0.6702373 ]
 [-0.6702373  -0.22534853]]
```

在两个二元高斯(一旦被截断并从顶部观察，可以想象成椭圆)中，主要成分是第二个(即第二列，对应于最大的特征值)。椭圆的偏心率由特征值之间的比值决定。如果这样的比例等于 1，形状就是圆，高斯是完全对称的；否则，它们会沿着一个轴拉伸。主要部件与 *x* 轴之间的角度(度)如下:

```py
import numpy as np

a1 = np.arccos(np.dot(nv1[:, 1], [1.0, 0.0]) / np.linalg.norm(nv1[:, 1])) * 180.0 / np.pi
a2 = np.arccos(np.dot(nv2[:, 1], [1.0, 0.0]) / np.linalg.norm(nv2[:, 1])) * 180.0 / np.pi
```

之前的公式是基于主成分 *v <sub class="calibre20">1</sub>* 与 *x* -versor、*e<sub class="calibre20">0</sub>T9】(即*【1，0】*)之间的点积:*

![](assets/6302cc74-9953-42e4-9bd4-8c0e0006f19b.png)

在显示最终结果之前，使用 k 均值对数据集进行聚类将会很有帮助:

```py
from sklearn.cluster import KMeans

km = KMeans(n_clusters=2, random_state=1000)
km.fit(X)
Y_pred_km = km.predict(X)
```

聚类结果如下图所示:

![](assets/23deac5e-1e50-4c67-a6aa-e83753613d06.png)

Gaussian mixture result (left) with the shapes of three horizontal sections; k-means result (right)

正如预期的那样，两种算法产生非常相似的结果，主要的差异是由于高斯分布的非对称性。特别地，对应于数据集左下部分的伪聚类在两个方向上具有较大的方差，并且对应的高斯是主导的。为了检查混合物的行为，让我们计算三个样本点的概率( *(0，-2)*； *(1，-1)*—一个临界样本；和 *(1，0)* ，使用`predict_proba()`方法:

```py
print('P([0, -2]=G1) = {:.3f} and P([0, -2]=G2) = {:.3f}'.format(*list(gm.predict_proba([[0.0, -2.0]]).squeeze())))
print('P([1, -1]=G1) = {:.3f} and P([1, -1]=G2) = {:.3f}'.format(*list(gm.predict_proba([[1.0, -1.0]]).squeeze())))
print('P([1, 0]=G1) = {:.3f} and P([1, 0]=G2) = {:.3f}'.format(*list(gm.predict_proba([[1.0, 0.0]]).squeeze())))
```

上一个块的输出如下:

```py
P([0, -2]=G1) = 0.987 and P([0, -2]=G2) = 0.013
P([1, -1]=G1) = 0.354 and P([1, -1]=G2) = 0.646
P([1, 0]=G1) = 0.068 and P([1, 0]=G2) = 0.932
```

我邀请读者通过使用其他协方差类型来重复这个例子，然后用 k 均值来比较所有的硬赋值。

# 用 AIC 和 BIC 评估高斯混合模型的性能

由于高斯混合模型是一个概率模型，找到最佳的组件数量需要一种不同于前面几章分析的方法。最广泛使用的技术之一是**阿卡克信息标准** ( **AIC** )，其基于信息论(首次在*阿卡克首次提出，统计模型识别的新观点，IEEE 自动控制交易，* 19 (6))。如果概率模型具有*n<sub class="calibre20">p</sub>T9】参数(即必须学习的单个值)并达到最大负对数似然， *L <sub class="calibre20">opt</sub>* ，则 AIC 定义如下:*

![](assets/be138ce2-232e-4f56-bea4-bb9522afbb79.png)

这种方法有两个重要的含义。第一个是关于价值本身；AIC 越小，分数越高。事实上，考虑到奥卡姆剃刀原理，模型的目标是以最小的参数数量实现最优似然。第二个含义与信息论严格相关(我们不讨论细节，这些细节在数学上很重要)，特别是数据生成过程和一般概率模型之间的信息损失。有可能证明 AIC 的渐近最小化(即当样本数趋于无穷大时)等价于信息损失的最小化。考虑基于不同数量的分量的几个高斯混合( *n <sub class="calibre20">p</sub>* 是所有权重、均值和协方差参数的总和)，具有最小 AIC 的配置对应于以最高精度再现数据生成过程的模型。AIC 的主要局限在于小数据集。在这种情况下，AIC 倾向于在大量参数下达到最小值，这与奥卡姆剃刀原理形成对比。然而，在大多数现实生活中，AIC 提供了一个有用的相对度量，可以帮助数据科学家排除许多配置，只分析最有希望的配置。

当需要强制参数数量保持较低时，可以采用**贝叶斯信息准则** ( **BIC** ，定义如下:

![](assets/21e81cf5-322e-49fe-ab4a-4128de243bf0.png)

上式中， *n* 为样本数(例如 *n = 1000* 且采用自然对数，罚值约为 6.9)；因此，BIC 几乎等同于 AIC，对参数的数量有更强的惩罚。然而，即使 BIC 倾向于选择较小的模型，结果通常也不如 AIC 可靠。BIC 的主要优点是当 *n → ∞* 时，数据生成过程 *p <sub class="calibre20">数据</sub>T9】与模型*p<sub class="calibre20">m</sub>T13】之间的 kulback-Leibler 散度(BIC 最小)趋于 0:**

![](assets/033ac37f-323b-46f8-ac50-bace7f37b555.png)

由于当两个分布相同时，库尔巴克-莱布勒散度为零，所以前一个条件意味着 BIC 趋向于渐近地选择精确再现数据生成过程的模型。

现在，让我们考虑前面的例子，检查 AIC 和 BIC 的不同数量的组件。Scikit-learn 将这些措施作为`GaussianMixture`课程的方法(`aic()`和`bic()`)纳入其中。此外，我们还想计算每个模型所达到的最终对数似然。这可以通过将通过`score()`方法获得的值(每样本对数似然平均值乘以样本数)相乘来实现，如下所示:

```py
from sklearn.mixture import GaussianMixture

n_max_components = 20

aics = []
bics = []
log_likelihoods = []

for n in range(1, n_max_components + 1):
 gm = GaussianMixture(n_components=n, random_state=1000)
 gm.fit(X)
 aics.append(gm.aic(X))
 bics.append(gm.bic(X))
 log_likelihoods.append(gm.score(X) * nb_samples)
```

结果图如下图所示:

![](assets/006b6068-1d96-4ada-86ca-44bdd17cd9b0.png)

AICs, BICs, and log-likelihoods for Gaussian mixtures with the number of components in the range (1, 20)

在这种情况下，我们知道数据集是由两个高斯分布生成的，但是让我们假设我们没有这条信息。AIC 和 BIC 的(当地)最低气温都是 *n <sub class="calibre20">c</sub> = 2* 。然而，当 BIC 不断变得越来越大时，AIC 的伪全球最小值为 *n <sub class="calibre20">c</sub> = 18* 。因此，如果我们信任 AIC，我们应该选择 18 个分量，这相当于用许多高斯子超分割数据集，具有小的方差。另一方面， *n <sub class="calibre20">c</sub> = 2* 与 *n <sub class="calibre20">c</sub> = 18* 相比其他数值的差异并不是很大，所以我们也可以更倾向于前者的配置，考虑到要简单得多。BIC 证实了这一选择。事实上，即使也有对应于 *n <sub class="calibre20">c</sub> = 18* 的局部最小值，其值也比 *n <sub class="calibre20">c</sub> = 2* 的 BIC 值大得多。正如我们之前解释的，这种行为是由于 BIC 强加的样本量的额外惩罚。由于每个二元高斯需要一个权重变量，两个均值变量，四个协方差矩阵变量，对于 *n <sub class="calibre20">c</sub> = 2* ，我们得到*n<sub class="calibre20">p</sub>= 2(1+2+4)= 14*，对于 *n <sub class="calibre20">c</sub> = 18* ，我们得到 *n <sub class="calibre20">p</sub> = 18(1 由于有 300 个样本，BIC 被罚 *log(300) ≈ 5.7* ，导致 BIC 增加约 350 个。当 *n <sub class="calibre20">c</sub>* 变大时，对数似然性增加(因为在极端情况下，每个点可以被认为是由具有零方差的单个高斯生成的，相当于狄拉克δ)，参数的数量在模型选择过程中起着主要作用。*

没有任何额外的惩罚，更大的模型很可能被选为最佳选择，但是在聚类过程中，我们也需要执行最大分离原则。这种情况部分与较少的组件有关，因此，BIC 应成为最佳方法。总的来说，我建议比较这两个标准，试图找到对应于 AIC 和 BIC 之间最大一致的*n<sub class="calibre20">c</sub>T3。此外，还应该考虑基本的背景知识，因为许多数据生成过程都有明确定义的行为，并且可以通过排除所有不现实的值来限制潜在组件的范围。我请读者用 *n <sub class="calibre20">c</sub> = 18* 重复前面的例子，画出所有高斯人，比较某些特定点的概率。*

# 基于贝叶斯高斯混合的组件选择

贝叶斯高斯混合模型是基于变分框架的标准高斯混合模型的扩展。这个题目相当高深，需要透彻的数学描述，超出了本书的范围(你可以在 Nasios N .和 Bors A. G .，*高斯混合模型的变分学习，IEEE Transactions On Systems，Man，and 控制论，* 36/ 4，08/2006 中找到)。然而，在我们讨论主要属性之前，理解主要概念和差异将是有帮助的。假设我们有一个数据集， *X* ，以及一个用向量 *θ* 参数化的概率模型。前面几节你看到了概率*p(X |**【θ】*)就是可能性 *L(θ|X)* ，它的最大化导致了一个生成概率最大的 *X* 的模型。但是，我们不对参数施加任何约束，它们的最终值完全取决于 *X* 。如果我们引入贝叶斯定理，我们会得到以下结果:

![](assets/f3b61fc8-f6b7-4d12-885b-577e5a179ee6.png)

左边是给定数据集的参数后验概率，我们知道它与参数的似然性乘以先验概率成正比。在一个标准的最大似然估计中，我们只使用 *p(X|θ)* ，但是，当然，我们也可以包含一段关于 *θ* (根据概率分布)的先验知识，并最大化 *p(θ|X)* 或比例代理函数。然而，一般来说， *p(θ|X)* 是难以处理的，之前的 *p(θ)* 往往很难定义，因为没有足够的关于高度可能区域的知识。为此，最好将参数建模为用 *η* (所有特定参数的集合，如平均值、系数等)参数化的概率分布，并引入**变分后验**、*q(θ| X；η* )近似真实分布。

这样的工具是一种称为**变分贝叶斯推理**的技术的关键要素(你可以在前面提到的论文中找到进一步的细节)，它允许我们轻松找到最佳参数，而不需要与实际的 *p(θ|X)* 一起工作。特别是，在高斯混合模型中，有三组不同的参数，每组参数都用适当的分布建模。在这种情况下，我们倾向于不讨论这些选择的细节，但是理解其基本原理是有用的。

在贝叶斯框架中，给定一个似然性， *p(X|θ)* ，一个概率密度函数， *p(θ)* ，属于同一个后验族， *p(θ|X)* ，被称为**共轭先验**。在这种情况下，程序显然被简化了，因为可能性的影响仅限于修改先前的参数。为此，由于似然性是正态的，为了对均值建模，我们可以采用正态分布(它是相对于均值的共轭先验)，对于协方差矩阵，我们可以采用威沙特分布(它是相对于协方差矩阵的逆的共轭先验)。在本讨论中，没有必要熟悉所有这些分布(除了正态分布)，但记住它们是共轭先验是有帮助的，因此，给定参数的初始猜测，可能性的作用是调整它们，以便在给定数据集的情况下最大化它们的联合概率。

由于混合的权重被归一化，因此它们的和必须总是等于 1，并且我们希望只自动选择大量分量的子集，因此我们可以使用狄利克雷分布，它具有稀疏的有用特性。换句话说，给定一组权重 *w <sub class="calibre20">1</sub> ，w<sub class="calibre20">2</sub>T5，...，以及 *w <sub class="calibre20">n</sub>* ，狄利克雷分布倾向于保持大部分权重的概率相当低，而非空权重的较小子组决定了主要贡献。狄利克雷过程提供了另一种选择，狄利克雷过程是一种产生概率分布的特殊随机过程。在这两种情况下，目标都是调整单个参数(称为**重量浓度参数**，该参数增加或减少具有稀疏分布(或简单地说，狄利克雷分布的稀疏性)的概率。*

Scikit-learn 实现了贝叶斯高斯混合(通过`BayesianGaussianMixture`类)，它可以基于狄利克雷过程和分布。在本例中，我们将保留默认值(`process`)并检查不同浓度值的行为(`weight_concentration_prior`参数)。也可以调整高斯的平均值作为均值，调整威沙特的自由度作为逆协方差。然而，在没有任何特定的先验知识的情况下，很难设置这些值(我们假设我们不知道均值可能位于何处或协方差矩阵的结构)，因此，最好保留从问题结构中导出的值。因此，平均值(高斯)将等于 *X* 的平均值(可以用`mean_precision_prior`参数控制位移；值 *< 1.0* 倾向于将单个均值向 *X* 的均值移动，而较大的值会增加位移)，自由度(Wishart)的数量被设置为等于特征的数量(维度为 *X* )。在许多情况下，这些参数是由学习过程自动调整的，没有必要改变它们的初始值。相反，`weight_concentration_prior`可以调整，以增加或减少活性成分的数量(即，其重量不接近零或比其他成分低得多)。

在这个例子中，我们将生成 500 个二维样本，使用 5 个部分重叠的高斯分布(特别是，其中 3 个共享一个非常大的重叠区域):

```py
from sklearn.datasets import make_blobs

nb_samples = 500
nb_centers = 5

X, Y = make_blobs(n_samples=nb_samples, n_features=2, center_box=[-5, 5], 
                  centers=nb_centers, random_state=1000)
```

先从大重量浓度参数(`1000`)开始，最大成分数等于`5`。在这种情况下，我们期望找到大量(可能是`5`)的活性成分，因为狄利克雷过程不能实现高度稀疏:

```py
from sklearn.mixture import BayesianGaussianMixture

gm = BayesianGaussianMixture(n_components=5, weight_concentration_prior=1000, 
                             max_iter=10000, random_state=1000)
gm.fit(X)

print('Weights: {}'.format(gm.weights_))
```

上一个片段的输出如下:

```py
Weights: [0.19483693 0.20173229 0.19828598 0.19711226 0.20803253]
```

正如所料，所有组件的重量大致相同。为了得到进一步的确认，我们可以检查(通过`argmax`功能)有多少样本几乎没有分配给每个样本，如下所示:

```py
Y_pred = gm.fit_predict(X)

print((Y_pred == 0).sum())
print((Y_pred == 1).sum())
print((Y_pred == 2).sum())
print((Y_pred == 3).sum())
print((Y_pred == 4).sum())
```

输出如下:

```py
96
102
97
98
107
```

因此，平均来说，所有高斯人产生的点数是相同的。最终配置如下图所示:

![](assets/c391f825-4d97-41f7-965e-79ccd22623fe.png)

Final configuration, with five active components

这种模式一般可以接受；然而，让我们假设我们知道潜在原因(即生成高斯分布)的数量可能是 4，而不是 5。我们可以尝试的第一件事是保持原有的最大组分数，降低重量浓度参数(即 0.1)。如果近似能够使用较少的高斯分布成功生成 *X* ，我们应该找到一个零权重:

```py
gm = BayesianGaussianMixture(n_components=5, weight_concentration_prior=0.1, 
                             max_iter=10000, random_state=1000)

gm.fit(X)

print('Weights: {}'.format(gm.weights_))
```

现在输出如下:

```py
Weights: [3.07496936e-01 2.02264778e-01 2.94642240e-01 1.95417680e-01 1.78366038e-04]
```

可以看到，第五个高斯的权重比其他高斯小得多，可以完全丢弃(我邀请你检查是否有几个样本几乎没有分配给它)。新配置包含四个活动组件，如下图所示:

![](assets/739c3008-1f01-46c2-81bf-6b710703a427.png)

Final configuration, with four active components

正如可以看到的，模型已经执行了组件数量的自动选择，并且它已经将较大的右斑点分成两部分，这两部分几乎是正交的。即使使用大量初始组件(例如，10 个)训练模型，该结果也保持不变。作为练习，我建议用其他值重复该示例，检查权重之间的差异。贝叶斯高斯混合模型非常强大，因为它们能够避免过度拟合。事实上，虽然标准高斯混合模型将通过减少它们的协方差来使用所有的分量，如果必要的话(以便覆盖密集的区域)，但是这些模型利用了狄利克雷过程/分布的属性，以避免激活太多的分量。例如，通过检查模型可实现的最小组件数量，可以很好地洞察潜在的数据生成过程。在没有任何其他先验知识的情况下，这样的值是最终配置的一个很好的候选值，因为较少数量的组件也会产生较低的最终可能性。当然，也可以将 AIC/BIC 与这种方法结合使用，以获得另一种形式的确认。然而，与标准高斯混合模型的主要区别在于，它可能包含来自专家的先验信息(例如，均值和协方差方面的原因结构)。为此，我邀请您通过更改`mean_precision_prior`的值来重复该示例。例如，可以将`mean_prior`参数设置为不同于 *X* 平均值的值，并调整`mean_precision_prior`，从而迫使模型基于一些先验知识实现不同的分割(即一个区域中的所有样本都应该由特定的分量生成)。

# 生成高斯混合

高斯混合模型主要是生成模型。这意味着训练过程的目标是优化参数，以便最大化模型生成数据集的可能性。如果假设是正确的，并且 *X* 已经从特定的数据生成过程中采样，则最终的近似值必须能够生成所有其他潜在的样本。换句话说，我们假设 *x <sub class="calibre20">i</sub> ∈ X* 为 IDD，*X<sub class="calibre20">I</sub>∨p<sub class="calibre20">数据</sub>*；因此，当已经找到最优近似 *p ≈ p <sub class="calibre20">数据</sub>* 时，所有在 *p* 下概率高的样本*x<sub class="calibre20">j</sub>T19】也很可能是由 *p <sub class="calibre20">数据</sub>* 生成的。*

在这个例子中，我们希望在半监督场景中使用高斯混合模型。这意味着我们有一个包含标记和未标记样本的数据集，我们希望利用标记样本作为基础事实，并找到可以生成整个数据集的最佳混合。当标注非常大的数据集非常困难和昂贵时，这种情况非常常见。为了克服这个问题，可以标记一个均匀采样的子集，并训练一个生成模型，该模型能够生成具有最大可能可能性的剩余样本。

我们将使用更新后的权重、均值和协方差矩阵公式，这些公式在主要部分通过一个简单的过程进行了讨论，如下所示:

*   所有被标记的样本都被认为是基本事实；所以，如果有 k 个类，我们还需要定义 *k 个*组件，并将每个类分配给其中一个。因此，如果*x<sub class="calibre20">I</sub>T5】是标有 *y <sub class="calibre20">i</sub> = {1，2，...，k}* ，对应的概率向量将为 *p(x <sub class="calibre20">i</sub> ) = (0，0，..., 1, 0, ...，0)* ，其中 1 对应于与 *y <sub class="calibre20">i</sub>* 类相关联的高斯。换句话说，我们信任已标记的样本，并强制单个高斯生成具有相同标签的子集。*
*   所有未标记的样本都以标准方式处理，概率向量是通过将权重乘以每个高斯下的概率来确定的。

让我们首先生成一个包含 500 个二维样本的数据集(`100`已标记，其余的未标记)，具有真实的标记，`0`和`1`，以及等于`-1`的未标记标记:

```py
from sklearn.datasets import make_blobs

nb_samples = 500
nb_unlabeled = 400

X, Y = make_blobs(n_samples=nb_samples, n_features=2, centers=2, cluster_std=1.5, random_state=100)

unlabeled_idx = np.random.choice(np.arange(0, nb_samples, 1), replace=False, size=nb_unlabeled)
Y[unlabeled_idx] = -1
```

此时，我们可以初始化高斯参数(权重被选择为相等，并且协方差矩阵必须是半正定的。如果读者不熟悉这个概念，我们可以说对称方阵*a∈ℜ<sup class="calibre27">n×n</sup>T3】是正半定的，如果:*

![](assets/65b86f60-db08-4f11-9cf4-8a820dd6e0ff.png)

而且所有的特征值都是非负的，特征向量生成一个正交基(这个概念在[第 7 章](07.html)、*降维和成分分析*讲 PCA 的时候会很有帮助)。

如果协方差矩阵是随机选择的，为了成为半正定的，需要将它们中的每一个乘以它的转置):

```py
import numpy as np

m1 = np.array([-2.0, -2.5])
c1 = np.array([[1.0, 1.0],
               [1.0, 2.0]])
q1 = 0.5

m2 = np.array([1.0, 3.0])
c2 = np.array([[2.0, -1.0],
               [-1.0, 3.5]])
q2 = 0.5
```

数据集和初始高斯分布如下图所示:

![](assets/4fcf89b7-072b-4d1b-b722-af2ee25e79f5.png)

Dataset (the unlabeled samples are marked with an x) and initial configuration

现在，我们可以按照之前定义的规则执行几次迭代(在我们的例子中是 10 次)(当然，也可以检查参数的稳定性，以便停止迭代)。使用 SciPy `multivariate_normal`类计算每个高斯分布下的概率:

```py
from scipy.stats import multivariate_normal

nb_iterations = 10

for i in range(nb_iterations):
    Pij = np.zeros((nb_samples, 2))

    for i in range(nb_samples):

        if Y[i] == -1:
            p1 = multivariate_normal.pdf(X[i], m1, c1, allow_singular=True) * q1
            p2 = multivariate_normal.pdf(X[i], m2, c2, allow_singular=True) * q2
            Pij[i] = [p1, p2] / (p1 + p2)
        else:
            Pij[i, :] = [1.0, 0.0] if Y[i] == 0 else [0.0, 1.0]

    n = np.sum(Pij, axis=0)
    m = np.sum(np.dot(Pij.T, X), axis=0)

    m1 = np.dot(Pij[:, 0], X) / n[0]
    m2 = np.dot(Pij[:, 1], X) / n[1]

    q1 = n[0] / float(nb_samples)
    q2 = n[1] / float(nb_samples)

    c1 = np.zeros((2, 2))
    c2 = np.zeros((2, 2))

    for t in range(nb_samples):
        c1 += Pij[t, 0] * np.outer(X[t] - m1, X[t] - m1)
        c2 += Pij[t, 1] * np.outer(X[t] - m2, X[t] - m2)

    c1 /= n[0]
    c2 /= n[1]
```

过程结束时的高斯混合参数如下:

```py
print('Gaussian 1:')
print(q1)
print(m1)
print(c1)

print('\nGaussian 2:')
print(q2)
print(m2)
print(c2)
```

上一个片段的输出如下:

```py
Gaussian 1:
0.4995415573662937
[ 0.93814626 -4.4946583 ]
[[ 2.53042319 -0.10952365]
 [-0.10952365  2.26275963]]

Gaussian 2:
0.5004584426337063
[-1.52501526  6.7917029 ]
[[ 2.46061144 -0.08267972]
 [-0.08267972  2.54805208]]
```

不出所料，由于数据集的对称性，权重几乎保持不变，同时更新了均值和协方差矩阵，以最大化可能性。最后的图显示在下面的截图中:

![](assets/4d7b1215-9880-410f-b1a4-f15a2d338d88.png)

Final configuration, after 10 iterations

可以看到，这两个 Gaussians 都已经成功优化，能够从几个扮演**可信向导**角色的标注样本开始生成整个数据集。这样的方法非常强大，因为它允许我们在模型中包含一些先验知识，而无需任何修改。然而，由于标记样本具有等于 1 的固定概率，该方法在异常值方面不是非常稳健。如果数据生成过程没有生成样本，或者样本受到噪声的影响，模型可能会导致高斯错误。然而，这种情况通常应该被忽略，因为任何先验知识，当包括在估计中时，总是必须被预先评估，以便检查它是否可靠。这样的步骤是必要的，以避免迫使模型只学习原始数据生成过程的一部分的风险。相反，当标记样本真正代表底层过程时，它们的包含减少了误差并加速了收敛。我邀请读者在介绍一些有噪声的点(例如，(-20，-10))后重复这个例子，并比较几个未标记测试样本的概率。

# 摘要

在本章中，我们介绍了一些最常见的软聚类方法，重点介绍了它们的属性和特点。模糊 c 均值是经典 k 均值算法的扩展，基于模糊集的概念。集群不被认为是互斥的分区，而是可以与其他一些集群重叠的灵活集合。所有的样本总是被分配给所有的聚类，但是权重向量决定了每个聚类的隶属度。连续的簇可以定义部分重叠的属性；因此，对于两个或多个聚类，给定样本可以具有非空权重。大小决定了它属于每个片段的多少。

高斯混合是一个生成过程，它基于一个假设，即可以用高斯分布的加权和来近似真实的数据生成过程。给定预定义数量的组件，为了最大化可能性，对模型进行训练。我们讨论了如何使用 AIC 和 BIC 作为性能度量，以便找到高斯分布的最佳数量。我们还简要介绍了贝叶斯高斯混合的概念，并研究了包含先验知识如何有助于自动选择一小部分活动组件。在最后一部分，我们讨论了半监督高斯混合的概念，展示了如何使用一些标记样本作为指导，以优化具有大量未标记点的训练过程。

在下一章中，我们将讨论核密度估计的概念及其在异常检测领域的应用。

# 问题

1.  软聚类和硬聚类的主要区别是什么？
2.  模糊 c 均值可以很容易地处理非凸聚类。这个说法对吗？
3.  高斯混合模型的主要假设是什么？
4.  假设两个模型达到相同的最优对数似然；然而，第一个的 AIC 是第二个的两倍。这是什么意思？
5.  考虑到前面的问题，我们更喜欢哪种模式？
6.  为什么我们要使用狄利克雷分布作为贝叶斯高斯混合模型权重的先验？
7.  假设我们有一个包含 1000 个标记样本的数据集，这些样本的值已经过专家认证。我们从相同的来源收集了 5000 个样本，但我们不想为额外的标签付费。我们可以做些什么来将它们纳入我们的模型？

# 进一步阅读

*   *理论神经科学*，*达扬·p .*，*麻省理工学院出版社，2005*
*   *通过 EM 算法不完全数据的最大似然*，*皇家统计学会杂志*，*登普斯特 A. P .，莱尔德 N. M .，鲁宾 D. B.* ，*系列 B. 39 (1)，1977*
*   *统计模型识别的新面貌*、*阿凯克 H.* 、 *IEEE 自动控制交易，19 (6)*
*   *高斯混合模型的变分学习*，*纳西欧和博思公司*， *IEEE 系统、人和控制论学报，36/ 4，08/2006*
*   *Belohlavek R .，Klir G. J.* (编著)，*概念与模糊逻辑*，*麻省理工学院出版社*，2011
*   *查佩尔·奥、斯科尔科夫·b、齐恩·a .*(编著)*半监督学习**麻省理工学院出版社*，2010
*   *掌握机器学习算法*，*博纳科索格*，*帕克特出版，2018*
*   *机器学习算法，第二版*，*博纳科索格*，*帕克特出版，2018*