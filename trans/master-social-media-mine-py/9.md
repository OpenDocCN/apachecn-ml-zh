# 第九章。链接数据和语义网

本章概述了语义网和相关技术。在本章中，我们将讨论以下主题:

*   讨论语义网作为数据网的基础
*   讨论微格式、链接数据和 RDF
*   从数据库中挖掘语义关系
*   从维基百科中挖掘地理信息
*   将地理信息绘制到谷歌地图中

# 数据之网

致力于开发网络标准的国际组织**万维网联盟** ( **W3C** )为语义网提出了一个简单的定义([https://www.w3.org/standards/semanticweb/data](https://www.w3.org/standards/semanticweb/data)，2016 年 4 月检索到:

> *“语义网是数据网”*

两者，术语*语义网*和这个定义，都是由网络发明者蒂姆·伯纳斯·李爵士([https://en.wikipedia.org/wiki/Tim_Berners-Lee](https://en.wikipedia.org/wiki/Tim_Berners-Lee))自己创造的，他也是 W3C 的主任。当他谈到他最伟大的发明时，他经常强调它的社会含义，以及网络如何更像是一种社会创造而不是一种技术创造(*编织网络*、*蒂姆·伯纳斯·李*、 *1999* )。

如果我们简要分析一下网络作为一个社交平台的演变，它的前景会变得更加清晰。在新千年的第一个十年里流行起来的关键词(或流行语)之一是 *Web 2.0* ，它诞生于 20 世纪 90 年代末，但后来被蒂姆·奥莱利普及。这个术语暗示了一个新版本的网络，但它并不涉及任何特定的技术更新或规范的变化，而是关注从旧的静态网络 1.0 到以用户为中心的理解网络和丰富用户体验的方法的逐渐演变。

从 Web 1.0 到 Web 2.0 的演变可以用一个词来概括——协作。蒂姆·伯纳斯·李不同意这种区别，因为 Web 2.0 中使用的技术和标准与旧 Web 1.0 中使用的技术和标准基本相同。此外，自从网络出现以来，协作和连接人们就是最初的目标。尽管有这种不同的观点，网络 2.0 这个术语——不管是不是行话——已经成为我们词汇的一部分，经常与社交网络这个术语结合在一起(更复杂的是，社交网络有时也被称为网络 2。十)。

那么我们把语义网放在哪里呢？根据网络的创造者，这是进化尺度上的又一步，也就是网络 3.0。

网络的自然进化是成为一个数据网络。对这一进程至关重要的是数据模型和数据表示，它们允许以机器可理解的方式共享知识。使用以一致和语义格式共享的数据，机器可以利用这些信息，并支持用户的复杂信息需求以及决策。

万维网上文档的主流格式是 HTML。这种格式是一种用于描述文档结构的标记约定，它结合了文本、图像或视频等多媒体对象以及文档之间的链接。

HTML 允许内容管理者在文档的标题中指定一些元数据。此类信息不一定用于显示目的，因为浏览器通常只显示文档的正文。元数据标签包括作者姓名、版权信息、文档的简短描述以及描述文档的关键词。所有这些细节都可以通过计算机来解释，从而对文件进行分类。HTML 在指定更复杂的信息方面有所欠缺。

例如，使用 HTML 元数据，我们可以描述一个文档是关于伍迪·艾伦的，但是语言不支持对更复杂的概念进行歧义消除，例如，该文档是关于伍迪·艾伦扮演的电影中的一个角色，还是伍迪·艾伦导演的电影，或者是其他人导演的关于伍迪·艾伦的纪录片？因为 HTML 的目的是描述文档的结构，所以它可能不是在这个粒度级别上表示知识的最佳工具。

另一方面，语义网技术允许我们更进一步，描述那些根据实体、属性以及它们之间的关系来表征我们的世界的概念。例如，艾伦从影记录的一部分如下表所示:

<colgroup><col> <col> <col></colgroup> 
| **艺人** | **角色** | **电影** |
| 伍迪·艾伦 | 导演 | 曼哈顿 |
| 伍迪·艾伦 | 行动者 | 曼哈顿 |
| 伍迪·艾伦 | 导演 | 匹配点 |

该表明确显示了名为*艺术家*和*电影*的实体类型之间的联系。表示这种结构化知识是语义网技术的领域。

本节的其余部分提供了与语义网和知识表示主题相关的主要概念和术语的概述。它介绍了旨在回答类似伍迪·艾伦例子中的复杂查询的技术。

## 语义网词汇

本节简要介绍和概括了语义技术中常用的一些基本词汇。

**标记语言**:这是一个用来标注文档的系统。给定一段文本，或者更一般地说，一段数据，标记语言允许内容管理者根据标记语言的特定含义来标记文本(数据)块。我们已经遇到的一种著名的标记语言是 HTML。标记语言可以包括表示性注释和描述性注释。前者与文档的显示方式有关，而后者提供了带注释数据的描述。在 HTML 的情况下，表示和描述标签都是可用的。例如，`<b>`或`<i>`标签代表一种风格，内容设计者可以使用它们来声明特定的文本必须以粗体或斜体显示。另一方面，`<strong>`和`<em>`标签提供了特定的语义，因为它们指示一段文本应该以某种方式分别被提供为强的或强调的。在普通浏览器中，`<b>`和`<strong>`都以粗体显示，就像`<i>`和`<em>`都以斜体显示一样。盲人用户不会从想象一个大胆的风格中受益，但是将一个短语标记为`<strong>`可以让屏幕阅读器理解该短语本身应该如何阅读。

**语义标注**:如前所述，事物的呈现方式和理解方式是有区别的。语义标记是描述所呈现信息的意义，而不是它的外观。在 HTML 和 XHTML 中，虽然不建议使用表示标记，但并不明确反对使用它们。HTML5 已经向语义标记迈进了一步，引入了一些语义标签，如`<article>`或`<section>`。与此同时，`<b>`、`<i>`等表象标签被保留了一个精确的含义(也就是说，在文体上不同于普通散文，不表达任何特别的重要性)。

**本体**:语义网的支柱之一就是本体。它们是特定业务领域实体的类型、属性和关系的正式命名和定义。本体的例子包括 WordNet([https://wordnet.princeton.edu/](https://wordnet.princeton.edu/))，一种词汇资源，其中术语根据其含义被分类成概念，以及 SNOMED CT([http://www.ihtsdo.org/snomed-ct](http://www.ihtsdo.org/snomed-ct))，一种医学词汇。

维基百科显示了来自 WordNet([https://en.wikipedia.org/wiki/WordNet](https://en.wikipedia.org/wiki/WordNet))的一个小摘录:

```py
dog, domestic dog, Canis familiaris 
  => canine, canid 
    => carnivore 
      => placental, placental mammal, eutherian, eutherian mammal 
        => mammal 
          => vertebrate, craniate 
            => chordate 
              => animal, animate being, beast, brute, creature, fauna 
                => ... 

```

在这个例子中，代表单词`dog`的一个意思，我们可以欣赏用来代表这一知识的细节水平:狗不仅仅是一种动物，它还根据属于特定的生物家族(犬科动物)或目(食肉动物)来分类。

本体可以使用语义标记来描述，例如，**网络本体语言** ( **OWL** 、[https://en.wikipedia.org/wiki/Web_Ontology_Language](https://en.wikipedia.org/wiki/Web_Ontology_Language))允许内容策展人表示知识。构建本体的挑战包括要表示的领域的广阔性，以及围绕人类知识和作为传递知识的载体的自然语言的固有的模糊性和不确定性。手动构建本体需要对被建模的领域有深入的理解。描述和表示知识自哲学诞生之初就一直是人类的基本问题之一，因此我们可以将语义网本体视为哲学本体的一种实际应用。

**分类学**:比本体更窄，分类法是指知识的层次化表示。换句话说，它们用于对定义明确的实体类进行分类，定义**是一个**，而**在类之间有一个**关系。本体和分类法之间的主要区别在于，本体模拟了更多种类的关系。

**Folksonomy** :这也叫社会分类学。构建大众分类法的实践指的是社会标签。用户可以使用特定的标签来标记一段内容，因为这种标签是对特定类别的解释。结果是一种分类法的民主化，这导致以用户感知的方式显示信息，而没有叠加的刚性结构。硬币的另一面是，野外的大众分类法可能完全没有条理，更难使用。它也可能代表一种特定的趋势，而不是对所描述的业务领域的深刻理解。

在社交媒体的背景下，推特给出了一个大众分类法的经典例子。使用标签，用户可以给他们的推文贴上属于特定主题的标签。这样，其他用户可以搜索特定的主题或关注某个事件。大众分类法的特点在推特标签的使用中非常明显——它们是由发布推文的用户选择的，它们没有层次，没有正式的组织，只有那些被认为有意义的东西才能被人群接受，才能成为潮流(民主化)。

**推理推理**:在逻辑领域，推理是从已知(或假设)为真的事实中推导出逻辑结论的过程。在这种情况下经常出现的另一个术语是*推理*。看待这种二分法的一种方式是将推理视为目标，将推理视为实现。

一个经典的推论例子是苏格拉底三段论。让我们考虑以下事实(被认为是真实的断言):

*   每个人都是凡人
*   苏格拉底是个男人

从这些前提出发，一个推理系统应该能够回答这样一个问题:*苏格拉底是凡人吗？*惊心动魄的答案是*是的。*

音节的一般情况如下:

*   每一个 *A* 就是 *B*
*   *C* 是 *A*
*   所以 *C* 就是 *B*

这里 *A* 、 *B* 、 *C* 可以是类别，也可以是个人。三段论是演绎推理的经典例子。总的来说，动词*从*、*从*进行推理，或者*对*进行推理，在我们的上下文中几乎是同义词，因为它们都需要从一些已知的(真实的)事实中产生新的知识。

鉴于前面关于苏格拉底和其他人的知识库，为了回答*这样的问题，柏拉图是否凡人？*，我们需要考虑下一段。

**封闭世界和开放世界假设**:推理系统可以包含关于宇宙的封闭世界或开放世界假设。这种差异对于从有限的已知事实中得出结论至关重要。在基于封闭世界假设的系统中，每个未知的事实都被认为是错误的。另一方面，基于开放世界假设的系统假设未知的事实可能是真的，也可能不是真的，所以它们不会强迫对未知的解释是假的。通过这种方式，他们可以处理不完整的知识，换句话说，只部分指定或随着时间的推移而完善的知识库。

比如经典的逻辑编程语言 Prolog(来源于《逻辑学》中的**编程**)就是基于闭世界假设。关于苏格拉底的知识库用序言语法表示如下:

```py
mortal(X) :- man(X). 
man(socrates). 

```

柏拉图是否是凡人这个问题可以用*不*来回答。这是因为系统没有任何关于柏拉图的知识，所以无法推断出任何进一步的事实。

封闭世界和开放世界假设之间的二元论也对不同知识库的融合方式产生了影响。如果两个知识库提供相反的事实，基于封闭世界假设的系统会触发错误，因为系统无法处理这种类型的不一致。另一方面，基于开放世界假设的系统将试图通过产生更多的知识并试图找到一种方法对不一致进行排序来从不一致中恢复。一种方法是给事实分配概率:简单来说，两个相互矛盾的事实可能有 50%是真的。

作为封闭世界假设的一个例子，我们可以考虑一个航空订票数据库。系统有关于特定域的完整信息。这就导致它以一种清晰的方式回答了一些问题:如果在办理登机手续时分配了座位，并且某个乘客预订了航班，但没有分配座位，我们可以推断该乘客还没有办理登机手续，并发送电子邮件提醒该乘客在线办理登机手续的选项。

另一方面，当系统没有给定领域的完整信息时，开放世界假设适用。例如，一个在线发布职位空缺广告的基于网络的平台知道给定职位的具体位置和申请人的所在地。如果一家公司为他们在纽约的办公室发布了一个职位广告，并且他们收到了一个来自欧洲的专业人士的申请，系统不能仅仅通过使用本节中描述的知识来推断该专业人士是否有权在美国工作。由于这个原因，这样的系统可能需要明确地询问申请人这个问题，然后他们才能继续申请。

逻辑和逻辑编程是广阔的研究领域，因此不可能在一小段时间内提炼出理解该主题所有方面所必需的所有知识。为了本章的目的，我们仅限于提及，一般来说，在网络上，假设知识库不完整会更安全，因为网络是一个信息不完整的系统。其实像**资源描述框架** ( **RDF** )这样的框架都是基于开放世界的假设。

## 微格式

微格式([http://microformats.org](http://microformats.org))扩展了 HTML，允许作者指定关于个人、组织、事件和几乎任何不同类型对象的机器可读语义注释。它们本质上是惯例，为内容创作者提供了在网页中嵌入明确的结构化数据的机会。最新的发展被纳入微格式 2([http://microformats.org/wiki/microformats2](http://microformats.org/wiki/microformats2))。

有趣的微格式包括以下示例:

*   `h-card`:代表人员、组织和相关联系信息
*   `h-event`:表示事件信息，如地点、开始时间等
*   `h-geo`:表示地理坐标，与`h-card``h-event`配合使用
*   **XHTML 朋友圈** ( **XFN** ):用于通过超链接表示人际关系
*   `h-resume`:在网上发布简历和履历，包括学历、经验和技能
*   `h-product`:描述品牌、价格或描述等产品相关数据

*   `h-recipe`:这代表网络上的食谱，包括配料、数量和说明
*   `h-review`:发布任何项目的评论(例如引用`h-card`、`h-event`、`h-geo`或`h-product`，包括评级信息和描述

下面的代码片段包括一个用于通过超链接表示人际关系的 XFN 标记示例:

```py
<div> 
  <a href="http://marcobonzanini.com" rel="me">Marco<a> 
  <a href="http://example.org/Peter" rel="friend met">Peter<a> 
  <a href="http://example.org/Mary" rel="friend met">Mary<a> 
  <a href="http://example.org/John" rel="friend">John<a> 
</div> 

```

XFN 通过`rel`属性增加了常规的 HTML。一些 HTML 链接已经根据人与人之间的关系进行了标注。第一个链接被标记为`me`，这意味着该链接与文档的作者相关。其他链接使用`friend`和`met`类进行标记，后者表示有人见过面(而不是仅在线连接)。其他类包括，例如，`spouse`、`child`、`parent`、`acquaintance`或`co-worker`。

以下片段包括一个用于表示伦敦地理坐标的`h-geo`标记示例:

```py
<p class="h-geo"> 
  <span class="p-latitude">51.50722</span>, 
  <span class="p-longitude">-0.12750</span> 
</p> 

```

虽然`h-geo`是这种微格式的最新版本，但旧版本(简称`geo`)仍然被广泛使用。前面的例子可以用旧格式改写如下:

```py
<div class="geo"> 
  <span class="latitude">51.50722</span>, 
  <span class="longitude">-0.12750</span> 
</div> 

```

如果这些微格式的例子看起来很容易理解，那是因为微格式意味着简单。微格式中的*微*代表了他们非常专注于一个非常特定的领域的特征。单个来说，微格式规范解决了受限领域中定义非常明确的问题(例如描述人际关系或提供地理坐标)，仅此而已。微格式的这一方面也使它们具有高度的可组合性——一个复杂的文档可以采用几种微格式以不同的方式丰富所提供的信息。此外，一些微格式可以嵌入其他微格式以提供更丰富的数据— `h-card`可以包括`h-geo`、`h-event`可以包括`h-card`和`h-geo`等等。

## 链接数据和开放数据

术语*链接数据*是指使用 W3C 标准在网络上发布结构化数据的一套原则，其方式有利于专门的算法来利用数据之间的联系。

由于语义网不仅仅是把数据放在网上，而且是允许人们(和机器)利用这些数据并探索它，蒂姆·伯纳斯·李提出了一套规则来促进链接数据的发布，并走向他所定义的链接开放数据，即通过开放许可获得的链接数据([https://www.w3.org/DesignIssues/LinkedData.html](https://www.w3.org/DesignIssues/LinkedData.html))。

类似于通过超链接链接在一起的文档网络，数据网络也基于发布在网络上的文档并链接在一起。另一方面，数据之网是关于最通用形式的数据，而不仅仅是关于文档。描述任意*事物*的格式选择是 RDF 而不是(X)HTML。

Berners-Lee 建议的四个规则可用作构建链接数据的实施指南，如下所示:

*   用 URIs 作为事物的名称
*   使用 HTTP URIs，这样人们就可以查找这些名字
*   当有人查找 URI 时，使用标准(RDF*，SPARQL)提供有用的信息
*   包括到其他 URIs 的链接，这样他们可以发现更多的东西

这些基本原则提供了一组期望，内容发布者应该满足这些期望才能生成链接数据。

为了鼓励人们接受链接数据，已经开发了一个五星评级系统，以便人们能够自我评估他们的链接数据，并了解链接开放数据的重要性。这对于与政府相关的数据所有者来说尤其如此，这是一种迈向透明的总体努力。五星评级体系描述如下:

*   **一星**:这可以在网上获得(任何格式)，但是需要开放许可才能成为开放数据
*   **双星**:这是机器可读的结构化数据(例如，在 Excel 中代替表格的图像扫描)
*   **三星级**:此为二星级，加非专有格式(例如 CSV 代替 Excel)
*   **四星**:这个和前面所有的评级一样，但是它也使用了 W3C (RDF 和 SPARQL)的开放标准来识别事物，这样人们就可以指向你的东西
*   **五星**:这个和前面所有的评分一样，但是它也把你的数据和别人的数据联系起来，提供上下文

达到五星后，数据集的下一步是提供额外的元数据(关于数据的数据)，如果是政府数据，则分别在美国、英国和欧盟的主要数据目录中列出，如[https://www.data.gov/](https://www.data.gov/)、[https://data.gov.uk/](https://data.gov.uk/)或[http://data.europa.eu/euodp/en/data](http://data.europa.eu/euodp/en/data)。

很明显，链接数据是语义网的一个基本概念。实现链接数据的数据集的一个著名例子是 DBpedia([http://dbpedia.org](http://dbpedia.org))，这是一个旨在从维基百科上发布的信息中提取结构化知识的项目。

## 资源描述框架

**资源描述框架** ( **RDF** )来自 W3C 规范家族，最初设计为元数据模型，用于结构化知识建模([https://en . Wikipedia . org/wiki/Resource _ Description _ Framework](https://en.wikipedia.org/wiki/Resource_Description_Framework))。

类似于经典的概念建模框架，如数据库设计中流行的实体关系模型，RDF 基于声明关于资源的语句的思想。这种语句被表示为*三元组*(也称为*三元组*)，因为它们有三个基本组成部分:主语、谓语和宾语。

以下是以主谓宾三元组表示的语句示例:

```py
(Rome, capital_of, Italy) 
(Madrid, capital_of, Spain) 
(Peter, friend_of, Mary) 
(Mary, friend_of, Peter) 
(Mary, lives_in, Rome) 
(Peter, lives_in, Madrid) 

```

前面的语法是任意的，没有遵循特定的 RDF 规范，但是它作为一个例子来理解数据模型的简单性。基于三元组的存储语句的方法允许表示任何种类的知识。RDF 语句的集合可以自然地看作一个图([https://en.wikipedia.org/wiki/Graph_theory](https://en.wikipedia.org/wiki/Graph_theory))，特别是标记有向多重图；标记是因为节点(资源)和边(谓词)与信息相关联，例如边的名称；定向是因为主谓宾关系有明确的方向；多重图因为相同节点之间的多个平行边是可能的，这意味着两个相同的资源可以彼此处于几种不同的关系中，所有这些关系都具有不同的语义。

*图 9.1* 允许我们以面向图的方式可视化前面例子中表达的知识:

![Resource Description Framework](graphics/2114_09_01-1.jpg)

图 9.1:基于三重知识的图形可视化

知识表示的下一层是 RDF 何时开始，因为它允许我们为三元组定义一些额外的结构。名为`rdf:type`的谓词允许我们声明某些对象属于某种类型。除此之外，RDF Schema 或 RDFS([https://en.wikipedia.org/wiki/RDF_Schema](https://en.wikipedia.org/wiki/RDF_Schema))允许我们定义类和类之间的关系。一种相关的技术 OWL 也允许我们表达类之间的关系，但是它提供的关于数据模型的信息在约束和注释方面更丰富。

RDF 可以被序列化(即导出)成许多不同的格式。虽然 XML 可能是最著名的方法，但也有其他选择，如 N3([https://en.wikipedia.org/wiki/Notation3](https://en.wikipedia.org/wiki/Notation3))。这是为了澄清 RDF 是一种用三元组表示知识的方式，而不仅仅是一种文件格式。

## JSON-LD

几乎在前面的所有章节中，我们都已经遇到了 JSON 格式，并欣赏它的灵活性。JSON 和链接数据之间的连接是通过 JSON-LD 格式([http://json-ld.org](http://json-ld.org))来实现的，这是一种基于 JSON 的轻量级数据格式，因此对于人和机器来说都很容易读写。

当链接来自不同数据源的数据时，我们可能会面临一个模糊问题。让我们考虑以下两个代表一个人的 JSON 文档:

```py
/* Document 1 */ 
{ 
  "name": "Marco", 
  "homepage": "http://marcobonzanini.com" 
} 
/* Document 2 */ 
{ 
  "name": "mb123", 
  "homepage": "http://marcobonzanini.com" 
} 

```

我们会注意到模式是相同的:两个文档都有`name`和`homepage`属性。两个数据源可能使用具有不同含义的相同属性名，这一事实产生了歧义。事实上，第一个文档似乎使用`name`作为人名，而第二个文档使用`name`作为登录名。

我们有一种感觉，这两个文档指的是同一个实体，因为`homepage`属性的值是相同的，但是没有进一步的信息，我们无法解决这个歧义。

JSON-LD 引入了一个叫做*上下文*的简单概念。当使用上下文时，我们能够通过利用一个我们已经熟悉的概念——网址来解决像这样的模糊情况。

让我们考虑以下摘自 JSON-LD 网站的片段:

```py
{ 
  "@context": "http://json-ld.org/contexts/person.jsonld", 
  "@id": "http://dbpedia.org/resource/John_Lennon", 
  "name": "John Lennon", 
  "born": "1940-10-09", 
  "spouse": "http://dbpedia.org/resource/Cynthia_Lennon" 
} 

```

文档使用`@context`属性作为其自身有效属性及其含义列表的引用。这样，我们期望在文档中看到什么样的属性以及它们的含义就不会有任何歧义。

前面的例子还介绍了 JSON-LD 中使用的另一个特殊属性，`@id`。此属性用作全局标识符。当引用同一个实体时，不同的应用程序可以使用全局标识符。这样，来自多个数据源的数据可以被消除歧义并链接在一起。例如，一个应用程序可以采用为一个人定义的标准模式([http://json-ld.org/contexts/person.jsonld](http://json-ld.org/contexts/person.jsonld))并用附加属性对其进行扩充。

*上下文*(属性的列表和含义)和全局识别实体的方式是链接数据的基础，JSON-LD 格式以简单优雅的方式解决了这个问题。

## Schema.org

2011 年作为一些主要搜索引擎公司(谷歌、必应和雅虎！后来 Yandex 加入了他们)，这个计划是一个协作努力，为要在网页上使用的结构化标记数据创建一组共享的模式。

该提案是关于使用`schema.org`([http://schema.org/](http://schema.org/))词汇以及微格式、基于 RDF 的格式或 JSON-LD 来用关于网站本身的元数据来扩充网站内容。使用建议的模式，内容管理者可以用额外的知识来标记他们的网站，这有助于搜索引擎和其他解析器更好地理解内容。

关于组织和个人的模式被用来影响系统，例如谷歌的知识图(Knowledge Graph)，这是一个知识库，每当特定实体是搜索的主题时，它都会用语义信息增强谷歌的搜索引擎结果。

# 从数据库中挖掘关系

DBpedia 是最著名的链接数据源之一。它以维基百科为基础，通过实体之间的语义联系来扩充流行的维基百科的内容。来自 DBpedia 的结构化信息可以通过网络使用一种类似 SQL 的语言来访问，这种语言被称为 SPARQL，一种 RDF 的语义查询语言。

在 Python 中，虽然我们可以选择使用 SPARQL 查询数据库，但是我们可以利用 RDFLib 包，这是一个用于处理 RDF 的库。

从我们的虚拟环境中，我们可以使用`pip`进行安装:

```py
$ pip install rdflib

```

考虑到语义网主题的复杂性，我们更喜欢挖掘一个例子，这样您就可以有 DBpedia 功能的味道，同时，获得如何使用 RDFLib 包的概述。

`rdf_summarize_entity.py`脚本查找给定的实体，并尝试向用户输出其摘要:

```py
# Chap09/rdf_summarize_entity.py 
from argparse import ArgumentParser 
import rdflib 

def get_parser(): 
  parser = ArgumentParser() 
  parser.add_argument('--entity') 
  return parser 

if __name__ == '__main__': 
  parser = get_parser() 
  args = parser.parse_args() 
  entity_url = 'http://dbpedia.org/resource/{}'.format(args.entity) 

  g = rdflib.Graph() 
  g.parse(entity_url)

  disambiguate_url = 'http://dbpedia.org/ontology/wikiPageDisambiguates'
  query = (rdflib.URIRef(entity_url),
           rdflib.URIRef(disambiguate_url),
           None)
  disambiguate = list(g.triples(query)) 
  if len(disambiguate) > 1: 
    print("The resource {}:".format(entity_url)) 
    for subj, pred, obj in disambiguate: 
      print('... may refer to: {}'.format(obj)) 
  else: 
    query = (rdflib.URIRef(entity_url), 
             rdflib.URIRef('http://dbpedia.org/ontology/abstract'), 
             None) 
    abstract = list(g.triples(query)) 
    for subj, pred, obj in abstract: 
      if obj.language == 'en': 
        print(obj) 

```

剧本照常使用`ArgumentParser`。`--entity`参数用于将实体名称传递给脚本。该脚本按照`dbpedia.org/resource/<entity-name>`模式在 DBpedia 上建立实体 URL。该网址最初用于使用`rdflib.Graph`类构建 RDF 图。

下一步是寻找任何歧义消除，或者换句话说，检查给定的名称是否可能引用多个实体。这是通过使用实体 URL 作为主题、`wikiPageDisambiguates`关系作为谓词和一个空对象来查询图中的三元组来完成的。为了构建主语、谓语和宾语，我们需要使用`rdflib.URIRef`类，该类将实体/关系的网址作为唯一的参数。作为三重元素之一的`None`对象被`triples()`方法用作任何可能值的占位符(本质上，它是一个通配符)。

如果找到歧义消除，列表将打印出来给用户。否则，将查找实体，特别是使用`triples()`方法检索其摘要，并留下`None`对象。一个给定的实体可以有多个摘要，因为内容可以用不同的语言提供。当我们迭代结果时，我们将只打印英文版本(标有`en`)。

例如，我们可以使用脚本来总结 Python 语言的描述:

```py
$ python rdf_summarize_entity.py \
 --entity "Python"

```

仅仅用`Python`这个词在维基百科上搜索，并不能直接导致我们所期待的结果，因为这个词本身就是模糊的。事实上，有几个实体可能与查询词匹配，因此脚本将打印出完整的列表。以下是输出的简短片段:

```py
The resource http://dbpedia.org/resource/Python: 
... may refer to: http://dbpedia.org/resource/Python_(film) 
... may refer to: http://dbpedia.org/resource/
Python_(Coney_Island,_Cincinnati,_Ohio) 
... may refer to: http://dbpedia.org/resource/Python_(genus) 
... may refer to: http://dbpedia.org/resource/Python_(programming_language) 
# (more disambiguations)

```

当我们将精确的实体标识为`Python_(programming_language)`时，我们可以使用正确的实体名称重新运行脚本:

```py
$ python rdf_summarize_entity.py \
 --entity "Python_(programming_language)"

```

这一次，输出正是我们想要的。输出的简短片段如下:

```py
Python is a widely used general-purpose, high-level programming language. Its design philosophy emphasizes code readability, (snip)

```

这个例子展示了如何将基于三元组的 RDF 的高级概念模型与 Pythonic 接口相结合是一项相对简单的任务。

# 挖掘地理坐标

如前所述，`geo`和`h-geo`是发布地理信息的微格式。在阅读一本关于社交媒体的书时，人们可能会问地理元数据是否属于社交数据的描述。在考虑地理数据时，要记住的核心思想是地理信息可以在许多应用中得到利用。例如，用户可能希望对企业执行特定于位置的搜索(如在 Yelp 中)，或者查找从特定位置拍摄的照片。更一般地说，每个人都在某个地方，或者在寻找某个地方。地理元数据使应用程序能够满足特定于位置的定制需求。在社交和移动数据时代，这些定制为应用程序开发人员打开了许多新的视野。

回到我们的语义标记数据之旅，本节描述如何使用维基百科从网页中提取地理元数据。提醒一下，经典的`geo`微格式标记如下:

```py
<p class="geo"> 
  <span class="latitude">51.50722</span>, 
  <span class="longitude">-0.12750</span> 
</p> 

```

较新的`h-geo`格式也非常相似，类名有一些变化。一些维基百科模板使用稍微不同的格式，如下所示:

```py
<span class="geo">51.50722; -0.12750</span> 

```

为了从维基百科页面中提取这些信息，基本上需要执行两个步骤，检索页面本身和解析 HTML 代码，寻找所需的类名。以下部分详细描述了该过程。

## 从维基百科中提取地理数据

从网页中提取地理数据的过程很容易从头开始，例如，使用我们已经讨论过的库，如**请求**和**美汤**。尽管很简单，但有一个 Python 包专门解决这个特殊问题，叫做 **mf2py** ，它提供了一个简单的界面来快速解析给定网址的网络资源。该库还提供了一些帮助函数，用于将解析后的数据从 Python 字典移动到 JSON 字符串。

从我们的虚拟环境中，我们可以像往常一样使用`pip`安装`mf2py`:

```py
$ pip install mf2py

```

该库提供了三种方法来解析语义标记的数据，因为我们可以将一段内容作为字符串、文件指针或网址传递给`parse()`函数。下面的示例显示了如何解析带有语义标记的字符串:

```py
>>> import mf2py 
>>> content = '<span class="geo">51.50722; -0.12750</span>' 
>>> obj = mf2py.parse(doc=content)

```

`obj`变量现在包含解析内容的字典。我们可以通过将它作为 JSON 字符串转储来检查它的内容，以便进行漂亮的打印:

```py
>>> import json 
>>> print(json.dumps(obj, indent=2)) 
{ 
  "items": [ 
  { 
    "type": [ 
      "h-geo" 
    ], 
    "properties": { 
      "name": [ 
        "51.50722; -0.12750" 
      ] 
    } 
  } 
  ], 
  "rels": {}, 
  "rel-urls": {} 
}

```

如果我们想将文件指针传递给解析函数，过程非常相似，如下所示:

```py
>>> with open('some_content.xml') as fp: 
...   obj = mf2py.parse(doc=fp)

```

另一方面，如果我们想向解析函数传递一个 URL，我们需要使用`url`参数而不是`doc`参数，如下所示:

```py
>>> obj = mf2py.parse(url='http://example.com/your-url')

```

mf2py 库的解析功能基于美丽的汤。在没有显式指定解析器的情况下调用美丽的汤时，库将尝试猜测可用的最佳选项。例如，如果我们已经安装了 **lxml** (正如我们在[第 6 章](6.html "Chapter 6.  Questions and Answers on Stack Exchange")、*堆栈交换问答*中所做的那样)，这将是所选择的选项。调用`parse()`函数将会触发一个警告:

```py
UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system ("lxml"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently. 
To get rid of this warning, change this: 

 BeautifulSoup([your markup]) 

to this: 

 BeautifulSoup([your markup], "lxml")

```

这个消息是由美丽的汤提出的，但是我们没有直接与这个库交互，所以关于如何修复这个问题的建议可能会令人困惑。抑制警告的解决方法是将所需的解析器名称显式传递给`parse()`函数，如下所示:

```py
>>> obj = mf2py.parse(doc=content, html_parser='lxml')

```

在长时间介绍 mf2py 库之后，我们可以将事情放在上下文中，并尝试从维基百科页面解析地理信息。

`micro_geo_wiki.py`脚本将维基百科的网址作为输入，并显示一个带有相关坐标的地点列表，如果在内容中发现任何地理标记:

```py
# Chap09/micro_geo_wiki.py 
from argparse import ArgumentParser 
import mf2py 

def get_parser(): 
  parser = ArgumentParser() 
  parser.add_argument('--url') 
  return parser 

def get_geo(doc): 
  coords = [] 
  for d in doc['items']: 
    try: 
      data = { 
        'name': d['properties']['name'][0], 
        'geo': d['properties']['geo'][0]['value'] 
      } 
      coords.append(data) 
    except (IndexError, KeyError): 
      pass 
  return coords 

if __name__ == '__main__': 
  parser = get_parser() 
  args = parser.parse_args() 

  doc = mf2py.parse(url=args.url) 
  coords = get_geo(doc) 
  for item in coords: 
    print(item) 

```

脚本使用`ArgumentParser`从命令行获取输入。`--url`参数用于传递我们要解析的页面的 URL。

然后脚本调用`mf2py.parse()`函数，传递这样的网址作为参数，获得带有微格式信息的字典。

提取地理坐标的逻辑核心由`get_geo()`函数处理，该函数将解析后的字典作为输入，并返回字典列表作为输出。输出中的每个字典都有两个键:`name`(地名)和`geo`(坐标)。

我们可以观察正在运行的脚本，例如，使用以下命令:

```py
$ python micro_geo_wiki.py \
 --url "https://en.wikipedia.org/wiki/London"

```

运行此命令将产生以下输出:

```py
{'name': 'London', 'geo': '51.50722; -0.12750'}

```

不出所料，在维基百科关于伦敦的页面上，我们找到的唯一坐标是以伦敦市为中心的坐标。

维基百科还提供了许多包含地点列表的页面，如下所示:

```py
$ python micro_geo_wiki.py --url \
 "https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population"

```

运行此命令将产生一个长输出(超过 300 行)，其中包含几个美国城市的地理信息。以下是输出的简短片段:

```py
{'geo': '40.6643; -73.9385', 'name': '1 New York City'} 
{'geo': '34.0194; -118.4108', 'name': '2 Los Angeles'} 
{'geo': '41.8376; -87.6818', 'name': '3 Chicago'} 
# (snip)

```

这个例子证明了从网页中提取地理数据并没有什么特别困难的。mf2py 库的简单性允许我们只用几行代码就能完成任务。

下面的部分更进一步:一旦我们有了一些地理信息，我们可以用它做什么？绘制地图似乎是一个简单的答案，所以我们将利用谷歌地图来可视化我们的地理数据。

## 在谷歌地图上绘制地理数据

谷歌地图([https://www.google.com/maps](https://www.google.com/maps))是一项非常受欢迎的服务，可能不需要太多介绍。在其众多功能中，它提供了创建带有兴趣点列表的自定义地图的可能性。

一种自动创建地图并保证不同应用程序之间互操作性的方法是使用一种通用格式来共享坐标。在本节中，我们将讨论使用锁眼标记语言(**KML**)([https://en.wikipedia.org/wiki/Keyhole_Markup_Language](https://en.wikipedia.org/wiki/Keyhole_Markup_Language))作为以谷歌地图识别的格式导出数据的方式。KML 格式是一种用二维和三维地图表达地理数据的 XML 符号。最初是为另一个谷歌产品谷歌地球开发的，现在被用于各种应用。

下面的代码片段显示了一个 KML 文档的例子，用来表示伦敦在地图上的位置:

```py
<?xml version="1.0" encoding="UTF-8"?> 
<kml > 
  <Document> 
    <Placemark> 
      <name>London</name> 
      <description>London</description> 
      <Point> 
        <coordinates>-0.12750,51.50722</coordinates> 
      </Point> 
    </Placemark> 
  </Document> 
</kml> 

```

我们注意到`coordinates`标签的值。虽然从维基百科提取的地理数据遵循`latitude; longitude`格式，但 KML 标记使用`longitude,latitude,altitude`-海拔是可选的，如果省略，则假设为零，如前例所示。

为了在谷歌地图上可视化地理坐标列表，我们的中间任务是以 KML 格式生成这样的列表。在 Python 中，有一个名为 **PyKML** 的库简化了这个过程。

从我们的虚拟环境中，我们可以像往常一样使用`pip`安装库:

```py
$ pip install pykml

```

以下`micro_geo2kml.py`脚本扩展了之前从维基百科提取的地理数据，以便输出 KML 格式的文件。

### 注

该脚本试图使用 lxml 库来处理字符串的 xml 序列化。如果库不存在，它将返回到元素树库，这是 Python 标准库的一部分。关于 lxml 库的详细信息，请参考[第 6 章](6.html "Chapter 6.  Questions and Answers on Stack Exchange")、*栈交换问答*。

```py
# Chap09/micro_geo2kml.py 
from argparse import ArgumentParser 
import mf2py 
from pykml.factory import KML_ElementMaker as KML 

  try: 
    from lxml import etree 
  except ImportError: 
    import xml.etree.ElementTree as etree 

  def get_parser(): 
    parser = ArgumentParser() 
    parser.add_argument('--url') 
    parser.add_argument('--output') 
    parser.add_argument('--n', default=20) 
    return parser 

  def get_geo(doc): 
    coords = [] 
    for d in doc['items']: 
      try: 
        data = { 
          'name': d['properties']['name'][0], 
          'geo': d['properties']['geo'][0]['value'] 
        } 
        coords.append(data) 
      except (IndexError, KeyError): 
        pass 
    return coords 

  if __name__ == '__main__': 
    parser = get_parser() 
    args = parser.parse_args() 

    doc = mf2py.parse(url=args.url) 
    coords = get_geo(doc) 
    folder = KML.Folder() 
    for item in coords[:args.n]: 
      lat, lon = item['geo'].split('; ') 
      place_coords = ','.join([lon, lat]) 
      place = KML.Placemark( 
                            KML.name(item['name']), 
                            KML.Point(KML.coordinates(place_coords)) 
                            ) 
      folder.append(place) 

    with open(args.output, 'w') as fout: 
      xml = etree.tostring(folder, 
                           pretty_print=True).decode('utf8') 
      fout.write(xml) 

```

脚本使用`ArgumentParser`从命令行解析三个参数:`--url`用于获取要解析的网页，`--output`用于指定用于转储地理信息的 KML 文件的名称，可选的`--n`参数默认为`20`，用于指定我们要在地图中包含多少个地方。

在主模块中，我们将解析一个给定的页面，并像以前一样使用`get_geo()`函数获取地理坐标。这个函数的返回值是一个字典列表，每个字典都有`name`和`geo`作为键。在 KML 术语中，一个地方被称为*地标*，而一个地方列表被称为*文件夹*。换句话说，脚本必须将 Python 字典列表翻译成地标文件夹。

该任务通过将`folder`变量初始化为空的`KML.Folder`对象来执行。当我们遍历带有地理信息的字典列表时，我们将为列表中的每个条目建立一个`KML.Placemark`对象。地标由一个`KML.name`和`KML.Point`组成，是持有地理坐标的物体。

为了使用 KML 格式构建坐标，我们需要拆分坐标字符串，最初采用`latitude; longitude`格式，然后交换两个值，并用逗号替换分号，这样存储在`place_coords`变量中的最终字符串就采用了`longitude,latitude`格式。

建立地标列表后，最后一步是将对象转储到文件中的 XML 表示中。这是使用`tostring()`方法执行的，该方法返回一个`bytes`对象，因此需要在写入文件之前进行解码。

我们可以使用以下示例运行脚本:

```py
$ python micro_geo2kml.py \
 --url "https://en.wikipedia.org /wiki/
    List_of_United_States_cities_by_population" \
 --output us_cities.kml \
 --n 20

```

地理数据现在保存在`us_cities.kml`文件中，可以加载到谷歌地图中以生成定制地图。

从谷歌地图菜单中，我们可以选择**您的地点**作为我们最喜欢的地点列表，然后选择**地图**作为自定义地图列表。*图 9.2* 显示带有定制地图列表的菜单:

![Plotting geodata on Google Maps](graphics/2114_09_02.jpg)

图 9.2:谷歌地图中定制地图的列表

一旦我们点击**创建地图**，我们就有机会从一个文件中导入数据，该文件可以是电子表格、CSV 或 KML 文件。当我们选择用我们的脚本生成的`us_cities.kml`文件时，结果显示在*图 9.3* 中:

![Plotting geodata on Google Maps](graphics/2114_09_03.jpg)

图 9.3:谷歌地图上显示的美国城市人口列表

每个地标都由地图上的一个大头针标识，单击一个大头针将突出显示其详细信息，在本例中是名称和坐标。

# 总结

在这一章的开头，我们提到了网络的创造者蒂姆·伯纳斯·李的观点。将网络视为一种社会创造而非技术创造是一种软化社会和语义数据边界的观点。

我们还描述了语义网的更大图景，以及多年来提出的不同技术如何结合在一起，并提供了实现这一愿景的机会。尽管在过去的 15-20 年里，关于语义网的炒作和希望相当高，但蒂姆·伯纳斯·李的愿景还不是无处不在的。

由 W3C、政府和其他私人公司(如`schema.org`)推动的社区努力试图朝着这个方向大力推进。虽然怀疑论者有大量的材料来讨论为什么语义网的承诺还没有完全实现，但我们更愿意关注我们目前拥有的机会。

社交媒体生态系统的当前趋势表明，社交和语义数据共享深层联系。这本书探讨了从几个社交媒体平台挖掘数据的主题，利用了它们的应用编程接口提供的机会，并使用了一些最流行的 Python 工具进行数据挖掘和数据分析。虽然一本书只能触及主题的表面，但我们希望，在这一点上，您可以欣赏社交媒体环境中为数据挖掘从业者和学者提供的所有机会。