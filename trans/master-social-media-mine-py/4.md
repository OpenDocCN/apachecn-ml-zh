# 第四章。脸书上的帖子、页面和用户交互

在一本关于社交媒体挖掘的书中，读者可能期待有一章是关于脸书的。脸书成立于 2004 年，最初仅限于哈佛学生，如今它已经是一家价值数十亿美元的公司，拥有近 15 亿月活跃用户。它的流行使它成为数据挖掘的一个非常有趣的游乐场。

在本章中，我们将讨论以下主题:

*   创建一个与脸书平台互动的应用程序
*   与脸书图形应用编程接口交互
*   从经过身份验证的用户中挖掘帖子
*   挖掘脸书页面、可视化帖子和衡量参与度
*   从一组帖子构建单词云

# 脸书图形应用编程接口

脸书图形应用编程接口是脸书平台的核心，也是支持第三方与脸书集成的主要组件之一。顾名思义，它提供了一个一致的类似图形的数据视图，表示对象和它们之间的联系。不同的平台组件允许开发人员访问脸书数据，并将脸书功能集成到第三方应用程序中。

就数据挖掘机会而言，2014 年随着 API 2.0 版本的发布，出现了重大转变。数据分析的主要兴趣对象之一是社交图，即用户之间的联系(友谊)列表。由于 2.0 版本的 Graph API，想要访问这些信息的应用程序必须明确请求`user_friends`权限，但 API 只会返回同样是给定应用程序用户的好友列表。

实际上，这一选择改变了过去数据分析的金矿。本节讨论用 Python 创建脸书应用程序以及与脸书图形应用编程接口交互的基础。

## 注册您的应用

通过注册的应用程序可以访问脸书应用编程接口。开发人员必须注册他们的应用程序，以便获得使用图形应用编程接口所需的凭据。作为脸书用户，你必须明确注册为开发者才能创建应用程序，你的账户必须通过手机或信用卡进行验证。

在脸书开发者网站([https://developers.facebook.com](https://developers.facebook.com))上，程序很简单:点击**我的应用**菜单下的**添加新应用**链接，会打开对话框，如图*图 4.1* 所示，我们提供`Social Media Mining`作为示例应用的显示名称(有 32 个字符的限制):

![Registering your app](graphics/2114_04_01.jpg)

图 4.1:创建新脸书应用程序的对话框窗口

一旦我们为我们的应用选择了名称和类别，我们就可以点击**创建应用标识**来确认它的创建。

此时，我们为应用选择的名称在**我的应用**菜单下可见，点击将打开应用仪表盘，如图*图 4.2* :

![Registering your app](graphics/2114_04_02.jpg)

图 4.2:应用仪表板上的视图

该面板提供访问应用编程接口所需的关键信息，如**应用程序标识**和**应用程序机密**。要查看**应用程序机密**，您需要提供您的脸书密码来确认您的身份。不用说，出于明显的安全原因，这些细节不会与任何人分享。

在 **App ID** 和 **App Secret** 之间，还有对 **API 版本**的引用(图 4.2*中的例子是使用 **v2.5** )。默认情况下，新应用的应用编程接口版本将是可用的最新版本。在 2014 年的 F8 会议上，脸书宣布他们决定为特定的 API 版本提供至少两年的支持。这是需要记住的一条重要信息，因为当某个特定的应用编程接口版本不再受支持时，如果您不更新它，您的应用程序可能会停止正常运行。*

### 型式

**脸书平台版本化**

文档中讨论了脸书平台的版本控制策略([https://developers.facebook.com/docs/apps/versions](https://developers.facebook.com/docs/apps/versions)，以及特定版本的日落日期([https://developers.facebook.com/docs/apps/changelog](https://developers.facebook.com/docs/apps/changelog))。

最初，您的应用程序设置为开发模式。此选项仅允许您(作为作者)和从**角色**菜单中明确指定为**开发人员**或**测试人员**的任何用户访问。花一些时间在仪表板上了解基本配置选项及其含义是值得的。

## 认证和安全

为了访问用户的配置文件信息，以及关于他们与其他对象(例如，页面、地点等)交互的信息，您的应用程序必须获得具有适当权限的访问令牌。

令牌对于用户-应用程序组合是唯一的，用于处理用户授予应用程序的权限。生成访问令牌需要用户交互，这意味着用户必须向脸书确认(通常通过对话窗口)他们正在向应用程序授予所需的权限。

出于测试目的，获取访问令牌的另一种方法是使用图形应用编程接口浏览器([https://developers.facebook.com/tools/explorer](https://developers.facebook.com/tools/explorer))，这是一个由脸书开发的工具，为开发人员提供了一个与图形应用编程接口交互的方便界面。*图 4.3* 展示了图形应用编程接口浏览器的使用，在我们从可用应用程序列表中选择我们的应用程序后，我们可以从**获取令牌**菜单中单击**获取用户访问令牌**:

![Authentication and security](graphics/2114_04_03.jpg)

图 4.3:从图形应用编程接口资源管理器生成访问令牌

这个动作会打开一个对话框窗口，就像*图 4.4* 中的那个，我们可以用它来指定我们想要在访问令牌中包含什么样的权限。确认应用程序的权限将为访问令牌生成字母数字字符串，该字符串从创建时起两小时内有效。单击令牌旁边的小信息图标将显示该信息。

![Authentication and security](graphics/2114_04_04.jpg)

图 4.4:从图形应用编程接口浏览器中选择访问令牌的权限

从*图 4.4* 我们可以看到，一个脸书应用的权限是特别细粒度的。这样，安装您的应用程序的用户将完全了解他们想要与您的应用程序共享的数据类型。

对于以下示例，我们将使用诸如用户全名、位置和帖子等字段。要让代码正确检索这些信息，我们需要勾选相应的权限(例如，`user_location`、`user_posts`等)。

### 注

用户第一次访问应用程序时，脸书会显示一个对话框来总结权限列表。用户将有机会查看应用程序请求的权限列表。

## 使用 Python 访问脸书图应用编程接口

一旦定义了应用细节，我们就可以通过 Python 以编程方式访问脸书图形应用编程接口。

脸书没有为 Python 提供官方客户端。使用**请求**库实现我们自己的客户端可能是一个有趣的练习，以便理解应用编程接口的特性，但是幸运的是，已经有一些选项可以简化这个过程。

对于我们的例子，我们将使用 **facebook-sdk** ，也是基于请求库，它提供了一个易于使用的界面来从网络服务中获取数据。在撰写本文时，PyPI 上可用的库的最新版本是 1.0.0，它完全支持 Python 3。以前的版本在 Python 3 兼容性方面出现了一些问题。我们可以从虚拟环境中使用`pip`安装库，如下所示:

```py
$ pip install facebook-sdk

```

按照上一节描述的步骤获得临时令牌后，我们可以立即测试该库。

首先，就像我们在[第 2 章](2.html "Chapter 2.  #MiningTwitter – Hashtags, Topics, and Time Series")、*# miningtwetter-Hashtags、Topics 和 Time Series* 以及[第 3 章](3.html "Chapter 3. Users, Followers, and Communities on Twitter")、*Twitter 上的用户、关注者和社区*中配置我们对 Twitter 的访问一样，让我们将令牌保存在一个将由脚本读取的环境变量中。在提示符下，使用以下命令:

```py
$ export FACEBOOK_TEMP_TOKEN="your-token"

```

以下脚本`facebook_my_profile.py`连接到图形应用编程接口，并查询经过身份验证的用户的配置文件:

```py
# Chap04/facebook_my_profile.py 
import os 
import json 
import facebook 

if __name__ == '__main__': 
  token = os.environ.get('FACEBOOK_TEMP_TOKEN') 

  graph = facebook.GraphAPI(token) 
  profile = graph.get_object('me', fields='name,location') 

  print(json.dumps(profile, indent=4)) 

```

该脚本不接受任何参数，因此只需使用以下命令即可运行:

```py
$ python facebook_my_profile.py

```

输出是由应用编程接口返回的 JSON 对象的转储:

```py
{ 
  "name": "Marco Bonzanini", 
  "location": { 
    "name": "London, United Kingdom", 
    "id": "106078429431815" 
  }, 
  "id": "10207505820417553" 
} 

```

`get_object()`函数将脸书图中特定对象的 ID 或名称作为第一个参数，并返回所需的信息。在我们的示例中，`me`标识只是经过身份验证的用户的别名。在不指定第二个参数和字段的情况下，应用编程接口将简单地返回对象的标识和名称。在这种情况下，我们明确要求将`name`和`location`包含在输出中。如您所见，`location`不仅仅是一个字符串，而是一个有自己字段的复杂对象(因为没有指定其他内容，所以该位置包含的字段只是`id`和`name`)。

从`GraphAPI`类获取数据的接口非常简单。该类还提供了发布和更新脸书数据的工具，允许应用程序与脸书平台交互(例如，通过在经过身份验证的用户的墙上张贴一些内容)。

我们关注的主要方法如下:

*   `get_object(id, **args)`:这将检索一个给定了`id`的对象，并接受可选的关键字参数
*   `get_objects(ids, **args)`:这将检索一个给定了`ids`列表的对象列表，并且还接受可选的关键字参数
*   `get_connections(id, connection_name, **args)`:这将检索一个对象列表，这些对象以`connection_name`关系连接到由`id`标识的对象，并且还带有可选的关键字参数
*   `request(path, args=None, post_args=None, files=None, method=None)`:这是一种通用方法，用于实现对 API 的特定请求，使用 API 文档中定义的`path`，可选参数定义如何执行 API 调用

`facebook_my_profile.py`脚本中的示例使用了`get_object()`方法下载当前用户的配置文件。在这种情况下，给出了一个可选的关键字参数`fields`，来指定我们想要从 API 中检索的属性。文档中规定了用户配置文件的完整属性列表。

遵循 API 规范，我们可以看到如何个性化字段的字符串，以便获得给定配置文件的更多信息。特别是，我们还可以执行嵌套请求，并包含连接到给定概要文件的对象的详细信息。在我们的示例中，我们检索了位置，它是类型为**Page**([https://developers . Facebook . com/docs/graph-API/reference/Page/](https://developers.facebook.com/docs/graph-api/reference/page/))的对象。由于每个页面都附加了一些属性，我们也可以将它们包含在我们的请求中，例如，更改`get_object()`请求:

```py
profile = graph.get_object("me", fields='name,location{location}') 

```

`first_level{second_level}`语法允许查询嵌套对象。在这个特殊的例子中，命名可能会令人困惑，因为`location`是我们正在检索的一级和二级属性的名称。理解这个小难题的解决方案是理解数据类型。第一级`location`是用户档案的一个属性，它的数据类型是一个脸书页面(有自己的 ID、名称和其他属性)。第二层`location`是前述脸书页面的一个属性，它是一个实际位置的描述符，由`latitude`和`longitude`等属性组成。带有额外二级`location`的前一个脚本的输出如下:

```py
{ 
  "name": "Marco Bonzanini", 
  "location": { 
    "name": "London, United Kingdom", 
    "id": "106078429431815" 
  }, 
  "id": "10207505820417553", 
  "location": { 
    "id": "106078429431815", 
    "location": { 
      "city": "London", 
      "latitude": 51.516434161634, 
      "longitude": -0.12961888255995, 
      "country": "United Kingdom" 
    } 
  } 
} 

```

### 注

一个`location`对象([https://developers . Facebook . com/docs/graph-API/reference/v 2.5/location](https://developers.facebook.com/docs/graph-api/reference/v2.5/location))默认检索到的属性为`city`、`country`、`latitude`和`longitude`。

如本节开头所述，随着脸书图形应用编程接口的更新版本，一些数据挖掘机会受到了限制。特别是，只有当所有相关的个人资料都是我们应用程序的用户时，挖掘社交图(即友谊关系)才有可能。以下脚本试图获取经过身份验证的用户的好友列表:

```py
# Chap04/facebook_get_friends.py 
import os 
import facebook 
import json 

if __name__ == '__main__': 
  token = os.environ.get('FACEBOOK_TEMP_TOKEN') 

  graph = facebook.GraphAPI(token) 
  user = graph.get_object("me") 
  friends = graph.get_connections(user["id"], "friends") 
  print(json.dumps(friends, indent=4)) 

```

即使对应用编程接口的调用需要`user_friends`权限才能授予我们的应用程序，但脚本无论如何都无法检索到许多关于朋友的数据，因为经过身份验证的用户(即`me`)目前是应用程序的唯一用户。以下是输出示例:

```py
{ 
  "data": [], 
  "summary": { 
    "total_count": 266 
  } 
} 

```

如我们所见，我们唯一能检索到的信息是给定用户的好友总数，而好友的数据由空列表表示。如果一些朋友决定使用我们的应用程序，我们将能够通过这次通话检索他们的个人资料。

我们将在本节结束时简要介绍图形应用编程接口强加的速率限制。如文档([https://developers . Facebook . com/docs/graph-API/advanced/限速](https://developers.facebook.com/docs/graph-api/advanced/rate-limiting))中所述，很少遇到限速。限额是按应用程序和每个用户计算的，也就是说，如果应用程序达到每日费率限额，应用程序发出的所有呼叫都将受到限制，而不仅仅是给定用户的呼叫。每日津贴是根据前一天和今天登录的用户数量计算的——这个总和就是基本用户数量。然后，在 60 分钟的窗口内，该应用程序允许每个用户调用 200 次应用编程接口。虽然这对于我们的示例来说已经足够了，但是建议您查看文档，以了解费率限制在您的应用程序中可能产生的影响。

在下一节中，我们将下载认证用户的所有帖子，并开始对这些数据进行一些数据分析。

# 挖掘你的岗位

在用一个简单的例子介绍了 Python facebook-sdk 之后，我们将开始挖掘数据挖掘的机会。第一个练习是下载我们自己的帖子(也就是通过身份验证的用户发布的帖子)。

`facebook_get_my_posts.py`脚本连接到图形应用编程接口，并获得由经过身份验证的用户`me`发布的帖子列表。帖子保存在`my_posts.jsonl`文件中，使用我们已经在[第 2 章](2.html "Chapter 2.  #MiningTwitter – Hashtags, Topics, and Time Series")、*# miningtwetter-Hashtags、主题和时间序列*和[第 3 章](3.html "Chapter 3. Users, Followers, and Communities on Twitter")、*推特上的用户、关注者和社区*中采用的 JSON Lines 格式(文件的每一行都是 JSON 文档):

```py
# Chap04/facebook_get_my_posts.py 
import os 
import json 
import facebook 
import requests 

if __name__ == '__main__': 
  token = os.environ.get('FACEBOOK_TEMP_TOKEN') 

  graph = facebook.GraphAPI(token) 
  posts = graph.get_connections('me', 'posts') 

  while True:  # keep paginating 
    try: 
      with open('my_posts.jsonl', 'a') as f: 
        for post in posts['data']: 
          f.write(json.dumps(post)+"\n") 
        # get next page 
        posts = requests.get(posts['paging']['next']).json() 
    except KeyError: 
      # no more pages, break the loop 
      break 

```

该脚本不采用任何命令行参数，因此只需使用以下命令即可运行:

```py
$ python facebook_get_my_posts.py

```

这个脚本提供了一个有趣的分页示例，由于帖子列表太长，无法通过一个 API 调用来收集，所以脸书提供了分页信息。

使用`get_connections()`方法执行的初始 API 调用返回帖子的第一页(存储在`posts['data']`中)，以及在不同页面之间循环所需的详细信息，可在`posts['paging']`中获得。由于分页功能没有在 Python facebook-sdk 库中实现，我们需要通过直接使用请求库来后退。幸运的是，图形应用编程接口提供的响应包含了我们需要请求的确切网址，以便获得下一页的帖子。事实上，如果我们检查`posts['paging']['next']`变量的值，我们将看到代表要查询的精确网址的字符串，包括访问令牌、应用编程接口版本号和所有必需的细节。

分页是在`while True`循环中执行的，当我们到达最后一页时，这个循环被`KeyError`异常中断。由于最后一页将不包含对`posts['paging']['next']`的引用，试图访问字典的这个键将引发异常，唯一的目的是打破循环。

一旦脚本执行完毕，我们就可以检查`my_posts.jsonl`文件的内容了。文件的每一行都是一个 JSON 文档，它包含一个唯一的标识、贴在帖子上的消息文本以及 ISO 8601 格式的创建时间。这是一个 JSON 文档的例子，代表一篇下载的文章:

```py
{ 
  "created_time": "2015-11-04T08:01:21+0000", 
  "id": "10207505820417553_10207338487234328", 
  "message": "The slides of my lighting talk at the PyData London 
    meetup last night\n" 
} 

```

就像`get_object()`函数一样，`get_connections()`也可以带一个`fields`参数，以便检索所需对象的更多属性。以下脚本重构了前面的代码，以便为我们的帖子获取更多有趣的属性:

```py
# Chap04/facebook_get_my_posts_more_fields.py 
import os 
import json 
import facebook 
import requests 

if __name__ == '__main__': 
  token = os.environ.get('FACEBOOK_TEMP_TOKEN') 

  graph = facebook.GraphAPI(token) 
  all_fields = [ 
    'message', 
    'created_time', 
    'description', 
    'caption', 
    'link', 
    'place', 
    'status_type' 
  ] 
  all_fields = ','.join(all_fields) 
  posts = graph.get_connections('me', 'posts', fields=all_fields) 

  while True:  # keep paginating 
    try: 
      with open('my_posts.jsonl', 'a') as f: 
        for post in posts['data']: 
          f.write(json.dumps(post)+"\n") 
        # get next page 
        posts = requests.get(posts['paging']['next']).json() 
    except KeyError: 
      # no more pages, break the loop 
      break 

```

我们想要检索的所有字段都在`all_fields`列表中声明，然后按照图形应用编程接口的要求，该列表被连接成一串逗号分隔的属性名称。然后，该值通过`fields`关键字参数传递给`get_connections()`方法。

下一节讨论关于帖子结构的更多细节。

## 柱子的结构

帖子是一个复杂的对象，因为它本质上可以是用户决定发布的任何内容。下表总结了帖子的有趣属性，并简要描述了它们的含义:

<colgroup><col> <col></colgroup> 
| **属性名称** | **描述** |
| `id` | 表示唯一标识符的字符串 |
| `application` | 带有用于发布此帖子的应用程序信息的`App`对象 |
| `status_type` | 代表帖子类型的字符串(例如`added_photos`或`shared_story`) |
| `message` | 表示帖子状态消息的字符串 |
| `created_time` | 以 ISO 8601 格式发布帖子的日期字符串 |
| `updated_time` | 以 ISO 8601 格式显示上次修改日期的字符串 |
| `message_tags` | 邮件中标记的配置文件列表 |
| `from` | 发布消息的配置文件 |
| `to` | 帖子中提到或针对的个人资料列表 |
| `place` | 贴在帖子上的位置信息 |
| `privacy` | 对象的隐私设置 |
| `story_tags` | 与`message_tags`相同 |
| `with_tags` | 标记为*的个人资料列表与帖子作者*在一起 |
| `properties` | 任何附加视频的属性列表(例如，视频的长度) |

一个`Post`对象的属性甚至比前一个表中的属性更多。官方文档中给出了完整的属性列表([https://developers . Facebook . com/docs/graph-API/reference/v 2.5/post](https://developers.facebook.com/docs/graph-api/reference/v2.5/post))，其中这个对象的复杂性更加清晰。

## 时频分析

下载完我们所有的帖子后，我们会根据不同帖子的创建时间进行第一次分析。该分析的目的是突出用户的行为，例如用户在一天中的什么时间在脸书发布最多的内容。

`facebook_post_time_stats.py`脚本使用`ArgumentParser`获得命令行输入，即带有帖子的`.jsonl`文件:

```py
# Chap04/facebook_post_time_stats.py 
import json 
from argparse import ArgumentParser 
import dateutil.parser 
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt 
from datetime import datetime 

def get_parser(): 
  parser = ArgumentParser() 
  parser.add_argument('--file', 
                      '-f', 
                      required=True, 
                      help='The .jsonl file with all the posts') 
  return parser 

if __name__ == '__main__': 
  parser = get_parser() 
  args = parser.parse_args() 
  with open(args.file) as f: 
    posts = [] 
    for line in f: 
      post = json.loads(line) 
      created_time = dateutil.parser.parse(post['created_time']) 
      posts.append(created_time.strftime('%H:%M:%S')) 
    ones = np.ones(len(posts)) 
    idx = pd.DatetimeIndex(posts) 
    # the actual series (a series of 1s for the moment) 
    my_series = pd.Series(ones, index=idx) 

    # Resampling into 1-hour buckets 
    per_hour = my_series.resample('1H', how='sum').fillna(0) 

    # Plotting 
    fig, ax = plt.subplots() 
    ax.grid(True) 
    ax.set_title("Post Frequencies") 
    width = 0.8 
    ind = np.arange(len(per_hour)) 
    plt.bar(ind, per_hour) 
    tick_pos = ind + width / 2 
    labels = [] 
    for i in range(24): 
      d = datetime.now().replace(hour=i, minute=0) 
      labels.append(d.strftime('%H:%M')) 
    plt.xticks(tick_pos, labels, rotation=90) 
    plt.savefig('posts_per_hour.png') 

```

该脚本可以从命令行运行，如下所示:

```py
$ python facebook_post_time_stats.py -f my_posts.jsonl

```

该脚本首先生成一个帖子列表，其中包含每个帖子的创建时间。`dateutil.parser.parse()`函数有助于将 ISO 8601 日期字符串读入`datetime`对象，然后使用`strftime()`函数将其转换为 HH:MM:SS 字符串。

然后，创建时间列表被用来索引熊猫系列，这最初只是一系列熊猫。然后，该系列按小时重新采样，总结帖子。在这一点上，我们有一系列的 24 个项目，一天中的每个小时一个，以及在那个特定的小时内发布的帖子的数量。脚本的最后一部分旨在将系列绘制为简单的条形图，以便可视化一天中帖子的分布。

*图 4.5* 为曲线图:

![Time frequency analysis](graphics/2114_04_05.jpg)

图 4.5:帖子的时间频率

如我们所见，出版时间在下午晚些时候和晚上之间达到高峰，但它在白天传播得相当好(在晚上和凌晨记录的频率最少)。需要考虑的一个方面是不考虑位置，即创建时间归一化为**协调世界时** ( **UTC** )，因此不考虑原时区。例如，如果下午 4 点在美国东海岸发布了一篇文章，其创建时间将记录为世界协调时晚上 9 点。这是因为**东部标准时间** ( **东部标准时间**)相当于 UTC - 05:00，即标准时间(秋冬季，不遵守夏令时)比 UTC 晚 5 小时。

# 挖掘脸书页面

脸书不仅被想与朋友和亲戚联系的个人使用，也被想与人交往的公司、品牌和组织使用。脸书页面由脸书个人帐户创建和管理，可以通过多种方式使用:

*   共享企业信息(例如，餐馆或网上商店)
*   代表名人(例如，足球运动员或摇滚乐队)
*   与观众联系(例如，作为在线社区的扩展)

与个人账户不同，Pages 可以发布公开可见的帖子。普通用户可以喜欢一个页面，这意味着他们将直接在他们的新闻提要(即他们的个性化脸书主页)上接收由该页面发布的内容更新。

例如，Packt Publishing 的脸书页面位于[https://www.facebook.com/PacktPub](https://www.facebook.com/PacktPub)，它包含了关于 PacktPub 的一般信息，以及与 PacktPub 发布的书籍、电子书和视频教程相关的帖子。

从 API 文档([https://developers . Facebook . com/docs/graph-API/reference/page](https://developers.facebook.com/docs/graph-api/reference/page))中，我们可以看到给定 Page 可用的各种信息。特别是，页面的有趣属性包括以下内容:

*   `id`:该页面的数字标识符
*   `name`:脸书显示的页面名称
*   `about`:页面的文字描述
*   `link`:本页的脸书链接
*   `website`:组织的网页(如果有)
*   `general_info`:页面一般信息的文本字段
*   `likes`:喜欢本页面的用户数量

一个`Page`对象也可以连接到其他对象，文档用与其他对象的边(即连接)来描述所有的可能性。例如:

*   `posts`:页面发布的帖子列表
*   `photos`:本页照片
*   `albums`:本页发布的相册列表
*   `picture`:本页简介图片

特定类型的 Pages 还可以显示针对其业务类型定制的附加信息(例如，餐厅可以显示其营业时间，乐队可以显示乐队成员列表，等等)。

以下脚本查询脸书图形应用编程接口来检索关于特定脸书页面的一些基本信息:

```py
# Chap04/facebook_get_page_info.py 
import os 
import json 
import facebook 
from argparse import ArgumentParser 

def get_parser(): 
  parser = ArgumentParser() 
  parser.add_argument('--page') 
  return parser 

if __name__ == '__main__': 
  parser = get_parser() 
  args = parser.parse_args() 

  token = os.environ.get('FACEBOOK_TEMP_TOKEN') 
  fields = [ 
    'id', 
    'name', 
    'about', 
    'likes', 
    'website', 
    'link' 
  ] 
  fields = ','.join(fields) 

  graph = facebook.GraphAPI(token) 
  page = graph.get_object(args.page, fields=fields) 

  print(json.dumps(page, indent=4)) 

```

该脚本使用`ArgumentParser`实例从命令行获取页面名称(或页面标识)，例如:

```py
$ python facebook_get_page_info.py --page PacktPub

```

输出如下:

```py
{ 
  "id": "204603129458", 
  "website": "http://www.PacktPub.com", 
  "likes": 6357, 
  "about": "Packt Publishing provides books, eBooks, video  
    tutorials, and articles for IT developers, administrators, and 
    users.", 
  "name": "Packt Publishing", 
  "link": "https://www.facebook.com/PacktPub/" 
} 

```

我们还可以使用脸书图形应用编程接口浏览器来获得可用字段的概述。

## 从页面获取帖子

在讨论了如何获取页面的基本信息后，我们将检查下载页面发布的帖子并以允许稍后分析帖子的常用 JSON Lines 格式存储它们的过程。

该过程类似于用于下载由经过身份验证的用户发布的帖子的过程，但是它还包括用于计算用户参与度的信息:

```py
# Chap04/facebook_get_page_posts.py 
import os 
import json 
from argparse import ArgumentParser 
import facebook 
import requests 

def get_parser(): 
  parser = ArgumentParser() 
  parser.add_argument('--page') 
  parser.add_argument('--n', default=100, type=int) 
  return parser 

if __name__ == '__main__': 
  parser = get_parser() 
  args = parser.parse_args() 

  token = os.environ.get('FACEBOOK_TEMP_TOKEN') 

  graph = facebook.GraphAPI(token) 
  all_fields = [ 
    'id', 
    'message', 
    'created_time', 
    'shares', 
    'likes.summary(true)', 
    'comments.summary(true)' 
  ] 
  all_fields = ','.join(all_fields) 
  posts = graph.get_connections('PacktPub', 
                                'posts', 
                                fields=all_fields) 

  downloaded = 0 
  while True:  # keep paginating 
    if downloaded >= args.n: 
      break 
    try: 
      fname = "posts_{}.jsonl".format(args.page) 
      with open(fname, 'a') as f: 
        for post in posts['data']: 
          downloaded += 1 
          f.write(json.dumps(post)+"\n") 
        # get next page 
        posts = requests.get(posts['paging']['next']).json() 
    except KeyError: 
      # no more pages, break the loop 
      break 

```

该脚本使用`ArgumentParser`的一个实例从命令行获取页面名称或页面 ID，以及我们想要下载的帖子数量。在示例代码中，帖子数量是可选的(默认为`100`)。正如我们之前从认证用户那里下载帖子时所做的那样，我们将定义想要包含在结果中的字段列表。特别是，由于我们将使用这些数据来执行一些与用户参与度相关的分析，我们希望结果包含关于帖子被喜欢、分享或评论的次数的信息。这是通过添加`shares`、`likes.summary(true)`和`comments.summary(true)`字段来实现的。对于喜欢和评论，额外的`summary(true)`属性是必需的，以包括汇总统计，即聚合计数。

下载在`while True`循环中执行，使用的方法类似于认证用户发布的方法。差异由`downloaded`计数器给出，该计数器针对每个检索到的帖子递增。这是用来限制我们想要下载的帖子数量的，主要是因为 Pages 经常发布大量的内容。

该脚本可以按如下方式运行:

```py
$ python facebook_get_page_posts.py --page PacktPub --n 500

```

运行前面的命令将查询脸书图形应用编程接口，并产生具有 500 个帖子的`posts_PacktPub.jsonl`文件，每行一个。下面的代码展示了一个印刷精美的单个帖子的例子(即`.jsonl`文件的一行):

```py
{ 
  "id": "post-id", 
  "created_time": "date in ISO 8601 format", 
  "message": "Text of the message", 
  "comments": { 
    "data": [ /* list of comments */ ], 
    "paging": { 
      "cursors": { 
        "after": "cursor-id", 
        "before": "cursor-id" 
      } 
    }, 
    "summary": { 
      "can_comment": true, 
      "order": "ranked", 
      "total_count": 4 
    } 
  }, 
  "likes": { 
    "data": [ /* list of users */ ], 
    "paging": { 
      "cursors": { 
        "after": "cursor-id", 
        "before": "cursor-id" 
      } 
    }, 
    "summary": { 
      "can_like": true, 
      "has_liked": false, 
      "total_count": 10 
    } 
  }, 
  "shares": { 
    "count": 9 
  } 
} 

```

我们可以看到，帖子是一个复杂的对象，有不同的嵌套信息。衡量用户参与度的字段有`shares`、`likes`和`comments`。`shares`字段仅报告分享故事的用户总数。由于隐私设置，其他细节不包括在内。当用户分享一条内容时，他们实际上是在创建自己的帖子，所以这条新帖子不应该被他们网络之外的其他用户看到。另一方面，`comments`和`likes`字段是连接到帖子本身的对象，因此有更多细节可供它们使用。

对于`comments`字段，`data`键包含带有注释相关信息的对象列表。特别是，每个注释如下所示:

```py
{ 
  "created_time": "date in ISO 8601 format", 
  "from": { 
    "id": "user-id", 
    "name": "user-name" 
  }, 
  "id": "comment-id", 
  "message": "text of the message" 
} 

```

`comments`对象还包括一个`paging`字段，用于保存对光标的引用，以防注释数量超过一页。给定原始请求，明确引用`summary(true)`属性，也包括简短的摘要统计。我们尤其对`total_count`感兴趣。

`likes`对象有点类似于`comments`对象，尽管在这种情况下数据没有那么复杂。具体来说，有一个喜欢这篇文章的用户的用户标识列表。同样，我们也有总喜欢数的汇总统计，因为在请求中指定了`summary(true)`属性。

数据下载后，我们可以执行不同类型的离线分析。

### 脸书反应和图形应用编程接口 2.6

这一章起草后不久，脸书推出了一个名为**反应**的新功能。Reactions 是 *Like* 按钮的延伸，它允许用户表达他们对特定帖子的感觉，而不仅仅是 Like。用户现在能表达的新感情叫爱，哈哈，哇，伤心，生气(还有喜欢)。*图 4.6* 向用户展示了新按钮的外观:

![Facebook Reactions and the Graph API 2.6](graphics/2114_04_06.jpg)

图 4.6:脸书反应的可视化

支持脸书反应的图形应用编程接口的第一个版本是 2.6。本章中的示例主要基于 API 的 2.5 版本，因此如果您计划在数据分析中包含此功能，您应该确保指向正确的版本。关于如何访问与 Reactions 相关的数据的信息可以在官方文档中找到([https://developers . Facebook . com/docs/graph-API/reference/post/Reactions](https://developers.facebook.com/docs/graph-api/reference/post/reactions))。

从开发人员的角度来看，Reactions 与 Likes 非常相似。如果我们请求关于对应用编程接口的反应的信息，我们为每个帖子返回的文档结构类似于下面的代码:

```py
{ 
  "message": "The content of the post", 
  "created_time": "creation date in ISO 8601", 
  "id": "the ID of the post", 
  "reactions": { 
    "data": [ 
      { 
        "id": "some-user-id", 
        "name": "John Doe", 
        "type": "WOW" 
      }, 
      { 
        "id": "another-user-id", 
        "name": "Jane Doe", 
        "type": "LIKE" 
      }, 
      /* more reactions... */ 
    ] 
  }, 
  "likes": { 
    "data": /* data about likes as before */ 
  } 
} 

```

虽然本章中的示例将点赞和评论作为理解参与度的一种方式，但它们可以很容易地扩展到包括这一新功能，该功能旨在帮助出版商更好地理解他们的用户和朋友如何看待他们的内容。

## 测量啮合

我们从分析脸书·佩奇发布的帖子开始，讨论了一些与用户参与度相关的问题。图形应用编程接口提供的数据包括以下细节:

*   帖子被分享的次数
*   喜欢这篇文章的用户数量
*   与帖子相关的用户数量
*   帖子上的评论数量

事实上，分享、喜欢和评论都是用户可以对他人发布的内容进行的操作。虽然很难理解具体行为背后的真正原因(例如，如何区分讽刺的喜欢和真实的喜欢？)，而对这类行为背后的心理进行深度分析，已经超出了本书的范围，所以为了这些数据挖掘的例子，我们就假设互动越多的帖子越*成功*。术语*成功*在这里是双引号，因为我们还没有定义*成功*的确切含义，所以在这一部分，我们只是计算用户与页面交互的次数。

以下脚本打印了一些关于脸书·佩奇发布的帖子的信息，该帖子的互动次数最多:

```py
# Chap04/facebook_top_posts.py 
import json 
from argparse import ArgumentParser 

def get_parser(): 
  parser = ArgumentParser() 
  parser.add_argument('--page') 
  return parser 

if __name__ == '__main__': 
  parser = get_parser() 
  args = parser.parse_args() 

  fname = "posts_{}.jsonl".format(args.page) 

  all_posts = [] 
  with open(fname) as f: 
    for line in f: 
      post = json.loads(line) 
      n_likes = post['likes']['summary']['total_count'] 
      n_comments = post['comments']['summary']['total_count'] 
      try: 
        n_shares = post['shares']['count'] 
      except KeyError: 
        n_shares = 0 
      post['all_interactions'] = n_likes + n_shares + n_comments 
      all_posts.append(post) 
  most_liked_all = sorted(all_posts, 
                          key=lambda x: x['all_interactions'], 
                          reverse=True) 
  most_liked = most_liked_all[0] 
  message = most_liked.get('message', '-empty-') 
  created_at = most_liked['created_time'] 
  n_likes = most_liked['likes']['summary']['total_count'] 
  n_comments = most_liked['comments']['summary']['total_count'] 
  print("Post with most interactions:") 
  print("Message: {}".format(message)) 
  print("Creation time: {}".format(created_at)) 
  print("Likes: {}".format(n_likes)) 
  print("Comments: {}".format(n_comments)) 
  try: 
    n_shares = most_liked['shares']['count'] 
    print("Shares: {}".format(n_shares)) 
  except KeyError: 
    pass 
  print("Total: {}".format(most_liked['all_interactions'])) 

```

脚本使用`ArgumentParser`获取命令行参数，可以如下运行:

```py
$ python facebook_top_posts.py --page PacktPub

```

当我们遍历帖子时，我们为每个帖子引入一个`all_interactions`键，计算为喜欢、分享和评论的总和。如果帖子还没有被任何用户分享，那么`shares`键将不会出现在字典中，因此对`post['shares']['count']`的访问已经被包括在`try/except`块中，如果该键不存在，该块默认共享数量为 0。

新的`all_interactions`键被用作`sorted()`功能中的排序选项，它返回按反向交互次数排序的帖子列表。

脚本的最后部分只是打印一些信息。输出如下:

```py
Post with most interactions: 
Message: It's back! Our $5 sale returns! 
Creation time: 2015-12-17T11:51:00+0000 
Likes: 10 
Comments: 4 
Shares: 9 
Total: 23

```

虽然找到互动次数最多的帖子是一个有趣的练习，但它并没有告诉我们太多关于整体情况的信息。

我们的下一步是验证一天中的特定时间是否比其他时间更成功，也就是说，在给定时间发布帖子是否会给我们带来更多的互动。

下面的脚本使用`pandas.DataFrame`来聚合交互的统计数据，并使用一个小时的时段来绘制结果，就像我们之前对经过身份验证的用户所做的那样:

```py
# Chap04/facebook_top_posts_plot.py 
import json 
from argparse import ArgumentParser 
import numpy as np 
import pandas as pd 
import dateutil.parser 
import matplotlib.pyplot as plt 
from datetime import datetime 

def get_parser(): 
  parser = ArgumentParser() 
  parser.add_argument('--page') 
  return parser 

if __name__ == '__main__': 
  parser = get_parser() 
  args = parser.parse_args() 

  fname = "posts_{}.jsonl".format(args.page) 

  all_posts = [] 
  n_likes = [] 
  n_shares = [] 
  n_comments = [] 
  n_all = [] 
  with open(fname) as f: 
    for line in f: 
      post = json.loads(line) 
      created_time = dateutil.parser.parse(post['created_time']) 
      n_likes.append(post['likes']['summary']['total_count']) 
      n_comments.append(post['comments']['summary']['total_count']) 
      try: 
        n_shares.append(post['shares']['count']) 
      except KeyError: 
        n_shares.append(0) 
      n_all.append(n_likes[-1] + n_shares[-1] + n_comments[-1]) 
      all_posts.append(created_time.strftime('%H:%M:%S')) 

    idx = pd.DatetimeIndex(all_posts) 
    data = { 
      'likes': n_likes, 
      'comments': n_comments, 
      'shares': n_shares, 
      'all': n_all 
    } 
    my_series = pd.DataFrame(data=data, index=idx) 

    # Resampling into 1-hour buckets 
    per_hour = my_series.resample('1h', how='sum').fillna(0) 

    # Plotting 
    fig, ax = plt.subplots() 
    ax.grid(True) 
    ax.set_title("Interaction Frequencies") 
    width = 0.8 
    ind = np.arange(len(per_hour['all'])) 
    plt.bar(ind, per_hour['all']) 
    tick_pos = ind + width / 2 
    labels = [] 
    for i in range(24): 
      d = datetime.now().replace(hour=i, minute=0) 
      labels.append(d.strftime('%H:%M')) 
    plt.xticks(tick_pos, labels, rotation=90) 
    plt.savefig('interactions_per_hour.png') 

```

可以使用以下命令运行该脚本:

```py
$ python facebook_top_posts_plot.py --page PacktPub

```

输出是保存在`interactions_per_hour.png`文件中的 matplotlib 图形，我们可以在*图 4.7* 中可视化。

代码在`.jsonl`文件中循环，建立列表来存储每个帖子的各种统计数据:点赞数、分享数、评论数以及它们的总和。这些列表中的每一个都是数据框中的一列，使用创建时间(只是时间，而不是日期)对其进行索引。使用之前已经应用的重采样技术，一个小时时段内发布的所有帖子将被聚合，并且它们的频率将被求和。出于本练习的目的，只考虑交互的总和，但也可以单独绘制单个统计数据:

![Measuring engagement](graphics/2114_04_07.jpg)

图 4.7:交互频率(每小时合计)

该图显示了分别标记为 **08:00** 和 **09:00** 的时段中的最高互动数量，这意味着在聚合中，上午 8 点之后和上午 10 点之前发布的帖子获得了最高互动数量。

观察数据集后，我们注意到上午 8 点到 10 点的时间段也是大多数帖子发布的时间。如果我们决定绘制帖子频率，在这种情况下，我们将观察到类似于在*图 4.7* 中观察到的分布(这是留给感兴趣的读者的练习)。

这种情况下，事情的核心是聚合模式，也就是重采样方法中的`how`属性。当我们使用`sum`时，上午 8 点到 10 点的时段似乎有更多的交互，仅仅是因为有更多的帖子可以交互，所以总的来说并不能真正告诉我们这些帖子是否普遍成功。解决方法很简单:我们可以将脚本修改为`how='mean'`并重新运行代码，而不是`how='sum'`。输出如下*图 4.8* 所示:

![Measuring engagement](graphics/2114_04_08.jpg)

图 4.8:交互频率(平均每小时)

*图 4.8* 显示了每小时聚集的平均交互次数的分布。在这个图中，上午 8 点到 10 点的时间段看起来没有之前那么成功。相反，最多的相互作用集中在凌晨 1 点至 5 点之间，有两个峰值。

这个简单的练习表明，以不同的方式聚合数据可以为同一个问题给出不同的结果。将讨论进行得更深入一点，需要注意的是，在这种情况下，我们没有两条重要的信息。首先，我们没有关于有多少人喜欢 PacktPub 页面的历史信息，也就是说，有多少人真正看到了他们新闻提要中的帖子。如果许多人最近才开始关注该页面，他们就不太可能与旧帖子互动，因此统计数据可能偏向于新帖子。其次，我们没有关于人口统计的信息，尤其是用户的位置:凌晨 1 点至 5 点的时间段似乎有点奇怪，但如前所述，创建时间被标准化为世界协调时时区。换句话说，世界协调时凌晨 1 点至 5 点对应于例如印度的早晨或太平洋海岸的下午晚些时候/晚上。

如果我们有关于用户人口统计的知识，我们也可以了解发布新帖子的最佳时间。例如，如果我们的大多数用户位于太平洋海岸，并且喜欢在下午晚些时候进行互动(根据他们的时区)，那么将帖子发布时间安排在世界协调时凌晨 1 点是值得的。

## 将帖子可视化为单词云

在分析了交互之后，我们将注意力转移回帖子的内容。

单词云，也称为标签云([https://en.wikipedia.org/wiki/Tag_cloud](https://en.wikipedia.org/wiki/Tag_cloud))，是文本数据的视觉表示。每个单词的重要性通常由它在图像中的大小来表示。

在本节中，我们将使用**单词云** Python 包，它提供了一种极其简单的方法来生成单词云。首先，我们需要使用以下命令在虚拟环境中安装库及其依赖项(一个映像库):

```py
$ pip install wordcloud
$ pip install Pillow

```

**枕头**是老的**蟒蛇成像库** ( **PIL** )项目的分叉，因为 PIL 显然已经停产了。在它的特性中，枕头支持 Python 3，所以在这个简短的安装之后，我们就可以开始了。

下面的脚本读取一个`.jsonl`文件，作为从 PacktPub 存储帖子的文件，并创建一个带有单词 cloud 的`.png`文件:

```py
# Chap04/facebook_posts_wordcloud.py 
import os 
import json 
from argparse import ArgumentParser 
import matplotlib.pyplot as plt 
from nltk.corpus import stopwords 
from wordcloud import WordCloud 

def get_parser(): 
  parser = ArgumentParser() 
  parser.add_argument('--page') 
  return parser 

if __name__ == '__main__': 
  parser = get_parser() 
  args = parser.parse_args() 

  fname = "posts_{}.jsonl".format(args.page) 

  all_posts = [] 
  with open(fname) as f: 
    for line in f: 
      post = json.loads(line) 
      all_posts.append(post.get('message', '')) 
  text = ' '.join(all_posts) 
  stop_list = ['save', 'free', 'today', 
               'get', 'title', 'titles', 'bit', 'ly'] 
  stop_list.extend(stopwords.words('english')) 
  wordcloud = WordCloud(stopwords=stop_list).generate(text) 
  plt.imshow(wordcloud) 
  plt.axis("off") 
  image_fname = 'wordcloud_{}.png'.format(args.page) 
  plt.savefig(image_fname) 

```

像往常一样，脚本使用`ArgumentParser`的实例来获取命令行参数(页面名称或页面标识)。

该脚本创建了一个列表`all_posts`，其中包含每个帖子的文本消息。我们使用`post.get('message', '')`而不是直接访问字典，因为`message`键可能不会出现在每个帖子中(例如，在没有评论的图像的情况下)，即使这个事件非常罕见。

帖子列表然后被连接成一个字符串，`text`，这将是生成单词云的主要输入。`WordCloud`对象采用一些可选参数来定义词云的某些方面。特别是，该示例使用`stopwords`参数来定义将从单词云中移除的单词列表。我们在此列表中包含的单词是在**自然语言工具包** ( **NLTK** )库中定义的标准英语停止词，以及一些在 PacktPub 帐户中经常使用但并不真正具有有趣含义的自定义关键词(例如，指向`bit.ly`的链接和特定标题的报价参考)。

输出图像示例如下图*图 4.9* :

![Visualizing posts as a word cloud](graphics/2114_04_09.jpg)

图 4.9:单词云示例

上图展示了 Packt Publishing 提供的一些产品(书籍、电子书和视频)，以及他们的出版物中讨论的一些主要技术(如 Python、JavaScript、R、Arduino、游戏开发等)，主要关键词为 **data** 。

# 总结

本章介绍了在社交媒体领域使用脸书的一些数据挖掘应用程序，他是一个大玩家，如果不是主要玩家的话。

在讨论了脸书图形应用编程接口的一些方面及其发展，以及在数据挖掘方面的含义后，我们构建了一个脸书应用程序，用于与脸书平台接口。

本章中显示的数据挖掘应用程序与经过身份验证的用户和脸书页面的配置文件相关。我们看到了如何计算与经过身份验证的用户的发布习惯以及关注给定页面更新的用户的交互相关的统计数据。使用简单的聚合和可视化技术，这种类型的分析可以用相对少量的代码来执行。最后，我们使用了一种称为词云的技术来可视化重要的关键词，这些关键词有望代表一堆已发布的帖子中的重要主题。

下一章的重点是谷歌开发的最新社交网络之一 Google+。