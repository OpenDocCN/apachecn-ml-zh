# 构建推荐引擎

像许多事情一样，它诞生于沮丧和僵硬的鸡尾酒。那是一个星期六，两个年轻人又一次陷入了没有约会对象的困境。当他们坐在那里倒饮料，分享悲伤时，这两位哈佛新生开始充实一个想法。如果他们可以使用计算机算法，而不是依靠随机的机会遇到合适的女孩呢？

他们认为，匹配人的关键是创造一组问题，提供每个人在第一次尴尬约会中真正想要的信息。通过匹配使用这些问卷的人，你可以排除那些最好避免的日期。这个过程会非常高效。

这个想法是向波士顿和全国各地的大学生推销他们的新服务。很快，他们就这么做了。

不久之后，他们建立的数字婚介服务取得了巨大成功。它受到了全国媒体的关注，并在接下来的几年里产生了数万场比赛。该公司非常成功，事实上，它最终被一家更大的公司收购，该公司希望使用其技术。

如果你认为我说的是 **OkCupid** ，那你就错了——而且会推迟 40 年。我所说的这家公司从 1965 年开始做所有这些事情——当时在 IBM 1401 主机上使用穿孔卡片进行计算匹配。仅仅运行计算也花了三天时间。

但奇怪的是，OkCupid 和它 1965 年的前身兼容性研究公司有联系。兼容性研究公司的联合创始人是杰夫·塔尔，他的女儿詹妮弗·塔尔是 OkCupid 联合创始人克里斯·科恩的妻子。世界真小。

但是为什么这些都与构建推荐引擎的章节有关呢？因为这很可能是第一次。虽然大多数人倾向于将推荐引擎视为寻找他们可能欣赏的密切相关的产品或音乐和电影的工具，但最初的化身是寻找潜在的伴侣。作为思考这些系统如何工作的模型，它提供了一个很好的参考框架。

在本章中，我们将探讨不同种类的推荐系统。我们将看看它们是如何在商业上实现的，以及它们是如何工作的。最后，我们将实现自己的推荐引擎来寻找 GitHub 存储库。

我们将在本章中讨论以下主题:

*   协同过滤
*   基于内容的过滤
*   混合系统
*   构建推荐引擎

# 协同过滤

2012 年初，一个故事发生了，一个男人走进明尼阿波利斯的塔吉特百货商店，抱怨一本书的优惠券被送到他家。事实上，他对这些优惠券相当恼火，这些优惠券是寄给当时还是高中生的女儿的。尽管这看起来像是对潜在省钱机会的奇怪反应，但了解到优惠券只针对产前维生素、尿布、婴儿配方奶粉、婴儿床等产品可能会改变你的看法。

经理听到投诉后，连连道歉。事实上，他感觉很糟糕，几天后他打电话跟进，解释这是怎么发生的。但是在经理还没来得及开始道歉的时候，父亲就开始向经理道歉。事实证明，他的女儿实际上已经怀孕了，她的购物习惯已经把她出卖了。

送走她的算法可能是基于——至少部分是基于——一种在推荐引擎中使用的算法，称为**协同过滤**。

# 那么，什么是协同过滤呢？

协同过滤是基于这样一种想法:在世界的某个地方，你有一个味觉二重身——一个对《星球大战》有多好和《T2》有多糟糕有着相同想法的人。

这个想法是，你给一些物品打分的方式和另一个人，这个二重身给它们打分的方式非常相似，但是你们每个人都给其他人没有给的物品打分。因为你已经确定了你的品味是相似的，推荐可以从你的二重身评价很高但你没有评价的项目中产生，反之亦然。这在某种程度上很像数字婚介，但结果是你喜欢的歌曲或产品，而不是真实的人。

因此，以我们怀孕的高中生为例，当她购买了无气味乳液、棉球和维生素补充剂的正确组合时，她很可能发现自己与后来继续购买婴儿床和尿布的人配对了。

让我们通过一个例子来看看这在实践中是如何工作的。

我们将从所谓的**效用矩阵**开始。这类似于**术语-文档矩阵**，但是，我们将代表产品和用户，而不是术语和文档。

在这里，我们假设我们有客户 *A-D* 和一组产品，他们已经从 0 到 5 进行了评级:

| **客户** | **斯纳克薯片** | **搜搜爽滑**T2**乳液** | **杜飞**T2**啤酒** | **自来水**
**自来水** | **【xxlargelivin】**
**泽西岛** | **雪天**T2**棉花**T5**丸子** | **一次性尿布**T2**尿布** |
| **A** | four |  | five | three | five |  |  |
| **B** |  | four |  | four |  | five |  |
| **C** | Two |  | Two |  | one |  |  |
| **D** |  | five |  | three |  | five | four |

我们之前已经看到，当我们想要找到相似的项目时，我们可以使用余弦相似度。让我们在这里试试。我们会找到最像用户 *A* 的用户。因为我们有一个包含许多未分级项目的稀疏向量，我们将不得不为那些丢失的值输入一些东西。我们这里用 0。我们将从比较用户*甲*和用户*乙*开始:

```py
from sklearn.metrics.pairwise import cosine_similarity 
cosine_similarity(np.array([4,0,5,3,5,0,0]).reshape(1,-1),\ 
                  np.array([0,4,0,4,0,5,0]).reshape(1,-1)) 
```

前面的代码产生以下输出:

![](assets/5672e7bc-e966-477e-9ab0-95cdadabfaec.png)

如你所见，这两个没有很高的相似性评级，这是有道理的，因为他们没有共同的评级。

现在我们来看看用户 C 与用户 *A* 的对比:

```py
cosine_similarity(np.array([4,0,5,3,5,0,0]).reshape(1,-1),\ 
                  np.array([2,0,2,0,1,0,0]).reshape(1,-1)) 
```

前面的代码产生以下输出:

![](assets/608f4fa0-0045-464c-b0a4-493eaa5c34ad.png)

在这里，我们看到他们有很高的相似性评级(记住 1 是完美相似性)，尽管事实上他们对相同产品的评级非常不同。为什么我们会得到这些结果？问题出在我们选择对未评级产品使用 0。它对那些未评级的产品表现出强烈(负面)的认同。在这种情况下，0 不是中性的。

那么，我们如何解决这个问题呢？

我们能做的不是仅仅用 0 来表示缺失的值，而是将每个用户的评分重新居中，使平均评分为 0 或中性。我们通过取每个用户的评分，并减去该用户所有评分的平均值来做到这一点。例如，对于用户 *A* ，平均值为 17/4，即 4.25。然后，我们从用户 *A* 提供的每个个人评分中减去该值。

一旦这样做了，我们就继续寻找其他用户的平均值，并将其从每个用户的评分中减去，直到每个用户都被处理完。

此过程将生成如下表。您会注意到每个用户行的总和为 0(忽略此处的舍入问题):

| **客户** | **斯纳克薯片** | **搜搜爽滑**T2**乳液** | **杜飞**T2**啤酒** | **自来水**
**自来水** | **【xxlargelivin】**
**泽西岛** | **雪天**T2**棉花**T5**丸子** | **一次性尿布**T2**尿布** |
| **A** | -.25 |  | .75 | -1.25 | .75 |  |  |
| **B** |  | -.33 |  | -.33 |  | .66 |  |
| **C** | .33 |  | .33 |  | -.66 |  |  |
| **D** |  | .75 |  | -1.25 |  | .75 | -.25 |

现在让我们在新的中心数据上尝试余弦相似性。我们再来做用户 *A* 对比用户 *B* 和 *C* 。

首先，我们来比较一下用户 *A* 和用户 *B* :

```py
cosine_similarity(np.array([-.25,0,.75,-1.25,.75,0,0])\ 
                  .reshape(1,-1),\ 
                  np.array([0,-.33,0,-.33,0,.66,0])\ 
                  .reshape(1,-1)) 
```

前面的代码产生以下输出:

![](assets/0f996df4-2bae-40a3-8a4d-e42e49ed0faa.png)

现在让我们在用户 *A* 和 *C* 之间进行尝试:

```py
cosine_similarity(np.array([-.25,0,.75,-1.25,.75,0,0])\ 
                  .reshape(1,-1),\ 
                  np.array([.33,0,.33,0,-.66,0,0])\ 
                  .reshape(1,-1)) 
```

前面的代码产生以下输出:

![](assets/be39db7c-65c3-435f-8a0e-e1def4064051.png)

我们可以看到的是 *A* 和 *B* 的相似度略有增加，而 *A* 和 *C* 的相似度则大幅下降。这正是我们所希望的。

这个居中过程，除了帮助我们处理缺失的值之外，还有一个好处，那就是帮助我们处理困难或容易的评分者，因为现在每个人都以平均值 0 为中心。你可能会注意到，这个公式相当于皮尔逊相关系数，就像那个系数一样，数值介于`-1`和`1`之间。

# 预测产品的评级

现在让我们利用这个框架来预测产品的评级。我们将我们的例子限制为三个用户，人 *X* ，人 *Y* ，人 *Z* 。我们来预测一个产品的评级，人 *X* 没有评级，但是人 *Y* 和 *Z* 非常相似的人 *X* 评级。

我们将从每个用户的基本评分开始，如下表所示:

| **客户** | **斯纳克薯片** | **搜搜爽滑**T2**乳液** | **杜飞**T2**啤酒** | **自来水**
**自来水** | **【xxlargelivin】**
**泽西岛** | **雪天**T2**棉花**T5**丸子** | **一次性尿布**T2**尿布** |
| **X** |  | four |  | three |  | four |  |
| **Y** |  | Three point five |  | Two point five |  | four | four |
| **Z** |  | four |  | Three point five |  | Four point five | Four point five |

接下来，我们将集中评分:

| **客户** | **斯纳克薯片** | **搜搜爽滑**T2**乳液** | **杜飞**T2**啤酒** | **自来水**
**自来水** | **【xxlargelivin】**
**泽西岛** | **雪天**T2**棉花**T5**丸子** | **一次性尿布**T2**尿布** |
| **X** |  | .33 |  | -.66 |  | .33 | ？ |
| **Y** |  | Zero |  | -1 |  | .5 | .5 |
| **Z** |  | -.125 |  | -.625 |  | .375 | .375 |

现在，我们想知道用户 *X* 可能会给**德普索斯的尿布**什么评分。使用来自用户 *Y* 和用户 *Z* 的评分，我们可以根据它们的中心余弦相似度进行加权平均来计算。

让我们首先得到这个数字:

```py
user_x = [0,.33,0,-.66,0,33,0] 
user_y = [0,0,0,-1,0,.5,.5] 

cosine_similarity(np.array(user_x).reshape(1,-1),\ 
                  np.array(user_y).reshape(1,-1)) 
```

前面的代码产生以下输出:

![](assets/a5611a3e-2da1-4821-a782-a64680fd5565.png)

现在，让我们为用户 *Z* 获取该图:

```py
user_x = [0,.33,0,-.66,0,33,0] 
user_z = [0,-.125,0,-.625,0,.375,.375] 

cosine_similarity(np.array(user_x).reshape(1,-1),\ 
                  np.array(user_z).reshape(1,-1)) 
```

前面的代码产生以下输出:

![](assets/2bf43f2d-e87c-46aa-a3c4-048480370cb7.png)

所以，现在我们有了用户 *X* 和用户 *Y* ( `0.42447212`)和用户 *Z* ( `0.46571861`)之间相似度的图。

综合起来，我们按照用户与 *X* 的相似度对每个用户评分进行加权，然后除以总相似度，如下所示:

*(. 42447212 *(4)+. 46571861 *(4.5))/(. 42447212+. 46571861)= 4.26*

并且我们可以看到用户 *X* 对**德普索斯纸尿裤**的预期评分为 4.26。(最好发个优惠券！)

到目前为止，我们只关注了用户对用户的协同过滤，但还有另一种方法可以使用。在实践中，该方法优于用户对用户过滤；叫做**项对项过滤**。方法是这样的:不是根据过去的评分将每个用户与其他相似的用户进行匹配，而是将每个评分的项目与所有其他项目进行比较，以找到最相似的项目，同样使用中心余弦相似度。

让我们看看这是如何工作的。

同样，我们有一个效用矩阵；这一次，我们来看看用户对歌曲的评分。用户沿着列，歌曲沿着行，如下所示:

| **实体** | **U1** | **U2** | **U3** | **U4** | **U5** |
| **S1** | Two |  | four |  | five |
| **S2** |  | three |  | three |  |
| **S3** | one |  | five |  | four |
| **S4** |  | four | four | four |  |
| **S5** | three |  |  |  | five |

现在，假设我们想知道用户 3 将为歌曲 5 分配的等级。我们将根据用户对歌曲的评价来寻找相似的歌曲，而不是寻找相似的用户。

我们来看一个例子。

首先，我们以每首歌曲行为中心，计算每首与我们的目标行的余弦相似度，即 *S5* ，如下所示:

| **实体** | **U1** | **U2** | **U3** | **U4** | **U5** | CNT rdcosim |
| **S1** | -1.66 |  | .33 |  | One point three three | .98 |
| **S2** |  | Zero |  | Zero |  | Zero |
| **S3** | -2.33 |  | One point six six |  | .66 | .72 |
| **S4** |  | Zero | Zero | Zero |  | Zero |
| **S5** | -1 |  | ？ |  | one | one |

您可以看到，最右边的列是用每一行的中心余弦相似度对行 *S5* 计算的。

我们接下来需要选择一个数字， *k* ，这是我们将用于为用户 3 评价歌曲的最近邻居的数量。我们在简单的例子中使用 *k = 2* 。

你可以看到歌曲 *S1* 和歌曲 *S3* 最相似，所以我们将使用这两个以及用户 3 对 *S1* 和 *S3* 的评分(分别为 4 和 5)。

现在让我们计算一下评级:

*(. 98 *(4)+. 72 *(5))/(. 98+. 72)= 4.42*

因此，基于这种项目到项目的协同过滤，我们可以看到，根据我们的计算，用户 3 很可能会将歌曲 *S5* 评为非常高的 4.42。

前面我说过，用户对用户的过滤不如项目对项目的过滤有效。为什么会这样？

很有可能你的朋友真的很喜欢你喜欢的一些东西，但是你们每个人都有其他感兴趣的领域，而另一个人绝对没有兴趣。

比如，或许你们俩都爱*权力的游戏*，但你的朋友也爱挪威死亡金属。然而，你宁愿死也不愿听挪威的死亡金属。如果你在用户对用户的推荐中，在很多方面都很相似——不包括死亡金属，你仍然会看到很多关于乐队名字的推荐，这些名字包含了像*燃烧的*、*斧头*、*头骨*和*大头棒*这样的词。有了逐项过滤，你很可能就不用提那些建议了。

到目前为止，在进行比较时，我们已经将用户和项目视为一个单一的实体，但是现在让我们继续看另一种方法，该方法将我们的用户和项目分解成所谓的**功能篮**。

# 基于内容的过滤

作为一名音乐家，蒂姆·韦斯特格伦多年来一直在路上听其他有才华的音乐家演奏，想知道为什么他们永远无法取得成功。他们的音乐很好——就像你在收音机里听到的一样好——然而，不知何故，他们就是没有抓住他们的大突破。他想这一定是因为他们的音乐从来没有在足够多的合适的人面前出现过。

蒂姆最终辞去了音乐人的工作，转而从事电影配乐作曲家的工作。正是在那里，他开始认为每首音乐都有一个独特的结构，可以分解成组成部分——一种音乐基因的形式。

考虑了一番后，他开始考虑围绕这个建立**音乐基因组**的想法创建一家公司。他的一个朋友提出了这个概念，他之前创建并出售了一家公司。朋友喜欢蒂姆的想法。事实上，以至于他开始帮助他写一份商业计划，并为这个项目筹集初始资金。这是一次尝试。

在接下来的几年里，他们雇佣了一小群音乐家，他们为 100 多万首音乐精心编纂了近 400 种不同的音乐特征。每一个特征都是用手在 0 到 5 分的等级上评定的(或者用耳朵来说是更好的方式)。每首三四分钟的歌都要花将近半个小时来分类。

这些特色包括主唱的声音有多沙哑，或者节奏有多快。他们的第一个原型花了将近一年的时间才完成。完全在 Excel 中使用 VBA 宏构建，仅返回一条推荐就花了将近四分钟。但最终，它奏效了，而且效果很好。

这家公司现在被称为潘多拉音乐，你可能听说过它，或者使用过它的产品，因为它在世界各地有数百万的日常用户。毫无疑问，这是一个基于内容过滤的成功例子。

不像在基于内容的过滤中那样，将每首歌曲视为一个不可分割的单元，而是将歌曲作为特征向量，可以使用我们的朋友余弦相似度进行比较。

另一个好处是，不仅歌曲会被分解成特征向量，听众也会被分解成特征向量。每个听众的口味特征成为这个空间中的一个向量，因此可以在他们的口味特征和歌曲本身之间进行测量。

对蒂姆·韦斯特格伦来说，这就是神奇之处，因为与其像许多推荐一样依赖音乐的流行度，不如说这个系统的推荐是基于内在的结构相似性。也许你没听过歌 *X* ，但是如果你是宋立科 *Y* ，那么你应该是宋立科 *X* ，因为它的*基因*几乎一模一样。这就是基于内容的过滤。

# 混合系统

我们现在已经研究了推荐系统的两种主要形式，但是您应该知道，在任何大规模生产环境中，您都有可能看到利用这两种形式的推荐。这就是所谓的**混合动力系统**，混合动力系统之所以更受欢迎，是因为它们有助于消除单独使用任何一种系统时可能存在的缺点。这两个系统共同创造了一个更强大的解决方案。

让我们来看看每种类型的优缺点。

# 协同过滤

协同过滤的优点如下:

*   没有必要手工制作特征

缺点如下:

*   如果没有大量的项目和用户，就不能很好地工作
*   当物品的数量远远超过可以购买的数量时，就会变得稀疏

# 基于内容的过滤

基于内容的过滤的优点如下:

*   它不需要大量用户

缺点如下:

*   定义正确的特性可能是一个挑战
*   缺乏意外的发现

正如您所看到的，当您还没有建立起庞大的用户群时，基于内容的过滤是一个更好的选择，但是随着您的成长，添加协作过滤可以帮助在推荐中引入更多的意外发现。

既然您已经熟悉了推荐引擎的类型和内部工作方式，让我们开始构建自己的推荐引擎。

# 构建推荐引擎

我喜欢偶然发现一个非常有用的 GitHub 存储库。你可以找到包含所有内容的存储库，从手工策划的机器学习教程到使用 **Elasticsearch** 时可以为你节省几十行代码的库。问题是，找到这些库比它应该的要困难得多。幸运的是，我们现在已经知道如何利用 GitHub API 来帮助我们找到这些代码宝石。

我们将使用 GitHub API 创建一个基于协同过滤的推荐引擎。我们的计划是获得我长期以来主演的所有存储库，然后让这些存储库的所有创建者找到他们主演的存储库。一旦这样做了，我们会发现哪些用户和我最相似(或者你，如果你是为你自己的存储库运行这个，我建议)。一旦我们有了最相似的用户，我们就可以使用他们主演的存储库来生成一组推荐，而我没有。

让我们开始吧:

1.  我们将导入所需的库:

```py
import pandas as pd 
import numpy as np 
import requests 
import json 
```

2.  您需要在 GitHub 上开设一个帐户，并在许多存储库中打上星号，这样才能为您的 GitHub 句柄工作，但您实际上不需要注册开发人员程序。您可以从您的配置文件中获得授权令牌，这将允许您使用该应用编程接口。您也可以让它与这段代码一起工作，但是限制太多，无法让它用于我们的示例。

3.  要创建一个与应用编程接口一起使用的令牌，请访问位于[https://github.com/settings/tokens](https://github.com/settings/tokens)的以下网址。在那里，您将在右上角看到一个按钮，如下所示:

![](assets/6093907c-b461-403b-b45a-1e54166869ad.png)

4.  您需要单击生成新令牌按钮。完成后，您需要选择权限，我选择了 public_repo。最后，复制它给你的令牌，用于下面的代码。请务必用引号将两者括起来:

```py
myun = YOUR_GITHUB_HANDLE 
mypw = YOUR_PERSONAL_TOKEN 
```

5.  我们将创建一个函数，该函数将提取您所标记的每个存储库的名称:

```py
my_starred_repos = [] 
def get_starred_by_me(): 
    resp_list = [] 
    last_resp = '' 
    first_url_to_get = 'https://api.github.com/user/starred' 
    first_url_resp = requests.get(first_url_to_get, auth=(myun,mypw)) 
    last_resp = first_url_resp 
    resp_list.append(json.loads(first_url_resp.text)) 

    while last_resp.links.get('next'): 
        next_url_to_get = last_resp.links['next']['url'] 
        next_url_resp = requests.get(next_url_to_get, auth=(myun,mypw)) 
        last_resp = next_url_resp 
        resp_list.append(json.loads(next_url_resp.text)) 

    for i in resp_list: 
        for j in i: 
            msr = j['html_url'] 
            my_starred_repos.append(msr) 
```

这里面有很多东西，但是，本质上，我们正在查询 API 来获得我们自己的星形存储库。GitHub 使用分页，而不是在一次调用中返回所有内容。因此，我们需要检查每个响应返回的`.links`。只要有下一个要调用的链接，我们就会继续这样做。

6.  我们只需要调用我们创建的函数:

```py
get_starred_by_me() 
```

7.  然后，我们可以看到星标存储库的完整列表:

```py
my_starred_repos 
```

前面的代码将产生类似如下的输出:

![](assets/d5f6c6d1-d138-4a60-8863-fce88149a145.png)

8.  我们需要解析出我们标记的每个库的用户名，以便我们可以检索它们标记的库:

```py
my_starred_users = [] 
for ln in my_starred_repos: 
    right_split = ln.split('.com/')[1] 
    starred_usr = right_split.split('/')[0] 
    my_starred_users.append(starred_usr) 

my_starred_users 
```

这将导致类似以下的输出:

![](assets/d61cd8d7-6229-474c-8904-b38d4e680787.png)

9.  现在我们已经有了所有我们主演的用户的句柄，我们将需要检索他们主演的所有存储库。下面的函数就可以做到这一点:

```py
starred_repos = {k:[] for k in set(my_starred_users)} 
def get_starred_by_user(user_name): 
    starred_resp_list = [] 
    last_resp = '' 
    first_url_to_get = 'https://api.github.com/users/'+ user_name +'/starred' 
    first_url_resp = requests.get(first_url_to_get, auth=(myun,mypw)) 
    last_resp = first_url_resp 
    starred_resp_list.append(json.loads(first_url_resp.text)) 

    while last_resp.links.get('next'): 
        next_url_to_get = last_resp.links['next']['url'] 
        next_url_resp = requests.get(next_url_to_get, auth=(myun,mypw)) 
        last_resp = next_url_resp 
        starred_resp_list.append(json.loads(next_url_resp.text)) 

    for i in starred_resp_list: 
        for j in i: 
            sr = j['html_url'] 
            starred_repos.get(user_name).append(sr) 
```

该函数的工作方式与我们之前调用的函数几乎相同，但调用了不同的端点。它会将他们的星形存储库添加到我们稍后将使用的字典中。

10.  现在就这么说吧。运行可能需要几分钟时间，这取决于每个用户所加入的存储库的数量。我实际上有一个超过 4000 个存储库:

```py
for usr in list(set(my_starred_users)): 
    print(usr) 
    try: 
        get_starred_by_user(usr) 
    except: 
        print('failed for user', usr) 
```

前面的代码将产生类似如下的输出:

![](assets/98699917-fcac-48cc-99db-1f272910481d.png)

请注意，在我调用之前，我已经将明星用户列表变成了一个集合。我注意到在一个用户句柄下由多个存储库导致的一些重复，因此遵循以下步骤来减少额外的调用是有意义的:

1.  我们现在需要构建一个功能集，其中包括我们主演过的每个人的所有明星资料库:

```py
repo_vocab = [item for sl in list(starred_repos.values()) for item in sl] 
```

2.  我们将把它转换成一个集合来删除可能存在于同一存储库中的多个用户的重复项:

```py
repo_set = list(set(repo_vocab)) 
```

3.  让我们看看会产生多少:

```py
len(repo_vocab) 
```

前面的代码应该会产生类似如下的输出:

![](assets/b1945589-999e-41b7-8532-67ae6570a9c7.png)

我已经标记了 170 个存储库，这些存储库的用户总共标记了超过 27，000 个独特的存储库。你可以想象如果我们再往前走一度，我们会看到多少。

现在我们有了完整的特性集，或者说存储库词汇表，我们需要运行每个用户来创建一个二进制向量，该向量包含一个用于他们已经加入的每个存储库的`1`和一个用于他们没有加入的每个存储库的`0`:

```py
all_usr_vector = [] 
for k,v in starred_repos.items(): 
    usr_vector = [] 
    for url in repo_set: 
        if url in v: 
            usr_vector.extend([1]) 
        else: 
            usr_vector.extend([0]) 
    all_usr_vector.append(usr_vector) 
```

我们所做的只是检查每个用户是否在我们的存储库词汇表中标记了每个存储库。如果他们有，他们会得到一个`1`，如果他们没有，他们会得到一个`0`。

此时，我们为每个用户提供了 27，098 个项目的二进制向量——全部 170 个。现在让我们把这个放入`DataFrame`中。行索引将是我们加了星号的用户句柄，列将是存储库词汇表:

```py
df = pd.DataFrame(all_usr_vector, columns=repo_set, index=starred_repos.keys()) 

df 
```

前面的代码将生成类似如下的输出:

![](assets/13164eb3-0140-47d5-b8d9-e0bb7ae78ff7.png)

接下来，为了与其他用户进行比较，我们需要在这个框架中添加我们自己的行。在这里，我添加了我的用户句柄，但是您应该添加自己的:

```py
my_repo_comp = [] 
for i in df.columns: 
    if i in my_starred_repos: 
        my_repo_comp.append(1) 
    else: 
        my_repo_comp.append(0) 

mrc = pd.Series(my_repo_comp).to_frame('acombs').T 

mrc 
```

前面的代码将生成类似如下的输出:

![](assets/88bf2a5c-5bd1-4ac6-b737-18bb530ac824.png)

我们现在需要添加适当的列名，并将其连接到另一个`DataFrame`:

```py
mrc.columns = df.columns 

fdf = pd.concat([df, mrc]) 

fdf 
```

前面的代码将产生类似如下的输出:

![](assets/877967fd-8853-45f8-80e8-e32663d021a3.png)

你可以在之前的截图中看到我已经被添加到`DataFrame`了。

从这里，我们只需要计算我们自己和我们主演的其他用户之间的相似度。我们现在将使用`pearsonr`函数来完成，我们需要从 SciPy 导入该函数:

```py
from scipy.stats import pearsonr 

sim_score = {} 
for i in range(len(fdf)): 
    ss = pearsonr(fdf.iloc[-1,:], fdf.iloc[i,:]) 
    sim_score.update({i: ss[0]}) 

sf = pd.Series(sim_score).to_frame('similarity') 
sf 
```

前面的代码将生成类似如下的输出:

![](assets/08e99592-0732-40f3-b1f3-5dba3b2360fa.png)

我们刚才所做的是将我们的向量，也就是`DataFrame`中的最后一个向量，与其他每个用户的向量进行比较，以生成一个中心余弦相似度(皮尔逊相关系数)。有些值必然是`NaN`，因为它们没有用星号标出存储库，因此导致计算中被零除:

1.  现在让我们对这些值进行排序，以返回最相似用户的索引:

```py
sf.sort_values('similarity', ascending=False) 
```

前面的代码将产生类似如下的输出:

![](assets/4e0c87e0-5c41-4154-8298-c1b8c54a0d1c.png)

所以我们有了，这些是最相似的用户，因此我们可以用来推荐我们可能喜欢的存储库。让我们来看看这些用户，看看他们有哪些我们可能会喜欢的明星。

2.  你可以忽略第一个拥有完美相似度分数的用户；那是我们自己的仓库。沿着列表往下，三个最接近的匹配是用户 6、用户 42 和用户 116。让我们看看每一个:

```py
fdf.index[6] 
```

前面的代码将产生类似如下的输出:

![](assets/7e4949d8-0add-4d51-83f2-e9c940281dba.png)

3.  让我们看看这是谁以及他们的存储库。从[https://github.com/cchi](https://github.com/cchi)我可以看到这个仓库属于以下哪个用户:

![](assets/4171a7e8-7065-45b6-98ce-4464443fc815.png)

这其实是我以前在的同事迟浩田，所以这并不奇怪。让我们看看他主演了什么:

1.  有几种方法可以做到这一点；我们可以使用我们的代码，或者直接点击他们图片下面的星星。让我们为这一个做两个，只是为了比较，并确保一切匹配。首先，让我们通过代码来实现:

```py
fdf.iloc[6,:][fdf.iloc[6,:]==1] 
```

这将产生以下输出:

![](assets/b2363349-f1b3-4e15-acab-0ad246dfbb49.png)

2.  我们看到了 30 个星形存储库。让我们把这些和 GitHub 网站上的进行比较:

![](assets/a67bbc65-7c4e-4a76-9d6c-c8ffc54cfa5a.png)

3.  在这里，我们可以看到它们是相同的，你会注意到你可以识别我们都主演过的存储库:它们被标记为 Unstar。
4.  不幸的是，只有 30 个星标存储库，没有太多的存储库来生成推荐。
5.  相似性方面的下一个用户是 42，Artem Golubin:

```py
fdf.index[42] 
```

前面的代码产生以下输出:

![](assets/9e826b79-e99b-468e-9bf5-ea87d04475a1.png)

下面是他的 GitHub 简介:

![](assets/2fd46a81-dbcc-4aec-9dae-74db19f2f4c1.png)

这里我们看到他主演的仓库:

![](assets/688655ef-721a-4503-8cb7-d90fd6ca56c1.png)

6.  Artem 已经主演了 500 多个存储库，所以肯定有一些推荐可以在那里找到。
7.  最后，让我们看看第三个最相似的用户:

```py
fdf.index[116] 
```

这将产生以下输出:

![](assets/80d07cbf-9bc3-4acc-8fc5-026dec03b83a.png)

这个用户，凯文·马卡姆，已经主演了大约 60 个存储库:

![](assets/d7801ab7-37c5-4a81-b8df-0bd6a2a39ef8.png)

我们可以在下图中看到星形存储库:

![](assets/2f7d43f9-fa57-4307-94f9-780a6c73fbdf.png)

这绝对是产生推荐的沃土。让我们现在就这样做；让我们利用这三个链接提出一些建议:

1.  我们需要收集他们主演的和我没有主演的资料库的链接。我们将创建一个`DataFrame`，其中包含我主演的存储库以及与我最相似的三个用户:

```py
all_recs = fdf.iloc[[6,42,116,159],:] 
all_recs.T 
```

前面的代码将产生以下输出:

![](assets/0c9c7b37-46ef-45c0-acd3-6edcdb94bb6a.png)

2.  如果看起来全是零，不用担心；这是一个稀疏矩阵，所以大多数都是 0。让我们看看是否有我们都主演过的存储库:

```py
all_recs[(all_recs==1).all(axis=1)] 
```

前面的代码将产生以下输出:

![](assets/fa2dbf18-0845-4435-92e9-f9c1b1db9616.png)

3.  如你所见，我们似乎都喜欢 scikit-learn 和机器学习存储库——这并不奇怪。让我们看看他们可能都主演了什么，我错过了。我们将从创建一个排除我的框架开始，然后我们将在其中查询常见的星形存储库:

```py
str_recs_tmp = all_recs[all_recs[myun]==0].copy() 
str_recs = str_recs_tmp.iloc[:,:-1].copy() 
str_recs[(str_recs==1).all(axis=1)] 
```

上述代码产生以下输出:

![](assets/aa86dd2f-5706-4d00-88d5-4c6329d650de.png)

4.  好吧，看起来我没有错过什么特别明显的东西。让我们看看是否有至少三分之二的用户主演的存储库。为了找到这一点，我们将对行进行求和:

```py
str_recs.sum(axis=1).to_frame('total').sort_values(by='total', ascending=False) 
```

前面的代码将产生类似如下的输出:

![](assets/9160ebd7-e513-4038-ad8a-8eff7cd4c253.png)

这看起来很有希望。有很多好的语言和人工智能库，老实说，我很惭愧我从来没有主演过模糊的东西，因为我经常使用它。

在这一点上，我不得不说我对结果印象深刻。这些肯定是我感兴趣的存储库，我会去看看。

到目前为止，我们已经使用协作过滤生成了推荐，并使用聚合进行了一些轻度的附加过滤。如果我们想走得更远，我们可以根据他们收到的星星总数来订购推荐。这可以通过再次调用 GitHub API 来实现。有一个端点可以提供这些信息。

我们可以做的另一件事是增加一层基于内容的过滤来改善结果。这是我们之前讨论的杂交步骤。我们需要从我们自己的存储库中创建一组特性，这些特性指示了我们感兴趣的事物的类型。实现这一点的一种方法是创建一个特性集，方法是将我们标记的存储库的名称及其描述标记化。

下面是我的星标存储库:

![](assets/f755b271-db4e-45e1-9e4f-8aa9779c097b.png)

正如您可能想象的那样，这将生成一组单词特征，我们可以使用它们来检查我们使用协作过滤找到的那些库。这将包括许多单词，如 *Python* 、*机器学习*和*数据科学*。这将确保与我们不太相似的用户仍然提供基于我们自己兴趣的推荐。这也会降低这些建议的意外收获，这是需要考虑的事情。也许有什么不一样的东西是我现在很想看到的。这当然是有可能的。

就数据帧而言，基于内容的过滤步骤会是什么样子？列将是单词特征(n-gram)，行将是我们的协同过滤步骤生成的存储库。我们将使用自己的存储库再次运行相似性过程进行比较。

# 摘要

在本章中，我们学习了推荐引擎。我们了解了目前使用的两种主要类型的系统:协作过滤和基于内容的过滤。我们还学习了如何将它们一起用于形成混合系统。我们还讨论了每种系统的优缺点。最后，我们一步一步地学习了如何使用 GitHub API 从头开始构建推荐引擎。

希望大家利用本章的指导构建自己的推荐引擎，希望大家找到对自己有用的资源。我知道我已经找到了一些我肯定会用到的东西。祝你旅途好运！