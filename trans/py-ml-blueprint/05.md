# 创建自定义新闻源

我经常阅读*。有些人甚至会强迫性地说。众所周知，我有时会消费一百多篇文章。但尽管如此，我还是经常发现自己在寻找更多可以阅读的东西。我受着这种潜移默化的怀疑，我错过了一些有趣的东西，我将永远遭受知识上的差距！*

 *如果你有类似的症状，不要害怕，因为在这一章中，我将揭示一个简单的技巧，找到你想读的所有文章，而不必去挖掘几十篇你不想读的文章。

在这一章结束时，你将学会如何建立一个系统，了解你的新闻品味，并将每天给你发送一份个人定制的时事通讯。

这就是我们将在本章中介绍的内容:

*   使用 Pocket 应用程序创建受监督的训练集
*   利用口袋应用编程接口检索故事
*   使用嵌入式应用编程接口提取故事主体
*   自然语言处理基础
*   支持向量机
*   与 RSS 源和谷歌表单的集成
*   建立每日个人简讯

# 使用 Pocket 创建监督训练集

在我们能够在新闻文章中创建我们品味的模型之前，我们需要训练数据。这些训练数据将被输入到我们的模型中，以便教会它区分我们感兴趣的文章和我们不感兴趣的文章。为了建立这个语料库，我们需要注释大量的文章来对应这些兴趣。我们会给每篇文章贴上`y`或`n`的标签，表明它是否是我们希望在日常摘要中发送给我们的文章类型。

为了简化这个过程，我们将使用 Pocket 应用程序。Pocket 是一个允许您保存故事以供以后阅读的应用程序。您只需安装浏览器扩展，然后当您想要保存一个故事时，单击浏览器工具栏中的口袋图标。文章将保存到您的个人存储库中。对于我们来说，Pocket 的一个很好的特性是能够保存带有您选择的标签的文章。我们用这个来标记有趣的文章为`y`，不有趣的文章为`n`。

# 安装袖珍镀铬加长件

我正在为此使用谷歌 Chrome，但其他浏览器应该也能类似地工作。按照步骤安装袖珍镀铬扩展:

1.  对于 Chrome，请访问谷歌应用商店并查找扩展部分:

![](assets/bec36363-4072-441d-9ee3-5e8d34d870e2.png)

Pocket Chrome Extention

2.  点击添加到铬。如果你已经有一个帐户，登录，如果没有，继续注册(这是免费的)。
3.  完成后，您应该会在浏览器的右上角看到口袋图标。
4.  它将灰显，但是一旦有你想要保存的文章，你可以点击它。保存文章后，它会变成红色:

![](assets/0600e60d-722d-40bc-ae27-2ac2aac6598e.png)

保存的页面如下所示:

![](assets/f0eba099-38cb-46dc-b437-6676aa2cbf4d.png)

The New York Times saved page

现在是有趣的部分！当你度过一天的时候，开始保存你想看的文章，以及那些你不想看的文章。有趣的用`y`标记，不有趣的用`n`标记。这需要一些工作。你的最终结果只会和你的训练集一样好，所以你需要为数百篇文章做这件事。如果你在保存的时候忘记给一篇文章贴标签，你可以随时去网站[http://www.get.pocket.com](https://getpocket.com/)，在那里贴标签。

# 使用口袋应用编程接口检索故事

现在，您已经努力将文章保存到 Pocket，下一步是检索它们。为了实现这一点，我们将使用口袋应用编程接口。你可以在[https://getpocket.com/developer/apps/new](https://getpocket.com/developer/apps/new)注册一个账户。遵循以下步骤来实现:

1.  点击左上角的创建一个新的应用程序，并填写详细信息，以获得您的应用程序接口密钥。

2.  确保单击所有权限，以便您可以添加、更改和检索文章:

![](assets/d70f782e-626c-411d-b2d7-111535a45741.png)

3.  填写并提交后，您将收到您的**消费者密钥**。
4.  你可以在左上角的“我的应用”下找到它。它看起来像下面的截图，但显然有一个真正的关键:

![](assets/2f8553e5-5a88-4939-8c58-1e115884f87e.png)

5.  设置好之后，您就可以进入下一步，即设置授权。我们现在就做。

6.  它要求您输入您的消费者密钥和重定向网址。重定向网址可以是任何东西。在这里，我使用了我的推特账户:

```
import requests 
import pandas as pd 
import json 
pd.set_option('display.max_colwidth', 200) 

CONSUMER_KEY = 'enter_your_consumer_key_here 

auth_params = {'consumer_key': CONSUMER_KEY, 'redirect_uri': 'https://www.twitter.com/acombs'} 

tkn = requests.post('https://getpocket.com/v3/oauth/request', data=auth_params) 

tkn.text 
```

前面的代码产生以下输出:

![](assets/e651dd33-b0ae-439a-8d0e-512fcdacdde3.png)

7.  输出将包含下一步所需的代码。在浏览器栏中放置以下内容:

[https://getpocket.com/auth/authorize?request _ token = some _ long _ access _ code&amp；redirect _ uri = https % 3A//www . Twitter . com/acom bs](https://getpocket.com/auth/authorize?request_token=some_long_access_code&redirect_uri=https%3A//www.twitter.com/acombs)

8.  如果您将重定向网址更改为您自己的网址，请确保对其进行网址编码(这是您在前面的网址中看到的`%3A`类型的内容)。
9.  此时，您应该会看到一个授权屏幕。继续批准它，然后我们可以继续下一步:

```
# below we parse out the access code from the tkn.text string 
ACCESS_CODE = tkn.text.split('=')[1] 

usr_params = {'consumer_key': CONSUMER_KEY, 'code': ACCESS_CODE} 

usr = requests.post('https://getpocket.com/v3/oauth/authorize', data=usr_params) 

usr.text 
```

前面的代码产生以下输出:

![](assets/f4c276fc-3893-4910-b350-f332d3d02864.png)

10.  我们将在这里使用输出代码，继续检索故事。首先，我们检索标记为`n`的故事:

```
# below we parse out the access token from the usr.text string 
ACCESS_TOKEN = usr.text.split('=')[1].split('&amp;')[0] 

no_params = {'consumer_key': CONSUMER_KEY, 
'access_token': ACCESS_TOKEN, 
'tag': 'n'} 

no_result = requests.post('https://getpocket.com/v3/get', data=no_params) 

no_result.text 
```

前面的代码产生以下输出:

![](assets/0ed35aed-75d2-4df9-8091-fc87965664d3.png)

你会注意到我们标记的所有文章上都有一个很长的 JSON 字符串`n`。这里面有几个关键点，但我们目前真的只对 URL 感兴趣。

11.  我们将继续创建一个所有网址的列表:

```
no_jf = json.loads(no_result.text) 
no_jd = no_jf['list'] 

no_urls=[] 
for i in no_jd.values(): 
    no_urls.append(i.get('resolved_url')) no_urls 
```

前面的代码产生以下输出:

![](assets/bac52869-eb17-41e6-99f6-e15668064bfd.png)

List of URLs

12.  这个列表包含了所有我们不感兴趣的故事的网址。现在让我们把它放在一个数据帧中，并这样标记它:

```
no_uf = pd.DataFrame(no_urls, columns=['urls']) 
no_uf = no_uf.assign(wanted = lambda x: 'n') no_uf 
```

前面的代码产生以下输出:

![](assets/7b330ba0-fb54-44e7-b899-a03304214597.png)

Tagging the URLs

13.  现在我们都有不想要的故事了。让我们对那些我们感兴趣的故事做同样的事情:

```
yes_params = {'consumer_key': CONSUMER_KEY, 
'access_token': ACCESS_TOKEN, 
'tag': 'y'} 
yes_result = requests.post('https://getpocket.com/v3/get', data=yes_params) 

yes_jf = json.loads(yes_result.text) 
yes_jd = yes_jf['list'] 

yes_urls=[] 
for i in yes_jd.values(): 
    yes_urls.append(i.get('resolved_url')) 

yes_uf = pd.DataFrame(yes_urls, columns=['urls']) 
yes_uf = yes_uf.assign(wanted = lambda x: 'y') 

yes_uf 
```

前面的代码产生以下输出:

![](assets/e0cf64cd-f8cc-4071-9d3b-ee31e97bdd51.png)

Tagging the URLs of stories we are interested in

14.  现在，我们的培训数据有了两种类型的故事，让我们将它们结合成一个单一的数据框架:

```
df = pd.concat([yes_uf, no_uf]) 

df.dropna(inplace=True) 

df 
```

前面的代码产生以下输出:

![](assets/7a12a25e-1a50-471b-b1af-6836de25339a.png)

Joining the URLs- both interested and not interested

现在我们已经在一个框架中设置了所有的网址和它们对应的标签，我们将继续下载每篇文章的 HTML。我们将为此使用另一种免费服务，称为 Embedly。

# 使用嵌入式应用编程接口下载故事正文

我们有故事的所有网址，但不幸的是，这不足以训练；我们需要完整的文章正文。如果我们想推出自己的刮刀，这本身可能会成为一个巨大的挑战，尤其是如果我们要从几十个网站中提取故事。我们需要编写代码来定位文章主体，同时小心避免围绕它的所有其他网站粘性。幸运的是，就我们而言，有许多免费服务可以为我们做到这一点。我将使用 Embedly 来实现这一点，但是您可以使用许多其他服务来代替。

第一步是注册 Embedly API 访问。你可以在 https://app.embed.ly/signup 做。这是一个简单的过程。一旦您确认注册，您将收到一个应用编程接口密钥。那真的是你所需要的。您只需在您的 HTTP 请求中使用该密钥。我们现在就开始吧:

```
import urllib 

EMBEDLY_KEY = 'your_embedly_api_key_here' 

def get_html(x): 
    try: 
        qurl = urllib.parse.quote(x) 
        rhtml = requests.get('https://api.embedly.com/1/extract?url=' + qurl + '&amp;key=' + EMBEDLY_KEY) 
        ctnt = json.loads(rhtml.text).get('content') 
    except: 
        return None 
    return ctnt 
```

前面的代码产生以下输出:

![](assets/6b4b9a5b-6e6e-4345-8a5c-766a71a49269.png)

HTTP requests

这样，我们就有了每个故事的 HTML。

由于内容嵌入在 HTML 标记中，并且我们希望将纯文本输入到我们的模型中，因此我们将使用解析器来剥离标记标签:

```
from bs4 import BeautifulSoup 

def get_text(x): 
    soup = BeautifulSoup(x, 'html5lib') 
    text = soup.get_text() 
    return text 

df.loc[:,'text'] = df['html'].map(get_text) 

df 
```

前面的代码产生以下输出:

![](assets/fb4aa1e9-f5f4-4a26-923c-b4cfa49d89e4.png)

就这样，我们已经准备好了训练。我们现在可以继续讨论如何将文本转换为模型可以处理的内容。

# 自然语言处理基础

如果机器学习模型只对数字数据进行操作，我们如何将文本转换为数字表示？这正是**自然语言处理** ( **NLP** )的重点。让我们简单了解一下这是如何实现的。

我们将从一个包含三个句子的小型语料库开始:

1.  这只新小猫和其他小猫一起玩耍
2.  她吃了午饭
3.  她爱她的小猫

我们首先将我们的语料库转换成一个**单词包** ( **BOW** )表示。我们暂时跳过预处理。将我们的语料库转换成 BOW 表示包括获取每个单词及其计数，以创建所谓的**术语文档矩阵**。在术语-文档矩阵中，每个唯一的单词被分配给一列，每个文档被分配给一行。两者的交叉点是伯爵:

| **先生否。** | **第** | **新增** | **小猫** | **播放了** | **同** | **其他** | **小猫** | **她** | **吃了** | **午餐** | **爱过** | **她** |
| one | one | one | one | one | one | one | one | Zero | Zero | Zero | Zero | Zero |
| Two | Zero | Zero | Zero | Zero | Zero | Zero | Zero | one | one | one | Zero | Zero |
| three | Zero | Zero | one | Zero | Zero | Zero | Zero | one | Zero | Zero | one | one |

注意，对于这三个短句，我们已经有了 12 个特性。正如你可能想象的那样，如果我们处理的是实际的文档，比如新闻文章甚至书籍，那么特征的数量将会激增到几十万个。为了缓解这种爆炸，我们可以采取一些步骤来删除那些对我们的分析几乎没有或根本没有信息价值的特征。

我们可以采取的第一步是删除**停止词**。这些词非常常见，通常不会告诉您文档的内容。常见的英语停止词有*的*、*的*、*的*、*的*和上的*。我们将删除这些，并重新计算术语文档矩阵:*

| **先生否。** | **新增** | **小猫** | **播放了** | **小猫** | **吃了** | **午餐** | **爱过** |
| one | one | one | one | one | Zero | Zero | Zero |
| Two | Zero | Zero | Zero | Zero | one | one | Zero |
| three | Zero | one | Zero | Zero | Zero | Zero | one |

如您所见，功能数量从 12 个减少到 7 个。这很好，但我们可以更进一步。我们可以执行**词干化**或**引理化**来进一步减少特征。请注意，在我们的矩阵中，我们同时拥有*小猫*和*小猫*。通过使用词干化或引理化，我们可以将它合并成仅仅*小猫咪*:

| **先生否。** | **新增** | **小猫** | **播放** | **吃饭** | **午餐** | **爱情** |
| one | one | Two | one | Zero | Zero | Zero |
| Two | Zero | Zero | Zero | one | one | Zero |
| three | Zero | one | Zero | Zero | Zero | one |

我们的新矩阵合并了*小猫*和*小猫*，但是也发生了一些其他的事情。我们失去了*玩*和*爱*的后缀，*吃*被转化为*吃*。为什么呢？这就是引理化的作用。如果你记得你小学的语法课，我们已经从单词的屈折形式变成了基本形式。如果那是引理化，那词干是什么？词干也有同样的目标，但使用的方法不那么复杂。这种方法有时会产生伪词，而不是实际的基本形式。比如引理，如果你要减少*小马*，你会得到*小马*，但是用炮泥，你会得到*小马*。

现在让我们进一步对矩阵进行另一种变换。到目前为止，我们已经使用了每个单词的简单计数，但是我们可以应用一种算法，该算法将对我们的数据进行过滤，以增强每个文档独有的单词。该算法称为**术语频率-逆文档频率** ( **tf-idf** ) **。**

我们计算矩阵中每个项的 tf-idf 比率。让我们举几个例子来计算一下。对于文件一中的*新*一词，频率一词只是计数，也就是`1`。反向文档频率计算为语料库中文档数量与该术语出现的文档数量的对数。对于*新*，这是 *log (3/1)* ，或者. 4471。所以，对于完整的 tf-idf 值，我们有 *tf * idf* ，或者，这里是 *1 x .4471* ，或者正好是. 4471。对于文档一中的单词 *kitten* ，tf-idf 为 *2 * log (3/2)* ，或. 3522。

为了完成其余条款和文件，我们有以下内容:

| **先生否。** | **新增** | **小猫** | **播放** | **吃饭** | **午餐** | **爱情** |
| one | .4471 | .3522 | .4471 | Zero | Zero | Zero |
| Two | Zero | Zero | Zero | .4471 | .4471 | Zero |
| three | Zero | .1761 | Zero | Zero | Zero | .4471 |

为什么会这样？假设，例如，我们有一个关于许多主题(医学、计算、食品、动物等)的文档语料库，我们希望将它们分类为主题。很少有文件会包含*血压计*这个词，它是用来测量血压的设备；所有的文件都可能与医学相关。显然，这个词在文件中出现的次数越多，就越有可能是关于医学的。因此，一个很少出现在我们整个语料库中，但在一个文档中多次出现的术语，很可能与该文档的主题紧密相关。这样，文档可以说是由那些具有高 tf-idf 值的术语来表示的。

在这个框架的帮助下，我们现在将把我们的训练集转换成 tf-idf 矩阵:

```
from sklearn.feature_extraction.text import TfidfVectorizer 

vect = TfidfVectorizer(ngram_range=(1,3), stop_words='english', min_df=3) 

tv = vect.fit_transform(df['text']) 
```

有了这三行，我们已经将所有文档转换为 tf-idf 向量。我们传入了一些参数:`ngram_range`、`stop_words`和`min_df`。让我们分别讨论一下。

首先，`ngram_range`是文档的标记化方式。在前面的例子中，我们使用每个单词作为标记，但是在这里，我们使用所有一到三个单词的序列作为标记。就拿我们的第二句话来说吧，*她吃了午饭*。我们暂时忽略停止词。这句话的 n-克数应该是:*她*、*她吃了*、*她吃了*、*吃了*、*吃了*午餐、*午餐*。

接下来，我们有`stop_words`。我们为此传递`english`以删除所有的英语停止词。如前所述，这将删除所有缺少信息内容的术语。

最后，我们有`min_df`。这将删除至少三个文档中没有出现的所有单词。添加这个可以删除非常罕见的术语，并减小矩阵的大小。

现在我们的文章语料库是一个可行的数字格式，我们将继续把它输入到我们的分类器中。

# 支持向量机

我们将在本章中使用一个新的分类器，一个线性**支持向量机** ( **SVM** )。SVM 算法是一种试图使用**最大边缘超平面**将数据点线性分类的算法。那是一口，让我们看看它真正的意思。

假设我们有两类数据，我们想用一条线把它们分开。(这里我们只讨论两个特征或维度。)放置那条线最有效的方法是什么？让我们看一个例子:

![](assets/13ff1cec-f4de-4170-b1c7-e2feadd686af.png)

在上图中，线 **H <sub>1</sub>** 没有有效区分这两个类，所以我们可以排除那一个。线 **H <sub>2</sub>** 能够干净利落的区分它们，但是 **H <sub>3</sub>** 是最大余量线。这意味着直线位于每个类的两个最近点之间的中心，这两个最近点被称为**支持向量**。这些可以看作下图中的虚线:

![](assets/30e96749-12d3-4f34-be03-81b0701fff2c.png)

如果数据不能如此整齐地分成类呢？如果点之间有重叠怎么办？在这种情况下，仍然有选择。一种是使用所谓的“软保证金 SVM”。这个公式仍然使边际最大化，但代价是落在边际错误一边的点数受到惩罚。另一个选择是使用所谓的**内核技巧**。这种方法将数据转换到一个更高维的空间，在那里数据可以线性分离。这里提供了一个示例:

![](assets/bb67ef6e-3a1e-4060-8da9-6e6fcfd5ea5f.png)

二维表示如下:

![](assets/74a17d51-a26b-4487-9d36-644d544e09fd.png)

我们采用了一维特征空间，并将其映射到二维特征空间。映射只是取每个 *x* 值，并将其映射到 *x* 、 *x <sup>2</sup>* 。这样做允许我们添加一个线性分离平面。

至此，让我们将 tf-idf 矩阵输入到我们的 SVM:

```
from sklearn.svm import LinearSVC 

clf = LinearSVC() 
model = clf.fit(tv, df['wanted']) 
```

`tv`是我们的矩阵，`df['wanted']`是我们的标签列表。记住这不是`y`就是`n`，表示我们对文章是否感兴趣。一旦运行，我们的模型就被训练好了。

本章中我们没有做的一件事是正式评估我们的模型。您应该总是有一个搁置集来评估您的模型，但是因为我们将不断更新我们的模型，并每天对其进行评估，所以我们将跳过本章的这一步。只要记住这通常是一个可怕的想法。

现在让我们继续设置每天的新闻源。

# IFTTT 与提要、谷歌表单和电子邮件的集成

我们使用 Pocket 来构建我们的训练集，但是现在我们需要一个文章流来运行我们的模型。为了设置这一点，我们将再次使用 IFTTT，以及谷歌表单，以及一个允许我们使用谷歌表单的 Python 库。

# 通过 IFTTT 设置新闻源和谷歌表单

希望此时您已经建立了一个 IFTTT 帐户，但是如果没有，现在就开始建立。完成后，您需要设置与 feed 和 Google Sheets 的集成:

1.  首先，在主页的搜索框中搜索提要，然后单击服务，并单击设置:

![](assets/fea512df-fbd0-44dd-a31d-fe6600f32ce9.png)

2.  您只需单击连接:

![](assets/6389031a-ce09-45cd-bf3d-41080cd1cc73.png)

3.  接下来，在服务下搜索`Google Drive`:

![](assets/03e1346f-8188-45e6-9a4a-3e44ae1bab67.png)

4.  点击那个。它会把你带到一个页面，在那里你选择你想连接的谷歌帐户。选择帐户，然后点按“允许”以启用 IFTTT 来访问您的 Google Drive 帐户。完成后，您应该会看到以下内容:

![](assets/f2445244-6fc6-4787-90a3-0ac7823c8d23.png)

5.  现在，通过连接我们的频道，我们可以设置我们的提要。点击右下角用户名下的下拉菜单中的新建小程序。这会把你带到这里:

![](assets/9bf9d660-7dba-4d1b-b951-8ebb2d523f60.png)

6.  点击+这个。搜索`RSS Feed`，然后点击。这应该会把你带到这里:

![](assets/edc23225-750c-4932-bf9f-5aeaf1001c77.png)

7.  从这里，单击新建订阅源项目:

![](assets/fe70de1e-5a10-47a3-8aea-e12bedde546d.png)

8.  然后，将网址添加到框中，并单击创建触发器。完成后，您将被带回来添加+那个动作:

![](assets/bff82527-32fb-4ed2-8e67-fcebfc39e102.png)

9.  点击+那个，搜索`Sheets`，然后点击它的图标。一旦完成，你会发现自己在这里:

![](assets/89fbc4e5-1aad-42d7-84c8-39773626cec5.png)

10.  我们希望我们的新闻项目流入谷歌驱动电子表格，所以点击添加行到电子表格。然后，您将有机会自定义电子表格:

![](assets/e9c7a419-8c0c-486a-97ad-22ed50fafff1.png)

我给这个电子表格起了个名字`NewStories`，并把它放在了一个名为`IFTTT`的谷歌驱动文件夹中。单击“创建操作”来完成制作方法，很快您将开始看到新闻项目流入您的谷歌驱动电子表格。请注意，它只会在新项目进入时添加新项目，而不会添加创建工作表时已存在的项目。我建议添加一些提要。你将需要为每个人创建单独的食谱。最好是为训练集中的站点添加提要，换句话说，就是用 Pocket 保存的站点。

给这些故事一两天的时间在纸上积累，然后它应该是这样的:

![](assets/4a6b042c-a32e-41a0-93a5-f9d4b27e1efd.png)

幸运的是，包含了完整的文章 HTML 正文。这意味着我们不必使用 Embedly 为每篇文章下载它。我们仍然需要从谷歌表单下载文章，然后处理文本以去除 HTML 标签，但这一切都可以很容易地完成。

为了下拉文章，我们将使用名为`gspread`的 Python 库。这可以 pip 安装。安装完成后，您需要按照设置 **OAuth 2** 的方向进行操作。这可以在[http://gspread.readthedocs.org/en/latest/oauth2.html](http://gspread.readthedocs.org/en/latest/oauth2.html)找到。您将最终下载一个 JSON 凭证文件。重要的是，一旦你有了那个文件，你就可以用`client_email`键找到里面的电子邮件地址。然后你需要分享你的电子邮件发送的`NewStories`电子表格。只需点击表格右上角的蓝色共享按钮，然后将电子邮件粘贴到那里。您最终会在 Gmail 帐户中收到一条*未能发送*的消息，但这是意料之中的。请确保在以下代码中交换文件的路径和文件名:

```
import gspread 

from oauth2client.service_account import ServiceAccountCredentials 
JSON_API_KEY = 'the/path/to/your/json_api_key/here' 

scope = ['https://spreadsheets.google.com/feeds', 
         'https://www.googleapis.com/auth/drive'] 

credentials = ServiceAccountCredentials.from_json_keyfile_name(JSON_API_KEY, scope) 
gc = gspread.authorize(credentials) 
```

现在，如果一切顺利，它应该运行没有错误。接下来，您可以下载这些故事:

```
ws = gc.open("NewStories") 
sh = ws.sheet1 

zd = list(zip(sh.col_values(2),sh.col_values(3), sh.col_values(4))) 

zf = pd.DataFrame(zd, columns=['title','urls','html']) 
zf.replace('', pd.np.nan, inplace=True) 
zf.dropna(inplace=True) 

zf 
```

前面的代码产生以下输出:

![](assets/cb48abf5-89de-4090-8fc2-47ddc3a33bf0.png)

这样，我们从提要中下载了所有文章，并将它们放入一个数据框中。我们现在需要去掉 HTML 标签。我们可以使用之前使用的函数来检索文本。然后，我们将使用 tf-idf 矢量器对其进行转换:

```
zf.loc[:,'text'] = zf['html'].map(get_text) 

zf.reset_index(drop=True, inplace=True) 

test_matrix = vect.transform(zf['text']) 

test_matrix 
```

前面的代码产生以下输出:

![](assets/0d864c05-e9fc-4aad-b78f-902084ce2524.png)

在这里，我们看到我们的矢量化是成功的。现在让我们将其传递到我们的模型中，以获得结果:

```
results = pd.DataFrame(model.predict(test_matrix), columns=['wanted']) 

results 
```

前面的代码产生以下输出:

![](assets/2292ef3e-aa9e-4b12-9703-f4e5ec5b44cc.png)

我们在这里看到每个故事都有结果。现在让我们将他们与故事本身联系起来，以便我们可以评估结果:

```
rez = pd.merge(results,zf, left_index=True, right_index=True) 

rez 
```

前面的代码产生以下输出:

![](assets/c5dd103e-6d29-4d96-afaf-3271f3696e49.png)

此时，我们可以通过检查结果并纠正错误来改进模型。你需要自己做这件事，但我是这样改变自己的:

```
change_to_no = [130, 145, 148, 163, 178, 199, 219, 222, 223, 226, 235, 279, 348, 357, 427, 440, 542, 544, 546, 568, 614, 619, 660, 668, 679, 686, 740, 829] 

change_to_yes = [0, 9, 29, 35, 42, 71, 110, 190, 319, 335, 344, 371, 385, 399, 408, 409, 422, 472, 520, 534, 672] 

for i in rez.iloc[change_to_yes].index: 
    rez.iloc[i]['wanted'] = 'y' 

for i in rez.iloc[change_to_no].index: 
    rez.iloc[i]['wanted'] = 'n' 

rez 
```

前面的代码产生以下输出:

![](assets/25c89735-a506-4343-834a-8ea97965d1c2.png)

这看起来可能有很多变化，但在评估的 900 多篇文章中，我必须改变的很少。通过进行这些修正，我们现在可以将这些反馈到我们的模型中，以进一步改进它。让我们将这些结果添加到之前的训练数据中，然后重建模型:

```
combined = pd.concat([df[['wanted', 'text']], rez[['wanted', 'text']]]) combined 
```

前面的代码产生以下输出:

![](assets/b97a3db6-da79-4fbc-9a5d-6216fbde7bd7.png)

用以下代码重新训练模型:

```
tvcomb = vect.fit_transform(combined['text'], combined['wanted']) 

model = clf.fit(tvcomb, combined['wanted']) 
```

现在我们已经用所有可用的数据重新训练了我们的模型。当你在几天或几周内得到更多的结果时，你可能想这样做很多次。你加的越多，结果就会越好。

在这一点上，我们假设您有一个训练有素的模型，并准备开始使用它。现在让我们看看如何部署它来设置个性化新闻源。

# 设置您的每日个人简讯

为了建立一个包含新闻故事的个人电子邮件，我们将再次使用 IFTTT。和以前一样，在[第 3 章](03.html)*中，我们将使用 Webhooks 频道发送`POST`请求。但这一次，有效载荷将是我们的新闻故事。如果您还没有设置 Webhooks 频道，请现在就设置。说明可以在[第 3 章](03.html)、*中找到，建立一个寻找廉价机票的应用程序*。你还应该设置 Gmail 频道。一旦完成，我们将添加一个食谱来结合这两者。按照以下步骤设置 IFTTT:*

1.  首先，从 IFTTT 主页点击新建小程序，然后点击+this。然后，搜索 Webhooks 频道:

![](assets/9dfbd355-91a9-4624-9ccf-155e0796cb1b.png)

2.  选择该选项，然后选择接收网络请求:

![](assets/35ed4dcf-c87b-42ae-b804-c13f352ac26a.png)

3.  然后，给请求起一个名字。我在用`news_event`:

![](assets/aaa279e1-2abc-42e7-97cd-4d2ff759aa9f.png)

4.  单击创建触发器完成。接下来，点击+以设置电子邮件。搜索 Gmail 并点击:

![](assets/0f4c3ac5-e8a7-4d0e-9eef-9e4da3764925.png)

5.  单击 Gmail 后，单击给自己发送电子邮件。在那里，您可以自定义您的电子邮件:

![](assets/36ee5b08-8d0d-45a2-a692-936f5e2b9b74.png)

输入主题行，并在邮件正文中包含`{{Value1}}`。我们将传递我们的故事标题，并将其与我们的`POST`请求联系起来。单击创建操作，然后单击完成将其完成。

现在，我们已经准备好生成将按计划运行的脚本，自动向我们发送感兴趣的文章。我们将为此创建一个单独的脚本，但是我们需要在现有代码中做的最后一件事是序列化我们的矢量器和模型，如下面的代码块所示:

```
import pickle 

pickle.dump(model, open(r'/input/a/path/here/to/news_model_pickle.p', 'wb')) 

pickle.dump(vect, open(r'/input/a/path/here/to/news_vect_pickle.p', 'wb')) 
```

这样，我们就从我们的模型中节省了所有我们需要的东西。在我们的新脚本中，我们将阅读这些内容来生成新的预测。我们将使用与我们在[第 3 章](03.html)*中使用的相同的调度库来运行代码，构建一个应用程序来查找便宜的机票*。综上所述，我们有以下脚本:

```
import pandas as pd 
from sklearn.feature_extraction.text import TfidfVectorizer 
from sklearn.svm import LinearSVC 
import schedule 
import time 
import pickle 
import json 
import gspread 
from oauth2client.service_account import ServiceAccountCredentials 
import requests 
from bs4 import BeautifulSoup 

def fetch_news(): 

    try: 
        vect = pickle.load(open(r'/your/path/to/news_vect_pickle.p', 'rb')) 
        model = pickle.load(open(r'/your/path/to /news_model_pickle.p', 'rb')) 

        JSON_API_KEY = r'/your/path/to/API KEY.json' 

        scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive'] 

        credentials = ServiceAccountCredentials.from_json_keyfile_name(JSON_API_KEY, scope) 
        gc = gspread.authorize(credentials) 

        ws = gc.open("NewStories") 
        sh = ws.sheet1 
        zd = list(zip(sh.col_values(2),sh.col_values(3), sh.col_values(4))) 
        zf = pd.DataFrame(zd, columns=['title','urls','html']) 
        zf.replace('', pd.np.nan, inplace=True) 
        zf.dropna(inplace=True) 

        def get_text(x): 
            soup = BeautifulSoup(x, 'html5lib') 
            text = soup.get_text() 
            return text 

        zf.loc[:,'text'] = zf['html'].map(get_text) 

        tv = vect.transform(zf['text']) 
        res = model.predict(tv) 

        rf = pd.DataFrame(res, columns=['wanted']) 
        rez = pd.merge(rf, zf, left_index=True, right_index=True) 

        rez = rez.iloc[:20,:] 

        news_str = '' 
 for t, u in zip(rez[rez['wanted']=='y']['title'], rez[rez['wanted']=='y']['urls']): 
            news_str = news_str + t + '\n' + u + '\n' 

        payload = {"value1" : news_str} 
        r = requests.post('https://maker.ifttt.com/trigger/news_event/with/key/bNHFwiZx0wMS7EnD425n3T', data=payload) 

        # clean up worksheet 
        lenv = len(sh.col_values(1)) 
        cell_list = sh.range('A1:F' + str(lenv)) 
        for cell in cell_list: 
            cell.value = "" 
        sh.update_cells(cell_list) 
        print(r.text) 

    except: 
        print('Action Failed') 

schedule.every(480).minutes.do(fetch_news) 

while 1: 
    schedule.run_pending() 
    time.sleep(1) 
```

这个脚本将每 4 小时运行一次，从 Google Sheets 中下拉新闻故事，通过模型运行这些故事，通过向 IFTTT 发送`POST`请求来生成一封电子邮件，请求那些预测感兴趣的故事，然后，最后，它将清除电子表格中的故事，这样在下一封电子邮件中只会发送新的故事。

恭喜你！你现在有自己的个性化新闻源了！

# 摘要

在本章中，我们学习了在训练机器学习模型时如何处理文本数据。我们还学习了自然语言处理和支持向量机的基础知识。

在下一章中，我们将进一步发展这些技能，并尝试预测什么样的内容会像病毒一样传播。*