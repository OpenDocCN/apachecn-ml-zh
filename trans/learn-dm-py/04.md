# 使用相似性分析推荐电影

在这一章中，我们将看一下**亲和度分析**，它决定了对象何时频繁出现在一起。这也被通俗地称为市场篮子分析，在一个常见的用例之后——确定商品在商店里经常一起购买的时间。

在[第 3 章](02.html) *【用决策树预测体育赢家】*中，我们将一个物体作为焦点，并使用特征来描述该物体。在本章中，数据有不同的形式。我们有一些事务，其中感兴趣的对象(电影，在本章中)以某种方式在这些事务中使用。目的是发现物体何时同时出现。如果我们想知道两部电影是由同一个评论者推荐的，我们可以使用相似性分析。

本章的关键概念如下:

*   产品推荐的相似性分析
*   基于 Apriori 算法的特征关联挖掘
*   推荐系统和固有挑战
*   稀疏数据格式及其使用方法

# 亲和力分析

相似性分析是确定对象何时以相似方式使用的任务。在上一章中，我们关注的是物体本身是否相似——在我们的例子中，游戏在本质上是否相似。关联性分析的数据通常以事务的形式描述。直观地说，这来自商店的交易——确定何时一起购买对象，作为向用户推荐他们可能购买的产品的一种方式。

然而，相似性分析可以应用于许多不使用这种意义上的事务的过程:

*   欺诈检测
*   客户细分
*   软件优化
*   产品推荐

亲和力分析通常比分类更具探索性。至少，我们经常只是简单地对结果进行排名，然后选择前五名推荐(或者其他一些数字)，而不是期待算法给我们一个具体的答案。

此外，我们通常没有许多分类任务所期望的完整数据集。例如，在电影推荐中，我们有不同人对不同电影的评论。然而，我们不太可能让每个审查者审查我们数据集中的所有电影。这给亲和分析留下了一个重要而困难的问题。如果评论者没有评论过一部电影，这是表明他们对这部电影不感兴趣(因此不会推荐它)还是仅仅表明他们还没有评论过这部电影？

思考数据集中的差距会导致这样的问题。反过来，这可能导致有助于提高你的方法效率的答案。作为一个初露头角的数据挖掘者，知道你的模型和方法需要改进的地方是创造伟大成果的关键。

# 亲和分析算法

我们在[第 1 章](03.html) *【数据挖掘入门*中介绍了一种亲和分析的基本方法，该方法测试了所有可能的规则组合。我们计算了每个规则的可信度和支持度，这反过来又让我们对它们进行排名，以找到最佳规则。

然而，这种方法效率不高。我们在[第 1 章](03.html) *【数据挖掘入门】*中的数据集只有 5 个待售项目。我们可以预计即使是一家小商店也会有数百种商品出售，而许多在线商店会有数千(或数百万！).通过一个简单的规则创建，比如我们之前在[第 1 章](03.html) *【数据挖掘入门】*中的算法，计算这些规则所需的时间呈指数级增长。随着我们添加更多的项目，计算所有规则所需的时间会显著加快。具体来说，规则的总可能数量为 *2n - 1* 。对于我们的五项数据集，有 31 个可能的规则。10 个项目，是 1023。对于仅仅 100 个项目，这个数字有 30 位数字。即使计算能力的急剧增长也不可能跟上在线存储项目数量的增长。因此，我们需要工作更智能的算法，而不是工作更努力的计算机。

亲和分析的经典算法称为 **Apriori 算法**。它解决了在数据库中创建频繁出现的项目集的指数问题，称为**频繁项目集**。一旦发现了这些频繁项集，创建关联规则就很简单了，我们将在本章后面部分看到。

Apriori 背后的直觉既简单又聪明。首先，我们确保规则在数据集中有足够的支持。定义最小支持度是 Apriori 的关键参数。为了建立频繁项目集，我们将较小的频繁项目集进行组合。要使项集(A，B)具有至少 30 的支持，A 和 B 必须在数据库中至少出现 30 次。这个属性也扩展到更大的集合。对于被认为是频繁的项目集(A，B，C，D)，该集(A，B，C)也必须是频繁的(D 也必须如此)。

这些频繁项集是可以建立的，可能的不频繁项集(其中有很多)将永远不会被测试。这在测试新规则时节省了大量时间，因为频繁项集的数量预计会大大少于可能项集的总数。

亲和分析的其他示例算法基于此或类似的概念，包括 **Eclat** 和 **FP-growth** 算法。数据挖掘文献中对这些算法有许多改进，进一步提高了方法的效率。在本章中，我们将重点介绍基本的 Apriori 算法。

# 总体方法

为了进行关联规则挖掘以进行相似性分析，我们首先使用 Apriori 算法来生成频繁项集。接下来，我们通过测试这些频繁项集中前提和结论的组合来创建关联规则(例如，*如果一个人推荐电影 X，他们也会推荐电影 Y* )。

1.  对于第一阶段，Apriori 算法需要一个值来表示项目集被认为是频繁的最小支持度。不考虑任何支持度较低的项目集。

Setting this minimum support too low will cause Apriori to test a larger number of itemsets, slowing the algorithm down. Setting it too high will result in fewer itemsets being considered frequent.

2.  第二阶段，在频繁项集被发现后，基于其置信度对关联规则进行测试。我们可以选择一个最低置信水平，一些要返回的规则，或者简单地返回所有的规则，让用户决定如何处理它们。

In this chapter, we will return only rules above a given confidence level. Therefore, we need to set our minimum confidence level. Setting this too low will result in rules that have a high support, but are not very accurate. Setting this higher will result in only more accurate rules being returned, but with fewer rules being discovered overall.

# 处理电影推荐问题

产品推荐是个大生意。在线商店利用它向顾客推荐他们可以购买的其他产品，从而进行促销。提出更好的建议会带来更好的销售。当网上购物每年向数百万客户销售时，通过向这些客户销售更多的商品，有很多潜在的钱可以赚。

产品推荐，包括电影和书籍，已经研究了很多年；然而，当网飞在 2007 年至 2009 年间管理他们的网飞奖时，这个领域获得了显著的提升。这场比赛旨在确定是否有人能比网飞更好地预测用户对电影的评价。该奖项颁给了一个比当前解决方案略好 10%的团队。虽然这看起来不是一个很大的进步，但这样的进步将在接下来的几年里为网飞带来数百万的收入。

# 获取数据集

自 Netflix 奖设立以来，明尼苏达大学的一个研究小组 Grouplens 已经发布了几个数据集，这些数据集通常用于测试该领域的算法。他们已经发布了几个版本的电影分级数据集，它们有不同的大小。有一个版本有 10 万评论，一个有 100 万评论，一个有 1000 万评论。

数据集可从[http://grouplens.org/datasets/movielens/](http://grouplens.org/datasets/movielens/)获得，我们将在本章中使用的数据集是 *MovieLens 100K 数据集*(有 100，000 条评论)。下载此数据集并将其解压缩到您的数据文件夹中。启动新的 Jupyter 笔记本并键入以下代码:

```py
import os
import pandas as pd
data_folder = os.path.join(os.path.expanduser("~"), "Data", "ml-100k")
ratings_filename = os.path.join(data_folder, "u.data")

```

确保`ratings_filename`指向解压缩文件夹中的 u.data 文件。

# 装载熊猫

`MovieLens`数据集状态良好；但是`pandas.read_csv`中的默认选项有一些我们需要做的改动。首先，数据由制表符而不是逗号分隔。接下来，没有标题行。这意味着文件中的第一行实际上是数据，我们需要手动设置列名。

加载文件时，我们将分隔符参数设置为 tab 字符，告诉 pandas 不要读取第一行作为标题(用`header=None`)，并用给定值设置列名。让我们看看下面的代码:

```py
all_ratings = pd.read_csv(ratings_filename, delimiter="t", header=None, names
            = ["UserID", "MovieID", "Rating", "Datetime"])

```

虽然我们不会在本章中使用它，但是您可以使用下面的行来正确解析日期时间戳。评论日期可能是推荐预测中的一个重要特征，因为一起排名的电影通常比单独排名的电影具有更相似的排名。考虑到这一点可以显著改进模型。

```py
all_ratings["Datetime"] = pd.to_datetime(all_ratings['Datetime'], unit='s')

```

您可以通过在新单元格中运行以下命令来查看前几条记录:

```py
all_ratings.head()

```

结果会是这样的:

|  | 使用者辩证码 | 电影 ID | 评级 | 日期时间 |
| Zero | One hundred and ninety-six | Two hundred and forty-two | three | 1997-12-04 15:55:49 |
| one | One hundred and eighty-six | Three hundred and two | three | 1998-04-04 19:22:22 |
| Two | Twenty-two | Three hundred and seventy-seven | one | 1997-11-07 07:18:36 |
| three | Two hundred and forty-four | Fifty-one | Two | 1997-11-27 05:02:03 |
| four | One hundred and sixty-six | Three hundred and forty-six | one | 1998-02-02 05:33:16 |

# 稀疏数据格式

该数据集采用稀疏格式。每一行都可以看作是前几章中使用的大型特征矩阵中的一个单元，其中行是用户，列是单个电影。第一栏是每个用户对第一部电影的评论，第二栏是每个用户对第二部电影的评论，依此类推。

这个数据集中大约有 1000 个用户和 1700 部电影，这意味着整个矩阵将相当大(近 200 万个条目)。我们可能会遇到将整个矩阵存储在内存中的问题，对其进行计算会很麻烦。但是这个矩阵有一个属性，就是大部分单元格都是空的，也就是大部分用户没有大部分电影的评论。不过，对于用户号 213 没有电影号 675 的评论，对于用户和电影的大多数其他组合也没有。

这里给出的格式代表了完整的矩阵，但更简洁。第一行指示用户号 196 查看了电影号 242，在 1997 年 12 月 4 日给它的排名是 3(五分之三)。

不在此数据库中的任何用户和电影组合都被认为不存在。这节省了大量空间，而不是在内存中存储一堆零。这种格式称为稀疏矩阵格式。根据经验，如果您期望大约 60%或更多的数据集为空或为零，稀疏格式将占用较少的存储空间。

When computing on sparse matrices, the focus isn't usually on the data we don't have—comparing all of the zeroes. We usually focus on the data we have and compare those.

# 理解 Apriori 算法及其实现

本章的目标是产生以下形式的规则:*如果有人推荐这套电影，他们也会推荐这套电影*。我们还将讨论扩展，其中推荐一组电影的人可能会推荐另一部特定的电影。

为此，我们首先需要确定一个人是否推荐一部电影。我们可以通过创建一个新的功能“好评”来做到这一点，如果这个人对一部电影给予了好评，这就是真的:

```py
all_ratings["Favorable"] = all_ratings["Rating"] > 3

```

我们可以通过查看数据集来查看新要素:

```py
all_ratings[10:15]

```

|  | 使用者辩证码 | 电影 ID | 评级 | 日期时间 | 有利的 |
| Ten | Sixty-two | Two hundred and fifty-seven | Two | 1997-11-12 22:07:14 | 错误的 |
| Eleven | Two hundred and eighty-six | One thousand and fourteen | five | 1997-11-17 15:38:45 | 真实的 |
| Twelve | Two hundred | Two hundred and twenty-two | five | 1997-10-05 09:05:40 | 真实的 |
| Thirteen | Two hundred and ten | Forty | three | 1998-03-27 21:59:54 | 错误的 |
| Fourteen | Two hundred and twenty-four | Twenty-nine | three | 1998-02-21 23:40:57 | 错误的 |

我们将对数据集进行采样以形成训练数据。这也有助于减少要搜索的数据集的大小，使 Apriori 算法运行得更快。我们获得了前 200 名用户的所有评论:

```py
ratings = all_ratings[all_ratings['UserID'].isin(range(200))]

```

接下来，我们可以创建一个仅包含样本中好评的数据集:

```py
favorable_ratings_mask = ratings["Favorable"]
favorable_ratings = ratings[favorable_ratings_mask]

```

我们将搜索用户对我们的项目集的好评。所以，接下来我们需要的是每个用户都给予好评的电影。我们可以通过`UserID`对数据集进行分组，并对每组中的电影进行迭代来计算:

```py
favorable_reviews_by_users = dict((k, frozenset(v.values)) for k, v in favorable_ratings.groupby("UserID")["MovieID"])

```

在前面的代码中，我们将值存储为`frozenset`，允许我们快速检查电影是否已经被用户评分。

对于这种类型的操作，集合比列表快得多，我们将在后面的代码中使用它们。

最后，我们可以创建一个`DataFrame`，告诉我们每部电影获得好评的频率:

```py
num_favorable_by_movie = ratings[["MovieID", "Favorable"]].groupby("MovieID").sum()

```

我们可以通过运行以下代码来查看前五部电影:

```py
num_favorable_by_movie.sort_values(by="Favorable", ascending=False).head()

```

让我们看看前五名的电影名单。我们现在只有身份证，将在本章稍后获得它们的标题。

| 电影 ID | 有利的 |
| Fifty | One hundred |
| One hundred | eighty-nine |
| Two hundred and fifty-eight | Eighty-three |
| One hundred and eighty-one | Seventy-nine |
| One hundred and seventy-four | Seventy-four |

# 研究 Apriori 算法的基础

Apriori 算法是我们的相似性分析方法的一部分，专门处理在数据中查找频繁项集。Apriori 的基本过程是从以前发现的频繁项集建立新的候选项集。测试这些候选项以查看它们是否频繁，然后算法按照这里的解释进行迭代:

1.  通过将每个项目放在自己的项目集中来创建初始频繁项目集。在此步骤中，仅使用至少具有最低支持的项目。
2.  通过查找现有频繁项目集的超集，从最近发现的频繁项目集中创建新的候选项目集。
3.  测试所有候选项目集，看它们是否频繁。如果一个候选人不经常出现，那么它就会被丢弃。如果这一步没有新的频繁项目集，请转到最后一步。
4.  存储新发现的频繁项集，进入第二步。
5.  返回所有发现的频繁项目集。

以下工作流程概述了该流程:

![](img/B06162_04_01.jpg)

# 实现 Apriori 算法

在 Apriori 的第一次迭代中，新发现的项目集的长度将为 2，因为它们将是第一步中创建的初始项目集的超集。在第二次迭代中(应用第四步并返回第二步后)，新发现的项目集的长度将为 3。这使我们能够根据第二步的需要快速识别新发现的项目集。

我们可以将发现的频繁项目集存储在字典中，其中关键是项目集的长度。这使我们能够在以下代码的帮助下快速访问给定长度的项目集，从而快速访问最近发现的频繁项目集:

```py
frequent_itemsets = {}

```

我们还需要定义项集被认为是频繁项集所需的最小支持。该值是根据数据集选择的，但尝试不同的值以了解其对结果的影响。我建议一次只改变 10%，因为算法运行的时间会有很大不同！让我们设置一个最小支持值:

```py
min_support = 50

```

To implement the first step of the Apriori algorithm, we create an itemset with each movie individually and test if the itemset is frequent. We use `frozenset`**,** as they allow us to perform faster set-based operations later on, and they can also be used as keys in our counting dictionary (normal sets cannot).

我们来看看下面的`frozenset`代码示例:

```py
frequent_itemsets[1] = dict((frozenset((movie_id,)), row["Favorable"])
 for movie_id, row in num_favorable_by_movie.iterrows()
 if row["Favorable"] > min_support)

```

为了提高效率，我们通过创建一个函数来实现第二步和第三步，该函数获取新发现的频繁项集，创建超集，然后测试它们是否频繁。首先，我们设置函数来执行这些步骤:

```py
from collections import defaultdict

def find_frequent_itemsets(favorable_reviews_by_users, k_1_itemsets, min_support):
    counts = defaultdict(int)
    for user, reviews in favorable_reviews_by_users.items():
        for itemset in k_1_itemsets:
            if itemset.issubset(reviews):
                for other_reviewed_movie in reviews - itemset:
                    current_superset = itemset | frozenset((other_reviewed_movie,))
                    counts[current_superset] += 1
    return dict([(itemset, frequency) for itemset, frequency in counts.items() if frequency >= min_support])

```

根据我们尽可能少地读取数据的经验法则，每次调用此函数时，我们都会遍历数据集一次。虽然这在这个实现中并不太重要(与普通计算机相比，我们的数据集相对较小)，但对于较大的应用程序来说，**单程**是一个很好的实践。

让我们详细看看这个函数的核心。我们遍历每个用户，以及每个之前发现的项目集，然后检查它是否是当前评论集的子集，这些评论存储在`k_1_itemsets`中(注意，这里 k_1 的意思是 *k-1* )。如果是，这意味着用户已经查看了项目集中的每部电影。这是通过`itemset.issubset(reviews)`线完成的。

然后，我们可以浏览用户已经看过的每一部电影(不在项目集中)，通过将项目集与新电影相结合来创建一个超集，并记录我们在计数字典中看到的这个超集。这些是 *k* 这个值的候选频繁项集。

我们通过测试哪些候选项目集有足够的支持度被认为是频繁的来结束我们的函数，并且只返回那些支持度超过我们的`min_support`值的项目集。

这个函数构成了我们的 Apriori 实现的核心，我们现在创建一个循环，重复更大算法的步骤，随着我们将 *k* 从 1 增加到最大值，存储新的项目集。在这个循环中，k 代表即将被发现的频繁项集的长度，允许我们通过使用关键字 *k - 1* 在我们的频繁项集字典中查找来访问以前发现最多的项集。我们创建频繁项集，并根据它们的长度将其存储在字典中。让我们看看代码:

```py
for k in range(2, 20):
    # Generate candidates of length k, using the frequent itemsets of length k-1
    # Only store the frequent itemsets
    cur_frequent_itemsets = find_frequent_itemsets(favorable_reviews_by_users,
                                                   frequent_itemsets[k-1], min_support)
    if len(cur_frequent_itemsets) == 0:
        print("Did not find any frequent itemsets of length {}".format(k))
        sys.stdout.flush()
        break
    else:
        print("I found {} frequent itemsets of length {}".format(len(cur_frequent_itemsets), k))
        sys.stdout.flush()
        frequent_itemsets[k] = cur_frequent_itemsets

```

如果我们确实找到了频繁项集，我们会打印出一条消息，让我们知道循环将再次运行。如果没有，我们停止迭代，因为 *k+1* 不能有频繁项集，如果 *k* 的当前值没有频繁项集，那么我们完成算法。

We use `sys.stdout.flush()` to ensure that the printouts happen while the code is still running. Sometimes, in large loops in particular cells, the printouts will not happen until the code has completed. Flushing the output in this way ensures that the printout happens when we want, rather than when the interface decides it can allocate the time to print. Don't flush too frequently though—the flush operation carries a computational cost (as does normal printing) and this will slow down the program. You can now run the above code.
The preceding code returns about 2000 frequent itemsets of varying lengths. You'll notice that the number of itemsets grows as the length increases before it shrinks. It grows because of the increasing number of possible rules. After a while, the large number of combinations no longer has the support necessary to be considered frequent. This results in the number shrinking. This shrinking is the benefit of the Apriori algorithm. If we search all possible itemsets (not just the supersets of frequent ones), we would be searching thousands of times more itemsets to see if they are frequent.

即使这种缩小没有发生，当发现所有电影组合在一起的规则时，该算法也绝对结束。因此，Apriori 算法将始终终止。

It may take a few minutes for this code to run, more if you have older hardware. If you find you are having trouble running any of the code samples, take a look at using an online cloud provider for additional speed. Details about using the cloud to do the work are given in Appendix, Next Steps.

# 提取关联规则

Apriori 算法完成后，我们有一个频繁项集列表。这些不完全是关联规则，但它们可以很容易地转换成这些规则。频繁项集是具有最小支持度的一组项，而关联规则有前提和结论。两者的数据是一样的。

We can make an *association rule from a frequent itemset* by taking one of the movies in the itemset and denoting it as the conclusion. The other movies in the itemset will be the premise. This will form rules of the following form: *if a reviewer recommends all of the movies in the premise, they will also recommend the conclusion movie*.

对于每个项目集，我们可以通过将每个电影设置为结论，剩余电影为前提来生成多个关联规则。

在代码中，我们首先通过迭代每个长度的发现的频繁项目集，从每个频繁项目集中生成所有规则的列表。作为结论，我们接着迭代项目集中的每一部电影。

```py
candidate_rules = []
for itemset_length, itemset_counts in frequent_itemsets.items():
    for itemset in itemset_counts.keys():
        for conclusion in itemset:
            premise = itemset - set((conclusion,))
            candidate_rules.append((premise, conclusion))

```

这将返回大量候选规则。通过打印列表中的前几条规则，我们可以看到一些:

```py
print(candidate_rules[:5])

```

结果输出显示了获得的规则:

```py
[(frozenset({79}), 258), (frozenset({258}), 79), (frozenset({50}), 64), (frozenset({64}), 50), (frozenset({127}), 181)]

```

在这些规则中，第一部分(第`frozenset` [)是前提中的电影列表，而后面的数字是结论。在第一种情况下，如果评论者推荐电影 79，他们也可能推荐电影 258。](03.html)

接下来，我们计算每个规则的置信度。这与[第 1 章](03.html) *【数据挖掘入门】*中的操作非常相似，唯一的变化是使用新数据格式进行计算所必需的变化。

计算置信度的过程从创建字典开始，以存储我们看到导致结论的前提的次数(规则的正确示例)和没有看到的次数(错误示例)。然后我们迭代所有的评论和规则，计算出规则的前提是否适用，如果适用，结论是否准确。

```py
correct_counts = defaultdict(int)
incorrect_counts = defaultdict(int)
for user, reviews in favorable_reviews_by_users.items():
    for candidate_rule in candidate_rules:
        premise, conclusion = candidate_rule
        if premise.issubset(reviews):
            if conclusion in reviews:
                correct_counts[candidate_rule] += 1
            else:
                incorrect_counts[candidate_rule] += 1

```

然后，我们通过将正确的计数除以规则被看到的总次数来计算每个规则的置信度:

```py
rule_confidence = {candidate_rule:
                    (correct_counts[candidate_rule] / float(correct_counts[candidate_rule] +  
                      incorrect_counts[candidate_rule]))
                  for candidate_rule in candidate_rules}

```

现在，我们可以通过对信心字典进行排序并打印结果来打印前五条规则:

```py
from operator import itemgetter
sorted_confidence = sorted(rule_confidence.items(), key=itemgetter(1), reverse=True)
for index in range(5):
    print("Rule #{0}".format(index + 1))
    premise, conclusion = sorted_confidence[index][0]
    print("Rule: If a person recommends {0} they will also recommend {1}".format(premise, conclusion))
    print(" - Confidence: {0:.3f}".format(rule_confidence[(premise, conclusion)]))
    print("")

```

最终的打印输出只显示电影标识，如果没有电影的名称，这就没什么用了。数据集附带了一个名为 u.items 的文件，该文件存储电影名称及其对应的电影 ID(以及其他信息，如流派)。

我们可以用熊猫从这个文件加载标题。数据集附带的自述文件中提供了有关文件和类别的其他信息。文件中的数据为 CSV 格式，但数据由|符号分隔；它没有头
，编码设置很重要。列名在自述文件中找到。

```py
movie_name_filename = os.path.join(data_folder, "u.item")
movie_name_data = pd.read_csv(movie_name_filename, delimiter="|", header=None,
                              encoding = "mac-roman")
movie_name_data.columns = ["MovieID", "Title", "Release Date", "Video Release", "IMDB", "<UNK>",
                           "Action", "Adventure", "Animation", "Children's", "Comedy", "Crime",
                           "Documentary", "Drama", "Fantasy", "Film-Noir", "Horror", "Musical",   
                           "Mystery", "Romance", "Sci-Fi", "Thriller", "War", "Western"]

```

获取电影标题是一个重要且经常使用的步骤，因此将它转化为功能是有意义的。我们将创建一个函数，从电影 ID 中返回电影的标题，省去我们每次查找的麻烦。让我们看看代码:

```py
def get_movie_name(movie_id):
    title_object = movie_name_data[movie_name_data["MovieID"] == movie_id]["Title"]
    title = title_object.values[0]
    return title

```

在一个新的 Jupyter Notebook 单元格中，我们调整了之前的代码，以打印出顶级规则，并包括标题:

```py
for index in range(5):
    print("Rule #{0}".format(index + 1))
    premise, conclusion = sorted_confidence[index][0]
    premise_names = ", ".join(get_movie_name(idx) for idx in premise)
    conclusion_name = get_movie_name(conclusion)
    print("Rule: If a person recommends {0} they will also recommend {1}".format(premise_names, conclusion_name))
    print(" - Confidence: {0:.3f}".format(rule_confidence[(premise, conclusion)]))
    print("")

```

结果可读性更强(仍有一些问题，但我们可以暂时忽略):

```py
Rule #1
Rule: If a person recommends Shawshank Redemption, The (1994), Silence of the Lambs, The (1991), Pulp Fiction (1994), Star Wars (1977), Twelve Monkeys (1995) they will also recommend Raiders of the Lost Ark (1981)
 - Confidence: 1.000

Rule #2
Rule: If a person recommends Silence of the Lambs, The (1991), Fargo (1996), Empire Strikes Back, The (1980), Fugitive, The (1993), Star Wars (1977), Pulp Fiction (1994) they will also recommend Twelve Monkeys (1995)
 - Confidence: 1.000

Rule #3
Rule: If a person recommends Silence of the Lambs, The (1991), Empire Strikes Back, The (1980), Return of the Jedi (1983), Raiders of the Lost Ark (1981), Twelve Monkeys (1995) they will also recommend Star Wars (1977)
 - Confidence: 1.000

Rule #4
Rule: If a person recommends Shawshank Redemption, The (1994), Silence of the Lambs, The (1991), Fargo (1996), Twelve Monkeys (1995), Empire Strikes Back, The (1980), Star Wars (1977) they will also recommend Raiders of the Lost Ark (1981)
 - Confidence: 1.000

Rule #5
Rule: If a person recommends Shawshank Redemption, The (1994), Toy Story (1995), Twelve Monkeys (1995), Empire Strikes Back, The (1980), Fugitive, The (1993), Star Wars (1977) they will also recommend Return of the Jedi (1983)
 - Confidence: 1.000

```

# 评估关联规则

从广义上讲，我们可以使用与分类相同的概念来评估关联规则。我们使用一组没有用于训练的测试数据，并根据它们在这个测试集中的表现来评估我们发现的规则。

为此，我们将计算测试集置信度，即测试集上每个规则的置信度。在这种情况下，我们不会应用正式的评估指标；我们只是检查规则并寻找好的例子。

形式评估可以包括通过确定预测用户是否对给定电影评价为有利的准确性来进行分类准确性。在这种情况下，如下所述，我们将非正式地查看规则，以找到更可靠的规则:

1.  首先，我们提取测试数据集，这是我们在训练集中没有使用的所有记录。我们将前 200 个用户(按 ID 值)用于训练集，将其余的全部用于测试数据集。与训练集一样，我们也将获得该数据集中每个用户的好评。让我们看看代码:

```py
test_dataset = all_ratings[~all_ratings['UserID'].isin(range(200))]
test_favorable = test_dataset[test_dataset["Favorable"]]
test_favorable_by_users = dict((k, frozenset(v.values)) for k, v in 
                               test_favorable.groupby("UserID")["MovieID"])

```

2.  然后我们计算前提导致结论的正确例子，就像我们之前做的一样。这里唯一的变化是使用了测试数据而不是训练数据。让我们看看代码:

```py
correct_counts = defaultdict(int)
incorrect_counts = defaultdict(int)
for user, reviews in test_favorable_by_users.items():
    for candidate_rule in candidate_rules:
        premise, conclusion = candidate_rule
        if premise.issubset(reviews):
            if conclusion in reviews:
                correct_counts[candidate_rule] += 1
            else:
                incorrect_counts[candidate_rule] += 1

```

3.  接下来，我们根据正确的计数计算每个规则的置信度，并对它们进行排序。让我们看看代码:

```py
test_confidence = {candidate_rule:
                             (correct_counts[candidate_rule] / float(correct_counts[candidate_rule] + incorrect_counts[candidate_rule]))
                             for candidate_rule in rule_confidence}
sorted_test_confidence = sorted(test_confidence.items(), key=itemgetter(1), reverse=True)

```

4.  最后，我们用标题而不是电影 id 打印出最佳关联规则:

```py
for index in range(10):
    print("Rule #{0}".format(index + 1))
    premise, conclusion = sorted_confidence[index][0]
    premise_names = ", ".join(get_movie_name(idx) for idx in premise)
    conclusion_name = get_movie_name(conclusion)
    print("Rule: If a person recommends {0} they will also recommend {1}".format(premise_names, conclusion_name))
    print(" - Train Confidence: {0:.3f}".format(rule_confidence.get((premise, conclusion), -1)))
    print(" - Test Confidence: {0:.3f}".format(test_confidence.get((premise, conclusion), -1)))
    print("")

```

我们现在可以看到哪些规则最适用于新的未知数据:

```py
Rule #1
Rule: If a person recommends Shawshank Redemption, The (1994), Silence of the Lambs, The (1991), Pulp Fiction (1994), Star Wars (1977), Twelve Monkeys (1995) they will also recommend Raiders of the Lost Ark (1981)
 - Train Confidence: 1.000
 - Test Confidence: 0.909

Rule #2
Rule: If a person recommends Silence of the Lambs, The (1991), Fargo (1996), Empire Strikes Back, The (1980), Fugitive, The (1993), Star Wars (1977), Pulp Fiction (1994) they will also recommend Twelve Monkeys (1995)
 - Train Confidence: 1.000
 - Test Confidence: 0.609

Rule #3
Rule: If a person recommends Silence of the Lambs, The (1991), Empire Strikes Back, The (1980), Return of the Jedi (1983), Raiders of the Lost Ark (1981), Twelve Monkeys (1995) they will also recommend Star Wars (1977)
 - Train Confidence: 1.000
 - Test Confidence: 0.946

Rule #4
Rule: If a person recommends Shawshank Redemption, The (1994), Silence of the Lambs, The (1991), Fargo (1996), Twelve Monkeys (1995), Empire Strikes Back, The (1980), Star Wars (1977) they will also recommend Raiders of the Lost Ark (1981)
 - Train Confidence: 1.000
 - Test Confidence: 0.971

Rule #5
Rule: If a person recommends Shawshank Redemption, The (1994), Toy Story (1995), Twelve Monkeys (1995), Empire Strikes Back, The (1980), Fugitive, The (1993), Star Wars (1977) they will also recommend Return of the Jedi (1983)
 - Train Confidence: 1.000
 - Test Confidence: 0.900

```

例如，第二个规则对训练数据有很好的信心，但是它只在 60%的测试数据情况下是准确的。前 10 名中的许多其他规则对测试数据有很高的可信度，这使它们成为提出建议的良好规则。

你可能还会注意到，这些电影往往是非常受欢迎的好电影。这给了我们一个可以比较的基线算法，也就是说，不要试图做个性化推荐，只推荐最喜欢的电影。试着实现这个算法 Apriori 算法比它强吗？强多少？另一个基准可能是简单地随机推荐同一类型的电影。

If you are looking through the rest of the rules, some will have a test confidence of -1\. Confidence values are always between 0 and 1\. This value indicates that the particular rule wasn't found in the test dataset at all.

# 摘要

在这一章中，我们进行了相似性分析，以便根据大量的评论者推荐电影。我们分两个阶段进行。首先，我们使用 Apriori 算法找到数据中的频繁项集。然后，我们从这些项目集创建关联规则。

由于数据集的大小，使用 Apriori 算法是必要的。在[第 1 章](00.html) *【数据挖掘入门】*中，我们使用了一种蛮力方法，这种方法计算更智能的方法所需的规则所需的时间呈指数级增长。这是数据挖掘的一种常见模式:对于小数据集，我们可以用蛮力的方式解决许多问题，但是需要更智能的算法来将概念应用到更大的数据集。

为了找到关联规则，我们对数据的一个子集进行了训练，然后在其余的数据上测试这些规则——一个测试集。根据我们在前面章节中讨论的内容，我们可以扩展这个概念，使用交叉验证来更好地评估规则。这将导致对每个规则的质量进行更可靠的评估。

为了进一步理解本章中的概念，请调查哪些电影获得了较高的总体评分(即大量推荐)，但没有足够的规则向新用户推荐它们。你会如何改变算法来推荐这些电影？

到目前为止，我们所有的数据集都是用特征来描述的。但是，并不是所有的数据集都是这样*预定义*的。在下一章中，我们将看看 scikit-learn 的变形金刚(它们在*第 3 章“用决策树预测体育赢家”*中介绍)作为一种从数据中提取特征的方法。我们将讨论如何实现我们自己的转换器，扩展现有的转换器，以及我们可以使用它们实现的概念。