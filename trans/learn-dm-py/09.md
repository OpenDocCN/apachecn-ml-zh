# 作者归属

**作者身份分析**是一项文本挖掘任务，旨在仅基于作者作品的内容来识别作者的某些方面。这可能包括年龄、性别或背景等特征。在具体的**作者归属**任务中，我们旨在确定一组作者中的哪一个写了一个特定的文档。这是一个经典的分类任务。在许多方面，作者身份分析任务是使用标准数据挖掘方法来执行的，例如交叉验证、特征提取和分类算法。

在本章中，我们将使用作者归属问题来拼凑我们在前面几章中开发的数据挖掘方法的各个部分。我们识别问题并讨论问题的背景和知识。这让我们可以选择要提取的特征，我们将构建一个管道来实现。我们将测试两种不同类型的特征:虚词和字符 n-gram。最后，我们将对结果进行深入分析。我们将首先使用书籍数据集，然后使用杂乱的真实电子邮件语料库。

我们将在本章中讨论的主题如下:

*   特征工程以及特征选择如何因应用而异
*   带着特定目标重新审视单词包模型
*   特征类型和字符 n-grams 模型
*   支持向量机
*   为数据挖掘清理混乱的数据集

# 将文件归属于作者

作者身份分析有一个背景在**文体学**，这是对一个作者的写作风格的研究。这个概念是基于这样一种想法，即每个人学习语言的方式略有不同，衡量人们写作中的这些细微差别将使我们能够仅使用他们写作的内容来区分他们。

作者身份分析在历史上(1990 年以前)一直使用可重复的手动分析和统计来执行，这很好地表明它可以通过数据挖掘来实现自动化。现代作者身份分析研究几乎完全基于数据挖掘，尽管仍有大量工作需要使用语言风格和风格计量学进行更多的人工驱动分析。当今特征工程的许多进步都是由风格计量学的进步推动的。换句话说，手动分析发现新的特征，然后将其编码并用作数据挖掘过程的一部分。

风格学的一个关键的潜在特征是**作者不变量**，这是一个特定作者在其所有文档中具有的特征，但是不与其他作者共享。在实践中，这些作者不变量似乎并不存在，因为作者风格会随着时间的推移而改变，但是数据挖掘的使用可以让我们接近基于这一原则的分类器。

作为一个领域，作者身份分析有许多子问题，主要有以下几个:

*   **作者简介:**这根据作品确定作者的年龄、性别或其他特征。例如，我们可以通过寻找一个人说英语的具体方式来检测他的第一语言。
*   **Authorship 验证:**这检查该文档的作者是否也写了另一个文档。这个问题是你在法庭上通常会想到的。例如，嫌疑人的写作风格(内容方面)将被分析，看看它是否符合赎金说明。
*   **Authorship clustering:** 这是 Authorship 验证的一个扩展，我们使用聚类分析将一个大集合中的文档进行聚类，每个聚类都是由同一个作者编写的。

However, the most common form of authorship analysis study is that of **authorship attribution**, a classification task where we attempt to predict which of a set of authors wrote a given document.

# 应用程序和用例

作者分析有许多**用例**。许多用例都与验证作者身份、证明共享作者身份/出处，或者将社交媒体简档与现实世界的用户联系起来等问题有关。

从历史的角度来看，我们可以使用作者身份分析来验证某些文件是否确实是由假定的作者写的。有争议的作者声明包括一些莎士比亚的戏剧，美国建国时期的联邦党人论文，以及其他历史文本。

Authorship studies alone cannot prove authorship but can provide evidence for or against a given theory, such as whether a particular person wrote a given document.

例如，我们可以分析莎士比亚的戏剧来确定他的写作风格，然后测试给定的十四行诗是否真的源于他(最近的一些研究表明他的一些作品有多个作者)。

一个更现代的用例是链接社交网络账户。例如，一个恶意的在线用户可以在多个在线社交网络上建立账户。能够将它们联系起来，当局就可以追踪给定账户的用户——例如，如果有人骚扰其他在线用户。

过去使用的另一个例子是作为骨干在法庭上提供专家证词，以确定给定的人是否写了一份文件。例如，嫌疑人可能被指控写电子邮件骚扰他人。使用作者身份分析可以确定这个人是否真的写了文档。另一个基于法庭的用途是解决盗用作者身份的索赔。例如，两位作者可能声称写了一本书，作者身份分析可以提供证据，证明哪个作者更有可能。

然而，作者身份分析并非万无一失。最近的一项研究发现，简单地要求未经训练的人隐藏他们的写作风格，将文件归属于作者会变得相当困难。这项研究还观察了一个框架练习，要求人们以另一个人的风格写作。对另一个人的这种诬陷被证明是相当可靠的，伪造的文件通常被认为是被诬陷的人。

尽管存在这些问题，作者身份分析在越来越多的领域被证明是有用的，并且是一个值得研究的有趣的数据挖掘问题。

Authorship attribution can be used in expert testimony, but by itself is hard to classify as hard evidence. Always check with a lawyer before using it for formal matters, such as authorship disputes.

# 作者归属

**作者身份归属**(区别于作者身份*分析*)是一个分类任务，通过它我们有一组候选作者、来自这些作者中每一个的一组文档，即**训练集**，以及一组未知作者身份的文档，也称为测试集。如果来路不明的文档确实属于候选文档之一，我们称之为封闭问题，如下图所示:

![](images/B06162_09_01.png)

如果我们不能确定实际作者是训练集的一部分，我们称之为开放问题。这种区别并不仅仅局限于作者的归属——任何数据挖掘应用程序，如果实际的类不在训练集中，都被认为是一个公开的问题，任务是找到候选作者或者不选择他们。如下图所示:

![](images/B06162_09_02.png)

在作者归属中，我们通常对任务有两个限制。它们如下:

*   首先，我们只使用文档中的内容信息，而不使用关于书写时间、交付、手写风格等的元数据。从这些不同类型的信息中组合模型有多种方法，但这通常不被认为是作者归属，更多的是**数据融合**应用。
*   第二个限制是我们不看文档的主题；相反，我们寻找更显著的特征，例如单词用法、标点符号和其他基于文本的特征。这里的推理是，一个人可以写很多不同的主题，所以担心他们写作的主题不会塑造他们实际的作者风格。查看主题词也会导致训练数据上的**过度拟合**——我们的模型可能在来自同一作者的文档上训练，也可能在同一主题上训练。例如，如果你通过阅读这本书来模仿我的写作风格，你可能会得出结论说*数据挖掘*代表了*我的*写作风格，而事实上，我也写其他主题。

从这里开始，执行作者归属的管道看起来很像我们在[第 6 章](05.html) *【使用朴素贝叶斯的社交媒体洞察】*中开发的管道。

1.  首先，我们从文本中提取特征。
2.  然后，我们对这些特征执行一些特征选择。
3.  最后，我们训练一个分类算法来适应一个模型，然后我们可以用它来预测文档的类别(在这个例子中是作者)。

There are some differences between classifying content and classifying authorship, mostly having to do with which features are used, that we will cover in this chapter. It is critical to choose features based on the application.

在我们深入研究这些问题之前，我们将定义问题的范围并收集一些数据。

# 获取数据

本章第一部分我们将使用的数据是来自[www.gutenberg.org](http://www.gutenberg.org)古腾堡计划的一套书籍，这是一个公共领域文学作品的存储库。我用于这些实验的书来自不同的作者:

*   布斯·塔金顿(22 部)
*   查尔斯·狄更斯(44 本)
*   伊迪丝·内斯比特(10 个头衔)
*   亚瑟·柯南·道尔(51 本)
*   马克·吐温(29 本)
*   理查德·弗朗西斯·伯顿爵士(11 个头衔)
*   埃米尔·加博里奥(10 本书)

总的来说，有来自 7 位作者的 177 个文档，提供了大量的文本。名为 getdata.py 的代码包中给出了标题的完整列表，以及下载链接和自动获取它们的脚本。如果运行代码得到的书籍比上面少得多，镜像可能会关闭。更多镜像网址可在脚本中尝试参见本网站:[https://www.gutenberg.org/MIRRORS.ALL](https://www.gutenberg.org/MIRRORS.ALL)

为了下载这些书，我们使用请求库将文件下载到数据目录中。

首先，在新的 Jupyter 笔记本中，设置数据目录，并确保以下代码链接到该目录:

```py
import os 
import sys 
data_folder = os.path.join(os.path.expanduser("~"), "Data", "books")

```

接下来，从 Packt 提供的代码包中下载数据包。将文件解压缩到此目录。图书文件夹应该直接包含每个作者的一个文件夹。

看完这些文件后，您会发现其中许多文件相当混乱——至少从数据分析的角度来看是如此。文件的开头有一个大型项目古腾堡免责声明。在我们进行分析之前，这需要删除。

例如，大多数书籍以如下信息开头:

查尔斯·狄更斯等人的《穆格比路口的古腾堡工程》电子书《T2》*，朱尔斯·a .
古德曼的《T4》*该电子书免费供任何人在任何地方使用，几乎没有任何限制。您可以将其复制、赠送或
*根据本电子书随附的《古腾堡计划许可证》条款*
*重新使用或在 www.gutenberg.org 在线*
*标题:穆格比路口*
*作者:查尔斯·狄更斯*
*发布日期:2009 年 1 月 28 日【电子书#27924】语言:英语*
*字符集编码*

在这一点之后，书的实际文本开始。使用一行开始* * *古腾堡计划是相当一致的，我们将使用它作为文本开始的提示——在这一行之前的任何内容都将被忽略。

我们可以改变磁盘上的单个文件来删除这些内容。然而，如果我们丢失了数据，会发生什么呢？我们将失去我们的变化，并可能无法复制这项研究。出于这个原因，我们将在加载文件时执行预处理——这使我们能够确保我们的结果是可复制的(只要数据源保持不变)。以下代码消除了书籍中的主要噪音源，这是古腾堡计划添加到文件中的前奏:

```py
def clean_book(document):
    lines = document.split("n")
    start= 0
    end = len(lines)
    for i in range(len(lines)):
        line = lines[i]
        if line.startswith("*** START OF THIS PROJECT GUTENBERG"):
            start = i + 1
        elif line.startswith("*** END OF THIS PROJECT GUTENBERG"):
            end = i - 1
    return "n".join(lines[start:end])

```

You may want to add to this function to remove other sources of noise, such as inconsistent formatting, footer information, and so on. Investigate the files to examine what issues they have.

我们现在可以使用下面的函数来获取我们的文档和类，该函数循环遍历这些文件夹，加载文本文档并记录一个分配给作者的数字作为目标类。

```py
import numpy as np

def load_books_data(folder=data_folder):
    documents = []
    authors = []
    subfolders = [subfolder for subfolder in os.listdir(folder)
                  if os.path.isdir(os.path.join(folder, subfolder))]
    for author_number, subfolder in enumerate(subfolders):
        full_subfolder_path = os.path.join(folder, subfolder)
        for document_name in os.listdir(full_subfolder_path):
            with open(os.path.join(full_subfolder_path, document_name), errors='ignore') as inf:
                documents.append(clean_book(inf.read()))
                authors.append(author_number)
    return documents, np.array(authors, dtype='int')

```

然后我们调用这个函数来实际加载书籍:

```py
documents, classes = load_books_data(data_folder)

```

This dataset fits into memory quite easily, so we can load all of the text at once. In cases where the whole dataset doesn't fit, a better solution is to extract the features from each document one-at-a-time (or in batches) and save the resulting values to a file or in-memory matrix

为了对数据的属性进行评估，我通常做的第一件事是创建一个简单的文档长度直方图。如果长度相对一致，这通常比完全不同的文档长度更容易学习。在这种情况下，文档长度有相当大的差异。要查看这一点，首先我们将长度提取到一个列表中:

```py
document_lengths = [len(document) for document in documents]

```

接下来，我们绘制这些。Matplotlib 有一个`hist`函数可以做到这一点，Seaborn 也是如此，默认情况下它会生成更好看的图形。

```py
import seaborn as sns
sns.distplot(document_lengths)

```

生成的图表显示了文档长度的变化:

![](images/B06162_09_06.png)

# 使用虚词

最早的特征类型之一是在单词包模型中使用虚词，这对于作者身份分析仍然非常有效。虚词是指本身意义不大，但创造(英语！)句子。例如 *this* 和*这两个词*实际上只是由它们在一个句子中所做的事情来定义的，而不是它们本身的意思。与此形成对比的是像*老虎*这样的实词，它有一个明确的含义，在句子中使用时会调用一只大猫的意象。

被认为是虚词的那组词并不总是显而易见的。一个好的经验法则是选择使用频率最高的单词(在所有可能的文档中，而不仅仅是来自同一作者的文档)。

Typically, the more frequently a word is used, the better it is for authorship analysis. In contrast, the less frequently a word is used, the better it is for content-based text mining, such as in the next chapter, where we look at the topic of different documents.

这里的图表给出了单词和频率关系之间的更好的概念:

![](images/B06162_09_03.png)

虚词的使用较少由文档的内容来定义，更多的是由作者做出的决定来定义。这使得它们成为区分不同用户之间的作者特征的很好的候选者。例如，虽然许多美国人特别在意*和*与*在一句话中的不同用法，但其他国家的人，比如澳大利亚，不太在意这种区别。这意味着一些澳大利亚人倾向于使用一个或另一个词，而其他人可能会使用更多的*。**

这种差异，加上成千上万的其他细微差别，构成了作者身份的模型。

# 计算虚词

我们可以使用我们在[第 6 章](05.html) *中使用的 CountVectorizer 类来计算虚词，使用朴素贝叶斯的社交媒体洞察*。这个类可以被传递一个词汇表，这是它要寻找的一组单词。如果一个词汇没有通过(我们在[第六章](05.html) *【使用朴素贝叶斯的社交媒体洞察】*的代码中没有通过)，那么它会从训练数据集中学习这个词汇。所有单词都在文档的训练集中(当然取决于其他参数)。

首先，我们建立我们的虚词词汇表，它只是一个包含每个虚词的列表。究竟哪些词是虚词，哪些不是，有待商榷。我从已发表的研究中发现以下列表相当不错，这些列表是我结合其他研究人员的单词列表进行研究后得出的。请记住，代码包可从 Packt publishing(或官方 github 频道)获得，因此您不需要键入:

```py
function_words = ["a", "able", "aboard", "about", "above", "absent", "according" , "accordingly", "across", "after", "against","ahead", "albeit", "all", "along", "alongside", "although", "am", "amid", "amidst", "among", "amongst", "amount", "an", "and", "another", "anti", "any", "anybody", "anyone", "anything", "are", "around", "as", "aside", "astraddle", "astride", "at", "away", "bar", "barring", "be", "because", "been", "before", "behind", "being", "below", "beneath", "beside", "besides", "better", "between", "beyond", "bit", "both", "but", "by", "can", "certain", "circa", "close", "concerning", "consequently", "considering", "could", "couple", "dare", "deal", "despite", "down", "due", "during", "each", "eight", "eighth", "either", "enough", "every", "everybody", "everyone", "everything", "except", "excepting", "excluding", "failing", "few", "fewer", "fifth", "first", "five", "following", "for", "four", "fourth", "from", "front", "given", "good", "great", "had", "half", "have", "he", "heaps", "hence", "her", "hers", "herself", "him", "himself", "his", "however", "i", "if", "in", "including", "inside", "instead", "into", "is", "it", "its", "itself", "keeping", "lack", "less", "like", "little", "loads", "lots", "majority", "many", "masses", "may", "me", "might", "mine", "minority", 
"minus", "more", "most", "much", "must", "my", "myself", "near", "need", "neither", "nevertheless", "next", "nine", "ninth", "no", "nobody", "none", "nor", "nothing", "notwithstanding", "number", "numbers", "of", "off", "on", "once", "one", "onto", "opposite", "or", "other", "ought", "our", "ours", "ourselves", "out", "outside", "over", "part", "past", "pending", "per", "pertaining", "place", "plenty", "plethora", "plus", "quantities", "quantity", "quarter", "regarding", "remainder", "respecting", "rest", "round", "save", "saving", "second", "seven", "seventh", "several","shall", "she", "should", "similar", "since", "six", "sixth", "so", "some", "somebody", "someone", "something", "spite","such", "ten", "tenth", "than", "thanks", "that", "the", "their", "theirs", "them", "themselves", "then", "thence", "therefore", "these", "they", "third", "this", "those","though", "three", "through", "throughout", "thru", "thus", "till", "time", "to", "tons", "top", "toward", "towards", "two", "under", "underneath", "unless", "unlike", "until", "unto", "up", "upon", "us", "used", "various", "versus","via", "view", "wanting", "was", "we", "were", "what", "whatever", "when", "whenever", "where", "whereas", "wherever", "whether", "which", "whichever", "while","whilst", "who", "whoever", "whole", "whom", "whomever", "whose", "will", "with", "within", "without", "would", "yet", "you", "your", "yours", "yourself", "yourselves"]

```

现在，我们可以设置一个提取器来获取这些虚词的计数。注意虚词列表作为`vocabulary`进入`CountVectorizer`初始化器。

```py
from sklearn.feature_extraction.text 
import CountVectorizer 
extractor = CountVectorizer(vocabulary=function_words)

```

对于这组虚词，这些文档中的出现频率非常高——正如您所期望的那样。我们可以使用提取器实例来获取这些计数，方法是将其拟合到数据上，然后调用`transform`(或者，使用`fit_transform`的快捷方式)。

```py
extractor.fit(documents)
counts = extractor.transform(documents)

```

在绘图之前，我们通过除以相关文档长度来标准化这些计数。下面的代码实现了这一点，得出了每个虚词所占的单词百分比:

```py
normalized_counts = counts.T / np.array(document_lengths)

```

然后，我们对所有文档的这些百分比进行平均:

```py
averaged_counts = normalized_counts.mean(axis=1)

```

最后，我们使用 Matplotlib 绘制它们(Seaborn 缺乏像这样简单的基本绘图界面)。

```py
from matplotlib import pyplot as plt
plt.plot(averaged_counts)

```

![](images/B06162_09_07.png)

# 用虚词分类

这里唯一的新东西是**支持向量机** ( **SVM** )的使用，我们将在下一节中介绍(目前，只将其视为标准分类算法)。

接下来，我们导入我们的类。我们导入 SVC 类，一个用于分类的 SVM，以及我们以前见过的其他标准工作流工具:

```py
from sklearn.svm import SVC 
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import Pipeline from sklearn import grid_search

```

支持向量机有许多参数。正如我所说的，在下一节详细讨论之前，我们将在这里盲目地使用一个。然后，我们使用字典来设置要搜索的参数。对于`kernel`参数，我们将尝试`linear`和`rbf`。对于 C，我们将尝试 1 和 10 的值(这些参数的描述将在下一节中介绍)。然后，我们创建一个网格搜索来搜索这些参数以获得最佳选择:

```py
parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}
svr = SVC()
grid = grid_search.GridSearchCV(svr, parameters)

```

Gaussian kernels (such as RBF) only work for reasonably sized data sets, such as when the number of features is fewer than about 10,000.

接下来，我们建立一个管道，使用`CountVectorizer`进行特征提取(仅使用虚词)，同时使用 SVM 进行网格搜索。代码如下:

```py
pipeline1 = Pipeline([('feature_extraction', extractor), ('clf', grid) ])

```

接下来，应用`cross_val_score`来获得我们对这个管道的交叉验证分数。结果是 0.811，这意味着我们大约有 80%的预测是正确的。

# 支持向量机

支持向量机是基于简单直观思想的分类算法，以一些复杂而创新的数学为后盾。支持向量机在两个类之间执行分类(尽管我们可以使用各种元算法将其扩展到更多的类)，只需在两者之间画一条分隔线(或者在更高维度上画一个超平面)。直观的想法是选择最好的分隔线，而不仅仅是任何特定的线。

假设我们的两个类可以用一条线分开，这样线以上的任何点都属于一个类，线以下的任何点都属于另一个类。支持向量机找到这条线，并将其用于预测，与线性回归的工作方式非常相似。然而，支持向量机找到了分离数据集的最佳线。在下图中，我们有三条线分隔数据集:蓝色、黑色和绿色。你认为哪个是最好的选择？

![](images/B06162_09_04.png)

直觉上，一个人通常会选择蓝线作为最佳选项，因为这样可以以最干净的方式分隔数据。更正式地说，它具有从直线到每个类中任何点的最大距离。找到最大分离线是一个优化问题，其基础是找到它们之间距离最大的边距线。解决这个优化问题是 SVM 训练阶段的主要任务。

The equations to solve SVMs is outside the scope of this book, but I recommend interested readers to go through the derivations at:
[http://en.wikibooks.org/wiki/Support_Vector_Machines](http://en.wikibooks.org/wiki/Support_Vector_Machines) for the details.
Alternatively, you can visit:
[http://docs.opencv.org/doc/tutorials/ml/introduction_to_svm/introduction_to_svm.html](http://docs.opencv.org/doc/tutorials/ml/introduction_to_svm/introduction_to_svm.html)

# 用支持向量机分类

训练模型后，我们有一条最大利润线。新样本的分类就是简单地问一个问题:它是落在线的上方，还是线的下方？如果落在线上方，则预测为一类。如果低于该线，则预测为其他类。

对于多个类，我们创建多个支持向量机——每个都是一个二进制分类器。然后，我们使用各种策略中的任何一种将它们联系起来。一个基本的策略是为每个类创建一个一对一的分类器，其中我们使用两个类来训练——给定的类和所有其他的样本。我们对每个类都这样做，并在新样本上运行每个分类器，从每个样本中选择最佳匹配。在大多数 SVM 实现中，这个过程是自动执行的。

我们在之前的代码中看到了两个参数: **C** 和内核。我们将在下一节讨论内核参数，但是 **C** 参数是拟合支持向量机的重要参数。 **C** 参数涉及分类器应该在多大程度上以正确预测所有训练样本为目标，存在过度拟合的风险。选择较高的 **C** 值会找到一条间隔较小的分隔线，旨在正确分类所有训练样本。选择较低的 **C** 值将导致一条具有较大余量的分隔线——即使这意味着一些训练样本分类不正确。在这种情况下，较低的 **C** 值表示过拟合的机会较低，有选择通常较差的分离线的风险

支持向量机(以其基本形式)的一个限制是，它们只分离线性可分的数据。如果数据不是，会发生什么？对于这个问题，我们使用内核。

# 核心

当数据不能线性分离时，诀窍是将其嵌入到更高维的空间中。这意味着，在很多细节问题上，向数据集添加新的特征，直到数据是线性可分的。如果你添加了正确的特征，这种线性分离最终会发生。

诀窍在于，当我们找到分离数据集的最佳线时，我们通常会计算样本的内部产量。给定一个使用点积的函数，我们可以有效地制造新特性，而不必实际定义这些新特性。这就是众所周知的内核技巧，它非常方便，因为我们不知道这些特性会是什么。我们现在将内核定义为一个函数，它本身是来自数据集的两个样本的函数的点积，而不是基于样本(和组成的特征)本身。

我们现在可以计算出点积是多少(或近似)，然后用它。

有许多常用的内核。**线性核**是最直接的，它只是两个样本特征向量、权重特征和一个偏差值的点积。还有一个**多项式核**，它将点积提升到给定的程度(例如，2)。其他包括**高斯** ( **径向基函数**)和**乙状结肠**功能。在我们之前的代码示例中，我们在**线性**内核和**径向基函数**内核选项之间进行了测试。

所有这些推导的最终结果是，这些核有效地定义了用于支持向量机中新样本分类的两个样本之间的距离。理论上，可以使用任何距离，尽管它可能不具有使 SVM 训练易于优化的相同特征。

在 scikit-learn 的 SVMs 实现中，我们可以定义内核参数来更改计算中使用的内核函数，就像我们在前面的代码示例中看到的那样。

# 字符 n-克

我们看到了如何使用虚词作为特征来预测文档的作者。另一种特征类型是字符 n-gram。一个 n-gram 是一系列 *n* 标记，其中 *n* 是一个值(对于文本，通常在 2 和 6 之间)。单词 n-grams 已经在许多研究中被使用，通常与文档的主题相关——如前一章所述。然而，字符 n-grams 已被证明是高质量的作者归属。

通过将文档表示为字符序列，可以在文本文档中找到字符 n-grams。然后从这个序列中提取这些 n-grams，并训练一个模型。对此有许多不同的模型，但是一个标准的模型与我们之前使用的单词包模型非常相似。

对于训练语料库中的每个不同的 n-gram，我们为其创建一个特征。n-gram 的一个例子是`<e t>`，它是字母 e，空格，然后是字母 t(尖括号用来表示 n-gram 的开始和结束，不是 n-gram 本身的一部分)。然后，我们使用训练文档中每个 n-gram 的频率训练我们的模型，并使用创建的特征矩阵训练分类器。

Character n-grams are defined in many ways. For instance, some applications only choose within-word characters, ignoring whitespace and punctuation. Some use this information (like our implementation in this chapter) for classification. Ultimately, this is the purpose of the model, chosen by the data miner (you!).

为什么字符 n-gram 起作用的一个常见理论是，人们更典型地写下他们可以轻松说出的单词，字符 n-gram(至少当 n 在 2 到 6 之间时)是**音素**的一个很好的近似——我们说单词时发出的声音。从这个意义上说，使用字符 n-grams 近似于单词的发音，这近似于你的写作风格。这是创建新功能时的常见模式。首先，我们有一个关于什么概念会影响最终结果(作者风格)的理论，然后创建特征来近似或测量这些概念。

字符 n-gram 矩阵的一个关键特征是它是稀疏的，并且随着更高的 n 值，稀疏度增加得非常快。对于 2 的 n 值，大约 75%的特征矩阵是零。对于 5 的 n 值，93%以上是零。虽然这通常比相同类型的单词 n-gram 矩阵稀疏，但是使用基于单词的分类器应该不会引起很多问题。

# 提取字符 n-g

我们将使用我们的`CountVectorizer`类来提取字符 n 克。为此，我们设置分析器参数，并为 n 指定一个值来提取 n-grams。

scikit-learn 中的实现使用 n-gram 范围，允许您同时提取多个大小的 n-gram。在这个实验中，我们不会深入研究不同的 n 值，所以我们只是设置相同的值。要提取大小为 3 的 n 克，需要指定(3，3)作为 n 克范围的值。

我们可以重用前面代码中的网格搜索。我们所需要做的就是在新的管道中指定新的特征提取器并运行它:

```py
pipeline = Pipeline([('feature_extraction', CountVectorizer(analyzer='char', ngram_range=(3,3))),
                     ('classifier', grid) ]
scores = cross_val_score(pipeline, documents, classes, scoring='f1') 
print("Score: {:.3f}".format(np.mean(scores)))

```

There is a lot of implicit overlap between function words and character n-grams, as character sequences in function words are more likely to appear. However, the actual features are very different and character n-grams capture punctuation, a characteristic that function words do not capture. For example, a character n-gram includes the full stop at the end of a sentence, while a function word-based method would only use the preceding word itself.

# 安然数据集

安然是 20 世纪 90 年代末世界上最大的能源公司之一，报告收入超过 1000 亿美元。它有 20，000 多名工作人员，截至 2000 年，似乎没有迹象表明有什么地方出了问题。

2001 年，发生了*安然丑闻*，人们发现安然正在进行系统性的欺诈性会计行为。这种欺诈是蓄意的，范围遍及整个公司，而且金额巨大。这一点被公开发现后，其股价从 2000 年的 90 多美元跌至 2001 年的不到 1 美元。安然公司很快在一片混乱中申请破产，这需要 5 年多的时间才能最终解决。

作为安然调查的一部分，美国联邦能源监管委员会公开了 60 多万封电子邮件。从那以后，这个数据集被用于从社交网络分析到欺诈分析的所有研究。这也是一个很好的作者分析数据集，因为我们能够从个人用户发送的文件夹中提取电子邮件。这允许我们创建一个比许多以前的数据集大得多的数据集。

# 访问安然数据集

整套安然电子邮件可在 https://www.cs.cmu.edu/~./enron/获得

The full dataset is quite large, and provided in a compression format called gzip. If you don't have a Linux-based machine to decompress (unzip) this file, get an alternative program, such as 7-zip ([http://www.7-zip.org/](http://www.7-zip.org/))

下载完整的语料库并解压到你的数据文件夹中。默认情况下，这将解压到一个名为`enron_mail_20110402`的文件夹中，该文件夹包含一个名为`maildir`的文件夹。在笔记本中，设置安然数据集的数据文件夹:

```py
enron_data_folder = os.path.join(os.path.expanduser("~"), "Data", "enron_mail_20150507", "maildir")

```

# 创建数据集加载器

当我们在寻找作者信息时，我们只想要我们可以归因于特定作者的电子邮件。因此，我们将查看每个用户的“已发送”文件夹，即他们发送的电子邮件。我们现在可以创建一个函数，随机选择几个作者，并返回他们发送的文件夹中的每封电子邮件。具体来说，我们在寻找有效载荷——也就是说，内容，而不是电子邮件本身。为此，我们需要一个电子邮件解析器。代码如下:

```py
from email.parser 
import Parser p = Parser()

```

我们稍后将使用它从数据文件夹中的电子邮件文件中提取有效负载。

有了数据加载功能，我们将有很多选择。其中大多数确保了我们的数据集相对平衡。一些作者在他们发送的邮件中会有成千上万封电子邮件，而其他人只有几十封。我们使用`min_docs_author`将搜索限制在至少有 10 封电子邮件的作者，并使用`max_docs_author`参数从每个作者处获取最多 100 封电子邮件。我们还使用`num_authors`参数指定我们想要获得多少作者-默认为 10。

功能如下。其主要目的是循环浏览作者，为该作者检索大量电子邮件，并将**文档**和**类**信息存储在一些列表中。我们还存储了作者姓名和他们的数字类值之间的映射，这让我们可以在以后检索这些信息。

```py
from sklearn.utils import check_random_state

```

```py
def get_enron_corpus(num_authors=10, data_folder=enron_data_folder, min_docs_author=10,
                     max_docs_author=100, random_state=None):
    random_state = check_random_state(random_state)
    email_addresses = sorted(os.listdir(data_folder))
    # Randomly shuffle the authors. We use random_state here to get a repeatable shuffle
    random_state.shuffle(email_addresses)
    # Setup structures for storing information, including author information
    documents = []
    classes = []
    author_num = 0
    authors = {}  # Maps author numbers to author names
    for user in email_addresses:
        users_email_folder = os.path.join(data_folder, user)
        mail_folders = [os.path.join(users_email_folder, subfolder)
                        for subfolder in os.listdir(users_email_folder)
                        if "sent" in subfolder]
        try:
            authored_emails = [open(os.path.join(mail_folder, email_filename),
                                    encoding='cp1252').read()
                               for mail_folder in mail_folders
                               for email_filename in os.listdir(mail_folder)]
        except IsADirectoryError:
            continue
        if len(authored_emails) < min_docs_author:
            continue
        if len(authored_emails) > max_docs_author:
            authored_emails = authored_emails[:max_docs_author]
        # Parse emails, store the content in documents and add to the classes list
        contents = [p.parsestr(email)._payload for email in authored_emails]
        documents.extend(contents)
        classes.extend([author_num] * len(authored_emails))
        authors[user] = author_num
        author_num += 1
        if author_num >= num_authors or author_num >= len(email_addresses):
            break
     return documents, np.array(classes), authors

```

It may seem odd that we sort the e-mail addresses, only to shuffle them around. The `os.listdir` function doesn't always return the same results, so we sort it first to get some stability. We then shuffle using a random state, which means our shuffling can reproduce a past result if needed.

在这个函数之外，我们现在可以通过调用下面的函数来获取数据集。我们将在这里使用随机状态 14(在本书中始终如此)，但是您可以尝试其他值或将其设置为无，以在每次调用函数时获得随机集:

```py
documents, classes, authors = get_enron_corpus(data_folder=enron_data_folder, random_state=14)

```

如果您看一下数据集，我们还需要进行进一步的预处理。我们的电子邮件相当混乱，但最糟糕的一点(从作者分析的角度来看)是这些电子邮件包含其他作者的作品，以附件回复的形式。以下面的邮件`documents[100]`为例:

*我想参加小组讨论，但我在会议上有冲突*

*日期。请明年记住我。*

*马克海厚*

Email is a notoriously messy format. Reply quoting, for instance, is sometimes (but not always) prepended with a > character. Other times, the reply is embedded in the original message. If you are doing larger scale data mining with email, be sure to spend more time cleaning the data to get better results.

与 books 数据集一样，我们可以绘制文档长度直方图，以了解文档长度分布:

```py
document_lengths = [len(document) for document in documents]
sns.distplot(document_lengths)

```

结果显示较短的文档有很强的分组性。虽然这是真的，但也表明一些文件非常非常长。这可能会扭曲结果，特别是如果一些作者倾向于写长文档。为了弥补这一点，这项工作的一个扩展可能是在进行训练之前将文档长度标准化为前 500 个字符。

![](images/B06162_09_08.png)

# 把它们放在一起

我们可以使用现有的参数空间和我们之前实验中的现有分类器——我们所需要做的就是在我们的新数据上重新调整它。默认情况下，scikit-learn 中的培训是从头开始的——后续对`fit()`的调用将丢弃任何先前的信息。

There is a class of algorithms called *online learning* that update the training with new samples and don't restart their training each time. 

和以前一样，我们可以使用`cross_val_score`计算我们的分数并打印结果。代码如下:

```py
scores = cross_val_score(pipeline, documents, classes, scoring='f1') 

print("Score: {:.3f}".format(np.mean(scores)))

```

结果是 0.683，对于如此混乱的数据集来说，这是一个合理的结果。添加更多的数据(例如在数据集加载中增加`max_docs_author`)可以改善这些结果，通过额外的清理提高数据质量也是如此。

# 估价

基于一个数字进行评估通常不是一个好主意。在 f 分数的情况下，它通常对*技巧*更稳健，这些技巧虽然没有用，但给出了好的分数。这方面的一个例子是准确性。正如我们在上一章中所说的，垃圾邮件分类器可以预测所有的东西都是垃圾邮件，并获得超过 80%的准确率，尽管该解决方案根本没有用。因此，通常值得对结果进行更深入的研究。

首先，我们将看看混淆矩阵，就像我们在[第 8 章](07.html) *中所做的那样，用神经网络击败验证码*。在此之前，我们需要预测一个测试集。前面的代码使用了`cross_val_score`，实际上并没有给我们一个可以使用的训练好的模型。所以，我们需要改装一个。为此，我们需要训练和测试子集:

```py
from sklearn.cross_validation import train_test_split training_documents, 

testing_documents, y_train, y_test = train_test_split(documents, classes, random_state=14)

```

接下来，我们将管道与我们的培训文档相匹配，并为测试集创建预测:

```py
pipeline.fit(training_documents, y_train) 
y_pred = pipeline.predict(testing_documents)

```

在这一点上，你可能想知道什么是参数的最佳组合。我们可以很容易地从我们的网格搜索对象(这是我们管道的分类器步骤)中提取出来:

```py
print(pipeline.named_steps['classifier'].best_params_)

```

结果给出了分类器的所有参数。然而，大多数参数都是我们没有触及的默认值。我们搜索的是 C 和内核，分别设置为 1 和线性。

现在我们可以创建一个混淆矩阵:

```py
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_pred, y_test)
cm = cm / cm.astype(np.float).sum(axis=1)

```

接下来，我们得到作者的名字，让我们可以正确地标记轴。为此，我们使用安然数据集加载的作者词典。代码如下:

```py
sorted_authors = sorted(authors.keys(), key=lambda x:authors[x])

```

最后，我们用 matplotlib 展示了混淆矩阵。与上一章相比，唯一的变化突出显示在下面；只需用本章实验的作者替换字母标签:

```py
%matplotlib inline 
from matplotlib import pyplot as plt 
plt.figure(figsize=(10,10))
plt.imshow(cm, cmap='Blues', interpolation='nearest')
tick_marks = np.arange(len(sorted_authors))
plt.xticks(tick_marks, sorted_authors) 
plt.yticks(tick_marks, sorted_authors) 
plt.ylabel('Actual') 
plt.xlabel('Predicted') 
plt.show()

```

结果如下图所示:

![](images/B06162_09_05.png)

我们可以看到作者在大多数情况下都是被正确预测的——有一条清晰的高值对角线。尽管有一些大的误差来源(较暗的值更大):例如，来自用户 rapp-b 的电子邮件通常被预测为来自 reitmeyer-j。

# 摘要

在这一章中，我们研究了基于文本挖掘的作者归属问题。为此，我们分析了两种类型的特征:虚词和字符 n-gram。对于虚词，我们能够使用单词包模型——仅仅限于我们预先选择的一组单词。这给了我们只有这些词的频率。对于字符 n-gram，我们使用了非常相似的工作流，使用了相同的类。然而，我们改变了分析器，看字符而不是单词。此外，我们使用了 n-grams，它是一行中 n 个标记的序列，在我们的例子中是字符。单词 n-grams 在某些应用程序中也值得测试，因为它们可以提供一种廉价的方式来获得单词使用的上下文。

对于分类，我们使用支持向量机，基于寻找最大利润的思想优化分类之间的分隔线。线以上是一个类，线以下是另一个类。与我们考虑的其他分类任务一样，我们有一组样本(在本例中是我们的文档)。

然后我们使用了一个非常混乱的数据集，安然的电子邮件。这个数据集包含许多工件和其他问题。这导致了比 books 数据集更低的精度，后者要干净得多。然而，我们有一半以上的时间能够从 10 个可能的作者中选择正确的作者。

为了进一步理解本章中的概念，请寻找包含作者信息的新数据集。例如，你能预测一篇博文的作者吗？一条推文的作者呢(你可能可以使用朴素贝叶斯重用你在[第六章](05.html)、*社交媒体洞察中的数据)？*

在下一章中，我们考虑如果我们没有目标类，我们能做什么。这叫做无监督学习，是一个探索性的问题，而不是预测性的问题。我们还继续处理杂乱的基于文本的数据集。