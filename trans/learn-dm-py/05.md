# 特色和科技-学习变形金刚

我们到目前为止使用的数据集已经按照*特征*进行了描述。在前一章中，我们使用了一个以事务为中心的数据集。然而，最终这只是表示基于特征的数据的不同格式。

还有许多其他类型的数据集，包括文本、图像、声音、电影，甚至真实对象。大多数数据挖掘算法依赖于具有数字或分类特征。这意味着在我们将这些类型输入数据挖掘算法之前，我们需要一种方法来表示它们。我们称这种表示为**模型**。

在这一章中，我们将讨论如何提取数字特征和分类特征，并在有特征时选择最佳特征。我们将讨论提取特征的一些常见模式和技术。选择合适的模型对数据挖掘练习的结果至关重要，比选择分类算法更重要。

本章介绍的关键概念包括:

*   从数据集提取要素
*   为您的数据创建模型
*   创建新功能
*   选择好的功能
*   为自定义数据集创建自己的转换器

# 特征抽出

提取特征是数据挖掘中最关键的任务之一，它通常比数据挖掘算法的选择更影响最终结果。不幸的是，没有硬性的规则来选择能够产生高性能数据挖掘的功能。要素的选择决定了您用来表示数据的模型。

Model creation is where the science of data mining becomes more of an art and why automated methods of performing data mining (there are several methods of this type) focus on algorithm choice and not model creation. Creating good models relies on intuition, domain expertise, data mining experience, trial and error, and sometimes a little luck.

# 用模型表现现实

鉴于我们到目前为止在书中所做的工作，我们很容易忘记，我们执行数据挖掘的原因是为了影响现实世界的对象，而不仅仅是操纵一个值矩阵。并非所有数据集都以特征的形式呈现。有时候，一个数据集只不过是由给定作者写的所有书组成的。有时，它是 1979 年上映的每部电影的电影。在其他时候，它是图书馆收藏的有趣的历史文物。

从这些数据集中，我们可能想要执行数据挖掘任务。对于书籍，我们可能想知道作者写的不同类别。在电影中，我们可能希望看到女性是如何被描绘的。在历史文物中，我们可能想知道它们是来自一个国家还是另一个国家。不可能只将这些原始数据集传递到决策树中，然后看看结果是什么。

为了让数据挖掘算法在这里帮助我们，我们需要将这些表示为**特征**。特征是创建模型的一种方式，模型以数据挖掘算法能够理解的方式提供对现实的近似。因此，模型只是现实世界某个方面的简化版本。举个例子，国际象棋是历史战争的简化模型(以游戏形式)。

Selecting features has another advantage: they reduce the complexity of the real world into a more manageable model.

想象一下，向一个没有项目背景知识的人正确、准确、完整地描述一个真实世界的对象需要多少信息。你需要描述尺寸、重量、质地、成分、年龄、瑕疵、用途、来源等等。

由于真实对象的复杂性对于当前的算法来说太多了，我们使用这些更简单的模型来代替。

这种简化也集中了我们在数据挖掘应用程序中的意图。在后面的章节中，我们将研究集群以及它在哪里是至关重要的。如果你放入随机特征，你会得到随机结果。

然而，这种简化也有缺点，因为它减少了细节，或者可能会删除我们希望对其执行数据挖掘的事物的良好指标。

应该始终考虑如何以模型的形式表现现实。您需要考虑数据挖掘练习的目标，而不是仅仅使用过去使用过的东西。你想达到什么目标？在[第 3 章](02.html)、*用决策树*预测体育赢家中，我们通过思考目标(预测赢家)来创建特征，并使用一点领域知识来提出新特征的想法。

Not all features need to be numeric or categorical. Algorithms have been developed that work directly on text, graphs, and other data structures. Unfortunately, those algorithms are outside the scope of this book. In this book, and normally in your data mining career, we mainly use numeric or categorical features.

*成人*数据集是一个很好的例子，它采用了一个复杂的现实，并试图使用特征对其建模。在这个数据集中，目标是估计某人年收入是否超过 5 万美元。

To download the dataset, navigate to [http://archive.ics.uci.edu/ml/datasets/Adult](http://archive.ics.uci.edu/ml/datasets/Adult) and click on the Data Folder link. Download the `adult.data` and `adult.names` into a directory named Adult in your data folder.

该数据集承担着复杂的任务，并在要素中进行描述。这些特征描述了这个人，他们的环境，他们的背景，以及他们的生活状态。

为本章打开一个新的 Jupyter 笔记本，设置数据文件名，并用熊猫加载数据:

```
import os
import pandas as pd
data_folder = os.path.join(os.path.expanduser("~"), "Data", "Adult")
adult_filename = os.path.join(data_folder, "adult.data")

```

```
adult = pd.read_csv(adult_filename, header=None, names=["Age", "Work-Class", "fnlwgt", 
                     "Education", "Education-Num", "Marital-Status", "Occupation",
                     "Relationship", "Race", "Sex", "Capital-gain", "Capital-loss",
                     "Hours-per-week", "Native-Country", "Earnings-Raw"])

```

大部分代码与前几章相同。

Don't want to type those heading names? Don't forget you can download the code from Packt Publishing, or alternatively from the author's GitHub repository for this book:
[https://github.com/dataPipelineAU/LearningDataMiningWithPython2](https://github.com/dataPipelineAU/LearningDataMiningWithPython2)

成人文件本身在文件末尾包含两行空白。默认情况下，pandas 会将倒数第二个新行解释为空(但有效)行。为了消除这种情况，我们移除任何带有无效数字的行(使用`inplace`只是确保相同的数据帧受到影响，而不是创建一个新的数据帧):

```
adult.dropna(how='all', inplace=True)

```

看看数据集，我们可以从`adult.columns`看到各种各样的特征:

```
adult.columns

```

结果显示了熊猫索引对象中存储的每个要素名称:

```
Index(['Age', 'Work-Class', 'fnlwgt', 'Education', 
'Education-Num', 'Marital-Status', 'Occupation', 'Relationship', 
'Race', 'Sex', 'Capital-gain', 'Capital-loss', 'Hours-per-week', 
'Native-Country', 'Earnings-Raw'], dtype='object')

```

# 常见特征模式

虽然有数以百万计的方法来创建模型，但是在不同的学科中有一些通用的模式。然而，选择合适的特性是很棘手的，值得考虑一个特性如何与最终结果相关联。正如一句众所周知的格言所说，*不要以封面来判断一本书*——如果你对书中包含的信息感兴趣，那么考虑一本书的大小可能是不值得的。

一些常用的特征集中在被研究的真实世界物体的物理属性上，例如:

*   空间属性，如对象的长度、宽度和高度
*   物体的重量和/或密度
*   对象或其组件的年龄
*   对象的类型
*   物体的质量

其他功能可能取决于对象的用法或历史记录:

*   对象的制作者、发行者或创建者
*   制造之年

其他功能根据数据集的组件来描述数据集:

*   给定子组件的频率，如书中的单词
*   子组件的数量和/或不同子组件的数量
*   子组件的平均大小，例如平均句子长度

序数特性允许我们对相似的值进行排序、排序和分组。正如我们在前面几章中看到的，特征可以是数字的，也可以是分类的。

数字特征通常被描述为序数。例如，爱丽丝、鲍勃和查理三个人的身高可能分别为 1.5 米、1.6 米和 1.7 米。我们会说爱丽丝和鲍勃的身高比爱丽丝和查理更相似。

我们在上一节中加载的成人数据集包含连续、有序特征的示例。例如，每周小时数功能跟踪人们每周工作的小时数。在这样的特性上，某些操作是有意义的。它们包括计算平均值、标准偏差、最小值和最大值。熊猫有一个功能，可以给出一些这种类型的基本统计数据:

```
adult["Hours-per-week"].describe()

```

结果告诉了我们一些关于这个特性的信息:

```
count 32561.000000
mean 40.437456
std 12.347429
min 1.000000
25% 40.000000
50% 40.000000
75% 45.000000
max 99.000000
dtype: float64

```

其中一些操作对其他功能没有意义。例如，计算这些人教育状况的总和是没有意义的。相比之下，计算每个顾客在网上商店的订单总数是有意义的。

还有一些特征不是数字的，但仍然是序数的。成人数据集中的教育功能就是一个例子。例如，学士学位是比完成高中更高的教育地位，这是比没有完成高中更高的地位。计算这些值的平均值不太有意义，但是我们可以通过取中值来创建一个近似值。数据集给出了一个有用的特征，`Education-Num`，它分配了一个基本上等于完成教育年限的数字。这使我们能够快速计算出中位数:

```
adult["Education-Num"].median()

```

结果是 10，也就是高中毕业一年。如果我们没有这个，我们可以通过对教育价值进行排序来计算中位数。

特征也可以是分类的。例如，球可以是网球、板球、足球或任何其他类型的球。分类特征也称为名义特征。对于标称特征，这些值要么相同，要么不同。虽然我们可以根据大小或重量对球进行排名，但仅仅是类别还不足以对事物进行比较。网球不是板球，也不是足球。我们可以说网球和板球更相似(比如大小)，但仅仅是类别并不能区分这一点——它们是相同的，或者不是。

我们可以使用一次性编码将分类特征转换为数字特征，正如我们在[第 3 章](02.html)、*用决策树预测体育赢家*中看到的那样。对于前面提到的球的类别，我们可以创建三个新的二元特征:是网球，是板球，是足球。这个过程就是我们在[第三章](02.html)、*用决策树*预测体育赢家时使用的一个热门编码。对于网球来说，矢量应该是`[1, 0, 0]`。板球有价值`[0, 1, 0]`，而足球有价值`[0, 0, 1]`。这些是二进制特征，但是可以被许多算法用作连续特征。这样做的一个关键原因是它很容易允许直接的数字比较(例如计算样本之间的距离)。

成人数据集包含几个分类特征，工作类就是一个例子。虽然我们可以说有些价值观比其他价值观更有地位(例如，一个有工作的人可能比一个没有工作的人有更好的收入)，但这并不是对所有价值观都有意义。例如，一个为州政府工作的人不太可能比在私营部门工作的人有更高的收入。

我们可以使用`unique()`函数查看数据集中该要素的唯一值:

```
adult["Work-Class"].unique()

```

结果显示此列中的唯一值:

```
array([' State-gov', ' Self-emp-not-inc', ' Private', ' Federal-gov',
' Local-gov', ' ?', ' Self-emp-inc', ' Without-pay',
' Never-worked', nan], dtype=object)

```

前面的数据中有一些丢失的值，但是它们不会影响我们在这个例子中的计算。您也可以使用`adult.value_counts()`功能查看每个值出现的频率。

使用新数据集的另一个非常有用的步骤是可视化它。下面的代码将创建一个群体图，给出教育和工作时间与最终分类(通过颜色识别)的关系:

```
%matplotlib inline
import seaborn as sns
from matplotlib import pyplot as plt
sns.swarmplot(x="Education-Num", y="Hours-per-week", hue="Earnings-Raw", data=adult[::50])

```

![](images/B06162_05_03.png)

在上面的代码中，我们使用`adult[::50]`数据集索引对数据集进行采样，以显示每 50 行。将此设置为仅`adult`将导致显示所有样本，但这也可能使图表难以阅读。

同样，我们可以通过称为**离散化**的过程将数值特征转换为分类特征，正如我们在[第 1 章](04.html)、*数据挖掘入门*中看到的那样。我们可以称任何身高超过 1.7 米的人为高个子，任何身高低于 1.7 米的人为矮个子。这给了我们一个分类特征(尽管仍然是序数特征)。我们确实丢失了一些数据。例如，两个人，一个身高 1.69 米，一个身高 1.71 米，将被分为两个不同的类别，并被我们的算法认为彼此有很大的不同。相比之下，身高 1.2 米的人会被认为与身高 1.69 米的人大致相同！这种细节的丢失是离散化的副作用，也是我们在创建模型时要处理的问题。

在成人数据集中，我们可以创建一个`LongHours`特征，它告诉我们一个人每周工作时间是否超过 40 小时。这将我们的连续特征(`Hours-per-week`)转化为分类特征，如果小时数超过 40，则为真，否则为假:

`adult["LongHours"] = adult["Hours-per-week"] > 40`

# 创建好的功能

建模带来的简化是我们没有可以简单应用于任何数据集的数据挖掘方法的关键原因。一个好的数据挖掘从业者将需要或获得他们正在应用数据挖掘的领域的领域知识。他们将研究问题和可用的数据，并提出一个模型来代表他们试图实现的目标。

例如，一个人的身高特征可能描述了一个人的一个组成部分，比如他们打篮球的能力，但可能不能很好地描述他们的学习成绩。如果我们试图预测一个人的成绩，我们可能不会费心去测量每个人的身高。

这就是数据挖掘变得更像艺术而不是科学的地方。提取好的特征是困难的，并且是有意义的和正在进行的研究的主题。选择更好的分类算法可以提高数据挖掘应用程序的性能，但是选择更好的特征通常是更好的选择。

In all data mining applications, you should first outline what you are looking for before you start designing the methodology that will find it. This will dictate the types of features you are aiming for, the types of algorithms that you can use, and the expectations in the final result.

# 特征选择

在初始建模之后，我们通常会有大量的特征可供选择，但我们希望只选择一小部分。这可能有许多原因:

*   **降低复杂度**:当特征数量增加时，很多数据挖掘算法需要明显更多的时间和资源。减少特征的数量是让算法运行得更快或使用更少资源的好方法。
*   **降低噪音**:增加额外的功能并不总能带来更好的性能。额外的特征可能会混淆算法，在没有任何实际意义的训练数据中找到相关性和模式。这在较小和较大的数据集中都很常见。只选择合适的特征是减少没有实际意义的随机相关性的好方法。
*   **创建可读模型**:虽然许多数据挖掘算法会很乐意为具有数千个特征的模型计算出答案，但结果对于人类来说可能很难解释。在这些情况下，使用更少的特征并创建一个人类可以理解的模型可能是值得的。

一些分类算法可以处理像前面描述的问题那样的数据。获得正确的数据并获得有效描述您正在建模的数据集的特征仍然可以帮助算法。

我们可以执行一些基本测试，比如确保特性至少是不同的。如果一个特征的值都相同，它就不能给我们额外的信息来执行我们的数据挖掘。

例如，`scikit-learn`中的变量阈值转换器将删除值中至少没有最小方差的任何特征。为了展示这是如何工作的，我们首先使用 NumPy 创建一个简单的矩阵:

```
import numpy as np
X = np.arange(30).reshape((10, 3))

```

结果是数字 0 到 29，分为三列 10 行。这表示一个包含 10 个样本和三个特征的合成数据集:

```
array([[ 0, 1, 2],
[ 3, 4, 5],
[ 6, 7, 8],
[ 9, 10, 11],
[12, 13, 14],
[15, 16, 17],
[18, 19, 20],
[21, 22, 23],
[24, 25, 26],
[27, 28, 29]])

```

然后，我们将整个第二列/特征设置为值 1:

```
X[:,1] = 1

```

结果在第一行和第三行有很多差异，但在第二行没有差异:

```
array([[ 0, 1, 2],
[ 3, 1, 5],
[ 6, 1, 8],
[ 9, 1, 11],
[12, 1, 14],
[15, 1, 17],
[18, 1, 20],
[21, 1, 23],
[24, 1, 26],
[27, 1, 29]])

```

我们现在可以创建一个`VarianceThreshold`转换器，并将其应用到我们的数据集:

```
from sklearn.feature_selection import VarianceThreshold
vt = VarianceThreshold()
Xt = vt.fit_transform(X)

```

现在，结果`Xt`没有第二列:

```
array([[ 0, 2],
[ 3, 5],
[ 6, 8],
[ 9, 11],
[12, 14],
[15, 17],
[18, 20],
[21, 23],
[24, 26],
[27, 29]])

```

我们可以通过打印`vt.variances_ attribute:`来观察每一列的差异

```
print(vt.variances_)

```

结果表明，虽然第一列和第三列至少包含一些信息，但第二列没有变化:

```
array([ 74.25, 0\. , 74.25])

```

当第一次看到数据时，像这样简单而明显的测试总是很好运行的。无差异的功能不会给数据挖掘应用程序增加任何价值；然而，它们会降低算法的性能并降低功效。

# 选择最佳的个人特征

如果我们有许多特征，找到最佳子集的问题是一项困难的任务。它涉及到多次解决数据挖掘问题本身。正如我们在[第 4 章](03.html)、*推荐 Movi* *es 使用亲和分析*中看到的，基于子集的任务随着特征数量的增加而呈指数级增长。对于寻找特征的最佳子集来说，这种所需时间的指数增长也是真实的。

解决这个问题的一个基本方法是不要寻找一个能很好地协同工作的子集，而不要只寻找最佳的单个特征。这种单变量特征选择根据特征本身的表现给我们打分。这通常用于分类任务，我们通常测量变量和目标类之间的某种关联。

scikit-learn 包有许多用于执行单变量特征选择的转换器。它们包括返回 k 个性能最好的特征的 SelectKBest 和返回前 R%特征的 SelectPercentile。在这两种情况下，都有许多计算特征质量的方法。

有许多不同的方法来计算单个特征与类值的关联效率。常用的方法是卡方( *χ2* )检验。其他方法包括互信息和熵。

我们可以使用我们的成人数据集来观察正在进行的单特征测试。首先，我们从熊猫数据帧中提取数据集和类值。我们可以选择以下功能:

```
X = adult[["Age", "Education-Num", "Capital-gain", "Capital-loss", "Hours-per-week"]].values

```

我们还将通过测试收入原始值是否超过 50，000 美元来创建一个目标类数组。如果是，该类将为真。否则，它将为假。让我们看看代码:

```
y = (adult["Earnings-Raw"] == ' >50K').values

```

接下来，我们使用 chi2 函数和 SelectKBest 转换器来创建我们的转换器:

```
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
transformer = SelectKBest(score_func=chi2, k=3)

```

运行`fit_transform`将调用 fit，然后用相同的数据集进行转换。
结果将创建一个新的数据集，只选择最佳的三个要素。
我们来看看代码:

```
Xt_chi2 = transformer.fit_transform(X, y)

```

生成的矩阵现在只包含三个特征。我们还可以得到每一列的分数
，让我们可以找出使用了哪些特征。我们来看看
的代码:

```
print(transformer.scores_)

```

打印结果给出了这些分数:

```
[ 8.60061182e+03 2.40142178e+03 8.21924671e+07 1.37214589e+066.47640900e+03]

```

第一、第三和第四列的最高值与年龄、资本收益和资本损失特征相关。基于单变量特征选择，这些是最佳选择特征。

If you'd like to find out more about the features in the Adult dataset, take a look at the adult.names file that comes with the dataset and the academic paper it references.

我们也可以实现其他相关性，如皮尔逊相关系数。这是在 SciPy 中实现的，SciPy 是一个用于科学计算的库(scikit-learn 将其用作基础)。

If scikit-learn is working on your computer, so is SciPy. You do not need to install anything further to get this sample working.

首先，我们从 SciPy 导入`pearsonr`函数:

```
from scipy.stats import pearsonr

```

前面的函数几乎适合 scikit-learn 的单变量变压器中需要使用的接口。该函数需要接受两个数组(在我们的例子中是 x 和 y)作为参数，并返回两个数组、每个特征的分数和相应的 p 值。我们之前使用的 chi2 函数只使用了所需的接口，这允许我们直接将其传递给 SelectKBest。

SciPy 中的 pearsonr 函数接受两个数组；然而，它接受的 X 数组只是一维的。我们将编写一个包装函数，允许我们将它用于多元数组，就像我们所拥有的一样。让我们看看代码:

```
def multivariate_pearsonr(X, y):
    scores, pvalues = [], []
    for column in range(X.shape[1]):
        # Compute the Pearson correlation for this column only
        cur_score, cur_p = pearsonr(X[:,column], y)
        # Record both the score and p-value.
        scores.append(abs(cur_score))
        pvalues.append(cur_p)
    return (np.array(scores), np.array(pvalues))

```

The Pearson value could be between -1 and 1\. A value of 1 implies a perfect correlation between two variables, while a value of -1 implies a perfect negative correlation, that is, high values in one variable give low values in the other and vice versa. Such features are really useful to have. For this reason, we have stored the absolute value in the scores array, rather than the original, signed value.

现在，我们可以像以前一样使用 transformer 类，使用 Pearson 相关系数对特征进行排序:

```
transformer = SelectKBest(score_func=multivariate_pearsonr, k=3)
Xt_pearson = transformer.fit_transform(X, y)
print(transformer.scores_)

```

这将返回一组不同的功能！以这种方式选择的特征是第一、第二和第五列:年龄、教育和每周工作时数。这表明对于什么是最好的特性没有一个确定的答案——它取决于所使用的度量和所采用的过程。

通过分类器运行它们，我们可以看到哪个特征集更好。请记住，结果仅指示哪个子集更适合特定的分类器和/或特征组合——在数据挖掘中，很少有一种方法在所有情况下都严格优于另一种方法的情况！让我们看看代码:

```
from sklearn.tree import DecisionTreeClassifier
from sklearn.cross_validation import cross_val_score
clf = DecisionTreeClassifier(random_state=14)
scores_chi2 = cross_val_score(clf, Xt_chi2, y, scoring='accuracy')
scores_pearson = cross_val_score(clf, Xt_pearson, y, scoring='accuracy')

print("Chi2 score: {:.3f}".format(scores_chi2.mean()))
print("Pearson score: {:.3f}".format(scores_pearson.mean()))

```

这里的 chi2 平均分是 0.83，而 Pearson 评分更低，为 0.77。对于这个组合，chi2 回报更好！

值得记住的是这一特定数据挖掘活动的目标:预测财富。将好的特征和特征选择结合起来，我们只需要使用一个人的三个特征就可以达到 83%的准确率！

# 特征创建

有时候，仅仅从我们拥有的东西中选择特征是不够的。我们可以用不同于现有功能的方式创建功能。我们之前看到的一次性编码方法就是一个例子。我们将创建三个新特性*而不是具有选项 A、B 和 C 的类别特性，是 A 吗？*、*是 B 吗？*、*是 C 吗？*。

创建新的要素似乎没有必要，也没有明显的好处——毕竟，信息已经在数据集中，我们只需要使用它。然而，当特征显著相关时，或者如果存在冗余特征时，一些算法会陷入困境。如果有多余的功能，他们也可能会很纠结。因此，有各种方法可以从我们已经拥有的特性中创建新的特性。

我们将加载一个新的数据集，所以现在是开始一个新的 Jupyter 笔记本的好时机。从[下载广告数据集，并保存到您的数据文件夹中。](http://archive.ics.uci.edu/ml/datasets/Internet+Advertisements)

接下来，我们需要加载带有熊猫的数据集。首先，我们将数据的文件名设置为始终:

```
import os
import numpy as np
import pandas as pd
data_folder = os.path.join(os.path.expanduser("~"), "Data")
data_filename = os.path.join(data_folder, "Ads", "ad.data")

```

这个数据集有几个问题使我们无法轻松加载它。通过尝试用`pd.read_csv`加载数据集，可以看到这些问题。首先，前几个特征是数字的，但是熊猫会把它们作为字符串加载。为了解决这个问题，我们需要编写一个转换函数，如果可能的话，将字符串转换成数字。否则，我们将得到一个**而不是一个数字**(NaN)——一个无效的值，这是一个特殊的值，表明该值不能被解释为一个数字。它类似于其他编程语言中的 none 或 null。

此数据集的另一个问题是缺少一些值。这些在数据集中用字符串表示？。幸运的是，问号不会转换成浮点数，所以我们可以使用相同的概念将它们转换成 NaNs。在接下来的章节中，我们将研究其他方法来处理像这样的缺失值。

我们将创建一个函数来为我们进行这种转换。它尝试将数字转换为浮点数，如果失败，它将返回 NumPy 的特殊 NaN 值，该值可以存储在浮点数的位置:

```
def convert_number(x):
    try:
        return float(x)
    except ValueError:
        return np.nan

```

现在，我们为转换创建一个字典。我们希望将所有功能转换为浮点型:

```
converters = {}
for i in range(1558):
    converters[i] = convert_number

```

此外，我们希望将最后一列，即类(列索引#1558)设置为二进制特征。在成人数据集中，我们为此创建了一个新功能。在数据集中，我们将在加载时转换要素:

```
converters[1558] = lambda x: 1 if x.strip() == "ad." else 0

```

现在我们可以使用`read_csv`加载数据集。我们使用转换器参数将自定义转换传递给熊猫:

```
ads = pd.read_csv(data_filename, header=None, converters=converters)

```

生成的数据集非常大，有 1，559 个要素和 3，000 多行。以下是通过在新单元格中插入`ads.head()`打印的一些特征值，前五个:

![](images/B06162_05_01.png)

该数据集描述网站上的图像，目的是确定给定的图像是否是广告。

该数据集中的要素没有通过标题进行很好的描述。ad.data 文件附带的两个文件有更多信息:`ad.DOCUMENTATION`和`ad.names`。前三个特征是图像尺寸的高度、宽度和比率。如果是广告，最后一个特征是 1，如果不是广告，最后一个特征是 0。

其他功能是 1 表示在 URL、替代文本或图像标题中存在某些单词。这些词，如单词赞助商，用于确定图像是否可能是广告。许多功能有相当大的重叠，因为它们是其他功能的组合。因此，这个数据集有很多冗余信息。

随着我们的数据集加载到`pandas`中，我们现在将为我们的分类算法提取`x`和`y`数据。`x`矩阵将是我们数据框中的所有列，除了最后一列。相比之下，`y`数组将只是最后一列，但在此之前，我们通过删除任何带有 NaN 值的行来简化数据集(只是为了本章的目的)。让我们看看代码:

```
ads.dropna(inplace=True)
X = ads.drop(1558, axis=1).values
y = ads[1558]

```

由于此命令，删除了 1000 多行，这对于我们的练习来说很好。对于现实世界的应用程序，如果可以的话，您不想丢弃数据——相反，您可以使用插值或值替换来填充 NaN 值。例如，您可以用该列的平均值替换任何缺失的值。

# 主成分分析

在某些数据集中，要素之间有很强的相关性。例如，速度和油耗在单挡卡丁车中密切相关。虽然在某些应用中找到这些相关性可能很有用，但数据挖掘算法通常不需要冗余信息。

ads 数据集具有高度相关的特征，因为许多关键词在替代文本和标题中重复出现。

主成分分析(PCA)算法旨在寻找在较少信息下描述数据集的特征组合。它旨在发现*主成分*，它们是彼此不相关的特征，并解释数据集的信息——特别是方差。这意味着我们通常可以用更少的要素获取数据集中的大部分信息。

我们应用主成分分析就像任何其他变压器。它有一个关键参数，即要查找的组件数量。默认情况下，它会生成与原始数据集中一样多的要素。但是，这些主成分是有排名的——第一个特征解释了数据集中最大的方差，第二个特征解释了较小的方差，依此类推。因此，仅找到前几个要素通常就足以解释大部分数据集。让我们看看代码:

```
from sklearn.decomposition import PCA
pca = PCA(n_components=5)
Xd = pca.fit_transform(X)

```

最终的矩阵 Xd 只有五个特征。但是，让我们看看这些特性所解释的差异量:

```
np.set_printoptions(precision=3, suppress=True)
pca.explained_variance_ratio_

```

结果`array([ 0.854, 0.145, 0.001, 0\. , 0\. ])`显示，第一个特征占数据集方差的 85.4%，第二个特征占 14.5%，依此类推。到了第四个特征，特征中包含的方差不到百分之一。其他 1553 个特性解释得更少(这是一个有序数组)。

用主成分分析转换数据的缺点是，这些特征通常是其他特征的复杂组合。例如，前面代码的第一个特征以`[-0.092, -0.995, -0.024],`开始，即原始数据集中的第一个特征乘以-0.092，第二个乘以-0.995，第三个乘以-0.024。此功能有 1，558 个这种形式的值，每个原始数据集一个值(尽管许多值为零)。人类无法区分这些特征，如果没有丰富的使用经验，很难从中收集到很多相关信息。

使用主成分分析可以生成不仅近似原始数据集的模型，还可以提高分类任务的性能:

```
clf = DecisionTreeClassifier(random_state=14)
scores_reduced = cross_val_score(clf, Xd, y, scoring='accuracy')

```

得到的分数是 0.9356，比我们原来模型的分数(略)高。PCA 不会总是给出这样的好处，但它往往会给出这样的好处。

We are using PCA here to reduce the number of features in our dataset. As a general rule, you shouldn't use it to reduce overfitting in your data mining experiments. The reason for this is that PCA doesn't take classes into account. A better solution is to use regularization. An introduction, with code, is available at [http://blog.datadive.net/selecting-good-features-part-ii-linear-models-and-regularization/](http://blog.datadive.net/selecting-good-features-part-ii-linear-models-and-regularization/)

另一个优势是，主成分分析允许你绘制数据集，否则你不能轻松可视化。例如，我们可以绘制 PCA 返回的前两个特征。

首先，我们告诉我们的笔记本在线显示图表:

```
%matplotlib inline
from matplotlib import pyplot as plt

```

接下来，我们得到数据集中所有不同的类(只有两个:是 ad 还是不是 ad):

```
classes = set(y)

```

我们还为每个类别分配颜色:

```
colors = ['red', 'green']

```

我们使用 zip 同时遍历两个列表，然后从该类中提取所有样本，并用适合该类的颜色绘制它们:

```
for cur_class, color in zip(classes, colors):
mask = (y == cur_class)
    plt.scatter(Xd[mask,0], Xd[mask,1], marker='o', color=color, label=int(cur_class))

```

最后，在循环之外，我们创建一个图例并显示图表，显示每个类的样本出现的位置:

```
plt.legend()
plt.show()

```

![](images/B06162_05_02.png)

# 创建自己的变压器

随着数据集的复杂性和类型的变化，您可能会发现您找不到符合您需求的现有特征提取转换器。我们将在[第 7 章](06.html)、*使用图挖掘*遵循建议中看到一个这样的例子，我们从图中创建新的特征。

变压器类似于转换功能。它将一种形式的数据作为输入，将另一种形式的数据作为输出返回。可以使用一些训练数据集来训练变压器，并且这些训练的参数可以用于转换测试数据。

变压器 API 相当简单。它将特定格式的数据作为输入，并将另一种格式(与输入相同或不同)的数据作为输出返回。对程序员没有太多要求。

# 变压器应用编程接口

变压器有两个关键功能:

*   `fit():`这将一组训练数据作为输入，并设置内部参数
*   `transform():`这执行转换本身。这可以采用训练数据集，也可以采用相同格式的新数据集

`fit()`和`transform()`函数都应该取相同的数据类型作为输入，但是`transform()`可以返回不同类型的数据而`fit()`总是返回自己。

我们将创建一个简单的转换器来展示正在运行的应用编程接口。转换器将取一个 NumPy 数组作为输入，并根据平均值将其离散化。任何高于(训练数据的)平均值的值将被赋予值 1，任何低于或等于平均值的值将被赋予值 0。

我们使用熊猫对成人数据集进行了类似的转换:我们采用了每周小时数特征，如果该值超过每周 40 小时，则创建一个长小时特征。这个变压器有两个不同的原因。首先，代码将符合 scikit-learn API，允许我们在管道中使用它。其次，代码将学习平均值，而不是将其作为一个固定值(例如 LongHours 示例中的 40)。

# 实现转换器

首先，打开我们用于成人数据集的 Jupyter 笔记本。然后，单击单元格菜单项，选择全部运行。这将重新运行所有单元格，并确保笔记本是最新的。

首先，我们导入 TransformerMixin，它为我们设置了 API。虽然 Python 没有严格的接口(与像 Java 这样的语言相反)，但是使用这样的混合允许 scikit-learn 确定该类实际上是一个转换器。我们还需要导入一个检查输入是否是有效类型的函数。我们很快就会用到它。

让我们看看代码:

```
from sklearn.base import TransformerMixin
from sklearn.utils import as_float_array

```

让我们完整地看一下我们的课程，然后我们将重温一些细节:

```
class MeanDiscrete(TransformerMixin):
    def fit(self, X, y=None):
        X = as_float_array(X)
        self.mean = X.mean(axis=0)
        return self

    def transform(self, X, y=None):
        X = as_float_array(X)
        assert X.shape[1] == self.mean.shape[0]
        return X > self.mean

```

我们的课将通过计算`X.mean(axis=0)`来学习拟合方法中每个特征的平均值，然后将其存储为对象属性。之后，fit 函数返回 self，符合 API (scikit-learn 使用它来允许链接函数调用)。

拟合后，变换函数取一个具有相同特征数的矩阵(由`assert`语句确认)，并简单地返回给定特征的大于平均值的值。

现在我们的类已经构建好了，我们现在可以创建这个类的一个实例，并使用它来转换我们的 X 数组:

```
mean_discrete = MeanDiscrete()
X_mean = mean_discrete.fit_transform(X)

```

尝试使用管道或不使用管道将这个转换器实现到工作流中。您将会看到，通过遵循 Transformer API，使用它来代替内置的 scikit-learn Transformer 对象是非常简单的。

# 单元测试

创建自己的函数和类时，做单元测试总是一个好主意。单元测试旨在测试代码的单个单元。在这种情况下，我们想要测试我们的变压器是否按照需要运行。

好的测试应该是独立可验证的。确认测试合法性的一个好方法是使用另一种计算机语言或方法来执行计算。在这种情况下，我使用 Excel 创建一个数据集，然后计算每个单元格的平均值。这些值然后被转移到单元测试。

一般来说，单元测试也应该小且运行快。因此，使用的任何数据都应该很小。我用来创建测试的数据集存储在前面的 Xt 变量中，我们将在测试中重新创建它。这两个特征的平均值分别为 13.5 和 15.5。

为了创建我们的单元测试，我们从 NumPy 的测试中导入`assert_array_equal`函数，该函数检查两个数组是否相等:

`from numpy.testing import assert_array_equal`

接下来，我们创建我们的函数。重要的是，测试的名称以 test_，
开头，因为这种命名法用于自动查找和运行测试的工具。我们还设置了测试数据:

```
def test_meandiscrete():
    X_test = np.array([[ 0, 2],
                       [ 3, 5],
                       [ 6, 8],
                       [ 9, 11],
                       [12, 14],
                       [15, 17],
                       [18, 20],
                       [21, 23],
                       [24, 26],
                       [27, 29]])
    # Create an instance of our Transformer
    mean_discrete = MeanDiscrete()
    mean_discrete.fit(X_test)
    # Check that the computed mean is correct
    assert_array_equal(mean_discrete.mean, np.array([13.5, 15.5]))
    # Also test that transform works properly
    X_transformed = mean_discrete.transform(X_test)
    X_expected = np.array([[ 0, 0],
                           [ 0, 0], 
                           [ 0, 0],
                           [ 0, 0],
                           [ 0, 0],
                           [ 1, 1],
                           [ 1, 1],
                           [ 1, 1],
                           [ 1, 1],
                           [ 1, 1]])
    assert_array_equal(X_transformed, X_expected)

```

我们可以通过简单地运行函数本身来运行测试:

```
test_meandiscrete()

```

如果没有错误，那么测试运行没有问题！您可以通过更改一些测试以故意使值不正确，并确认测试失败来验证这一点。记得把它们改回来，这样考试就通过了！

如果我们有多个测试，使用一个测试框架是值得的，比如 py.test 或者 nose 来运行我们的测试。使用这样的框架超出了本书的范围，但是作为程序员，它们管理运行测试、记录失败，并向您提供反馈，以帮助您改进代码。

# 把它们放在一起

现在我们有了一台经过测试的变压器，是时候将它付诸行动了。利用到目前为止所学的知识，我们创建了一个管道，将第一步设置为均值离散转换器，第二步设置为决策树分类器。然后，我们运行交叉验证并打印出结果。让我们看看代码:

```
from sklearn.pipeline import Pipeline
pipeline = Pipeline([('mean_discrete', MeanDiscrete()), ('classifier', DecisionTreeClassifier(random_state=14))])
scores_mean_discrete = cross_val_score(pipeline, X, y, scoring='accuracy')
print("Mean Discrete performance: {0:.3f}".format(scores_mean_discrete.mean()))

```

结果是 0.917，没有以前那么好了，但是对于一个简单的二值特征模型来说非常好。

# 摘要

在本章中，我们研究了特性和转换器，以及它们如何在数据挖掘管道中使用。我们讨论了什么是好的特征，以及如何从标准集中通过算法选择好的特征。然而，创造好的功能更多的是艺术而不是科学，通常需要领域知识和经验。

然后，我们使用一个接口创建了自己的转换器，该接口允许我们在 scikit-learn 的助手函数中使用它。我们将在后面的章节中创建更多的转换器，这样我们就可以使用现有的功能进行有效的测试。

为了进一步学习本章中的经验教训，我建议注册在线数据挖掘竞赛网站[Kaggle.com](http://Kaggle.com)，并尝试一些竞赛。他们推荐的起点是泰坦尼克号数据集，它允许你练习本章的特征创建方面。许多特征不是数字特征，需要在应用数据挖掘算法之前将其转换为数字特征。

在下一章中，我们将对文本文档语料库进行特征提取。文本有许多变形器和特征类型，各有优缺点。