# 基于深度神经网络的图像目标检测

我们在[第 8 章](07.html) *中使用了基本的神经网络，用神经网络击败了验证码*。神经网络的研究正在许多领域创造一些最先进和最精确的分类算法。本章介绍的概念与[第 8 章](07.html) *介绍的概念之间的区别在于*的复杂性*。在这一章中，我们将研究深度神经网络，即具有许多隐藏层的网络，以及用于处理特定类型信息(如图像)的更复杂的层类型。*

这些进步来自于计算能力的提高，使我们能够训练更大更复杂的网络。然而，这些进步不仅仅是在这个问题上投入更多的计算能力。除了计算能力之外，新的算法和图层类型极大地提高了性能。代价是这些新的分类器需要比其他数据挖掘分类器更多的数据来学习。

在这一章中，我们将研究确定图像中代表什么对象。像素值将被用作输入，然后神经网络将自动找到有用的像素组合，以形成更高级别的特征。这些将用于实际分类。

总的来说，在本章中，我们将研究以下内容:

*   对图像中的对象进行分类
*   不同类型的深度神经网络
*   TensorFlow 和 Keras 库构建和训练神经网络
*   使用图形处理器提高算法速度
*   使用基于云的服务为数据挖掘增加马力

# 对象分类

计算机视觉正在成为未来技术的重要组成部分。例如，我们将在不久的将来获得自动驾驶汽车——汽车制造商计划在 2017 年发布自动驾驶车型，并且已经部分实现了自动驾驶。为了实现这一点，汽车的电脑需要能够看到它周围的情况；识别障碍物、其他交通和天气状况；然后用它来计划一次安全的旅行。

虽然我们可以很容易地检测到是否有障碍物，例如使用雷达，但我们知道那个物体是什么也很重要。如果是路上的动物，我们可以停下来，让它让开；如果是建筑，这种策略不会很奏效！

# 用例

计算机视觉用于许多场景。下面是一些应用程序非常重要的例子。

*   谷歌地图等在线地图网站使用计算机视觉的原因有很多。一个原因是自动模糊他们发现的任何人脸，以便给被拍照的人一些隐私，作为他们街景功能的一部分。
*   人脸检测在很多行业也有应用。现代相机自动检测人脸，作为提高照片质量的一种手段(用户最常希望聚焦在可见的人脸上)。人脸检测也可以用于身份识别。例如，脸书可以自动识别照片中的人，从而可以轻松标记朋友。
*   如前所述，自动驾驶汽车高度依赖计算机视觉来识别路径和避开障碍物。计算机视觉是一个关键问题，不仅在自动驾驶汽车的研究中得到解决，不仅在消费者使用中，而且在采矿和其他行业中也是如此。
*   其他行业也在使用计算机视觉，包括仓库自动检查货物的缺陷。
*   航天工业也在使用计算机视觉，帮助自动收集数据。这对有效使用航天器至关重要，因为从地球向火星上的漫游者发送信号可能需要很长时间，并且在某些时候是不可能的(例如，当两颗行星不相对时)。随着我们开始更频繁地从更远的距离处理天基飞行器，增加这些航天器的自主性是绝对必要的，计算机视觉是其中的一个关键部分。下图为美国宇航局设计使用的火星车；它大量使用计算机视觉来识别它在一个陌生、不适宜居住的星球上的环境。

![](img/B06162_11_03.jpg)

# 应用场景

在本章中，我们将构建一个系统，该系统将把图像作为输入，并对其中的对象进行预测。我们将扮演汽车视觉系统的角色，观察路上或路边的任何障碍物。图像的形式如下:

![](img/B06162_11_01.png)

该数据集来自一个名为 CIFAR-10 的流行数据集。它包含 60，000 幅 32 像素宽、32 像素高的图像，每个像素都有一个红绿蓝(RGB)值。数据集已经被分为训练和测试，尽管我们在完成训练之前不会使用测试数据集。

The CIFAR-10 dataset is available for download at [http://www.cs.toronto.edu/~kriz/cifar.html](http://www.cs.toronto.edu/~kriz/cifar.html)
Download the python version, which has already been converted to NumPy arrays.

打开一个新的 Jupyter 笔记本，我们可以看到数据是什么样子。首先，我们设置数据文件名。我们将只担心第一批开始，并在接近结束时扩展到完整的数据集大小；

```py
import os
data_folder = os.path.join(os.path.expanduser("~"), "Data", "cifar-10-batches-py") 
batch1_filename = os.path.join(data_folder, "data_batch_1")

```

接下来，我们创建一个可以读取存储在批处理中的数据的函数。批处理已经用 pickle 保存，这是一个保存对象的 python 库。通常我们只需要在文件上调用`pickle.load(file)`就可以得到对象。但是，这个数据有一个小问题:它保存在 Python 2 中，但是我们需要在 Python 3 中打开它。为了解决这个问题，我们将编码设置为`latin`(即使我们以字节模式打开它):

```py
import pickle
# Bugfix thanks to: http://stackoverflow.com/questions/11305790/pickle-incompatability-of-numpy-arrays-between-python-2-and-3 
def unpickle(filename): 
    with open(filename, 'rb') as fo: 
        return pickle.load(fo, encoding='latin1')

```

使用这个函数，我们现在可以加载批处理数据集:

```py
batch1 = unpickle(batch1_filename)

```

这个批次是一个字典，包含 NumPy 数组中的实际数据、相应的标签和文件名，以及一个说明它是哪个批次的注释(例如，这是训练批次 1/5)。

我们可以使用批次数据关键字中的索引提取图像:

```py
image_index = 100 
image = batch1['data'][image_index]

```

图像数组是一个 NumPy 数组，有 3，072 个条目，从 0 到 255。每个值都是图像中特定位置的红色、绿色或蓝色强度。

图像的格式不同于 matplotlib 通常使用的格式(用于显示图像)，因此为了显示图像，我们首先需要重塑数组并旋转矩阵。这对训练我们的神经网络来说并不重要(我们将以适合数据的方式定义我们的网络)，但是为了 matplotlib，我们确实需要转换它:

```py
image = image.reshape((32,32, 3), order='F') 
import numpy as np 
image = np.rot90(image, -1)

```

现在，我们可以使用 matplotlib 显示图像:

```py
%matplotlib inline

from matplotlib import pyplot as plt 
plt.imshow(image)

```

将显示结果图像，即一艘船:

![](img/B06162_11_02.png)

这张图像的分辨率很低，只有 32 像素宽和 32 像素高。尽管如此，大多数人还是会看着图片，看到一艘船。我们能让电脑做同样的事情吗？

您可以更改图像索引以显示不同的图像，从而感受数据集的属性。

在这一章中，我们项目的目标是建立一个分类系统，可以拍摄这样的图像，并预测其中的物体是什么。在此之前，我们将绕道学习我们将要使用的分类器:**深度神经网络**。

# 深层神经网络

我们在[第八章](07.html) *中使用的神经网络，用神经网络*击败验证码，有一些神奇的*理论*属性。例如，学习任何映射只需要一个隐藏层(尽管中间层的大小可能需要非常非常大)。由于这种理论上的完善，神经网络在 70 年代和 80 年代是一个非常活跃的研究领域。然而，有几个问题导致它们失宠，特别是与其他分类算法如支持向量机相比。这里列出了几个主要的例子:

*   主要问题之一是，运行许多神经网络所需的计算能力超过了其他算法，也超过了许多人所能获得的能力。
*   另一个问题是培训网络。虽然反向传播算法已经为人所知有一段时间了，但它在较大的网络中存在问题，在权重确定之前需要大量的训练。

Each of these issues has been addressed in recent times, leading to a resurgence in popularity of neural networks. Computational power is now much more easily available than 30 years ago, and advances in algorithms for training mean that we can now readily use that power.

# 直觉

将**深度神经网络**与我们在[第八章](07.html) *中看到的更基本的神经网络*区别开来的方面是大小。

A neural network is considered deep when it has two or more hidden layers. In practice, a deep neural network is often much larger, both in the number of nodes in each layer and also the number of layers. While some of the research of the mid -2000s focused on very large numbers of layers, smarter algorithms are reducing the actual number of layers needed.

大小是一个区别点，但新的层类型和神经网络结构有助于为特定区域创建深度神经网络。我们已经看到了由**密层**组成的前馈神经网络。这意味着我们有一系列的层，按顺序，其中一层的每个神经元连接到另一层的每个神经元。其他类型包括:

*   **卷积神经网络** ( **CNN** )进行图像分析。在这种情况下，图像的一小部分被作为单个输入，该输入被传递到汇集层以组合这些输出。这有助于解决图像的旋转和平移等问题。我们将在本章中使用这些网络。
*   **用于文本和时间序列分析的递归神经网络** ( **RNN** )。在这种情况下，神经网络的先前状态被记住并用于改变当前输出。想象句子中的前一个单词修改了短语中当前单词的输出:*美国*。最流行的类型之一是 LSTM 循环网络，代表**长短期记忆**。
*   **自动编码器**，从输入中学习映射，通过隐藏层(通常节点较少)回到输入。这找到了输入数据的压缩，这一层可以在其他神经网络中重用，减少了所需的标记训练数据量。

神经网络的类型还有很多很多。对深度神经网络的应用和理论的研究每月都在发现越来越多的神经网络形式。有些是为通用学习设计的，有些是为特定任务设计的。此外，还有多种方法可以组合图层、调整参数以及改变学习策略。例如，**脱落层**在训练过程中随机将一些权重降为零，迫使神经网络的所有部分学习好的权重。

尽管有这些差异，神经网络通常被设计成以非常基本的特征作为输入——在计算机视觉的情况下，它是简单的像素值。当这些数据被组合并通过网络推送时，这些基本特征组合成更复杂的特征。有时，这些特征对人类来说意义不大，但它们代表了计算机进行分类时所寻找的样本的各个方面。

# 实现深度神经网络

Implementing these deep neural networks can be quite challenging due to their size. A bad implementation will take significantly longer to run than a good one, and may not even run at all due to memory usage.

神经网络的一个基本实现可能是从创建一个节点类开始，并将一组节点类收集到一个层类中。然后，每个节点使用*边*类的实例连接到下一层的节点。这种基于类的实现很好地展示了网络是如何运行的，但是对于大型网络来说效率太低。神经网络只是有太多的运动部分，这种策略是有效的。

Instead, most neural networks operations can be expressed as mathematical expressions on matrices. The weights of the connections between one network layer and the next can be represented as a matrix of values, where the rows represent nodes in the first layer and the columns represent the nodes in the second layer (the transpose of this matrix is used sometimes too). The value is the weight of the edge between one layer and the next. A network can then be defined as a set of these weight matrices. In addition to the nodes, we add a bias term to each layer, which is basically a node that is always on and connected to each neuron in the next layer.

这种洞察力允许我们使用矩阵运算来构建、训练和使用神经网络，而不是创建基于类的实现。这些数学运算非常棒，因为已经编写了许多高度优化的代码库，我们可以用它们来尽可能高效地执行这些计算。

我们在[第 8 章](07.html) *中使用的 scikit-learn 实现“用神经网络击败 CAPTCHAs”*确实包含了一些构建神经网络的特性，但缺少该领域的一些最新进展。然而，对于更大、更定制的网络，我们需要一个能给我们更多能量的库。我们将使用 **Keras** 库来创建我们的深层神经网络。

在本章中，我们将首先用 Keras 实现一个基本的神经网络，然后(几乎)复制我们在[第 8 章](07.html) *中的实验，用神经网络*击败 CAPTCHAs，来预测图像中的哪个字母。最后，我们将使用复杂得多的卷积神经网络来对 CIFAR 数据集执行图像分类，这也将包括在 GPU 而不是 CPU 上运行这一过程，以提高性能。

Keras 是使用图形计算库实现深度神经网络的高级接口。图形计算库概述了一系列操作，然后计算数值。这些对于矩阵运算非常有用，因为它们可以用来表示数据流，将这些数据流分布在多个系统中，并执行其他优化。Keras 可以在引擎盖下使用两个图形计算库中的任何一个。第一个叫**antao**，稍微老一点，有很强的追随者(本书第一版就用过)，第二个叫 **TensorFlow** ，谷歌最近发布的，是为他们深度学习提供动力的图书馆。最终，您可以在本章中使用其中任何一个库。

# TensorFlow导论

TensorFlow 是一个由谷歌工程师设计的图形计算库，并开始为谷歌最近在**深度学习**和**人工智能**方面的许多进展提供动力。

图形计算库有两个步骤。它们如下所示:

1.  定义获取输入数据、对其进行操作并转换为输出的操作序列(或更复杂的图形)。
2.  在给定输入的情况下，计算从步骤 1 获得的图形。

许多程序员日常并不使用这种类型的编程，但他们中的大多数人与使用这种编程的相关系统进行交互。关系数据库，特别是基于 SQL 的数据库，使用一个类似的概念，称为声明性范例。虽然程序员可能会在数据库上用`WHERE`子句定义一个`SELECT`查询，但数据库会对其进行解释，并根据许多因素创建一个优化的查询，例如`WHERE`子句是否应用于主键、数据存储格式以及其他因素。程序员定义他们想要什么，系统决定如何去做。

You can install TensorFlow using Anaconda: conda install tensorflow
For more options, Google has a detailed installation page at [https://www.tensorflow.org/get_started/os_setup](https://www.tensorflow.org/get_started/os_setup) 

使用 TensorFlow，我们可以定义许多类型的函数来处理标量、数组和矩阵，以及其他数学表达式。例如，我们可以创建一个图表来计算给定二次方程的值:

```py
import tensorflow as tf

# Define the parameters of the equation as constant values
a = tf.constant(5.0)
b = tf.constant(4.5)
c = tf.constant(3.0)

# Define the variable x, which lets its value be changed
x = tf.Variable(0., name='x')  # Default of 0.0

# Define the output y, which is an operation on a, b, c and x
y = (a * x ** 2) + (b * x) + c

```

这个 *y* 对象是一个张量对象。它还没有一个值，因为这个值还没有计算出来。我们所做的就是创建一个图表，说明:

*当我们计算 y 时，首先取 x 值的平方，乘以 a，加上 b 乘以 x，然后加上 c 得到结果。*

图表本身可以通过TensorFlow来查看。以下是在 Jupyter 笔记本中可视化此图的一些代码，由 StackOverflow 用户 Yaroslav Bulatov 提供(参见此答案:[http://stackoverflow.com/a/38192374/307363](http://stackoverflow.com/a/38192374/307363)):

```py
from IPython.display import clear_output, Image, display, HTML

def strip_consts(graph_def, max_const_size=32):
    """Strip large constant values from graph_def."""
    strip_def = tf.GraphDef()
    for n0 in graph_def.node:
        n = strip_def.node.add() 
        n.MergeFrom(n0)
        if n.op == 'Const':
            tensor = n.attr['value'].tensor
            size = len(tensor.tensor_content)
            if size > max_const_size:
                tensor.tensor_content = "<stripped %d bytes>"%size
    return strip_def

def show_graph(graph_def, max_const_size=32):
    """Visualize TensorFlow graph."""
    if hasattr(graph_def, 'as_graph_def'):
        graph_def = graph_def.as_graph_def()
    strip_def = strip_consts(graph_def, max_const_size=max_const_size)
    code = """
        <script>
          function load() {{
            document.getElementById("{id}").pbtxt = {data};
          }}
        </script>
        <link rel="import" href="https://tensorboard.appspot.com/tf-graph-basic.build.html" onload=load()>
        <div style="height:600px">
          <tf-graph-basic id="{id}"></tf-graph-basic>
        </div>
    """.format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))

    iframe = """
        <iframe seamless style="width:1200px;height:620px;border:0" srcdoc="{}"></iframe>
    """.format(code.replace('"', '&quot;'))
    display(HTML(iframe))

```

然后，我们可以在一个新的单元格中使用该代码执行实际的可视化:

```py
show_graph(tf.get_default_graph().as_graph_def())

```

结果显示了这些操作在有向图中是如何链接的。观想平台叫做 **TensorBoard** ，自带 TensorFlow:

![](img/B06162_11_04.png)

当我们想要计算 y 的值时，我们需要通过图中的其他节点传递 x 的值，这些节点在上面的图中称为 OpNodes，简称为 *Operation Node* 。

至此，我们已经定义了图形本身。下一步是计算数值。我们可以通过多种方式做到这一点，尤其是考虑到 x 是一个变量。为了计算 y，使用 x 的当前值，我们创建一个 TensorFlow Session 对象，然后要求它运行 y:

```py
model = tf.global_variables_initializer()
with tf.Session() as session:
    session.run(model)
    result = session.run(y)
print(result)

```

第一行初始化变量。TensorFlow 允许您指定操作范围和命名空间。在这一点上，我们只是使用全局命名空间，这个函数是一个方便的快捷方式来正确初始化该范围，这可以被认为是 TensorFlow 编译该图所需的一个步骤。

第二个创建一个新的会话来运行模型本身。`tf.global_variables_initializer()`的结果本身就是图上的操作，必须执行才能发生。下一行实际上运行变量 y，它计算计算 y 值所需的必要运算节点。在我们的例子中，这是所有的节点，但是更大的图可能不需要计算所有的节点——TensorFlow将只做足够的工作来获得答案，仅此而已。

If you get an error that `global_variables_initializer` is not defined, replace it with `initialize_all_variables` - the interface was recently changed.

打印结果给我们的值是 3。

我们还可以执行其他操作，例如更改 x 的值。例如，我们可以创建一个赋值操作，为现有变量赋值。在这个例子中，我们将 x 的值改为 10，然后计算 y，结果是 548。

```py
model = tf.global_variables_initializer()
with tf.Session() as session:
    session.run(model)
    session.run(x.assign(10))
    result = session.run(y)
print(result)

```

虽然这个简单的例子看起来并不比我们用 Python 已经能做的更强大，但是 TensorFlow(和 antio)有大量的分发选项，可以在许多计算机上计算更大的网络，并且可以优化以高效地完成它。这两个库还包含用于保存和加载网络的额外工具，包括值，允许我们保存在这些库中创建的模型。

# 使用 Keras

TensorFlow 不是直接构建神经网络的库。同理，NumPy 也不是执行数据挖掘的库；它只是做繁重的工作，通常从另一个图书馆使用。TensorFlow 包含一个内置库，称为 TensorFlow 学习构建网络和执行数据挖掘。其他库，如 Keras，也是考虑到这一点而构建的，并在后端使用 TensorFlow。

Keras 实现了许多现代类型的神经网络层和构建它们的构件。在这一章中，我们将使用卷积层，它被设计来模仿人类视觉的工作方式。他们使用连接的神经元的小集合，只分析输入值的一部分——在这个例子中，是一幅图像。这允许网络处理标准变更，例如处理图像的翻译。在基于视觉的实验中，卷积层处理的改变的一个例子是平移图像。

In contrast, a traditional neural network is often heavily connected—all neurons from one layer connect to all neurons in the next layer. This is referred to as a dense layer.

Keras 中神经网络的标准模型是一个**序列**模型，它是通过传递一个层列表来创建的。在标准前馈配置中，输入(X_train)被给予第一层，其输出被给予下一层，以此类推。

在 Keras 中构建神经网络比仅仅使用 TensorFlow 构建要容易得多。除非你正在对神经网络结构进行高度定制化的修改，否则我强烈建议使用 Keras。

为了展示将 Keras 用于神经网络的基础知识，我们将实现一个基本网络来依赖 Iris 数据集，我们在[第 1 章](00.html) *【数据挖掘入门】*中看到了该数据集。Iris 数据集非常适合测试新算法，甚至是复杂的算法，如深度神经网络。

首先，打开一个新的 Jupyter 笔记本。我们将在本章的稍后部分返回到包含 CIFAR 数据的笔记本电脑。

接下来，我们加载数据集:

```py
import numpy as np
from sklearn.datasets import load_iris 
iris = load_iris() 
X = iris.data.astype(np.float32) 
y_true = iris.target.astype(np.int32)

```

当处理像 TensorFlow 这样的库时，最好明确数据类型。虽然 Python 很乐意从一种数字数据类型隐式转换为另一种数据类型，但像 TensorFlow 这样的库是低级代码(在本例中是 C++)的包装器。这些库不能总是在数字数据类型之间转换。

我们的输出目前是分类值的单个数组(0、1 或 2，具体取决于类)。神经网络*可以*开发成输出这种格式的数据，但是*的常规*是神经网络有 *n* 个输出，其中 *n* 个在类的数量上。因此，我们使用一热编码将我们的分类 y 转换为一热编码`y_onehot`:

```py
from sklearn.preprocessing import OneHotEncoder

y_onehot = OneHotEncoder().fit_transform(y_true.reshape(-1, 1))
y_onehot = y_onehot.astype(np.int64).todense()

```

然后，我们分成训练和测试数据集:

```py
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, random_state=14)

```

接下来，我们通过创建不同的层来构建我们的网络。我们的数据集包含四个输入变量和三个输出类。这给了我们第一层和最后一层的大小，但没有中间的层。玩弄这个数字会给出不同的结果，值得跟踪不同的值看看会发生什么。我们将首先创建一个小型网络，其规模如下:

```py
input_layer_size, hidden_layer_size, output_layer_size = 4, 6, 3

```

接下来，我们创建隐藏层和输出层(输入层是隐式的)。对于本例，我们将使用密集层:

```py
from keras.layers import Dense
hidden_layer = Dense(output_dim=hidden_layer_size, input_dim=input_layer_size, activation='relu')
output_layer = Dense(output_layer_size, activation='sigmoid')

```

我鼓励你玩激活值，看看这如何影响结果。如果您没有关于您的问题的进一步信息，这里的值是很好的默认值。即隐藏图层使用`relu`，输出图层使用`sigmoid`。

然后，我们将这些层组合成一个顺序模型:

```py
from keras.models import Sequential
model = Sequential(layers=[hidden_layer, output_layer])

```

从这里开始的一个必要步骤是编译网络，这将创建图表。在编译步骤中，我们获得了关于如何训练和评估网络的信息。这里的值定义了神经网络试图训练的确切内容，在下面的例子中，它是输出神经元和它们的期望值之间的均方误差。优化器的选择在很大程度上影响了它的效率，通常需要在速度和内存使用之间进行权衡。

```py
model.compile(loss='mean_squared_error',
              optimizer='adam',
              metrics=['accuracy'])

```

然后我们使用`fit`函数训练我们的模型。Keras 模型从`fit()`返回一个历史对象，这允许我们在细粒度级别查看数据。

```py
history = model.fit(X_train, y_train)

```

你会得到相当多的产出。神经网络将训练 10 个时期，这些时期是获取训练数据、通过神经网络运行、更新权重和评估结果的训练周期。如果您调查历史对象(尝试`print(history.history)`)，您将会看到在这些时期中的每一个之后损失函数的分数(越低越好)。还包括精度，越高越好。你可能还会注意到，它并没有真正改善那么多。

我们可以使用`matplotlib`绘制出历史对象:

```py
import seaborn as sns
from matplotlib import pyplot as plt

plt.plot(history.epoch, history.history['loss'])
plt.xlabel("Epoch")
plt.ylabel("Loss")

```

![](img/B06162_11_05.png)

虽然训练损失在减少，但并没有减少多少。这是神经网络的一个问题——它们训练得很慢。默认情况下，fit 函数将只执行 10 个时期，这对于几乎任何应用程序来说都远远不够。要了解这一点，请使用神经网络来预测测试集并运行分类报告:

```py
from sklearn.metrics import classification_report
y_pred = model.predict_classes(X_test)
print(classification_report(y_true=y_test.argmax(axis=1), y_pred=y_pred))

```

结果相当差，整体 f1 得分为 0.07，分类器仅预测所有实例的类别 2。起初，神经网络似乎没有那么好，但让我们看看当我们训练 1000 个时代时会发生什么:

```py
history = model.fit(X_train, y_train, nb_epoch=1000, verbose=False)

```

再次可视化每个时期的损失，当运行迭代算法(如神经网络)时，这是一个非常有用的可视化，使用上面的代码显示了一个非常不同的故事:

![](img/B06162_11_06.png)

最后，我们再次执行分类报告以查看结果:

```py
y_pred = model.predict_classes(X_test)
print(classification_report(y_true=y_test.argmax(axis=1), y_pred=y_pred))

```

太好了。

# 卷积神经网络

为了开始用 Keras 进行图像分析，我们将重新实现我们在[第 8 章](07.html) *中使用的例子，用神经网络*击败验证码，来预测图像中代表哪个字母。我们将重现我们在[第八章](07.html) *中使用的密集神经网络，用神经网络*击败验证码。首先，我们需要在笔记本中再次输入数据集构建代码。关于该代码的作用，请参考[第 8 章](07.html) *、用神经网络击败验证码* ( *记得更新 Coval 字体的文件位置*):

```py
import numpy as np 
from PIL import Image, ImageDraw, ImageFont 
from skimage import transform as tf

```

```py
def create_captcha(text, shear=0, size=(100, 30), scale=1):
    im = Image.new("L", size, "black")
    draw = ImageDraw.Draw(im)
    font = ImageFont.truetype(r"bretan/Coval-Black.otf", 22) 
    draw.text((0, 0), text, fill=1, font=font)
    image = np.array(im)
    affine_tf = tf.AffineTransform(shear=shear)
    image = tf.warp(image, affine_tf)
    image = image / image.max()
    shape = image.shape
    # Apply scale
    shapex, shapey = (shape[0] * scale, shape[1] * scale)
    image = tf.resize(image, (shapex, shapey))
    return image

```

```py
from skimage.measure import label, regionprops
from skimage.filters import threshold_otsu
from skimage.morphology import closing, square

def segment_image(image):
    # label will find subimages of connected non-black pixels
    labeled_image = label(image>0.2, connectivity=1, background=0)
    subimages = []
    # regionprops splits up the subimages
    for region in regionprops(labeled_image):
        # Extract the subimage
        start_x, start_y, end_x, end_y = region.bbox
        subimages.append(image[start_x:end_x,start_y:end_y])
    if len(subimages) == 0:
        # No subimages found, so return the entire image
        return [image,]
    return subimages

```

```py
from sklearn.utils import check_random_state
random_state = check_random_state(14) 
letters = list("ABCDEFGHIJKLMNOPQRSTUVWXYZ")
assert len(letters) == 26
shear_values = np.arange(0, 0.8, 0.05)
scale_values = np.arange(0.9, 1.1, 0.1)

```

```py
def generate_sample(random_state=None): 
    random_state = check_random_state(random_state) 
    letter = random_state.choice(letters) 
    shear = random_state.choice(shear_values)
    scale = random_state.choice(scale_values)
    return create_captcha(letter, shear=shear, size=(30, 30), scale=scale), letters.index(letter)

```

```py
dataset, targets = zip(*(generate_sample(random_state) for i in range(1000)))
dataset = np.array([tf.resize(segment_image(sample)[0], (20, 20)) for sample in dataset])
dataset = np.array(dataset, dtype='float') 
targets = np.array(targets)

```

```py
from sklearn.preprocessing import OneHotEncoder 
onehot = OneHotEncoder() 
y = onehot.fit_transform(targets.reshape(targets.shape[0],1))
y = y.todense()

X = dataset.reshape((dataset.shape[0], dataset.shape[1] * dataset.shape[2]))

from sklearn.model_selection import train_test_split 
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.9)

```

在重新运行所有这些代码之后，您将拥有一个类似于[第 8 章](07.html) *、用神经网络击败验证码*实验的数据集。接下来，我们将使用 Keras，而不是使用 scikit-learn 来做我们的神经网络。

首先，我们创建我们的两个**密集**层，并将它们组合在一个**连续**模型中。我选择在隐藏层放 100 个神经元。

```py
from keras.layers import Dense
from keras.models import Sequential
hidden_layer = Dense(100, input_dim=X_train.shape[1])
output_layer = Dense(y_train.shape[1])
# Create the model
model = Sequential(layers=[hidden_layer, output_layer])
model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])

```

然后，我们拟合模型。和以前一样，你会想要更多的时代。我又用了 1000，如果你想要更好的结果，你可以增加这个数字。

```py
model.fit(X_train, y_train, nb_epoch=1000, verbose=False)
y_pred = model.predict(X_test)

```

您还可以收集生成的历史对象，就像我们对 Iris 示例所做的那样，以进一步研究训练。

```py
from sklearn.metrics import classification_report
print(classification_report(y_pred=y_pred.argmax(axis=1),
y_true=y_test.argmax(axis=1)))

```

再次，完美。

At least, it was on my machine but your results may differ slightly.

# 图形处理器优化

神经网络的规模可以变得相当大。这对记忆的使用有一些影响；然而，像稀疏矩阵这样的高效结构意味着我们通常不会遇到在内存中拟合神经网络的问题。

The main issue when neural networks grow large is that they take a very long time to compute. In addition, some datasets and neural networks will need to run many epochs of training to get a good fit for the dataset.

我们将在本章中训练的神经网络在我的功能相当强大的计算机上每个纪元需要 8 分钟以上，我们预计将运行几十个，可能几百个纪元。一些较大的网络可能需要几个小时来训练单个纪元。为了获得最佳表现，你可能会考虑数千个训练周期。

神经网络的规模导致训练时间长。

一个积极的方面是，神经网络的核心充满了浮点运算。也有大量的操作可以并行执行，因为神经网络训练主要由矩阵运算组成。这些因素意味着在图形处理器上计算是加快这种训练的一个有吸引力的选择。

# 何时使用图形处理器进行计算

图形处理器最初是为显示图形而设计的。这些图形用矩阵和矩阵上的数学方程表示，然后转换成我们在屏幕上看到的像素。这个过程涉及大量并行计算。现代的中央处理器可能有很多核心(你的电脑可能有 2 个、4 个，甚至 16 个——甚至更多！)，GPU 有数千个专门为显卡设计的小内核。

中央处理器更适合顺序任务，因为核心往往更快，访问计算机内存等任务更有效。老实说，让中央处理器来承担重任也更容易。几乎每个机器学习库都默认使用 CPU，在使用 GPU 进行计算之前，还需要做额外的工作。好处可能相当大。

因此，图形处理器更适合同时对数字进行大量小操作的任务。许多机器学习任务都是这样的，通过使用图形处理器来提高效率。

让你的代码在图形处理器上运行是一种令人沮丧的体验。这在很大程度上取决于你有什么类型的 GPU，它是如何配置的，你的操作系统，以及你是否准备对你的计算机进行一些低级别的更改。

Luckily, Keras will automatically use a GPU for operations, if the operation suits and a GPU can be found (and if you use TensorFlow as the backend). However, you still need to setup your computer such that the GPU can be found by Keras and TensorFlow.

有三种主要途径可以选择:

*   首先是看看你的电脑，搜索适合你的图形处理器和操作系统的工具和驱动程序，探索现有的许多教程，并找到一个适合你的场景。这是否有效取决于你的系统是什么样的。也就是说，这个场景比几年前容易得多，有了更好的工具和驱动程序来执行支持 GPU 的计算。
*   第二个途径是选择一个系统，找到关于设置它的好文档，然后购买一个与之匹配的系统。这将工作得更好，但可能相当昂贵——在大多数现代计算机中，图形处理器是最昂贵的部件之一。如果你想从系统中获得出色的性能，尤其如此——你需要一个非常好的图形处理器，这可能非常昂贵。如果你是一家企业(或者有更大的资金可花)，你可以购买专门用于深度学习的高端 GPU，并更直接地与供应商交谈，以确保你获得正确的硬件。
*   第三种途径是使用虚拟机，它已经为此目的进行了配置。例如，Altoros Systems 创建了这样一个运行在亚马逊网络服务上的系统。这个系统运行起来会花你很多钱，但是价格比一台新电脑要低得多。根据你的位置，你得到的确切系统和你使用它的程度，你可能每小时看不到 1 美元，而且通常要少得多。如果您在亚马逊的网络服务中使用 spot 实例，您可以以每小时几美分的价格运行它们(尽管您需要开发代码来分别在 spot 实例上运行)。

If you aren't able to afford the running costs of a virtual machine, I recommend that you look into the first avenue, with your current system. You may also be able to pick up a good second-hand GPU from family or a friend who constantly updates their computer (gamer friends are great for this!).

# 在图形处理器上运行我们的代码

我们将走本章的第三条路，基于 Altoros Systems 的基础系统创建一个虚拟机。这将在亚马逊的 EC2 服务上运行。还有许多其他的网络服务可以使用，每个服务的过程都略有不同。在本节中，我将概述亚马逊的过程。

如果你想使用自己的计算机，并将其配置为运行支持图形处理器的计算，请随意跳过这一部分。

You can get more information on how this was set up, see [https://aws.amazon.com/marketplace/pp/B01H1VWUOY?qid=1485755720051&sr=0-1&ref_=srh_res_product_title](https://aws.amazon.com/marketplace/pp/B01H1VWUOY?qid=1485755720051&sr=0-1&ref_=srh_res_product_title) 

1.  首先，进入 https://console.aws.amazon.com/console/home?的自动气象站控制台地区=美国-东部-1
2.  使用您的亚马逊帐户登录。如果您没有，系统会提示您创建一个，您需要这样做才能继续。
3.  接下来，转到位于 https://console.aws.amazon.com/ec2/v2/home?的 EC2 服务控制台:[地区=美国-东部-1。](https://console.aws.amazon.com/ec2/v2/home?region=us-east-1.)
4.  单击启动实例，并在右上角的下拉菜单中选择加州北部作为您的位置。

5.  点击 Community AMIs，搜索带有 TensorFlow (GPU)的 Ubuntu x64 AMI，这是 Altoros Systems 创建的机器。然后，单击选择。在下一个屏幕上，选择 g 2.2x 大作为机器类型，然后单击查看并启动。在下一个屏幕上，单击启动。
6.  在这一点上，你将被收费，所以请记住关闭你的机器，当你完成他们。您可以转到 EC2 服务，选择机器，然后停止它。不运行的机器不会向您收费。
7.  系统会提示您一些如何连接到实例的信息。如果您以前没有使用过 AWS，您可能需要创建一个新的密钥对来安全地连接到您的实例。在这种情况下，请为您的密钥对命名，下载 pemfile，并将其存储在安全的地方—如果丢失，您将无法再次连接到您的实例！
8.  有关使用 pem 文件连接到实例的信息，请单击连接。最有可能的情况是您将 ssh 与以下命令一起使用:

```py
ssh -i <certificante_name>.pem ubuntu@<server_ip_address>

```

# 设置环境

接下来，我们需要把代码放到机器上。有很多方法可以把这个文件放到你的电脑上，但是最简单的方法之一就是复制粘贴内容。

首先，打开我们以前用过的 Jupyter 笔记本(在你的电脑上，而不是在亚马逊虚拟机上)。笔记本上有一个菜单。单击文件，然后单击下载为。选择 Python 并将其保存到您的计算机上。该过程将 Jupyter 笔记本中的代码作为 python 脚本下载，您可以从命令行运行该脚本。

打开此文件(在某些系统中，您可能需要右键单击并用文本编辑器打开)。选择所有内容并将其复制到剪贴板。

在亚马逊虚拟机上，移动到主目录，用新文件名打开 nano:

```py
$ cd~/

```

```py
$ nano chapter11script.py

```

nano 程序将会打开，这是一个命令行文本编辑器。

打开该程序后，将剪贴板的内容粘贴到该文件中。在某些系统上，您可能需要使用 ssh 程序的文件选项，而不是按 Ctrl+ V 来粘贴。

在 nano 中，按 Ctrl+ O 将文件保存在磁盘上，然后按 Ctrl+ X 退出程序。

您还需要字体文件。最简单的方法是从原始位置再次下载。为此，请输入以下内容:

```py
$ wget http://openfontlibrary.oimg/downloads/bretan/680bc56bbeeca95353ede363a3744fdf/bretan.zip

```

```py
$ sudo apt-get install unzip

```

```py
$ unzip -p bretan.zip

```

当仍在虚拟机中时，您可以使用以下命令运行程序:

```py
$ python chapter11script.py

```

该程序将像在 Jupyter 笔记本中一样运行，结果将打印到命令行。

结果应该和以前一样，但是神经网络的实际训练和测试会快得多。请注意，在程序的其他方面，它不会快得多——我们没有编写验证码数据集创建来使用图形处理器，因此我们不会在那里获得加速。

You may wish to shut down the Amazon virtual machine to save some money; we will be using it at the end of this chapter to run our main experiment, but will be developing the code on your main computer first.

# 应用

现在回到您的主计算机上，打开我们在本章中创建的第一个 Jupyter Notebook 我们加载了 CIFAR 数据集的那个。在这个主要实验中，我们将获取 CIFAR 数据集，创建一个深度卷积神经网络，然后在我们基于 GPU 的虚拟机上运行它。

# 获取数据

首先，我们将拍摄 CIFAR 图像，并使用它们创建数据集。与之前不同的是，我们将保留像素结构，即行和列。首先，将所有批次加载到列表中:

```py
import os
import numpy as np 

data_folder = os.path.join(os.path.expanduser("~"), "Data", "cifar-10-batches-py")

batches = [] 
for i in range(1, 6):
    batch_filename = os.path.join(data_folder, "data_batch_{}".format(i))
    batches.append(unpickle(batch_filename)) 
    break

```

最后一行，休息，是测试代码——这将大大减少训练示例的数量，允许您快速查看代码是否工作。在您测试完代码后，我会提示您删除这一行。

接下来，通过将这些批次堆叠在一起来创建数据集。我们使用 NumPy 的 vstack，它可以被可视化为向数组的末尾添加行:

```py
X = np.vstack([batch['data'] for batch in batches])

```

然后，我们将数据集规范化为 0 到 1 的范围，然后强制该类型为 32 位浮点型(这是启用 GPU 的虚拟机可以运行的唯一数据类型):

```py
X = np.array(X) / X.max() 
X = X.astype(np.float32)

```

然后我们对类做同样的事情，除了我们执行一个 hstack，这类似于在数组的末尾添加列。然后我们可以使用 OneHotEncoder 将它变成一个单热阵列。我将在这里展示一个替代方法，使用 Keras 中的实用函数，但是结果是一样的:

```py
from keras.utils import np_utils
y = np.hstack(batch['labels'] for batch in batches).flatten()
nb_classes = len(np.unique(y))
y = np_utils.to_categorical(y, nb_classes)

```

接下来，我们将数据集分为训练集和测试集:

```py
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

```

接下来，我们重塑数组以保留原始数据结构。原始数据是 32×32 像素的图像，每个像素有 3 个值(红色、绿色和蓝色值)。虽然标准的前馈神经网络只接受单个输入数据阵列(参见验证码示例)，但卷积神经网络是为图像构建的，并接受三维图像数据(二维图像和另一个包含颜色深度的维度)。

```py
X_train = X_train.reshape(-1, 3, 32, 32)
X_test = X_test.reshape(-1, 3, 32, 32)
n_samples, d, h, w = X_train.shape  # Obtain dataset dimensions
# Convert to floats and ensure data is normalised.
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')

```

我们现在有了一个熟悉的训练和测试数据集，以及每个数据集的目标类。我们现在可以构建分类器了。

# 创建神经网络

我们现在将构建卷积神经网络。我已经进行了一些修补，并找到了一个运行良好的布局，但请随意尝试更多(或更少)的层，不同类型和不同大小的层。较小的网络训练速度更快，但较大的网络可以获得更好的结果。

首先，我们创建神经网络的层次:

```py
from keras.layers import Dense, Flatten, Convolution2D, MaxPooling2D
conv1 = Convolution2D(32, 3, 3, input_shape=(d, h, w), activation='relu')
pool1 = MaxPooling2D()
conv2 = Convolution2D(64, 2, 2, activation='relu')
pool2 = MaxPooling2D()
conv3 = Convolution2D(128, 2, 2, activation='relu')
pool3 = MaxPooling2D()
flatten = Flatten()
hidden4 = Dense(500, activation='relu')
hidden5 = Dense(500, activation='relu')
output = Dense(nb_classes, activation='softmax')
layers = [conv1, pool1,
          conv2, pool2,
          conv3, pool3,
          flatten, hidden4, hidden5,
          output]

```

按照正常的前馈神经网络，我们对最后三层使用密集层，但是在此之前，我们使用卷积层和汇集层相结合。我们有三套这样的。

对于每对卷积 2D 和最大池 2D 层，会发生以下情况:

1.  卷积 2D 网络获取输入数据的补丁。这些数据通过一个过滤器，一个类似于支持向量机使用的核算子的矩阵变换。滤波器是一个较小的矩阵，大小为 k 乘 n(在上面的卷积 2D 初始化器中指定为 3x3)，应用于图像中的每个 k 乘 n 模式。结果是卷积特征。
2.  最大池 2D 层从卷积 2D 层获取结果，并为每个卷积特征找到最大值。

虽然这确实会丢弃大量信息，但这实际上有助于图像检测。如果图像的对象是右边几个像素，标准的神经网络会认为它是一幅全新的图像。相比之下，卷积层会找到它并报告几乎相同的输出(当然，这取决于各种各样的其他因素)。

在通过这些对层之后，进入网络密集部分的特征是表示图像抽象概念的元特征，而不是具体质量。通常这些可以被可视化，产生像*这样的特征，一点点直线指向*。

接下来，我们将这些层放在一起构建我们的神经网络并对其进行训练。这次培训将比以前的培训花费更长的时间。我建议从 10 个时代开始，确保代码一直有效，然后用 100 个时代重新运行。此外，一旦您确认代码有效并且您得到了预测，返回并删除我们在创建数据集时输入的`break`行(它在批处理循环中)。这将允许代码对所有样本进行训练，而不仅仅是第一批样本。

```py
model = Sequential(layers=layers)
model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])
import tensorflow as tf
history = model.fit(X_train, y_train, nb_epoch=25, verbose=True,
validation_data=(X_test, y_test),batch_size=1000))

```

最后，我们可以用网络预测和评估。

```py
y_pred = model.predict(X_test)
from sklearn.metrics import classification_report
print(classification_report(y_pred=y_pred.argmax(axis=1),
 y_true=y_test.argmax(axis=1)))

```

跑了 100 个时代，在这种情况下还是不太完美，但还是一个优秀的成绩。如果你有时间(比如说，一夜之间)，试着运行 1000 个时代的代码。准确性有所提高，但投入的时间回报却在减少。一个(不太好的)经验法则是，要将误差减半，你需要加倍训练时间。

# 把它们放在一起

现在我们的网络代码可以工作了，我们可以在远程机器上用我们的训练数据集来训练它。如果您使用本地机器运行神经网络，则可以跳过这一部分。

我们需要将脚本上传到我们的虚拟机。和以前一样，单击文件|下载为 Python，并将脚本保存在计算机上的某个位置。启动并连接到虚拟机，然后像前面一样上传脚本(我称我的脚本为`chapter11cifar.py`—如果您的脚本命名不同，只需更新以下代码)。

接下来我们需要的是数据集在虚拟机上。最简单的方法是转到虚拟机并键入:

```py
$ wget http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz

```

这将下载数据集。下载完成后，您可以先创建数据文件夹，然后解压其中的数据，从而将数据提取到该文件夹中:

```py
$ mkdir Data

```

```py
$ tar -zxf cifar-10-python.tar.gz -C Data

```

最后，我们可以用以下内容运行我们的示例:

```py
$ python3 chapter11cifar.py

```

你会注意到的第一件事是急剧加速。在我的家用电脑上，每个纪元运行时间超过 100 秒。在支持 GPU 的虚拟机上，每个纪元只需 16 秒！如果我们尝试在我的计算机上运行 100 个纪元，将需要近三个小时，而在虚拟机上只需要 26 分钟。

这种急剧的加速使得尝试不同的模型要快得多。通常在试用机器学习算法时，单个算法的计算复杂度并不重要。一个算法可能需要几秒钟、几分钟或几小时才能运行。如果你只运行一个模型，这个训练时间不太可能太重要——尤其是因为预测，就像大多数机器学习算法一样，非常快，这也是机器学习模型最常用的地方。

然而，当你有许多参数要运行时，你会突然需要训练成千上万个参数略有不同的模型——突然之间，这些速度的提高变得更加重要。

经过 100 个阶段的训练，用了整整 26 分钟，你会得到一个打印输出的最终结果:

```py
0.8497

```

还不错！我们可以增加训练阶段的数量来进一步改善这一点，或者我们可以尝试改变参数；也许，更多的隐藏节点，更多的卷积层，或者一个额外的密集层。喀拉斯还有其他类型的图层可以尝试；虽然一般来说，卷积层对视觉更好。

# 摘要

在这一章中，我们研究了使用深度神经网络，特别是卷积网络来执行计算机视觉。我们通过 Keras 包做到了这一点，该包使用 Tensorflow 或 antao 作为其计算后端。使用喀拉的助手功能构建网络相对容易。

卷积网络是为计算机视觉设计的，所以结果相当准确应该不足为奇。最后的结果表明，使用今天的算法和计算能力，计算机视觉确实是一个有效的应用。

我们还使用了一个支持图形处理器的虚拟机来大大加快这个过程，对我的机器来说几乎是 10 倍。如果您需要额外的能量来运行其中的一些算法，云提供商的虚拟机可能是实现这一点的有效方法(通常每小时不到一美元)——只需记住在完成后关闭它们！

为了扩展本章的工作，请尝试使用网络结构来进一步提高精度。另一个可以用来提高准确性的方法是创建更多的数据，要么通过自己拍照(慢)，要么通过修改现有的照片(快得多)。要进行修改，您可以颠倒图像，旋转，剪切等。Keras 有一个非常有用的功能。参见[https://keras.io/preprocessing/image/](https://keras.io/preprocessing/image/)的文件

另一个值得研究的领域是神经网络结构的变化，更多的节点，更少的节点，更多的层等等。还要试验不同的激活类型、不同的层类型和不同的组合。

这一章的重点是一个非常复杂的算法。卷积网络训练时间长，训练参数多。最终，相比之下，数据的规模很小；虽然这是一个很大的数据集，但我们甚至不用稀疏矩阵就可以将它全部加载到内存中。在下一章中，我们将使用一个简单得多的算法，但是一个大得多的数据集，无法存储在内存中。这是大数据的基础，也是数据挖掘在许多大型行业(如采矿和社交网络)中应用的基础。