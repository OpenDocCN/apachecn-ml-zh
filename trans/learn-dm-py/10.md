# 新闻文章聚类

在前面的大部分章节中，我们执行了数据挖掘，知道我们在寻找什么。我们对*目标类*的使用允许我们在训练阶段学习我们的特征如何对这些目标建模，这允许算法设置内部参数以最大化其学习。这种我们有训练目标的学习方式被称为**监督学习**。在本章中，我们将考虑没有这些目标我们会做什么。这是**无监督学习**，它更像是一个探索性的任务。无监督学习的目标不是想用我们的模型进行分类，而是探索数据以找到见解。

在这一章中，我们将研究新闻文章的聚类，以发现数据中的趋势和模式。我们将研究如何使用链接聚合网站从不同网站提取数据，以展示各种新闻故事。

本章涵盖的关键概念包括:

*   使用 reddit API 收集有趣的新闻故事
*   从任意网站获取文本
*   无监督数据挖掘的聚类分析
*   从文档中提取主题
*   在线学习，无需再培训即可更新模型
*   将不同的模型组合在一起

# 趋势主题发现

在本章中，我们将构建一个系统，该系统获取新闻文章的实时提要，并将它们分组在一起，以便这些组具有相似的主题。您可以在几周(或更长时间)内多次运行该系统，以查看趋势如何随时间变化。

我们的系统将从流行的链接聚合网站([https://www.reddit.com](https://www.reddit.com/))开始，它存储了其他网站的链接列表，以及一个评论区供讨论。reddit 上的链接分为几类，称为**子链接**。有专门针对特定电视节目、滑稽图像和许多其他事物的子分类。我们感兴趣的是新闻的子领域。我们将在本章中使用 */r/worldnews* 子循环，但是代码应该与任何其他基于文本的子循环一起工作。

在这一章中，我们的目标是下载流行的故事，然后将它们聚集在一起，以查看发生的任何主要主题或概念。这将让我们深入了解流行焦点，而不必手动分析数百个单独的故事。一般流程是:

1.  从 reddit 收集最近流行新闻故事的链接。
2.  从这些链接下载网页。
3.  只从下载的网站上提取新闻故事。
4.  进行聚类分析，找出故事的聚类。
5.  分析这些集群以发现趋势。

# 使用网络应用编程接口获取数据

在前面的几章中，我们已经使用了基于网络的 API 来提取数据。例如，在[第 7 章](06.html) *【使用图挖掘遵循建议】*中，我们使用了 Twitter 的 API 来提取数据。收集数据是数据挖掘管道的关键部分，基于 web 的 API 是收集各种主题的数据的绝佳方式。

使用基于网络的应用编程接口收集数据时，需要考虑三件事:授权方法、速率限制和应用编程接口端点。

**授权方法**允许数据提供者知道谁在收集数据，以确保它们受到适当的速率限制，并且可以跟踪数据访问。对于大多数网站来说，个人帐户通常足以开始收集数据，但有些网站会要求您创建一个正式的开发人员帐户来获得这种访问权限。

**限速**适用于数据采集，尤其是免费服务。在使用应用编程接口时，了解规则是很重要的，因为它们可以而且确实会随着网站的不同而变化。Twitter 的 API 限制是每 15 分钟 180 个请求(取决于特定的 API 调用)。我们将在后面看到，Reddit 允许每分钟 30 个请求。其他网站实行每日限额，而其他网站则以每秒为单位进行限额。即使在网站内部，不同的 API 调用也有很大的差异。例如，谷歌地图对每个资源有较小的限制和不同的应用编程接口限制，每小时的请求数量有不同的限额。

If you find you are creating an app or running an experiment that needs more requests and faster responses, most API providers have commercial plans that allow for more calls. Contact the provider for more details.

**API 端点**是您用来提取信息的实际网址。这些因网站而异。大多数情况下，基于网络的应用编程接口遵循 RESTful 接口(代表状态转移的简称)。RESTful 接口经常使用与 HTTP 相同的动作:`GET`、`POST`和`DELETE`是最常见的。例如，要检索资源信息，我们可以使用以下(仅示例)API 端点:
[www.dataprovider.com/api/resource_type/resource_id/](http://www.dataprovider.com/api/resource_type/resource_id/)

为了获取信息，我们只需向该网址发送一个 HTTP `GET`请求。这将返回具有给定类型和标识的资源信息。大多数 API 都遵循这种结构，尽管在实现上有一些不同。大多数带有应用程序接口的网站都会对它们进行适当的记录，为您提供可以检索的所有应用程序接口的详细信息。

首先，我们设置连接到服务的参数。为此，您需要 reddit 的开发者密钥。要拿到这个钥匙，登录[https://www.reddit.com/login](https://www.reddit.com/login)站点，前往[https://www.reddit.com/prefs/apps](https://www.reddit.com/prefs/apps)。点击这里，你是开发者吗？创建应用程序...并填写表单，将类型设置为脚本。您将获得您的客户 ID 和一个秘密，您可以将其添加到新的 Jupyter 笔记本中:

```py
CLIENT_ID = "<Enter your Client ID here>" 
CLIENT_SECRET = "<Enter your Client Secret here>"

```

Reddit 还要求您(当您使用他们的 API 时)将用户代理设置为包含您的用户名的唯一字符串。创建唯一标识应用程序的用户代理字符串。我使用了书的名字，第 10 章，和 0.1 的版本号来创建我的用户代理，但是它可以是任何你喜欢的字符串。请注意，不这样做可能会导致您的连接受到严重的速率限制:

```py
USER_AGENT = "python:<your unique user agent> (by /u/<your reddit username>)"

```

此外，您需要使用您的用户名和密码登录 reddit。如果你还没有，就注册一个新的(这是免费的，你也不需要用个人信息来验证)。

You will need your password to complete the next step, so be careful before sharing your code to others to remove it. If you don't put your password in, set it to none and you will be prompted to enter it. 

现在让我们创建用户名和密码:

```py
from getpass import getpass
USERNAME = "<your reddit username>" 
PASSWORD = getpass("Enter your reddit password:")

```

接下来，我们将创建一个函数来记录这些信息。reddit 登录 API 将返回一个令牌，您可以将其用于进一步的连接，这将是这个函数的结果。该代码获取登录 reddit 所需的信息，设置用户代理，然后获取一个访问令牌，我们可以在以后的请求中使用:

```py
import requests
def login(username, password):
    if password is None:
        password = getpass.getpass("Enter reddit password for user {}: ".format(username))    
    headers = {"User-Agent": USER_AGENT}
    # Setup an auth object with our credentials
    client_auth = requests.auth.HTTPBasicAuth(CLIENT_ID, CLIENT_SECRET)
    # Make a post request to the access_token endpoint
    post_data = {"grant_type": "password", "username": username, "password": password}
    response = requests.post("https://www.reddit.com/api/v1/access_token", auth=client_auth,     
                             data=post_data, headers=headers) 
    return response.json()

```

我们现在可以调用我们的函数来获取访问令牌:

```py
token = login(USERNAME, PASSWORD)

```

这个令牌对象只是一个字典，但是它包含`access_token`字符串，我们将在未来的请求中传递这个字符串。它还包含其他信息，如令牌的范围(可能是所有内容)和到期时间，例如:

```py
{'access_token': '<semi-random string>', 'expires_in': 3600, 'scope': '*', 'token_type': 'bearer'}

```

If you are creating a production-level app, make sure you check the expiry of the token and to refresh it if it runs out. You'll also know this has happened if your access token stops working when trying to make an API call.

# Reddit 作为数据源

Reddit 是一个全球数百万人使用的链接聚合网站，尽管英文版本以美国为中心。任何用户都可以向他们感兴趣的网站提供链接，以及该链接的标题。其他用户可以投上一票，表示他们喜欢这个链接，或者投下一票，表示他们不喜欢这个链接。投票最高的链接被移到页面顶部，而较低的链接不显示。随着时间的推移，旧的链接会从首页上删除，这取决于它有多少 uptown。拥有向上投票的故事的用户赚取称为因果报应的积分，提供了只提交好故事的激励。

Reddit 还允许非链接内容，称为自我发布。这些包含标题和提交者输入的一些文本。这些用于提问和开始讨论。对于这一章，我们将只考虑基于链接的帖子，而不是基于评论的帖子。

帖子被分成网站的不同部分，称为子页面。子循环是相关帖子的集合。当用户向 reddit 提交链接时，他们会选择链接进入哪个子页面。子循环有自己的管理员，并且有自己的规则来决定该子循环的有效内容。

默认情况下，帖子按 **Hot** 排序，这是一个帖子的年龄、向上票数、已获得的向下票数以及内容自由程度的函数。还有**新**，它只给你最近发布的故事(因此包含大量垃圾邮件和不良帖子)，以及 **Top** ，它是给定时间段内投票最高的故事。在这一章中，我们将使用 Hot，它将为我们提供最新的、更高质量的故事(New 中确实有很多低质量的链接)。

使用我们之前创建的令牌，我们现在可以从子循环中获取链接集。为此，我们将使用/r/ <subredditname>API 端点，默认情况下，该端点返回热点故事。我们将使用/r/worldnews 子链接:</subredditname>

```py
subreddit = "worldnews"

```

上一个端点的 URL 允许我们创建完整的 URL，我们可以使用字符串格式设置它:

```py
url = "https://oauth.reddit.com/r/{}".format(subreddit)

```

接下来，我们需要设置标题。有两个原因需要这样做:允许我们使用之前收到的授权令牌，并设置用户代理来阻止我们的请求受到严格限制。代码如下:

```py
headers = {"Authorization": "bearer {}".format(token['access_token']), 
"User-Agent": USER_AGENT}

```

然后，像以前一样，我们使用请求库进行调用，确保我们设置了标题:

```py
response = requests.get(url, headers=headers)

```

对此调用`json()`将产生一个 Python 字典，其中包含 reddit 返回的信息。它将包含给定子循环的前 25 个结果。我们可以通过迭代这个响应中的故事来获得标题。故事本身存储在字典的数据键下。代码如下:

```py
result = response.json()
for story in result['data']['children']: 
    print(story['data']['title'])

```

# 获取数据

我们的数据集将由来自/r/worldnews 子列表热点列表的帖子组成。我们在上一节中看到了如何连接到 reddit 以及如何下载链接。为了把这一切放在一起，我们将创建一个函数，该函数将提取给定子循环中每个项目的标题、链接和分数。

我们将遍历子循环，一次最多得到 100 个故事。我们也可以做分页来得到更多的结果。在 reddit 阻止我们之前，我们可以阅读大量页面，但我们会将其限制在 5 页以内。

由于我们的代码将对一个应用编程接口进行重复调用，记住对我们的调用进行速率限制是很重要的。为此，我们需要睡眠功能:

```py
from time import sleep

```

我们的函数将接受一个子域名和一个授权令牌。我们也将接受阅读的页数，尽管我们将默认设置为 5:

```py
def get_links(subreddit, token, n_pages=5):
    stories = []
    after = None
    for page_number in range(n_pages):
        # Sleep before making calls to avoid going over the API limit
        sleep(2)
        # Setup headers and make call, just like in the login function
        headers = {"Authorization": "bearer {}".format(token['access_token']), "User-Agent": USER_AGENT} 
        url = "https://oauth.reddit.com/r/{}?limit=100". format(subreddit)
        if after:
            # Append cursor for next page, if we have one
            url += "&after={}".format(after)
        response = requests.get(url, headers=headers)
        result = response.json()
        # Get the new cursor for the next loop
        after = result['data']['after']
        # Add all of the news items to our stories list
        for story in result['data']['children']:
            stories.append((story['data']['title'], story['data']['url'], story['data']['score']))
    return stories

```

We saw in [Chapter 7](06.html)*, Follow Recommendations Using Graph Mining*, how pagination works for the Twitter API. We get a cursor with our returned results, which we send with our request. Twitter will then use this cursor to get the next page of results. The reddit API does almost exactly the same thing, except it calls the parameter after. We don't need it for the first page, so we initially set it to None. We will set it to a meaningful value after our first page of results.

调用 stories 函数是传递授权令牌和子域名的一个简单例子:

```py
stories = get_links("worldnews", token)

```

返回的结果应该包含标题、网址和 500 个故事，我们现在将使用它们从结果网站中提取实际的文本。以下是我通过运行脚本收到的标题示例:

*俄罗斯考虑禁止向任何 2015 年后出生的人出售香烟*
*瑞士穆斯林女孩必须和男孩一起游泳*
*报道:俄罗斯在瑞典散布假新闻和造谣——根据瑞典国际事务研究所研究人员的一份报告，俄罗斯在过去 2 年里协调了一场运动，通过使用造谣、宣传和虚假文件来影响瑞典的决策。*
*现在荷兰火车 100%依靠风能运行。荷兰提前一年实现了可再生能源目标。*
*针对英国全面监控法的法律挑战迅速众筹*
*根据对美国在世界各地的打击进行的分析，2016 年，一块约有特拉华州大小的 1000 英尺厚的冰块正在从南极洲脱落*
*美国平均每天投掷 72 枚炸弹——相当于一个小时 3 枚。美国在 2016 年轰炸了伊拉克、叙利亚、巴基斯坦、阿富汗、利比亚、也门、索马里*
*德国政府正在调查最近假新闻激增的情况，此前有消息称，俄罗斯正试图干预该国今年晚些时候的议会选举。*
*在巴西农村，杀虫剂在几天内杀死了 1000 多万只蜜蜂*
*欧洲伊斯兰国恐怖袭击的美国受害者家属起诉推特，指控这家社交媒体巨头允许这个恐怖组织在网上扩散*
*尽管气候变化，但全球范围内的汽油税下降；油&amp；天然气行业获得 5000 亿美元补贴；美国最近一次征收新的汽油税是在 1993 年*
*捷克政府告诉公民武装自己并射杀穆斯林恐怖分子，以防“超级大屠杀”*
*巴解组织威胁说，如果美国大使馆迁往耶路撒冷，他们将取消对以色列的承认*
*欧洲三分之二的新增艾滋病毒病例仅在一个国家记录在案——俄罗斯:目前有 100 多万俄罗斯人感染了这种病毒，预计这一数字将增加近一倍 未来十年*
*捷克政府告诉其公民如何打击恐怖分子:自己毙了他们| 内政部正在推动一项宪法改革，将允许公民使用枪支打击恐怖分子*
*摩洛哥禁止出售布卡*
*大规模杀伤性武器布雷维克在权利上诉案中向纳粹致敬*
*索罗斯组织风险清洗在特朗普获胜后助长了匈牙利的气焰*
*尼日利亚在腐败大扫荡中从国家工资单上清除了 5 万名“幽灵工人”*
*酒精广告具有攻击性，与青少年饮酒有关， 研究发现|社会*
*英国政府在分散人们注意力的同时，悄悄发起了“对自由的攻击”，法律挑战背后的活动家说——调查权力法案在去年年底成为法律， 并赋予间谍阅读每个人整个互联网历史的权力*
*俄罗斯储备基金在 2016 年下降了 70%*
*一名俄罗斯外交官在雅典被发现死亡*
*在喀布尔阿富汗议会附近发生的两起爆炸中，至少有 21 人死亡(大部分是平民)45 人受伤*
*随着货币收回可疑荣誉*英镑的衰落加深**

世界新闻通常不是最乐观的地方，但它确实提供了对世界各地正在发生的事情的洞察，这个子分类上的趋势通常是世界趋势的指示。

# 从任意网站提取文本

我们从 reddit 获得的链接可以进入许多不同组织运行的任意网站。更难的是，这些页面是设计给人类阅读的，而不是电脑程序。当试图获得这些结果的实际内容/故事时，这可能会引起问题，因为现代网站有很多背景。调用 JavaScript 库，应用样式表，使用 AJAX 加载广告，向边栏添加额外的内容，以及做各种其他的事情来使现代网页成为一个复杂的文档。这些特性让现代网络变成了现在的样子，但是很难自动从中获取好的信息！

# 在任意网站上寻找故事

首先，我们将从每个链接下载完整的网页，并将它们存储在我们的数据文件夹中的 raw 子文件夹下。稍后我们将处理这些信息以提取有用的信息。这种结果缓存确保了我们在工作时不必持续下载网站。首先，我们设置数据文件夹路径:

```py
import os 
data_folder = os.path.join(os.path.expanduser("~"), "Data", "websites", "raw")

```

We are going to use MD5 hashing to create unique filenames for our articles, by hashing the URL, and we will import `hashlib` to do that. A `hash` function is a function that converts some input (in our case a string containing the title) into a string that is seemingly random. The same input will always return the same output, but slightly different inputs will return drastically different outputs. It is also impossible to go from a hash value to the original value, making it a one-way function.

```py
import hashlib

```

对于本章的实验，我们将简单地跳过任何失败的网站下载。为了确保我们这样做不会丢失太多信息，我们维护了一个简单的错误发生次数计数器。我们将抑制任何可能导致禁止下载的系统问题的错误。如果这个错误计数器太高，我们可以看看这些错误是什么，并尝试修复它们。例如，如果计算机没有互联网接入，所有 500 次下载都将失败，您可能应该在继续之前修复它！

```py
number_errors = 0

```

接下来，我们遍历每个故事，下载网站，并将结果保存到一个文件中:

```py
for title, url, score in stories:
    output_filename = hashlib.md5(url.encode()).hexdigest() 
    fullpath = os.path.join(data_folder, output_filename + ".txt")
    try: 
        response = requests.get(url) 
        data = response.text 
        with open(fullpath, 'w') as outf: 
            outf.write(data)
        print("Successfully completed {}".format(title))
    except Exception as e:
        number_errors += 1
        # You can use this to view the errors, if you are getting too many:
        # raise

```

如果在获取网站时出现错误，我们只需跳过这个网站，继续前进。这段代码将在大量网站上运行，这对于我们的应用程序来说已经足够好了，因为我们正在寻找一般趋势，而不是精确。

Note that sometimes you do care about getting 100 percent of responses, and you should adjust your code to accommodate more errors. Be warned though that there is a significant increase in effort required to create code the works reliably on data from the internet. The code to get those final 5 to 10 percent of websites will be significantly more complex.

在前面的代码中，我们只是捕捉发生的任何错误，记录错误并继续前进。

如果发现错误太多，请将打印(e)行改为只键入 raise。这将导致调用异常，允许您调试问题。

完成后，我们的`raw`子文件夹中会有一堆网站。查看这些页面后(在文本编辑器中打开创建的文件)，您可以看到内容在那里，但是有 HTML、JavaScript、CSS 代码以及其他内容。由于我们只对故事本身感兴趣，我们现在需要一种方法从这些不同的网站中提取这些信息。

# 提取内容

在我们获得原始数据后，我们需要找到每个数据中的故事。有几种复杂的算法可以做到这一点，也有一些简单的算法。我们将在这里坚持一个简单的方法，记住简单的算法足够好。这是数据挖掘的一部分——知道何时使用简单的算法来完成工作，而不是使用更复杂的算法来获得额外的性能。

首先，我们得到了`raw`子文件夹中每个文件名的列表:

```py
filenames = [os.path.join(data_folder, filename) for filename in os.listdir(data_folder)]

```

接下来，我们为将要提取的纯文本版本创建一个输出文件夹:

```py
text_output_folder = os.path.join(os.path.expanduser("~"), "Data", "websites", "textonly")

```

接下来，我们开发将从文件中提取文本的代码。我们将使用 lxml 库来解析 HTML 文件，因为它有一个很好的 HTML 解析器来处理一些格式错误的表达式。代码如下:

```py
from lxml import etree

```

提取文本的实际代码基于三个步骤:

1.  我们遍历 HTML 文件中的每个节点，并提取其中的文本。
2.  我们跳过任何 JavaScript、样式或注释节点，因为这不太可能包含我们感兴趣的信息。
3.  我们确保内容至少有 100 个字符。这是一个很好的基线，但为了获得更准确的结果，可以对其进行改进。

正如我们之前所说的，我们对脚本、风格或评论不感兴趣。因此，我们创建一个列表来忽略这些类型的节点。任何在此列表中具有类型的节点都不会被视为包含故事。代码如下:

```py
skip_node_types = ["script", "head", "style", etree.Comment]

```

我们现在将创建一个函数，将一个 HTML 文件解析成一个 lxml `etree`，然后我们将创建另一个函数，解析这个树寻找文本。第一个函数非常简单；只需打开文件，并使用 lxml 库对 HTML 文件的解析功能创建一个树。代码如下:

```py
parser = etree.HTMLParser()

def get_text_from_file(filename):
    with open(filename) as inf:
        html_tree = etree.parse(inf, parser) 
    return get_text_from_node(html_tree.getroot())

```

在该函数的最后一行，我们调用`getroot()`函数来获取树的根节点，而不是完整的`etree`。这允许我们编写我们的文本提取函数来接受任何节点，并因此编写一个递归函数。

该函数将在任何子节点上调用自己，从中提取文本，然后返回任何子节点文本的串联。

If the node where this function is passed doesn't have any child nodes, we just return the text from it. If it doesn't have any text, we just return an empty string. Note that we also check here for our third condition—that the text is at least 100 characters long.

用于检查文本长度是否至少为 100 个字符的代码如下:

```py
def get_text_from_node(node):
    if len(node) == 0: 
        # No children, just return text from this item
        if node.text: 
            return node.text 
        else:
            return ""
    else:
        # This node has children, return the text from it:
        results = (get_text_from_node(child)
                   for child in node
                   if child.tag not in skip_node_types)
    result = str.join("n", (r for r in results if len(r) > 1))
    if len(result) >= 100:
        return result
    else:
        return ""

```

此时，我们知道该节点有子节点，因此我们在每个子节点上递归调用该函数，然后在结果返回时加入它们。

返回行中的最后一个条件是停止返回空行(例如，当节点没有子节点也没有文本时)。我们还使用了一个生成器，它只在需要的时候抓取文本数据，即最终的返回语句，而不是创建许多子列表，从而使代码更加高效。

我们现在可以在所有原始 HTML 页面上运行这段代码，方法是遍历它们，在每个页面上调用文本提取函数，并将结果保存到纯文本子文件夹中:

```py
for filename in os.listdir(data_folder):
    text = get_text_from_file(os.path.join(data_folder, filename)) 
    with open(os.path.join(text_output_folder, filename), 'w') as outf: 
        outf.write(text)

```

您可以通过打开纯文本子文件夹中的每个文件并检查其内容来手动评估结果。如果你发现太多的结果有非故事内容，尝试增加最小 100 个字符的限制。如果您仍然无法获得好的结果，或者您的应用需要更好的结果，请尝试*附录 A“下一步”中列出的方法。*

# 将新闻文章分组

本章的目的是通过将新闻文章聚集或分组来发现它们的趋势。为此，我们将使用 k-means 算法，这是一种最初于 1957 年开发的经典机器学习算法。

**聚类**是一种无监督的学习技术，我们经常使用聚类算法来探索数据。我们的数据集包含大约 500 个故事，单独检查每个故事将是相当困难的。使用聚类可以让我们将相似的故事组合在一起，并且我们可以独立探索每个聚类中的主题。

We use clustering techniques when we don't have a clear set of target classes for our data. In that sense, clustering algorithms have little direction in their learning. They learn according to some function, regardless of the underlying meaning of the data.

出于这个原因，选择好的功能是至关重要的。在监督学习中，如果选择了较差的特征，学习算法可以选择不使用那些特征。例如，支持向量机不会给分类中没有用的特征太多的权重。然而，通过聚类，所有的特征都被用在最终的结果中——即使这些特征没有给我们提供我们想要的答案。

在对真实数据执行聚类分析时，了解哪种特性适合您的场景总是一个好主意。在本章中，我们将使用单词包模型。我们正在寻找基于主题的组，因此我们将使用基于主题的特性来建模文档。我们知道这些特性之所以有效，是因为其他人在我们问题的监督版本中所做的工作。相比之下，如果我们要执行基于作者身份的聚类，我们将使用诸如在[第 9 章](08.html) *【作者身份属性】*实验中发现的特征。

# k-均值算法

k-means 聚类算法使用迭代过程找到最能代表数据的质心。该算法从一组预定义的质心开始，这些质心通常是从训练数据中获取的数据点。k-means 中的 **k** 是要寻找的质心数量以及算法会找到多少个簇。例如，将 k 设置为 3 将在数据集中找到三个聚类。

k-means 有两个阶段:**分配**和**更新**。它们解释如下:

*   在**分配**步骤中，我们为数据集中的每个样本设置一个标签，将其链接到最近的质心。对于最接近质心 1 的每个样本，我们指定标签 1。对于距离质心 2 最近的每个样本，我们为 k 个质心中的每一个指定一个标签 2，依此类推。这些标签形成了簇，所以我们说标签为 1 的每个数据点都在簇 1 中(仅在此时，因为分配会随着算法的运行而改变)。
*   在**更新**步骤中，我们获取每个聚类并计算质心，质心是该聚类中所有样本的平均值。

然后，该算法在分配步骤和更新步骤之间迭代；每次更新步骤发生时，每个质心都会少量移动。这将导致赋值发生轻微变化，导致质心在下一次迭代中发生少量移动。这样重复，直到达到某个停止标准。

It is common to stop after a certain number of iterations, or when the total movement of the centroids is very low. The algorithm can also complete in some scenarios, which means that the clusters are stable—the assignments do not change and neither do the centroids.

在下图中，k-means 是对随机创建的数据集执行的，但数据中有三个聚类。恒星代表质心的起始位置，质心是通过从数据集中随机选取一个样本而随机选择的。在 k-means 算法的 5 次迭代中，质心移动到由三角形表示的位置。

![](img/B06162_10_01.png)

k-means 算法因其数学特性和历史意义而引人入胜。这是一个(大致)只有单一参数的算法，而且相当有效，使用频率很高，甚至在发现 50 多年后也是如此。

scikit-learn 中有一个 k-means 算法，我们从 scikit-learn 中的`cluster`模块导入:

```py
from sklearn.cluster import KMeans

```

我们还导入了`CountVectorizer`类的近亲`TfidfVectorizer`。该矢量器根据每个术语出现在多少个文档中，对每个术语的计数应用权重，使用公式:tf / log(df)，其中 tf 是术语的频率(它在当前文档中出现的次数)，df 是术语的文档频率(它出现在我们的语料库中的文档数量)。许多文档中出现的术语权重较低(通过将值除以其出现的文档数量的日志)。对于许多文本挖掘应用程序，使用这种类型的加权方案可以非常可靠地提高性能。代码如下:

```py
from sklearn.feature_extraction.text import TfidfVectorizer

```

然后，我们为我们的分析建立管道。这有两个步骤。第一个是应用我们的矢量器，第二个是应用我们的 k-means 算法。代码如下:

```py
from sklearn.pipeline import Pipeline
n_clusters = 10 
pipeline = Pipeline([('feature_extraction', TfidfVectorizer(max_df=0.4)),
                                     ('clusterer', KMeans(n_clusters=n_clusters)) ])

```

`max_df`参数设置为 0.4 的低值，表示忽略超过 40%的文档中出现的任何单词。这个参数对于移除那些本身没有多少基于主题的意义的虚词来说是非常宝贵的。

Removing any word that occurs in more than 40 percent of documents will remove function words, making this type of preprocessing quite useless for the work we saw in [Chapter 9](08.html)*, Authorship Attribution*.

```py
documents = [open(os.path.join(text_output_folder, filename)).read()
             for filename in os.listdir(text_output_folder)]

```

然后我们拟合并预测这条管道。到目前为止，对于分类任务，我们在本书中已经多次遵循这个过程，但是这里有一个区别——我们没有将数据集的目标类交给 fit 函数。这就是为什么这是一个无监督的学习任务！代码如下:

```py
pipeline.fit(documents)
labels = pipeline.predict(documents)

```

标签变量现在包含每个样本的聚类数。具有相同标签的样本被称为属于同一聚类。应该注意的是，聚类标签本身是没有意义的:聚类 1 和 2 不比聚类 1 和 3 更相似。

我们可以使用`Counter`类看到每个集群中放置了多少个样本:

```py
from collections import Counter
c = Counter(labels) 
for cluster_number in range(n_clusters): 
    print("Cluster {} contains {} samples".format(cluster_number, c[cluster_number]))

```

```py
Cluster 0 contains 1 samples
Cluster 1 contains 2 samples
Cluster 2 contains 439 samples
Cluster 3 contains 1 samples
Cluster 4 contains 2 samples
Cluster 5 contains 3 samples
Cluster 6 contains 27 samples
Cluster 7 contains 2 samples
Cluster 8 contains 12 samples
Cluster 9 contains 1 samples

```

Many of the results (keeping in mind that your dataset will be quite different to mine) consist of a large cluster with the majority of instances, several medium clusters, and some clusters with only one or two instances. This imbalance is quite normal in many clustering applications.

# 评估结果

聚类主要是探索性分析，因此很难有效地评估聚类算法的结果。一种简单的方法是根据算法试图学习的标准来评估算法。

If you have a test set, you can evaluate clustering against it. For more details, visit [http://nlp.standford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html](http://nlp.standford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html)

在 k-means 算法的情况下，它在开发质心时使用的标准是最小化从每个样本到其最近质心的距离。这称为算法的惯性，可以从任何调用了 fit 的 KMeans 实例中检索:

```py
pipeline.named_steps['clusterer'].inertia_

```

我的数据集上的结果是 343.94。不幸的是，这个值本身是毫无意义的，但是我们可以用它来确定我们应该使用多少个集群。在上例中，我们将`n_clusters`设置为 10，但这是最佳值吗？下面的代码用`n_clusters`的每个值从 2 到 20 运行 k-means 算法 10 次，需要一些时间来完成大量的运行。

对于每次运行，它都会记录结果的惯性。

您可能会注意到以下代码，我们不使用管道；相反，我们分开了步骤。我们只从文本文档中为`n_clusters`的每个值创建一次 X 矩阵，以(大幅)提高该代码的速度。

```py
inertia_scores = [] 
n_cluster_values = list(range(2, 20)) 
for n_clusters in n_cluster_values: 
    cur_inertia_scores = [] 
    X = TfidfVectorizer(max_df=0.4).fit_transform(documents) 
 for i in range(10): 
        km = KMeans(n_clusters=n_clusters).fit(X) 
        cur_inertia_scores.append(km.inertia_) 
    inertia_scores.append(cur_inertia_scores)

```

`inertia_scores`变量现在包含 2 到 20 之间的每个 n_clusters 值的惯性分数列表。我们可以将此绘制成图，以了解该值如何与`n_clusters`相互作用:

```py
%matplotlib inline
from matplotlib import pyplot as plt
inertia_means = np.mean(inertia_scores, axis=1)
inertia_stderr = np.std(inertia_scores, axis=1)
fig = plt.figure(figsize=(40,20))
plt.errorbar(n_cluster_values, inertia_means, inertia_stderr, color='green')
plt.show()

```

![](img/B06162_10_02.png)

总的来说，惯性的值应该随着集群数量的增加而减少，这可以从这些结果中大致看出。值 6 到 7 之间的增加只是由于选择质心的随机性，这直接影响最终结果的好坏。尽管如此，总的趋势是(就我的数据而言；您的结果可能会有所不同),大约 6 个集群是惯性上一次发生的重大改进。

在这一点之后，对惯性只有轻微的改进，尽管像这样模糊的标准很难具体说明。寻找这种类型的图案被称为肘状规则，因为我们在图中寻找肘状弯曲。一些数据集有更明显的肘部，但是这个特征甚至不能保证出现(一些图形可能是平滑的！).

基于此分析，我们将`n_clusters`设置为 6，然后重新运行算法:

```py
n_clusters = 6 
pipeline = Pipeline([('feature_extraction', TfidfVectorizer(max_df=0.4)),
                     ('clusterer', KMeans(n_clusters=n_clusters)) ])
pipeline.fit(documents) 
labels = pipeline.predict(documents)

```

# 从聚类中提取主题信息

现在，我们将目光放在集群上，试图发现每个集群中的主题。

我们首先从特征提取步骤中提取术语列表:

```py
terms = pipeline.named_steps['feature_extraction'].get_feature_names()

```

我们还设置了另一个计数器来计算每个类的大小:

```py
c = Counter(labels)

```

迭代每个集群，我们像以前一样打印集群的大小。

It is important to keep in mind the sizes of the clusters when evaluating the results—some of the clusters will only have one sample, and are therefore not indicative of a general trend.

接下来(仍然在循环中)，我们迭代这个集群最重要的术语。为此，我们从质心获取五个最大的值，这是通过找到质心本身具有最高值的特征而获得的。

```py
for cluster_number in range(n_clusters): 
    print("Cluster {} contains {} samples".format(cluster_number, c[cluster_number]))
    print(" Most important terms")
    centroid = pipeline.named_steps['clusterer'].cluster_centers_[cluster_number]
    most_important = centroid.argsort()
    for i in range(5):
        term_index = most_important[-(i+1)]
        print(" {0}) {1} (score: {2:.4f})".format(i+1, terms[term_index], centroid[term_index]))

```

结果很能说明当前的趋势。在我的结果(2017 年 1 月获得)中，这些群组对应于卫生事务、中东紧张局势、朝鲜紧张局势和俄罗斯事务。这些是这个时候新闻经常出现的主要话题——尽管这几年来几乎没有改变！

你可能会注意到一些没有提供太多价值的词出现在顶部，比如*你、她*和*T4 先生【这些虚词对于作者身份分析来说很棒——正如我们在[第 9 章](08.html)、*作者身份归属*中看到的，但是对于主题分析来说一般不是很好。将虚词列表传递到我们上面管道中的 **TfidfVectorizer** 的`stop_words`参数中将忽略那些词。以下是构建这种管道的更新代码:*

```py
function_words = [... list from Chapter 9 ...]

pipeline = Pipeline([('feature_extraction', TfidfVectorizer(max_df=0.4, stop_words=function_words)),
                     ('clusterer', KMeans(n_clusters=n_clusters)) ])

```

# 使用聚类算法作为转换器

顺便提一下，k-means 算法(以及任何聚类算法)的一个有趣的特性是，您可以将其用于特征约简。有很多方法可以减少特征的数量(或者创建新的特征来嵌入数据集)，比如**主成分分析**、**潜在语义索引**等等。这些算法的一个问题是它们通常需要大量的计算能力。

在前面的示例中，术语列表中有超过 14，000 个条目，这是一个相当大的数据集。我们的 k-means 算法将这些转化为仅仅六个聚类。然后，我们可以通过将到每个质心的距离作为一个要素来创建一个要素数量少得多的数据集。

为此，我们在 KMeans 实例上调用转换函数。我们的管道适合这个目的，因为它在最后有一个 k-means 实例:

```py
X = pipeline.transform(documents)

```

这将在管道的最后一步调用 transform 方法，这是 k-means 的一个实例。这导致矩阵具有六个特征，样本的数量与文档的长度相同。

然后，您可以对结果执行自己的第二级聚类，或者如果您有目标值，则使用它进行分类。一种可能的工作流程是使用监督数据执行一些特征选择，使用聚类将特征数量减少到更易于管理的数量，然后在分类算法(如支持向量机)中使用结果。

# 聚类集成

在[第 3 章](02.html) *【用决策树预测体育赢家】*中，我们研究了使用随机森林算法的分类集成，这是许多低质量的基于树的分类器的集成。集合也可以使用聚类算法来执行。这样做的一个关键原因是平滑算法多次运行的结果。正如我们之前看到的，运行 k-means 的结果是不同的，这取决于初始质心的选择。通过多次运行算法，然后合并结果，可以减少变化。

Ensembling also reduces the effects of choosing parameters on the final result. Most clustering algorithms are quite sensitive to the parameter values chosen for the algorithm. Choosing slightly different parameters results in different clusters.

# 证据积累

作为一个基本的集合，我们可以首先对数据进行多次聚类，并记录每次运行的标签。然后，我们记录每对样本在新矩阵中聚集在一起的次数。这就是**证据累积聚类** ( **EAC** )算法的本质。

东非共同体有两个主要步骤。

1.  第一步是使用较低级别的聚类算法(如 k-means)对数据进行多次聚类，并记录每次迭代中样本在同一聚类中的频率。这存储在一个**关联矩阵**中。
2.  第二步是对得到的共关联矩阵进行聚类分析，这是使用另一种称为层次聚类的聚类算法进行的。这有一个有趣的基于图论的属性，因为它在数学上与找到将所有节点链接在一起的树并移除弱链接是一样的。

我们可以通过迭代每个标签并记录两个样本具有相同标签的位置，从标签数组中创建一个联合矩阵。我们使用 SciPy 的`csr_matrix`，这是一种稀疏矩阵:

```py
from scipy.sparse import csr_matrix

```

我们的函数定义采用一组标签，然后记录每个匹配的行和列。我们在列表中做这些。稀疏矩阵通常只是记录非零值位置的一组列表，`csr_matrix`就是这种稀疏矩阵的一个例子。对于每对具有相同标签的样品，我们记录两个样品在列表中的位置:

```py
import numpy as np
def create_coassociation_matrix(labels):
    rows = [] 
    cols = []
    unique_labels = set(labels) 
    for label in unique_labels:
        indices = np.where(labels == label)[0]
        for index1 in indices:
            for index2 in indices:
                rows.append(index1)
                cols.append(index2)
    data = np.ones((len(rows),)) 
    return csr_matrix((data, (rows, cols)), dtype='float')

```

为了从标签中获得协关联矩阵，我们简单地调用这个函数:

```py
C = create_coassociation_matrix(labels)

```

从这里，我们可以将这些矩阵的多个实例加在一起。这使得我们可以将 k-means 的多次运行结果结合起来。打印出`C`(只需在你的 Jupyter 笔记本的一个新单元格中输入 C 并运行它)就会告诉你有多少单元格中有非零值。在我的例子中，大约一半的单元格中有值，因为我的聚类结果有一个大的聚类(聚类越均匀，非零值的数量越少)。

下一步涉及到共关联矩阵的层次聚类。我们将通过在这个矩阵上找到最小生成树并移除权重低于给定阈值的边来实现这一点。

在图论中，生成树是将所有节点连接在一起的图上的一组边。**最小生成树** (MST)就是总权重最低的生成树。对于我们的应用程序，图中的节点是来自数据集的样本，边缘权重是这两个样本聚集在一起的次数，即来自我们的协关联矩阵的值。

在下图中，显示了六个节点的图上的一个 MST。图中的节点可以连接到 MST 中的多个节点，只要所有节点都连接在一起。

![](img/B06162_10_03.png)

为了计算 MST，我们使用 SciPy 的`minimum_spanning_tree`函数，该函数可以在稀疏包中找到:

```py
from scipy.sparse.csgraph import minimum_spanning_tree

```

`mst`函数可以直接在我们的协关联函数返回的稀疏矩阵上调用:

```py
mst = minimum_spanning_tree(C)

```

然而，在我们的共关联矩阵 C 中，较高的值表示样本更经常地聚集在一起——一个相似性值。相比之下，`minimum_spanning_tree`将输入视为距离，分数越高则被扣分。出于这个原因，我们改为在协关联矩阵的逆矩阵上计算最小生成树:

```py
mst = minimum_spanning_tree(-C)

```

前一个函数的结果是一个与协关联矩阵大小相同的矩阵(行数和列数与我们数据集中的样本数相同)，只保留了 MST 中的边，其他的都被移除了。

然后，我们移除权重小于预定义阈值的任何节点。为此，我们迭代 MST 矩阵中的边，移除任何小于特定值的边。我们不能仅用联合矩阵中的一次迭代来测试这一点(值要么是 1，要么是 0，因此没有太多可使用的东西)。因此，我们将首先创建额外的标签，创建联合矩阵，然后将两个矩阵相加。代码如下:

```py
pipeline.fit(documents) 
labels2 = pipeline.predict(documents) 
C2 = create_coassociation_matrix(labels2) 
C_sum = (C + C2) / 2

```

然后，我们计算最大似然距离，并移除在这两个标签中没有出现的任何边缘:

```py
mst = minimum_spanning_tree(-C_sum) 
mst.data[mst.data > -1] = 0

```

我们想要切断的阈值是不在两个聚类中的任何边，也就是说，值为 1。然而，当我们否定联合矩阵时，我们也不得不否定阈值。

最后，我们找到所有连接的组件，这是一种简单的方法，在我们移除低权重的边之后，找到仍然通过边连接的所有样本。第一个返回值是连接组件的数量(即簇的数量)，第二个是每个样本的标签。代码如下:

```py
from scipy.sparse.csgraph import connected_components 
number_of_clusters, labels = connected_components(mst)

```

在我的数据集中，我获得了八个聚类，聚类与之前大致相同。这并不奇怪，因为我们只使用了两次 k-means 迭代；使用更多的 k-means 迭代(正如我们在下一节中所做的那样)将导致更多的方差。

# 它是如何工作的

在 k-means 算法中，每个特征的使用都不考虑其权重。本质上，所有的特征都被假定在同一尺度上。我们在[第 2 章](09.html) *中看到了不缩放特征的问题，用 scikit-learn 估计器进行分类*。其结果是，k-means 正在寻找圆形集群，如下图所示:

![](img/B06162_10_04.png)

椭圆形星团也可以通过 k-means 发现。这种分离通常不太平滑，但是可以通过特征缩放使其更容易。这种形状集群的示例如下:

![](img/B06162_10_05.png)

正如我们在前面的截图中看到的，并不是所有的集群都有这种形状。蓝色簇是圆形的，是 k-means 非常擅长拾取的类型。红色的星团是一个椭圆。k-means 算法可以通过一些特征缩放来拾取这种形状的聚类。

下面的第三个簇不是偶数凸起的——这是一个奇怪的形状，k-means 将很难发现，但仍然会被认为是一个*簇*，至少对大多数人来说是这样的:

![](img/B06162_10_06.png)

聚类分析是一项艰巨的任务，大部分困难只是试图定义问题。很多人凭直觉理解它的含义，但试图用精确的术语来定义它(机器学习所必需的)是非常困难的。就连人们也经常对这个术语产生分歧！

EAC 算法的工作原理是将特征重新映射到一个新的空间，本质上是将 k-means 算法的每一次运行都转换成一个转换器，使用的原理与上一节使用 k-means 进行特征约简的原理相同。然而，在这种情况下，我们只使用实际的标签，而不使用到每个质心的距离。这是记录在联合关联矩阵中的数据。

结果是 EAC 现在只关心事物之间的接近程度，而不关心它们在原始特征空间中的放置方式。未缩放功能仍然存在问题。要素缩放很重要，无论如何都应该进行(我们在本章中使用 tf **-** idf 进行了缩放，这导致要素值具有相同的缩放比例)。

我们在[第 9 章](08.html) *【作者属性】*中看到了类似类型的转换，通过在支持向量机中使用核。这些转换非常强大，对于复杂的数据集应该牢记在心。然而，将数据重新映射到新特征空间的算法并不需要复杂，正如您将在 EAC 算法中看到的那样。

# 履行

将所有这些放在一起，我们现在可以创建一个适合 scikit-learn 界面的聚类算法，它执行 EAC 中的所有步骤。首先，我们使用 scikit-learn 的 *ClusterMixin* 创建类的基本结构。

我们的参数是在第一步(创建联合关联矩阵)中要执行的 k-means 聚类的数量、要截断的阈值以及要在每个 k-means 聚类中找到的聚类的数量。我们设置了一个 n_clusters 的范围，以便在我们的 k-means 迭代中获得大量的方差。一般来说，用系综术语来说，方差是好事；没有它，解决方案不会比单个集群更好(也就是说，高方差并不表明集成会更好)。

我将首先介绍整个课程，然后概述每个功能:

```py
from sklearn.base import BaseEstimator, ClusterMixin
class EAC(BaseEstimator, ClusterMixin):
    def __init__(self, n_clusterings=10, cut_threshold=0.5, n_clusters_range=(3, 10)): 
        self.n_clusterings = n_clusterings
        self.cut_threshold = cut_threshold
        self.n_clusters_range = n_clusters_range

    def fit(self, X, y=None):
        C = sum((create_coassociation_matrix(self._single_clustering(X))
                 for i in range(self.n_clusterings)))
        mst = minimum_spanning_tree(-C)
        mst.data[mst.data > -self.cut_threshold] = 0
        mst.eliminate_zeros()
        self.n_components, self.labels_ = connected_components(mst)
        return self

    def _single_clustering(self, X):
        n_clusters = np.random.randint(*self.n_clusters_range)
        km = KMeans(n_clusters=n_clusters)
        return km.fit_predict(X)

    def fit_predict(self, X):
        self.fit(X)
        return self.labels_

```

`fit`函数的目标是多次执行 k-means 聚类，合并联合矩阵，然后通过找到 MST 将其拆分，正如我们之前在 EAC 示例中看到的那样。然后，我们使用 k-means 执行低级聚类，并对每次迭代得到的共关联矩阵求和。我们在生成器中这样做是为了节省内存，只在需要时创建联合矩阵。在这个生成器的每次迭代中，我们用数据集创建一个新的 k-means 运行，然后为它创建协关联矩阵。我们用`sum`把这些加在一起。

和以前一样，我们创建 MST，移除任何小于给定阈值的边(如前所述正确地否定值)，并找到连接的组件。与 scikit-learn 中的任何 fit 函数一样，我们需要返回 self，以便类在管道中有效地工作。

`_single_clustering`函数被设计为对我们的数据执行 k 均值的单次迭代，然后返回预测的标签。为此，我们使用 NumPy 的`randint`函数和设置可能值范围的`n_clusters_range`参数随机选择一些要查找的聚类。然后，我们使用 k 均值对数据集进行聚类和预测。这里的返回值将是来自 k 均值的标签。

最后，`fit_predict`函数简单地调用 fit，然后返回文档的标签。

我们现在可以通过像以前一样设置一个管道并使用 EAC 在我们以前使用 KMeans 实例作为管道的最后阶段来在我们以前的代码上运行它。代码如下:

```py
pipeline = Pipeline([('feature_extraction', TfidfVectorizer(max_df=0.4)),
                     ('clusterer', EAC()) ])

```

# 在线学习

在某些情况下，在我们开始学习之前，我们没有训练所需的所有数据。有时，我们在等待新数据的到来，可能是我们拥有的数据太大，无法放入内存，或者我们在做出预测后收到了额外的数据。在这种情况下，随着时间的推移，在线学习是培训模型的一种选择。

**在线学习**是随着新数据的到来，模型的增量更新。支持在线学习的算法可以一次对一个或几个样本进行训练，并随着新样本的到来而更新。相比之下，非 T2 在线的算法需要一次访问所有数据。标准的 k-means 算法是这样的，我们到目前为止在本书中看到的大多数算法也是这样的。

在线版本的算法有一种方法，只需几个样本就可以部分更新模型。神经网络是以在线方式工作的算法的标准例子。当给神经网络一个新的样本时，网络中的权重根据学习速率来更新，该学习速率通常是非常小的值，例如 0.01。这意味着任何一个单独的实例只对模型做了一个小的(但有希望改进的)改变。

神经网络也可以以批处理模式进行训练，在这种模式下，一组样本同时给出，训练在一个步骤中完成。算法在批处理模式下通常更快，但使用更多内存。

同样，我们可以在单个或小批量样本后稍微更新 k-means 质心。为此，我们在 k-means 算法的更新步骤中对质心移动应用学习速率。假设样本是从群体中随机选择的，质心应该倾向于向它们在标准、离线和 k-means 算法中的位置移动。

在线学习与基于流的学习相关；然而，有一些重要的区别。在线学习能够在模型中使用较旧的样本后对其进行审查，而基于流的机器学习算法通常只能通过一次，也就是说，有一次机会查看每个样本。

# 履行

scikit-learn 包包含允许在线学习的**迷你批处理工具**算法。这个类实现了一个 partial_fit 函数，该函数获取一组样本并更新模型。相比之下，调用 fit()将删除任何以前的训练，并且只在新数据上重新调整模型。

MiniBatchKMeans 遵循与 scikit-learn 中其他算法相同的聚类格式，因此创建和使用它与其他算法非常相似。

该算法的工作原理是对它所看到的所有点进行流式平均。为了计算这一点，我们只需要跟踪两个值，即所有观察点的当前总和和观察点的数量。然后，我们可以使用这些信息，结合一组新的点，在更新步骤中计算新的平均值。

因此，我们可以通过使用`TfIDFVectorizer`从我们的数据集提取特征来创建矩阵 X，然后从中进行采样以增量更新我们的模型。代码如下:

```py
vec = TfidfVectorizer(max_df=0.4) 
X = vec.fit_transform(documents)

```

然后，我们导入迷你批次并创建其实例:

```py
from sklearn.cluster import MiniBatchKMeans 
mbkm = MiniBatchKMeans(random_state=14, n_clusters=3)

```

接下来，我们将从我们的 X 矩阵中随机取样，以模拟来自外部来源的数据。每次我们得到一些数据，我们就更新模型:

```py
batch_size = 10 
for iteration in range(int(X.shape[0] / batch_size)): 
    start = batch_size * iteration 
    end = batch_size * (iteration + 1) 
    mbkm.partial_fit(X[start:end])

```

然后，我们可以通过要求实例预测来获取原始数据集的标签:

```py
labels = mbkm.predict(X)

```

然而，在这个阶段，我们不能在管道中这样做，因为`TfidfVectorizer`不是在线算法。为了克服这一点，我们使用`HashingVectorizer`。`HashingVectorizer`类是散列算法的一个聪明的应用，它大大减少了计算单词包模型的内存。我们只记录这些名称的散列，而不是记录特征名称，例如文档中的单词。这允许我们在查看数据集之前就知道我们的特征，因为它是所有可能散列的集合。这是一个非常大的数字，通常为 2 <sup>18</sup> 的数量级。使用稀疏矩阵，我们甚至可以非常容易地存储和计算这种大小的矩阵，因为矩阵的很大一部分将具有值 0。

目前，`Pipeline`类不允许在在线学习中使用。不同的应用程序有一些细微差别，这意味着没有一个明显的一刀切的方法可以实现。相反，我们可以创建自己的管道子类，允许我们将其用于在线学习。我们首先从管道中派生我们的类，因为我们只需要实现一个函数:

```py
class PartialFitPipeline(Pipeline):
    def partial_fit(self, X, y=None):
        Xt = X
        for name, transform in self.steps[:-1]:
            Xt = transform.transform(Xt)
        return self.steps[-1][1].partial_fit(Xt, y=y)

```

我们唯一需要实现的函数是`partial_fit`函数，通过首先做所有的变换步骤，然后在最后一步调用部分拟合(应该是分类器或者聚类算法)来执行。所有其他函数都与普通的管道类相同，所以我们(通过类继承)引用这些函数。

我们现在可以创建一个管道，在我们的`HashingVectorizer`旁边使用我们的`MiniBatchKMeans`进行在线学习。除了使用我们的新类`PartialFitPipeline`和`HashingVectorizer`，这与本章其余部分使用的过程相同，只是我们一次只适合几个文档。代码如下:

```py
from sklearn.feature_extraction.text import HashingVectorizer

pipeline = PartialFitPipeline([('feature_extraction', HashingVectorizer()),
                               ('clusterer', MiniBatchKMeans(random_state=14, n_clusters=3)) ])
batch_size = 10 
for iteration in range(int(len(documents) / batch_size)): 
    start = batch_size * iteration end = batch_size * (iteration + 1)
    pipeline.partial_fit(documents[start:end]) 
labels = pipeline.predict(documents)

```

这种方法有一些缺点。首先，我们不能轻易找出哪些词对每个聚类最重要。我们可以通过拟合另一个`CountVectorizer`并获取每个单词的散列来解决这个问题。然后我们通过散列而不是单词来查找值。这有点麻烦，并且会抵消使用哈希矢量器带来的内存增益。此外，我们不能使用我们之前使用的`max_df`参数，因为它要求我们知道特征的含义并随着时间的推移对它们进行计数。

We also can't use tf-idf weighting when performing training online. It would be possible to approximate this and apply such weighting, but again this is a cumbersome approach. `HashingVectorizer` is still a very useful algorithm and a great use of hashing algorithms.

# 摘要

在这一章中，我们研究了聚类，这是一种无监督的学习方法。我们使用无监督学习来探索数据，而不是为了分类和预测。在这里的实验中，我们在 reddit 上找到的新闻条目没有主题，所以我们无法进行分类。我们使用 k-means 聚类将这些新闻故事组合在一起，以在数据中找到共同的主题和趋势。

在从 reddit 提取数据时，我们必须从任意网站提取数据。这是通过寻找大的文本片段来实现的，而不是完全成熟的机器学习方法。对于这项任务，有一些有趣的机器学习方法可以改进这些结果。在这本书的附录中，我为每一章列出了超越这一章的范围和改进结果的途径。这包括参考其他信息来源和每章中更难应用的方法。

我们还研究了一个简单的集成算法 EAC。集成通常是处理结果差异的好方法，尤其是如果你不知道如何选择好的参数(聚类总是很难)。

最后，我们介绍了在线学习。这是通向更大的学习练习的大门，包括大数据，这将在本书的最后两章中讨论。这些最终实验相当大，需要管理数据以及从中学习模型。

作为本章工作的扩展，尝试将 EAC 实现为在线学习算法。这不是一个琐碎的任务，将涉及到一些关于当算法更新时应该发生什么的思考。另一个扩展是从更多的数据源(如其他子分类或直接从新闻网站或博客)收集更多的数据，并寻找总体趋势。

在下一章，我们将远离无监督学习，回到分类。我们将研究深度学习，这是一种基于复杂神经网络的分类方法。