# 用 scikit 学习估计量进行分类

scikit-learn 库是一个数据挖掘算法的集合，用 Python 编写并使用 a .该库允许用户轻松尝试不同的算法，并利用标准工具进行有效的测试和参数搜索。scikit-learn 中有许多算法和实用程序，包括许多现代机器学习中常用的算法。

在本章中，我们将重点讨论如何为运行数据挖掘过程建立一个良好的框架。我们将在后面的章节中使用这个框架，重点是在这些情况下使用的应用程序和技术。

本章介绍的关键概念如下:

*   **估值器:**这是执行分类、聚类和回归
*   **变压器**:这是进行预处理和数据变更
*   **管道**:这是把你的工作流程组合成一个可复制的格式

# scikit-learn 估计器

**估计器**允许算法的标准化实现和测试一个通用的轻量级接口，供分类器遵循。通过使用这个接口，我们可以将这些工具应用于任意分类器，而无需担心算法如何工作。

估计器必须具有以下两个重要功能:

*   `fit()`:该功能执行算法的训练——设置内部参数的值。`fit()`获取两个输入，训练样本数据集和这些样本对应的类。
*   `predict()`:这是我们作为唯一输入提供的测试样本的类别。该函数返回一个带有每个输入测试样本预测的`NumPy`数组。

大多数 scikit-learn 估计器使用`NumPy`数组或相关格式进行输入和输出。然而，这是惯例，不需要使用接口。

在 scikit-learn 和其他使用相同接口的开源项目中实现了许多评估器。这些(SVM)，随机森林。我们将在后面的章节中使用这些算法中的许多
。在本章中，我们将使用最近邻
算法。

For this chapter, you will need to install a new library called `matplotlib`. The easiest way to install it is to use `pip3`, as you did in [Chapter 1](00.html), *Getting Started with Data Mining*, to install scikit-learn:
`**$pip3 install matplotlib**`
If you have `matplotlib`, seek the official installation instructions at:
[http://matplotlib.org/users/installing.html](http://matplotlib.org/users/installing.html%22http://matplotlib.org/users/installing.html)

# 最近的邻居

最近邻算法是我们的新样本。我们提取了最相似的样本
并预测了这些邻近样本中的大多数所具有的相同类别。这种投票通常只是简单的计数，尽管确实存在更复杂的方法，如加权投票。

作为下图中的一个例子，我们希望根据三角形更像哪个类来预测三角形的类(这里用相似的物体靠得更近来表示)。我们寻找三个最近的邻居，即画出的圆内的两个菱形和一个正方形。钻石比圆多，因此三角形的预测类别是钻石:

![](img/image_02_001-1.jpg)

最近邻用于几乎任何数据集，然而，计算所有样本对之间的距离在计算上可能很昂贵。例如，如果数据集中有十个样本，则需要计算 45 个唯一距离。然而，如果有 1000 个样本，就有近 50 万个！存在各种方法来提高这种速度，例如使用树结构进行距离计算。其中一些算法可能相当复杂，但谢天谢地，scikit-learn 已经实现了一个版本，使我们能够在更大的数据集上进行分类。因为这些树结构是 scikit-learn 中的默认结构，所以我们不需要配置任何东西来使用它。

最近邻在**基于分类的数据集**中表现不佳，具有分类特征，应该使用另一种算法来代替。最近邻的问题是由于分类值的差异比较困难，最好留给一个算法来衡量每个特征的重要性。比较分类特征可以通过一些距离度量或预处理步骤来完成，例如我们在后面章节中使用的一种热编码。为任务选择正确的算法是数据挖掘中的难题之一，通常测试一组算法并查看哪一个在您的任务中表现最佳是最容易的。

# 距离度量

数据挖掘中的一个关键基础概念是**距离**。如果有两个样本，需要回答*这样的问题，这两个样本比另外两个更相似吗？*回答这样的问题对于数据挖掘练习的结果很重要。

最常用的是**欧氏**距离，也就是两个物体之间的*现实世界*距离。如果你在图上画出点并用尺子测量距离，结果将是欧几里得距离。

A little more formally, the Euclidean distances between points a and b is the square root of the sum of the squared distances for each feature.

欧几里德距离是直观的，但如果某些要素的值大于 0，则精度较差，这就是所谓的稀疏矩阵。

还有其他正在使用的距离指标；两个常用的是曼哈顿距离和余弦距离。

The **Manhattan** distance is the sum of the absolute differences in each feature (with no use of square distances).

直觉上，我们可以将曼哈顿距离想象为一个鲁克棋子(也称为城堡)的移动次数，如果它被限制为一次移动一个方块的话。虽然如果某些要素比其他要素具有更大的值，曼哈顿距离确实会受到影响，但如果仅限于一次移动一个正方形，其影响不会像欧几里德点那样显著。虽然如果某些要素的值比其他要素大，曼哈顿距离确实会受到影响，但这种影响并不像欧几里得那样显著。

The **Cosine** distance is better suited to cases where some features are larger than others and when there are lots of zeros in the dataset.

直观地说，我们从原点到每个样本画一条线，并测量这些线之间的角度。我们可以观察到下图中算法之间的差异:

![](img/image_02_002-1.jpg)

在本例中，每个灰色圆圈与白色圆圈的距离完全相同。在(a)中，距离是欧几里得的，因此，类似的距离适合于一个圆。这个距离可以用尺子测量。在(b)中，距离是曼哈顿，也称为城市街区。我们通过移动行和列来计算距离，就像国际象棋中的车(城堡)移动一样。最后，在(c)中，我们有余弦距离，它是通过计算从样本绘制的直线与矢量之间的角度来测量的，忽略了直线的实际长度。

The distance metric chosen can have a large impact on the final performance.

例如，如果你有许多特征，随机样本之间的欧氏距离就会收敛(由于著名的*维数灾难*)。高维欧氏距离很难比较样本，因为距离总是几乎相同！

在这些情况下，曼哈顿距离可以更稳定，但是如果一些特征具有非常大的值，这可以*否决*其他特征中的大量相似性。例如，如果要素 A 的值介于 1 和 2 之间，而另一个要素 B 的值介于 1000 和 2000 之间，在这种情况下，要素 A 不太可能对结果产生任何影响。这个问题可以通过归一化来解决，归一化使得曼哈顿(和欧几里德)距离在不同的特征下更加可靠，我们将在本章后面看到。

最后，余弦距离是比较具有许多特征的项目的一个很好的度量，但是它丢弃了一些关于向量长度的信息，这在一些应用中是有用的。由于文本挖掘中固有的大量特征，我们经常在文本挖掘中使用余弦距离(参见[第 6 章](05.html)、*使用朴素贝叶斯的社交媒体洞察*)。

Ultimately, either a theoretical approach is needed to determine which distance method is needed, or an empirical evaluation is needed to see which performed more effectively. I prefer the empirical approach, but either approach can yield good results.

对于这一章，我们将继续使用欧几里德距离，在后面的章节中使用其他度量。如果你想做实验，那么试着把度量标准设置为曼哈顿，看看这会对结果产生什么影响。

# 正在加载数据集

数据集`Ionosphere`，其中高频天线。这些天线的目的是确定电离层和高层大气区域是否有结构。我们认为有结构的阅读是好的，而没有结构的阅读被认为是坏的。该应用程序的目的是构建一个数据挖掘分类器，可以确定图像是好还是坏。

![](img/image_02_003.png)

(Image Credit: https://www.flickr.com/photos/geckzilla/16149273389/)

您可以为不同的数据挖掘应用程序下载此数据集。前往[http://archive.ics.uci.edu/ml/datasets/Ionosphere](http://archive.ics.uci.edu/ml/datasets/Ionosphere)点击数据文件夹。将`ionosphere.data`和`ionosphere.names`文件下载到电脑的文件夹中。对于这个例子，我假设您已经将数据集放在了您的`home`文件夹中一个名为`Data`的目录中。您可以将数据放在另一个文件夹中，只需确保更新您的数据文件夹(在这里，以及所有其他章节中)。

The location of your home folder depends on your operating system. For Windows, it is usually at `C:Documents` and `Settingsusername`. For Mac or Linux machines, it is usually at `/home/username`. You can get your home folder by running this python code inside a Jupyter Notebook:

```py
import os print(os.path.expanduser("~"))

```

对于数据集中的每一行，有 35 个值。前 34 个是从 17 个天线获得的测量值(每个天线两个值)。最后一个不是‘g’就是‘b’；分别代表好和坏。

启动 Jupyter 笔记本服务器，创建一个名为电离层最近邻居的新笔记本。首先，我们加载代码所需的`NumPy`和`csv`库，并设置代码所需的数据文件名。

```py
import numpy as np 
import csv 
data_filename = "data/ionosphere.data"

```

然后我们创建`X`和`y` `NumPy`数组来存储数据集。这些数组的大小是从数据集中知道的。如果您不知道未来数据集的大小，请不要担心——我们将在未来的章节中使用其他方法加载数据集，并且您不需要事先知道这个大小:

```py
X = np.zeros((351, 34), dtype='float') 
y = np.zeros((351,), dtype='bool')

```

数据集采用**逗号分隔值** ( **CSV** )格式，这是数据集的常用格式。我们将使用`csv`模块加载该文件。导入它并设置一个`csv`读取器对象，然后在文件中循环，为数据集的每一行在`X`中设置适当的行，在`y`中设置类值:

```py
with open(data_filename, 'r') as input_file: 
    reader = csv.reader(input_file) 
    for i, row in enumerate(reader): 
        # Get the data, converting each item to a float 
        data = [float(datum) for datum in row[:-1]] 
        # Set the appropriate row in our dataset 
        X[i] = data 
        # 1 if the class is 'g', 0 otherwise 
        y[i] = row[-1] == 'g'

```

我们现在在`X`中有一个样本和特征的数据集，在`y`中有相应的类，就像我们在[第 1 章](00.html)的分类示例中所做的那样，数据挖掘入门。

首先，尝试将第 1 章、*中的 OneR 算法应用到这个数据集。它不会很好地工作，因为这个数据集中的信息分散在某些特征的相关性中。OneR 只对单个特征的值感兴趣，不能很好地提取更复杂数据集的信息。包括最近邻在内的其他算法会合并来自多个要素的信息，使其适用于更多场景。缺点是它们的计算成本通常更高。*

# 迈向标准工作流程

评估者 scikit-learn 有两个和`predict()`。我们在测试集上使用
`predict()`方法训练算法。我们使用我们测试设备上的`predict()`方法对其进行评估。

1.  首先，我们需要创建这些训练和测试集。和以前一样，导入并运行`train_test_split`功能:

```py
from sklearn.cross_validation import train_test_split 
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=14)

```

2.  然后，我们导入`nearest neighbor`类并为其创建一个实例。我们暂时将参数保留为默认值，并将在本章稍后测试其他值。默认情况下，算法将选择五个最近的邻居来预测测试样本的类别:

```py
from sklearn.neighbors import KNeighborsClassifier estimator = KNeighborsClassifier()

```

3.  在创建我们的`estimator`之后，我们必须将它放在我们的训练数据集中。对于`nearest neighbor`类，这个训练步骤只是记录我们的数据集，允许我们通过将一个新数据点与训练数据集进行比较来找到该数据点的最近邻居:

```py
estimator.fit(X_train, y_train)

```

4.  然后，我们使用测试集训练算法，并使用测试集进行评估:

```py
y_predicted = estimator.predict(X_test) 
accuracy = np.mean(y_test == y_predicted) * 100     
print("The accuracy is {0:.1f}%".format(accuracy))

```

该模型的准确率为 86.4%，对于默认算法和几行代码来说，这是令人印象深刻的！大多数 scikit-learn 默认参数都是特意选择的，以便在一系列数据集上很好地工作。但是，您应该始终根据应用实验的知识来选择参数。我们将在后面的章节中使用策略进行**参数搜索**。

# 运行算法

基于我们的测试数据集，基于测试集，前面的结果相当不错。然而，如果我们幸运地选择了一个简单的测试集，会发生什么呢？或者，如果特别麻烦呢？我们可以丢弃一个好的模型，因为这种不吉利的数据分割会导致糟糕的结果。

**交叉验证**框架是解决选择单一测试集问题的一种方式，也是数据挖掘中标准的*最佳实践*方法。该过程通过用不同的训练和测试分割做许多实验来工作，但是在一个测试集中只使用每个样本一次。程序如下:

1.  将整个数据集分成几个称为折叠的部分。
2.  对于数据中的每个文件夹，执行以下步骤:
    1.  将该文件夹设置为当前测试集
    2.  在剩余的折叠上训练算法
    3.  对当前测试集进行评估

3.  报告所有的评估分数，包括平均分数。

In this process, each sample is used in the testing set only once, reducing (but not eliminating) the likelihood of choosing lucky testing sets. Throughout this book, the code examples build upon each other within a chapter. Each chapter's code should be entered into the same Jupyter Notebook unless otherwise specified in-text.

scikit-learn 库包含一些交叉验证方法。给出了一个`helper`函数来执行前面的程序。我们现在可以在 Jupyter 笔记本中导入它:

```py
from sklearn.cross_validation import cross_val_score

```

By `cross_val_score` uses a specific methodology called **Stratified K-Fold** to create folds that have approximately the same proportion of classes in each fold, again reducing the likelihood of choosing poor folds. Stratified K-Fold is a great default -we won't mess with it right now.

接下来，我们使用这个新函数，通过交叉验证来评估我们的模型:

```py
scores = cross_val_score(estimator, X, y, scoring='accuracy') 
average_accuracy = np.mean(scores) * 100 
print("The average accuracy is {0:.1f}%".format(average_accuracy))

```

我们的新代码返回了 82.3%的稍微温和的结果，但是考虑到我们还没有尝试设置更好的参数，这仍然是相当好的。在下一节中，我们将了解如何改变参数以获得更好的结果。

当执行数据挖掘和尝试重复实验时，结果的变化是很自然的。这是由于褶皱是如何产生的以及一些分类算法中固有的随机性。我们可以通过设置随机状态(我们将在后面的章节中进行)来故意选择精确地复制一个实验。在实践中，多次重新运行实验来了解所有实验的平均结果和结果的分布(平均值和标准偏差)是一个好主意。

# 设置参数

用户可以设置的几乎所有参数，让算法更多地关注特定的数据集，而不是只适用于小范围和特定的问题。设置这些参数可能相当困难，因为选择好的参数值通常高度依赖数据集的特征。

最近邻算法有几个参数，但最重要的一个是预测未知属性类别时使用的最近邻数。在`-learn`中，这个参数被称为`n_neighbors`。在下图中，我们表明当这个数字太低时，随机标记的样本会导致错误。相反，当它太高时，实际最近邻对结果的影响较低:

![](img/image_02_004-1.jpg)

在图(a)的左侧，我们通常期望将测试样本(三角形)分类为圆形。但是，如果`n_neighbors`为 1，则该区域中的单个红色菱形(可能是有噪声的样本)会导致样本被预测为菱形。在图(b)的右侧，我们通常期望将测试样本分类为钻石。但是如果`n_neighbors`为 7，则三个最近邻(都是菱形)会被大量的圆样本覆盖。最近邻是一个很难解决的问题，因为参数可以产生巨大的差异。幸运的是，大多数时候具体的参数值并不会对最终结果产生很大的影响，标准值(通常是 5 或 10)往往是足够接近的*。*

考虑到这一点，我们可以测试一系列的值，并研究这个参数对性能的影响。如果我们想要测试`n_neighbors`参数的多个值，例如，从 1 到 20 的每个值，我们可以通过设置`n_neighbors`并观察结果来多次重新运行实验。下面的代码将这些值存储在`avg_scores`和`all_scores`变量中。

```py
avg_scores = [] 
all_scores = [] 
parameter_values = list(range(1, 21))  # Include 20 
for n_neighbors in parameter_values: 
    estimator = KNeighborsClassifier(n_neighbors=n_neighbors) 
    scores = cross_val_score(estimator, X, y, scoring='accuracy')     avg_scores.append(np.mean(scores))     
all_scores.append(scores)

```

然后我们可以绘制`n_neighbors`的值和准确度之间的关系。首先，我们告诉 Jupyter 笔记本，我们想在笔记本本身展示剧情`inline`:

```py
%matplotlib inline

```

然后，我们从`matplotlib`库中导入`pyplot`，并绘制参数值和平均分数:

```py
from matplotlib import pyplot as plt plt.plot(parameter_values,  avg_scores, '-o')

```

![](img/image_02_005.png)

虽然有很大的差异，但随着邻居数量的增加，该图显示出减少的趋势。关于方差，每当你做这种性质的评估时，你可以预期大量的方差。为了补偿，根据`n_neighbors`的值，更新代码以运行 100 个测试。

# 预处理

当测量现实世界的物体时，我们经常可以得到不同范围内的特征。例如，如果我们测量动物的质量，我们可能有几个特征，如下所示:

*   **腿数**:这对于大多数动物来说是在 0-8 之间的范围，而有些动物的腿数更多！更多！更多！
*   **体重**:这个在只有几微克的范围之间，一直到体重 19 万公斤的蓝鲸！
*   **心的数量**:这可以在 0 到 5 之间，对于蚯蚓来说。

对于基于数学的算法来比较这些特征中的每一个，尺度、范围和单位的差异可能难以解释。如果我们在许多算法中使用上述特征，权重可能是最有影响的特征，因为只有较大的数字，与特征的实际有效性没有任何关系。

其中一个可能的策略*对特征进行归一化*，使它们都具有相同的范围，或者将值转化为类似于*小*、*中*和*大*的类别。突然之间，特征类型的巨大差异对算法的影响变小了，并且会导致精确度的大幅提高。

预处理还可以用于仅选择更有效的特征，创建新的特征，等等。scikit-learn 中的预处理是通过`Transformer`对象完成的，这些对象以一种形式获取数据集，并在对数据进行一些转换后返回一个经过修改的数据集。这些不必是数字的，因为变形金刚也被用来提取特征——然而，在这一节中，我们将继续进行预处理。

我们可以通过*打破*`Ionosphere`数据集来展示一个问题的例子。虽然这只是一个例子，但许多真实数据集都存在这种形式的问题。

1.  首先，我们创建一个数组副本，这样就不会改变原始数据集:

```py
X_broken = np.array(X)

```

2.  接下来，我们通过将每一秒的特征除以`10`来打破数据集:

```py
X_broken[:,::2] /= 10

```

理论上，这应该不会对结果产生很大影响。毕竟，这些特性的值还是相对相同的。主要问题是比例发生了变化，现在奇数特征*比偶数特征*大。通过计算精确度，我们可以看到其效果:

```py
estimator = KNeighborsClassifier() 
original_scores = cross_val_score(estimator, X, y,scoring='accuracy') 
print("The original average accuracy for is {0:.1f}%".format(np.mean(original_scores) * 100)) 
broken_scores = cross_val_score(estimator, X_broken, y,   scoring='accuracy') 
print("The 'broken' average accuracy for is   {0:.1f}%".format(np.mean(broken_scores) * 100))

```

这种测试方法对原始数据集给出了 82.3%的评分，而对破碎数据集的评分下降到了 71.5%。我们可以通过将所有特征缩放到范围`0`到`1`来解决这个问题。

# 标准预处理

我们将为这个实验执行的预处理被称为基于特征的归一化，我们使用 scikit-learn 的`MinMaxScaler`类来执行。继续本章剩余部分的 Jupyter 笔记本，首先，我们导入这个类:

```py
fromsklearn.preprocessing import MinMaxScaler

```

该类获取每个特征并将其缩放至范围`0`至`1`。该预处理器根据线性映射，用`0`替换最小值，用`1`替换最大值，以及介于两者之间的其他值。

为了应用我们的预处理器，我们在其上运行`transform`功能。变形金刚通常需要首先被训练，就像分类器一样。我们可以通过运行`fit_transform`功能来合并这些步骤:

```py
X_transformed = MinMaxScaler().fit_transform(X)

```

在这里，`X_transformed`将具有与`*X*`相同的形状。然而，每列最多有`1`和最少`0`。

以这种方式进行规范化还有各种其他形式，这对于其他应用程序和功能类型是有效的:

*   使用`sklearn.preprocessing.Normalizer`，确保每个样本的值之和等于 1
*   使用`sklearn.preprocessing.StandardScaler`，强制每个特征的均值为零，方差为 1，这是一个常用的标准化起点
*   使用`sklearn.preprocessing.Binarizer`将数字特征转换为二进制特征，其中任何高于阈值的值为 1，任何低于阈值的值为 0

我们将在后面的章节中使用这些预处理器的组合，以及其他类型的`Transformers`对象。

Pre-processing is a critical step in the data mining pipeline and one that can mean the difference between a bad and great result.

# 把它们放在一起

现在，我们可以通过结合前面部分的代码，使用前面计算的破碎数据集来创建工作流:

```py
X_transformed = MinMaxScaler().fit_transform(X_broken) 
estimator = KNeighborsClassifier() 
transformed_scores = cross_val_score(estimator, X_transformed, y,    scoring='accuracy') 
print("The average accuracy for is {0:.1f}%".format(np.mean(transformed_scores) * 100))

```

我们现在恢复了原来 82.3%的准确率。`MinMaxScaler`导致了相同规模的特征，这意味着没有特征仅仅因为更大的价值而压倒其他特征。虽然最近邻算法可能会与较大的特征混淆，但一些算法可以更好地处理尺度差异。相比之下，有的就差很多了！

# 管道

随着实验的增加，操作的复杂性也在增加。我们可以分割数据集、二值化特征、执行基于特征的缩放、执行基于样本的缩放以及更多操作。

跟踪这些操作可能会非常混乱，并可能导致无法复制结果。问题包括忘记一个步骤、错误地应用一个转换或者添加一个不需要的转换。

另一个问题是代码的顺序。在前一节中，我们创建了我们的`X_transformed`数据集，然后为交叉验证创建了一个新的估计器。如果我们有多个步骤，我们需要在代码中跟踪数据集的这些变化。

管道是解决这些问题(以及其他问题，我们将在下一章中看到)的结构。管道存储数据挖掘工作流中的步骤。他们可以接收您的原始数据，执行所有必要的转换，然后创建预测。这允许我们在诸如`cross_val_score`之类的函数中使用管道，在这些函数中，他们期望一个估计器。首先，导入`Pipeline`对象:

```py
fromsklearn.pipeline import Pipeline

```

管道将步骤列表作为输入，表示数据挖掘应用程序的链。最后一步需要是一个评估者，而之前的所有步骤都是变形金刚。每个 Transformer 都会改变输入数据集，一个步骤的输出就是下一个步骤的输入。最后，我们用最后一步的估计量对样本进行分类。在我们的管道中，我们有两个步骤:

1.  使用`MinMaxScaler`将特征值从 0 缩放到 1
2.  使用`KNeighborsClassifier`作为分类算法

然后我们用一个元组`('name',` `step)`来表示每个步骤。然后，我们可以创建我们的管道:

```py
scaling_pipeline = Pipeline([('scale', MinMaxScaler()), 
                             ('predict', KNeighborsClassifier())])

```

这里的关键是元组列表。第一个元组是我们的缩放步骤，第二个元组是预测步骤。我们给每一步起一个名字:第一步我们叫`scale`，第二步我们叫`predict`，但是你可以自己选择名字。元组的第二部分是实际的`Transformer`或`estimator`对象。

使用之前的交叉验证代码，运行这个管道现在非常容易:

```py
scores = cross_val_score(scaling_pipeline, X_broken, y, scoring='accuracy') 
print("The pipeline scored an average accuracy for is {0:.1f}%".format(np.mean(transformed_scores) * 100))

```

这给了我们与以前相同的分数(82.3%)，这是意料之中的，因为我们运行的步骤完全相同，只是界面有所改进。

在后面的章节中，我们将使用更先进的测试方法，设置管道是确保代码复杂性不会失控增长的好方法。

# 摘要

在本章中，我们使用了 scikit-learn 的几种方法来构建标准工作流，以运行和评估数据挖掘模型。我们引入了最近邻算法，它在 scikit-learn 中作为估计器实现。使用这个类相当容易；首先，我们在训练数据上调用`fit`函数，其次，我们使用`predict`函数来预测测试样本的类别。

然后，我们通过修复较差的特征缩放来研究预处理。这是使用`Transformer`对象和`MinMaxScaler`类完成的。这些函数还有一个`fit`方法，然后是一个转换，它将一种形式的数据作为输入，并返回一个转换后的数据集作为输出。

为了进一步研究这些变形，试着把`MinMaxScaler`换成其他提到的变形。哪个最有效，为什么会这样？

scikit-learn 中也存在其他变形金刚，我们将在本书后面使用，例如 PCA。参考 scikit-learn 在[https://scikit-learn.org/stable/modules/preprocessing.html](https://scikit-learn.org/stable/modules/preprocessing.html)的优秀文档，也可以尝试其中一些

在下一章中，我们将在一个更大的例子中使用这些概念，使用真实世界的数据来预测体育比赛的结果。