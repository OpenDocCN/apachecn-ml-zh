# 使用朴素贝叶斯的社交媒体洞察

基于文本的文档包含大量信息。例子包括书籍、法律文件、社交媒体和电子邮件。从基于文本的文档中提取信息对现代人工智能系统至关重要，例如在搜索引擎、法律人工智能和自动新闻服务中。

从文本中提取有用的特征是一个难题。文本本质上不是数字，因此必须使用模型来创建可用于数据挖掘算法的特征。好消息是，有一些简单的模型在这方面做得很好，包括我们将在本章中使用的单词包模型。

在本章中，我们将研究从文本中提取特征以用于数据挖掘应用。我们在这一章要解决的具体问题是社交媒体上的术语歧义消除——根据一个单词的上下文来确定它的含义。

我们将在本章中讨论以下主题:

*   从社交网络 API 下载数据
*   文本数据的转换器和模型
*   朴素贝叶斯分类器
*   使用 JSON 保存和加载数据集
*   用于特征创建的 NLTK 库
*   评价的模糊测度

# 解疑

文本数据通常被称为非结构化格式。文字中有很多信息，但只是*在那里*；没有标题、没有要求的格式(除了正常的语法规则)、松散的语法和其他问题阻碍了从文本中轻松提取信息。数据也是高度关联的，有很多提及和交叉引用——只是不是以一种允许我们轻松提取它的格式！即使是看似简单的问题，比如确定一个单词是否是名词，也有很多奇怪的边缘情况，这使得它很难可靠地完成。

我们可以将存储在书中的信息与存储在大型数据库中的信息进行比较，看看有什么不同。书中有人物、主题、地点和大量信息。然而，一本书需要阅读和解释，有文化背景，才能获得这些信息。相比之下，服务器上的数据库具有列名和数据类型。所有信息都在那里，提取特定信息所需的解释水平相当低。

关于数据的信息，如数据的类型或含义，称为元数据。文本缺少元数据。一本书也以目录和索引的形式包含一些元数据，但是这些部分包含的信息程度明显低于数据库。

处理文本的问题之一是**术语歧义**。当一个人使用*银行*这个词时，这是金融信息还是环境信息(比如河岸)？这种类型的歧义消除在许多情况下对人类来说相当容易(尽管仍然有麻烦)，但对计算机来说要困难得多。

在这一章中，我们将研究如何消除术语 Python 在 Twitter 流中的使用歧义。当人们谈论 Python 时，他们可能会谈论以下内容:

*   编程语言 Python
*   蒙蒂 Python，经典喜剧组合
*   蛇蟒
*   一种叫蟒蛇鞋

可以有很多其他的东西叫做 Python。我们实验的目的是拍摄一条提到 Python 的推文，并仅根据推文的内容来确定它是否在谈论编程语言。

A message on Twitter is called a *tweet* and is limited to 140 characters. Tweets include lots of metadata, such as the time and date of posting, who posted it, and so on. However in regards to the topic of the tweet, there is not much in this regard.

在本章中，我们将执行数据挖掘实验，包括以下步骤:

1.  从推特上下载一组推文。
2.  手动对它们进行分类以创建数据集。
3.  保存数据集，以便我们可以复制我们的研究。
4.  使用朴素贝叶斯分类器创建一个分类器来执行术语消歧。

# 从社交网络下载数据

我们首先将从推特上下载一个数据语料库，并使用它从有用的内容中筛选出垃圾邮件。Twitter 为从其服务器收集信息提供了一个强大的应用编程接口，并且该接口对小规模使用是免费的。然而，如果你开始在商业环境中使用推特的数据，你需要注意一些条件。

首先，你需要注册一个推特账户(免费)。去[http://twitter.com](http://twitter.com)注册一个账户，如果你还没有的话。

接下来，您需要确保每分钟只发出一定数量的请求。这个限制目前是每 15 分钟 15 个请求(这取决于确切的 API)。确保不突破这个限制可能很棘手，因此强烈建议您使用库来与 Twitter 的 API 进行对话。

If you are using your own code (that is making the web calls with your own code) to connect with a web-based API, ensure that you read the documentation about rate limiting their documentation and understand the limitations. In Python, you can use the `time` library to perform a pause between calls to ensure you do not breach the limit.

然后你需要一把钥匙来访问推特的数据。前往[http://twitter.com](http://twitter.com)登录您的账户。登录后，进入[https://apps.twitter.com/](https://apps.twitter.com/)，点击【新建应用】。为您的应用程序创建名称和描述，以及网站地址。如果您没有可以使用的网站，请插入一个占位符。将此应用程序的回拨网址字段留空—我们不需要它。同意使用条款(如果你同意)，然后点击创建你的推特应用程序。

保持生成的网站打开，您将需要此页面上的访问键。接下来，我们需要一个图书馆来和推特交流。有很多选择；我喜欢的那个简单的叫`twitter`，是官方的 Twitter Python 库。

You can install `twitter` using pip3 install twitter (on the command line) if you are using pip to install your packages. At the time of writing, Anaconda does not include twitter, therefore you can't use `conda` to install it. If you are using another system or want to build from source, check the documentation at [https://github.com/sixohsix/twitter](https://github.com/sixohsix/twitter)

创建一个新的 Jupyter 笔记本来下载数据。我们将在本章中为各种不同的目的创建几个笔记本，因此创建一个文件夹来跟踪它们可能是个好主意。这第一个笔记本，`ch6_get_twitter`，专门用来下载新的推特数据。

首先，我们导入 twitter 库并设置我们的授权令牌。消费者密钥和消费者秘密将出现在你的推特应用页面的密钥和访问令牌标签上。要获取访问令牌，您需要单击同一页面上的创建我的访问令牌按钮。在下列代码中的适当位置输入密钥:

```py
import twitter
consumer_key = "<Your Consumer Key Here>"
consumer_secret = "<Your Consumer Secret Here>"
access_token = "<Your Access Token Here>"
access_token_secret = "<Your Access Token Secret Here>"
authorization = twitter.OAuth(access_token, access_token_secret, consumer_key, consumer_secret)

```

我们将从推特的搜索功能中获取我们的推文。我们将创建一个使用我们的授权连接到 twitter 的阅读器，然后使用该阅读器执行搜索。在笔记本中，我们设置了存储推文的文件名:

```py
import os
output_filename = os.path.join(os.path.expanduser("~"), "Data", "twitter", "python_tweets.json")

```

接下来，创建一个可以从 Twitter 读取的对象。我们用之前设置的授权对象创建这个对象:

```py
t = twitter.Twitter(auth=authorization)

```

然后我们打开输出文件进行写入。我们打开它进行附加——这允许我们重新运行脚本来获得更多的推文。然后，我们使用我们的推特连接来搜索 Python 这个词。我们只希望返回数据集的状态。这段代码获取 tweet，使用 json 库使用 dumps 函数创建一个字符串表示，然后将其写入文件。然后，它会在推文下面创建一个空行，这样我们就可以轻松区分一条推文在文件中的开始和结束位置:

```py
import json
with open(output_filename, 'a') as output_file:
    search_results = t.search.tweets(q="python", count=100)['statuses']
    for tweet in search_results:
        if 'text' in tweet:
            output_file.write(json.dumps(tweet))
            output_file.write("nn")

```

在前面的循环中，我们还执行了一项检查，以查看推文中是否有文本。并非 twitter 返回的所有对象都是真实的推文(例如，一些响应将是删除推文的操作)。关键的区别是包含文本作为一个键，这是我们测试的。运行几分钟将导致 100 条推文被添加到输出文件中。

You can keep re-running this script to add more tweets to your dataset, keeping in mind that you may get some duplicates in the output file if you rerun it too fast (that is before Twitter gets new tweets to return!). For our initial experiment, 100 tweets will be enough, but you will probably want to come back and rerun this code to get that up to about 1000.

# 加载和分类数据集

在我们收集了一组推文(我们的数据集)之后，我们需要标签来执行分类。我们将通过在 Jupyter 笔记本中设置一个允许我们输入标签的表单来标记数据集。我们通过加载我们在前面部分收集的推文，迭代它们，并(手动)提供它们是否引用 Python 编程语言的分类来做到这一点。

我们存储的数据集几乎是 **JSON** 格式，但不完全是。JSON 是一种数据格式，它不会在内容上强加太多的结构，只是在语法上。JSON 背后的思想是数据是用 JavaScript 直接可读的格式(因此得名， *JavaScript 对象符号*)。JSON 定义了基本的对象，如数字、字符串、列表和字典，如果它们包含非数字数据，那么它是存储数据集的好格式。如果数据集是完全数值化的，那么使用基于矩阵的格式(如 NumPy)可以节省空间和时间。

A key difference between our dataset and real JSON is that we included newlines between tweets. The reason for this was to allow us to easily append new tweets (the actual JSON format doesn't allow this easily). Our format is a JSON representation of a Tweet, followed by a newline, followed by the next Tweet, and so on.

为了解析它，我们可以使用 json 库，但是我们必须首先通过换行符来拆分文件，以获得实际的 tweet 对象本身。建立一个新的 Jupyter 笔记本，我称之为我的 ch6_label_twitter。在其中，我们将首先通过迭代文件从输入文件名加载数据，在循环时存储推文。下面的代码做了一个基本的检查，在推文中有实际的文本。如果有，我们使用 json 库加载推文，然后将其添加到列表中:

```py
import json
import os

# Input filename
input_filename = os.path.join(os.path.expanduser("~"), "Data", "twitter", "python_tweets.json")
# Output filename
labels_filename = os.path.join(os.path.expanduser("~"), "Data", "twitter", "python_classes.json")

tweets = []
with open(input_filename) as inf:
    for line in inf:
        if len(line.strip()) == 0:
            continue
        tweets.append(json.loads(line))

```

我们现在感兴趣的是手动分类一个项目是否与我们相关(在这种情况下，相关指的是编程语言 Python)。我们将利用 Jupyter Notebook 嵌入 HTML 的能力，以及 JavaScript 和 Python 之间的对话，创建一个推文查看器，让我们可以轻松快速地将推文归类为垃圾邮件或不垃圾邮件。代码会向用户(你)呈现一条新的推文，并要求标注:*是否相关？*然后它将存储输入并呈现下一条要标记的推文。

首先，我们创建一个存储标签的列表。无论给定的推文是否引用了编程语言 Python，这些标签都将被存储，它将允许我们的分类器学习如何区分含义。

我们还会检查我们是否已经有任何标签，并加载它们。如果您需要在贴标签的中途合上笔记本，这将有所帮助。这段代码将从您停止的地方加载标签。一般来说，考虑如何在中途为这样的任务节省时间是一个好主意。没有什么比失去一个小时的工作更痛苦的了，因为你的电脑在你保存标签之前就崩溃了！执行此加载的代码如下:

```py
labels = []
if os.path.exists(labels_filename):
    with open(labels_filename) as inf:
        labels = json.load(inf)

```

你第一次运行这个，什么都不会发生。手动分类一些示例后，您可以保存进度，然后关闭笔记本。之后，您可以重新打开笔记本并返回到之前的位置。

If you make one or two mistakes classifying, don't worry too much. If you make lots of mistakes and want to start again, delete just python_classes.json and the above code will pick up with an empty set of classifications. If you need to delete all of your data and start again with new tweets, make sure to delete (or move) both files - python_tweets.json and python_classes.json. Otherwise, this Notebook will get confused, giving classes from the old dataset to the new tweets.

接下来，我们创建一个简单的函数，它将返回下一条需要标记的推文。我们可以通过找到第一条还没有贴标签的推文来找出下一条推文。代码相当简单。我们决定我们已经标记了多少条推文(用`len(labels)`)，并获得推文 _ 样本列表中的下一条推文:

```py
def get_next_tweet():
    return tweets[len(labels)]['text']

```

我们实验的下一步是从用户(你！)哪些推文是指 Python(编程语言)，哪些不是。

As of yet, there is not a good, straightforward way to get interactive feedback with pure Python in Jupyter Notebooks for such a large number of text documents. For this reason, we will use some JavaScript and HTML to get this input from the user. There are many ways to do this, below is just one example.

为了获得反馈，我们需要一个 JavaScript 组件来加载下一条推文并显示它。我们还需要一个 HTML 组件来创建显示推文的 HTML 元素。除了给出一般的工作流程之外，我不会在这里讨论代码的细节:

1.  获取下一条需要用`load_next_tweet`分类的推文
2.  用`handle_output`展示给用户
3.  等待用户用`$("input#capture").keypress`按 0 或 1
4.  用`set_label`将结果存储在班级列表中

这种情况一直发生，直到我们到达列表的末尾(此时会出现索引错误，表明我们没有更多的推文需要分类)。代码如下(请记住，您可以从 Packt 或官方 GitHub 存储库中获取代码):

```py
%%html
<div name="tweetbox">
 Instructions: Click in text box. Enter a 1 if the tweet is relevant, enter 0 otherwise.<br>
 Tweet: <div id="tweet_text" value="text"></div><br>
 <input type=text id="capture"></input><br>
</div>

<script>
function set_label(label){
 var kernel = IPython.notebook.kernel;
 kernel.execute("labels.append(" + label + ")");
 load_next_tweet();
}

function load_next_tweet(){
 var code_input = "get_next_tweet()";
 var kernel = IPython.notebook.kernel;
 var callbacks = { 'iopub' : {'output' : handle_output}};
 kernel.execute(code_input, callbacks, {silent:false});
}

function handle_output(out){
 console.log(out);
 var res = out.content.data["text/plain"];
 $("div#tweet_text").html(res);
}

$("input#capture").keypress(function(e) {
 console.log(e);
 if(e.which == 48) {
 // 0 pressed
 set_label(0);
 $("input#capture").val("");
 }else if (e.which == 49){
 // 1 pressed
 set_label(1); 
 $("input#capture").val("");
 }
});

load_next_tweet();
</script>

```

您需要将所有这些代码输入到一个单元格中(或者从代码包中复制)。它包含了 HTML 和 JavaScript 的混合，这是从你那里获得输入来手动分类推文所必需的。如果需要停止或保存进度，请在下一个单元格中运行以下代码。它会保存你的进度(也不会中断上面的 HTML 代码，可以让它继续运行):

```py
with open(labels_filename, 'w') as outf:
    json.dump(labels, outf)

```

# 从推特创建一个可复制的数据集

在数据挖掘中，有很多变量。这些不是数据挖掘算法的参数——它们是数据收集的方法、环境是如何设置的以及许多其他因素。能够复制你的结果很重要，因为它使你能够验证或改进你的结果。

用算法 X 在一个数据集上获得 80%的准确率，用算法 Y 在另一个数据集上获得 90%的准确率并不意味着 Y 更好。我们需要能够在相同的条件下对相同的数据集进行测试，以便能够进行适当的比较。通过运行前面的代码，您将获得与我创建和使用的数据集不同的数据集。主要原因是推特会根据你执行搜索的时间，为你返回和我不同的搜索结果。

甚至在那之后，你给推文贴的标签可能和我做的不一样。虽然有明显的例子表明某条推文与 python 编程语言有关，但总会有灰色区域标注不明显。我遇到的一个棘手的灰色地带是我看不懂的非英语推文。在这个特定的例子中，推特的应用编程接口中有设置语言的选项，但即使是这些也不会是完美的。

由于这些因素，很难在从社交媒体中提取的数据库上复制实验，推特也不例外。Twitter 明确禁止直接共享数据集。一个解决办法是只分享推文 id，可以自由分享。在本节中，我们将首先创建一个可以自由共享的推文 ID 数据集。然后，我们将看到如何从这个文件下载原始推文来重建原始数据集。首先，我们保存可复制的推文标识数据集。

创建另一个新的 Jupyter 笔记本后，首先像以前一样设置文件名。这与我们标记的方式相同，但是有一个新的文件名，我们可以在其中存储可复制的数据集。代码如下:

```py
import os
input_filename = os.path.join(os.path.expanduser("~"), "Data", "twitter", "python_tweets.json")
labels_filename = os.path.join(os.path.expanduser("~"), "Data", "twitter", "python_classes.json")
replicable_dataset = os.path.join(os.path.expanduser("~"), "Data", "twitter", "replicable_dataset.json")

```

我们加载推文和标签，就像在之前的笔记本中一样:

```py
import json
tweets = []
with open(input_filename) as inf:
    for line in inf:
        if len(line.strip()) == 0:
            continue
        tweets.append(json.loads(line))
if os.path.exists(labels_filename):
    with open(labels_filename) as inf:
        labels = json.load(inf)

```

现在，我们通过同时遍历推文和标签并将其保存在列表中来创建数据集。这段代码的一个重要副作用是，通过将标签放在 zip 函数的第一位，它只会为我们创建的标签加载足够的推文。换句话说，您可以在部分分类的数据上运行此代码:

```py
dataset = [(tweet['id'], label) for label, tweet in zip(labels, tweets)]

```

最后，我们将结果保存在文件中:

```py
with open(replicable_dataset, 'w') as outf:
    json.dump(dataset, outf)

```

现在我们已经保存了推文标识和标签，我们可以重新创建原始数据集。如果您想重新创建我在本章中使用的数据集，可以在本书附带的代码包中找到。加载前面的数据集并不难，但可能需要一些时间。

启动一个新的 Jupyter 笔记本，像以前一样设置数据集、标签和推文 ID 文件名。我已经调整了这里的文件名，以确保您不会覆盖您以前收集的数据集，但是如果您想覆盖，请随意更改这些文件名。

代码如下:

```py
import os
tweet_filename = os.path.join(os.path.expanduser("~"), "Data", "twitter", "replicable_python_tweets.json")
labels_filename = os.path.join(os.path.expanduser("~"), "Data", "twitter", "replicable_python_classes.json")
replicable_dataset = os.path.join(os.path.expanduser("~"), "Data", "twitter", "replicable_dataset.json")

```

接下来，使用 JSON 从文件中加载推文标识:

```py
import json
with open(replicable_dataset) as inf:
    tweet_ids = json.load(inf)

```

保存标签非常容易。我们只需遍历这个数据集并提取标识。我们只需两行代码(打开文件并保存推文)就可以很容易地做到这一点。然而，我们不能保证我们会得到所有我们正在寻找的推文(例如，自从收集数据集以来，一些可能已经被更改为私有的)，因此标签将根据数据被错误地索引。举个例子，我试图在收集数据集一天后重新创建数据集，已经有两条推文丢失了(它们可能被用户删除或变成私有的)。因此，重要的是只打印出我们需要的标签。

为此，我们首先创建一个空的实际标签列表来存储我们从推特上实际恢复的推文的标签，然后创建一个字典来将推文标识映射到标签。代码如下:

```py
actual_labels = []
label_mapping = dict(tweet_ids)

```

接下来，我们将创建一个 twitter 服务器来收集所有这些推文。这需要一点时间。导入我们之前使用的 twitter 库，创建一个授权令牌，并使用它来创建 twitter 对象:

```py
import twitter
consumer_key = "<Your Consumer Key Here>"
consumer_secret = "<Your Consumer Secret Here>"
access_token = "<Your Access Token Here>"
access_token_secret = "<Your Access Token Secret Here>"
authorization = twitter.OAuth(access_token, access_token_secret, consumer_key, consumer_secret)
t = twitter.Twitter(auth=authorization)

```

接下来，我们将遍历每个推文 id，并要求推文恢复原始推文。twitter 的 API 的一个很好的特性是，我们可以一次请求 100 条推文，大大减少了 API 调用的数量。有趣的是，从推特的角度来看，只要是一个请求，一条推文或 100 条推文的呼叫数量是相同的。

下面的代码将以 100 人为一组循环遍历我们的推文，将 id 值连接在一起，并获得每条推文的信息。

```py
all_ids = [tweet_id for tweet_id, label in tweet_ids]
with open(tweet_filename, 'a') as output_file:
    # We can lookup 100 tweets at a time, which saves time in asking twitter for them
    for start_index in range(0, len(all_ids), 100):
        id_string = ",".join(str(i) for i in all_ids[start_index:start_index+100])
        search_results = t.statuses.lookup(_id=id_string)
        for tweet in search_results:
            if 'text' in tweet:
                # Valid tweet - save to file
                output_file.write(json.dumps(tweet))
                output_file.write("nn")
                actual_labels.append(label_mapping[tweet['id']])

```

在这段代码中，我们检查每条推文，看看它是否是有效的推文，如果是，就保存到我们的文件中。我们的最后一步是保存生成的标签:

```py
with open(labels_filename, 'w') as outf:
    json.dump(actual_labels, outf)

```

# 文本转换器

既然我们有了数据集，我们将如何对它进行数据挖掘？

基于文本的数据集包括书籍、论文、网站、手稿、编程代码和其他形式的书面表达。到目前为止，我们看到的所有算法都处理数字或分类特征，那么我们如何将文本转换成算法可以处理的格式呢？可以进行多种测量。

例如，平均单词和平均句子长度用于预测文档的可读性。然而，有许多特征类型，例如我们现在将要研究的单词出现。

# 单词包模型

最简单但高效的模型之一是简单地计算数据集中的每个单词。我们创建一个矩阵，其中每行代表数据集中的一个文档，每列代表一个单词。单元格的值是该单词在文档中出现的频率。这就是所谓的**单词包模型**。

这是《指环王》的节选，托尔金:

<q>Three Rings for the Elven-kings under the sky,</q> <q>Seven for the Dwarf-lords in halls of stone, Nine for Mortal Men, doomed to die,</q> <q>One for the Dark Lord on his dark throne In the Land of Mordor where the Shadows lie.</q> <q>One Ring to rule them all, One Ring to find them,</q> <q>One Ring to bring them all and in the darkness bind them.</q> *In the Land of Mordor where the Shadows lie.* <q>                                                                  - J.R.R. Tolkien's epigraph to The Lord of The Rings</q>

这个词在这段引文中出现了九次，而 in、for、to 和 one 则各出现了四次。环这个字出现三次，的字也是。

我们可以由此创建一个数据集，选择一个单词子集并计算频率:

| 单词 | 这 | 一个 | 戒指 | 到 |
| 频率 | nine | four | three | four |

要对单个文档中的所有单词执行此操作，我们可以使用 **Counter** 类。当计算单词时，将所有字母转换为小写是正常的，我们在创建字符串时会这样做。代码如下:

```py
 s = """Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in halls of stone, Nine for Mortal Men, doomed to die, One for the Dark Lord on his dark throne In the Land of Mordor where the Shadows lie. One Ring to rule them all, One Ring to find them, One Ring to bring them all and in the darkness bind them. In the Land of Mordor where the Shadows lie. """.lower()
words = s.split()
from collections import Counter
c = Counter(words)
print(c.most_common(5))

```

打印`c.most_common(5)`给出了最常出现的五个单词的列表。领带处理不好，因为只有五个给了和非常多的话都分享一个第五名的领带。

单词包模型有三种主要类型，有许多变化和变更。

*   首先是使用原始频率，如前面的例子所示。这与任何具有高方差的非标准化数据字具有相同的缺点，因为高的整体值(例如)*遮蔽了较低频率(因此较低方差)的字，即使单词*的存在很少具有很大的重要性。**
***   第二个模型是使用归一化频率，其中每个文档的总和等于 1。这是一个更好的解决方案，因为文档的长度并不重要，但它仍然意味着像盖过较低频率的单词这样的单词。第三种类型是简单地使用二进制特征——如果出现，值为 1，否则为 0。我们将在本章中使用二进制表示。*   另一种(可以说是更流行的)执行归一化的方法被称为**术语频率-逆文档频率** ( **tf-idf** )。在这个加权方案中，术语计数首先被归一化为频率，然后除以它在语料库中出现的文档数量。我们将在[第 10 章](09.html) *中使用 tf-idf，聚类新闻文章*。**

 **# n-gram 特征

标准单词包模型的一种变体叫做 n-gram 模型。n-grams 模型解决了单词包模型中上下文的不足。使用单词包模型，只有单个单词被自己计数。这意味着常见的词对，如*美国*，会失去它们在句子中的意义，因为它们被视为单独的词。

有一些算法可以读取一个句子，将其解析成树状结构，并使用它来创建单词背后含义的非常准确的表示。不幸的是，这些算法计算量很大。这使得它们难以应用于大型数据集。

为了弥补这些背景和复杂性的问题，n-grams 模型适合中间立场。它比单词包模型有更多的上下文，但计算成本稍高。

n-gram 是连续的、重叠的记号的子序列。在这个实验中，我们使用单词 n-grams，它是单词标记的 n-grams。它们的计数方式与单词包相同，n 克组成一个放在包中的*单词*。该数据集中单元格的值是特定 n-gram 在给定文档中出现的频率。

The value of n is a parameter. For English, setting it to between 2 to 5 is a good start, although some applications call for higher values. Higher values for n result in sparse datasets, as when n increases it is less likely to have the same n-gram appear across multiple documents. Having n=1 results in simply the bag-of-words model.

作为一个例子，`for n=3`，我们提取以下引用的前几个 n 克:

*Always look on the bright side of life.*

第一个 n 克(3 号)是*永远看着*，第二个是*看着*，第三个是*亮着*。如你所见，n-grams 重叠，每个覆盖三个单词。单词 n-grams 比使用单个单词有优势。这个简单的概念通过考虑单词的本地环境为单词的使用引入了一些上下文，而不需要大量的计算来理解语言。

使用 n-gram 的一个缺点是矩阵变得更加稀疏——单词 n-gram 不太可能出现两次(尤其是在推文和其他短文档中！).特别是对于社交媒体和其他短文档，word n-grams 不太可能出现在太多不同的推文中，除非是转发。然而，在较大的文档中，word n-grams 对许多应用程序都非常有效。文本文档的 n-gram 的另一种形式是字符 n-gram。也就是说，你很快就会看到 n-grams 这个词在实践中非常有效。

我们不使用词集，而是简单地使用字符集(尽管字符 n-grams 有很多计算方法可供选择！).这种类型的模型可以帮助识别拼写错误的单词，并为分类提供其他好处。我们将在本章中测试字符 n-grams，并在[第 9 章](08.html) *、Authorship Attribution】中再次看到它们。*

# 其他文本功能

还有其他可以提取的特征。这些包括句法特征，如句子中特定单词的用法。词性标签对于需要理解文本含义的数据挖掘应用程序也很受欢迎。这类功能类型不会在本书中涉及。如果你有兴趣了解更多，我推荐 *Python 3 文本处理搭配 NLTK 3 Cookbook，Jacob Perkins，Packt 出版物*。

Python 中有许多处理文本数据的库。最常见的一种叫做自然语言工具包。scikit-learn 库中也有执行类似动作的 CountVectorizer 类，建议大家看一下(我们将在[第 9 章](08.html)*【Authorship attribute】*中使用)。NLTK 在单词标记化和词性标注(即识别哪些单词是名词、动词等)方面有更多的特性。

我们要用的图书馆叫 spaCy。它是从头开始设计的，可以快速可靠地进行自然语言处理。它的知名度不如 NLTK，但正在迅速普及。它还简化了一些决策，但是与 NLTK 相比，它使用的语法稍微困难一些。

For production systems, I recommend using spaCy, which is faster than NLTK. NLTK was built for teaching, while spaCy was built for production. They have different syntaxes, meaning it can be difficult to port code from one library to another. If you aren't looking into experimenting with different types of natural language parsers, I recommend using spaCy.

# 朴素贝叶斯

朴素贝叶斯是一个概率模型，不出所料，它建立在贝叶斯统计的朴素解释之上。尽管有幼稚的一面，该方法在大量的上下文中表现得非常好。因为幼稚的方面，它工作得相当快。它可以用于许多不同特征类型和格式的分类，但我们将在本章中重点讨论一个:单词包模型中的二进制特征。

# 理解贝叶斯定理

对我们大多数人来说，当我们被教授统计学时，我们是从一个**常客方法**开始的。在这种方法中，我们假设数据来自某个分布，我们的目标是确定该分布的参数。然而，这些参数(可能是不正确的)被认为是固定的。我们使用我们的模型来描述数据，甚至测试来确保数据符合我们的模型。

相反，贝叶斯统计模拟了人们(至少是非经常光顾的统计学家)实际上是如何推理的。我们有一些数据，我们使用这些数据来更新我们的模型，了解事情发生的可能性。在贝叶斯统计中，我们使用数据来描述模型，而不是使用模型并用数据来确认它(根据频繁者方法)。

需要注意的是，频繁者统计和贝叶斯统计问和答的问题略有不同。直接比较并不总是正确的。

贝叶斯定理计算 P(A|B)的值。也就是知道 B 已经发生，事件 A 发生的概率是多少。大多数情况下，B 是观测到的事件如*昨天下雨*，A 是预测今天会下雨。对于数据挖掘，B 通常是*我们观察到这个样本*，A 是*这个样本是否属于这个类*(类预测)。我们将在下一节看到如何使用贝叶斯定理进行数据挖掘。

贝叶斯定理的公式如下:

![](images/B06162_06_01.png)

举个例子，我们想确定一封包含“药物”一词的电子邮件是垃圾邮件的概率(因为我们认为这样的推文可能是药物垃圾邮件)。

*A* ，在这个上下文中，就是这条推文是垃圾邮件的概率。我们可以通过计算数据集中垃圾推文的百分比，直接从训练数据中计算出 *P(A)* ，称为先验信念。如果我们的数据集每 100 封电子邮件包含 30 封垃圾邮件，则 *P(A)* 为 30/100 或 0.3。

*B* ，在这个上下文中，就是这条推文中包含了*药物*这个词。同样，我们可以通过计算数据集中包含毒品一词的推文的百分比来计算 *P(B)* 。如果我们的训练数据集中每 100 封电子邮件中有 10 封包含“药物”一词， *P(B)* 为 10/100 或 0.1。请注意，在计算该值时，我们不在乎电子邮件是否是垃圾邮件。

*P(B|A)* 是如果是垃圾邮件，则电子邮件中包含毒品一词的概率。这也很容易从我们的训练数据集中计算出来。我们在训练集中查找垃圾邮件，并计算它们中包含毒品一词的百分比。在我们的 30 封垃圾邮件中，如果有 6 封包含毒品一词，那么 *P(B|A)* 计算为 6/30 或 0.2。

从这里，我们用贝叶斯定理来计算 *P(A|B)* ，即一条含有毒品这个词的推文是垃圾邮件的概率。利用前面的等式，我们看到的结果是 0.6。这表明，如果一封电子邮件中包含“毒品”一词，则有 60%的可能性是垃圾邮件。

Note the empirical nature of the preceding example—we use evidence directly from our training dataset, not from some preconceived distribution. In contrast, a frequentist view of this would rely on us creating a distribution of the probability of words in tweets to compute similar equations.

# 朴素贝叶斯算法

回顾我们的贝叶斯定理方程，我们可以用它来计算给定样本属于给定类别的概率。这允许方程被用作分类算法。

在我们的数据集中，将 *C* 作为给定的类， *D* 作为样本，我们创建了贝叶斯定理以及随后的朴素贝叶斯所必需的元素。朴素贝叶斯是一种分类算法，它利用贝叶斯定理来计算新数据样本属于特定类别的概率。

*P(D)* 是给定数据样本的概率。这可能很难计算，因为示例是不同特性之间的复杂交互，但幸运的是，它在所有类中都是恒定的。因此，我们根本不需要计算它，因为我们在最后一步所做的只是比较相对值。

*P(D|C)* 是数据点属于该类的概率。由于不同的特性，这也很难计算。然而，这是我们介绍朴素贝叶斯算法的朴素部分的地方。我们天真地认为每个特征都是相互独立的。我们不是计算 *P(D|C)* 的全概率，而是计算每个特征 *D1，D2，D3，...*以此类推。然后，我们将它们相乘:

p〔t0〕(d | c)= p(D1 | c)x p(D2 | c)...我...x P(Dn|C)

使用二进制特征，这些值中的每一个都相对容易计算；我们只需计算样本数据集中相等次数的百分比。

In contrast, if we were to perform a non-naive Bayes version of this part, we would need to compute the correlations between different features for each class. Such computation is infeasible at best, and nearly impossible without vast amounts of data or adequate language analysis models.

从这里开始，算法很简单。我们为每个可能的类计算 P(C|D)，完全忽略 P(D)项。然后我们选择概率最高的班级。由于 P(D)项在每个类中都是一致的，忽略它对最终预测没有影响。

# 它是如何工作的

例如，假设我们的数据集中有来自样本的以下(二进制)特征值:[0，0，0，1]。

我们的训练数据集包含两个类，其中 75%的样本属于类 0，25%属于类 1。每个类别的特征值的可能性如下:

对于等级 0: [0.3，0.4，0.4，0.7]

对于类别 1: [0.7、0.3、0.4、0.9]

这些值将被解释为:对于特征 1，在 30%的情况下，0 类样本的值为 1。在 70%的 1 级样本中，该值为 1。

我们现在可以计算这个样本属于 0 类的概率。P(C=0) = 0.75，即类为 0 的概率。同样，朴素贝叶斯算法不需要 P(D)，只需将其从等式中移除即可。让我们看一下计算:

```py
P(D|C=0) = P(D1|C=0) x P(D2|C=0) x P(D3|C=0) x P(D4|C=0) 
= 0.3 x 0.6 x 0.6 x 0.7 
= 0.0756

```

The second and third values are 0.6, because the value of that feature in the sample was 0\. The listed probabilities are for values of 1 for each feature. Therefore, the probability of a 0 is its inverse: *P(0) = 1 – P(1)*. 

现在，我们可以计算数据点属于这个类的概率。让我们看一下计算:

```py
P(C=0|D) = P(C=0) P(D|C=0) = 0.75 * 0.0756 = 0.0567 

```

现在，我们为类 1 计算相同的值:

```py
P(D|C=1) = P(D1|C=1) x P(D2|C=1) x P(D3|C=1) x P(D4|C=1)
         = 0.7 x 0.7 x 0.6 x 0.9
         = 0.2646 P(C=1|D) 
         = P(C=1)P(D|C=1)
         = 0.25 * 0.2646
         = 0.06615

```

Normally, P(C=0|D) + P(C=1|D) should equal to 1\. After all, those are the only two possible options! However, the probabilities are not 1 due to the fact we haven't included the computation of P(D) in our equations here.

由于 *P(C=1|D)* 的值大于 *P(C=0|D)* ，该数据点应归为 1 类。无论如何，你可能已经猜到了这一点；然而，你可能会有点惊讶，最终的决定如此接近。毕竟，计算 *P(D|C)* 的概率要高得多，对于 1 类来说要高得多。这是因为我们引入了一个先验信念，即大多数样本通常属于 0 类。

如果类的大小相等，那么得到的概率就会大不相同。您可以自己尝试，将相同班级规模的 *P(C=0)* 和 *P(C=1)* 都更改为 0.5，然后再次计算结果。

# 朴素贝叶斯的应用

我们现在将创建一个管道，接收一条推文，并仅根据该推文的内容来确定它是否相关。

为了执行单词提取，我们将使用 spaCy，这是一个包含大量工具的库，用于对自然语言进行分析。我们也将在以后的章节中使用 spaCy。

To get spaCy on your computer, use pip to install the package: pip install spacy
If that doesn't work, see the spaCy installation instructions at [https://spacy.io/](https://spacy.io/) for information specific to your platform.

我们将创建一个管道来提取单词特征，并使用朴素贝叶斯对推文进行分类。我们的管道有以下步骤:

*   使用 spaCy 的单词标记化将原始文本文档转换为计数字典。
*   使用 scikit-learn 中的`DictVectorizer`转换器将这些字典转换成向量矩阵。这对于使朴素贝叶斯分类器能够读取第一步中提取的特征值是必要的。
*   训练朴素贝叶斯分类器，正如我们在前面章节中看到的。

我们需要创建另一个笔记本(这一章的最后一个！)调用`ch6_classify_twitter`进行分类。

# 提取字数

我们将使用 spaCy 提取我们的字数。我们仍然希望在管道中使用它，但是 spaCy 不符合我们的转换器接口。我们需要创建一个基本的转换器来实现这一点，以获得拟合和转换方法，使我们能够在管道中使用它。

首先，设置变压器类。我们不需要在这个类中放入任何东西，因为这个转换器只是提取文档中的单词。因此，我们的 fit 是一个空函数，除了它返回 self，这是 transformer 对象符合 scikit-learn API 所必需的。

我们的转换有点复杂。我们希望从每个文档中提取每个单词，并在发现时记录“真”。我们在这里只使用二进制特性——如果在文档中，则为真，否则为假。如果我们想使用频率，我们会建立计数字典，就像我们在过去几章中所做的那样。

让我们看看代码:

```py
import spacy
from sklearn.base import TransformerMixin

# Create a spaCy parser
nlp = spacy.load('en')

class BagOfWords(TransformerMixin):
    def fit(self, X, y=None):
        return self

    def transform(self, X):
        results = []
        for document in X:
            row = {}
            for word in list(nlp(document, tag=False, parse=False, entity=False)):
                if len(word.text.strip()): # Ignore words that are just whitespace
                    row[word.text] = True
                    results.append(row)
        return results

```

结果是字典列表，其中第一个字典是第一条推文中的单词列表，以此类推。每个字典都有一个单词作为关键字，值“真”表示这个单词被发现。任何不在字典中的单词都将被认为没有出现在推文中。明确声明一个单词的出现为 False 也是可行的，但是会占用不必要的存储空间。

# 将字典转换为矩阵

下一步将按照上一步构建的字典转换成可与分类器一起使用的矩阵。通过 scikit-learn 中提供的 DictVectorizer 转换器，这个步骤变得非常简单。

`DictVectorizer`类只需获取一个字典列表，并将它们转换成一个矩阵。该矩阵中的特征是每个字典中的关键字，这些值对应于每个样本中这些特征的出现。字典很容易在代码中创建，但是许多数据算法实现更喜欢矩阵。这使得`DictVectorizer`成为一个非常有用的类。

在我们的数据集中，每个字典都有单词作为关键字，并且只有当单词实际出现在推文中时才会出现。因此，如果单词出现在推文中，我们的矩阵将把每个单词作为一个特征，并在单元格中取值为真。

要使用`DictVectorizer`，只需使用以下命令导入即可:

```py
from sklearn.feature_extraction import DictVectorizer

```

# 把它们放在一起

最后，我们需要设置一个分类器，我们将在本章中使用朴素贝叶斯。由于我们的数据集只包含二进制特征，因此我们使用为二进制特征设计的`BernoulliNB`分类器。作为一个分类器，它非常容易使用。与`DictVectorizer`一样，我们只需导入它并将其添加到我们的管道中:

```py
from sklearn.naive_bayes import BernoulliNB

```

现在是时候把所有这些碎片放在一起了。在我们的 Jupyter 笔记本中，像以前一样设置文件名并加载数据集和类。为两条推文本身设置文件名(不是 id！)以及我们分配给他们的标签。代码如下:

```py
import os
input_filename = os.path.join(os.path.expanduser("~"), "Data", "twitter", "python_tweets.json")
labels_filename = os.path.join(os.path.expanduser("~"), "Data", "twitter", "python_classes.json")

```

自己加载推文。我们只对推文的内容感兴趣，所以我们提取文本值并只存储它。代码如下:

```py
import json

tweets = []
with open(input_filename) as inf:
    for line in inf:
        if len(line.strip()) == 0: continue
        tweets.append(json.loads(line)['text'])

with open(labels_filename) as inf:
    labels = json.load(inf)

# Ensure only classified tweets are loaded
tweets = tweets[:len(labels)]

```

现在，创建一个管道，将以前的组件放在一起。我们的管道有三个部分:

1.  我们创造的 NLTKBOW 变压器。
2.  一个独裁者转换器。
3.  一个 BernoulliNB 分类器。

代码如下:

```py
from sklearn.pipeline import Pipeline

pipeline = Pipeline([('bag-of-words', BagOfWords()), ('vectorizer', DictVectorizer()), ('naive-bayes', BernoulliNB()) ])

```

我们现在几乎可以运行我们的管道，我们将像以前多次做的那样使用`cross_val_score`来做。在我们执行数据挖掘之前，我们将引入一个比之前使用的准确性度量更好的评估度量。正如我们将看到的，当每个类中的样本数量不同时，准确性的使用对于数据集是不够的。

# 使用 F1 分数进行评估

When choosing an evaluation metric, it is always important to consider cases where that evaluation metric is not useful. Accuracy is a good evaluation metric in many cases, as it is easy to understand and simple to compute. However, it can be easily faked. In other words, in many cases, you can create algorithms that have a high accuracy but have a poor utility.

虽然我们的推文数据集(通常，您的结果可能会有所不同)包含大约 50%的编程相关和 50%的非编程，但许多数据集并不像这样*平衡*。

例如，电子邮件垃圾邮件过滤器可能会发现超过 80%的传入电子邮件是垃圾邮件。一个简单地将所有东西都标记为垃圾邮件的垃圾邮件过滤器是非常无用的；然而，它将获得 80%的准确率！

为了解决这个问题，我们可以使用其他评估指标。最常用的一种叫做 **f1 分数**(也称为 f 分数、f 度量或该术语的许多其他变体之一)。

The F1-score is defined on a per-class basis and is based on two concepts: the precision and recall. The precision is the percentage of all the samples that were predicted as belonging to a specific class, that were actually from that class. The recall is the percentage of samples in the dataset that are in a class and actually labeled as belonging to that class.

在我们的应用程序中，我们可以计算两个类的值(python 编程和非 python 编程)。

我们的精度计算变成了一个问题:<q>在所有预测相关的推文中，有多少百分比是真正相关的？</q>

同样，召回变成了一个问题:<q>在数据集中所有相关的推文中，有多少被预测为相关？</q>

计算精度和召回率后，f1 分数是精度和召回率的调和平均值:

要在 scikit-learn 方法中使用 f1 评分，只需将评分参数设置为 f1。默认情况下，这将返回标签为 1 的类的 f1 分数。在数据集上运行代码，我们只需使用以下代码行:

```py
from sklearn.cross_validation import cross_val_score
scores = cross_val_score(pipeline, tweets, labels, scoring='f1')
# We then print out the average of the scores:
import numpy as np
print("Score: {:.3f}".format(np.mean(scores)))

```

结果是 0.684，这意味着我们可以准确地确定使用 Python 的推文是否有近 70%的时间与编程语言相关。这是使用一个只有 300 条推文的数据集。

回去多收集数据，你会发现结果增加了！请记住，您的数据集可能不同，因此您的结果也会不同。

More data usually means a better accuracy, but it is not guaranteed!

# 从模型中获取有用的特征

你可能会问，决定一条推文是否相关的最佳特征是什么？我们可以从我们的朴素贝叶斯模型中提取这些信息，并根据朴素贝叶斯找出哪些特征是最好的。

首先，我们安装了一个新模型。虽然`cross_val_score`在不同的交叉验证测试数据中给了我们一个分数，但它并不容易给我们训练好的模型本身。为此，我们只需将我们的管道与推文相匹配，创建一个新模型。代码如下:

```py
 model = pipeline.fit(tweets, labels)

```

Note that we aren't really evaluating the model here, so we don't need to be as careful with the training/testing split. However, before you put these features into practice, you should evaluate on a separate test split. We skip over that here for the sake of clarity.

管道通过`named_steps`属性和步骤名称(我们在创建管道对象本身时自己定义了这些名称)让您可以访问各个步骤。例如，我们可以得到朴素贝叶斯模型:

```py
nb = model.named_steps['naive-bayes']
feature_probabilities = nb.feature_log_prob_

```

从这个模型中，我们可以提取每个单词的概率。这些被存储为对数概率，简单来说就是*对数(P(A|f))* ，其中 *f* 是给定的特征。

这些被存储为对数概率的原因是因为实际值非常低。例如，第一个值是-3.486，与低于 0.03%的概率相关。对数概率用于计算像这样的小概率，因为它们阻止了非常小的值被舍入为零的下溢误差。假设所有的概率相乘，单个值 0 将导致整个答案总是 0！不管怎样，价值观之间的关系还是一样的；该值越高，该功能就越有用。

通过对对数概率数组进行排序，我们可以得到最有用的特征。我们想要降序，所以我们先简单地否定这些值。代码如下:

```py
top_features = np.argsort(-nb.feature_log_prob_[1])[:50]

```

前面的代码将只给我们指数，而不是实际的特征值。这不是很有用，所以我们将把特征的索引映射到实际值。关键是管道的字典矢量器步骤，它为我们创建了矩阵。幸运的是，这也记录了映射，允许我们找到与不同列相关的特征名。我们可以从管道的这一部分提取特征:

```py
dv = model.named_steps['vectorizer']

```

从这里，我们可以通过在字典矢量器的`feature_names_`属性中查找来打印出顶级特征的名称。在新单元格中输入以下行，并运行它以打印出顶级功能的列表:

```py
for i, feature_index in enumerate(top_features):
    print(i, dv.feature_names_[feature_index], np.exp(feature_probabilities[1][feature_index]))

```

前几个特性包括:RT，甚至 Python。根据我们收集的数据，这些很可能是噪音(尽管在编程之外使用冒号并不常见)。收集更多数据对于解决这些问题至关重要。浏览列表，我们得到了许多更明显的编程特性:

```py
9 for 0.175
14 ) 0.10625
15 ( 0.10625
22 jobs 0.0625
29 Developer 0.05

```

还有一些其他术语在工作环境中引用 Python，因此可能是指编程语言(虽然自由蛇处理程序也可能使用类似的术语，但它们在推特上不太常见)。

最后一个通常是这样的格式:<q>我们正在寻找这份工作的候选人</q>。

浏览这些特性给了我们很多好处。我们可以训练人们识别这些推文，寻找共性(这可以让他们洞察一个话题)，甚至去掉没有意义的特征。例如，RT 这个词在这个列表中出现得相当高；然而，这是一个常见的推特转发短语(即在别人的推特上转发)。专家可以决定从列表中删除这个词，使分类器不太容易受到我们通过小数据集引入的噪声的影响。

# 摘要

在这一章中，我们研究了文本挖掘——如何从文本中提取特征，如何使用这些特征，以及扩展这些特征的方法。在这样做的时候，我们把一条推文放在上下文中——这条推文提到 python 是指编程语言吗？我们从基于网络的应用编程接口下载数据，从流行的微博网站推特上获取推文。这给了我们一个数据集，我们使用直接在 Jupyter 笔记本中构建的表单对其进行了标记。

我们还观察了实验的再现性。虽然推特不允许你向其他人发送你的数据副本，但它允许你发送推文的标识。利用这一点，我们创建了保存标识的代码，并重新创建了大部分原始数据集。并非所有推文都被退回；自创建标识列表和复制数据集以来，有些已被删除。

我们使用朴素贝叶斯分类器来执行文本分类。这是建立在贝叶斯定理的基础上的，贝叶斯定理使用数据来更新模型，不像经常从模型开始的频繁方法。这允许模型合并和更新新的数据，并合并先前的信念。此外，天真的部分允许轻松计算频率，而无需处理特征之间的复杂相关性。

我们提取的特征是单词出现——这个单词出现在这条推文中吗？这种模式被称为单词包。虽然这样会丢弃关于单词使用位置的信息，但它在许多数据集上仍然可以获得很高的准确性。将单词包模型与朴素贝叶斯一起使用的整个流程非常健壮。你会发现它可以在大多数基于文本的任务中获得相当好的分数。在尝试更高级的模型之前，这是一个很好的基线。作为另一个优点，朴素贝叶斯分类器没有任何需要设置的参数(尽管如果您希望做一些修补，也有一些)。

为了扩展我们在本章中所做的工作，首先从收集更多数据开始。您还需要手动对这些进行分类，但是您会发现推文之间的一些相似之处，这可能会使分类更加容易。例如，有一个名为“位置敏感哈希”的研究领域，可以确定两条推文是否相似。两条类似的推文很可能是关于同一个话题。扩展研究的另一种方法是考虑如何建立一个模型，将推特用户的历史纳入等式中——换句话说，如果用户经常在推特上谈论 python 作为编程语言，那么他们更有可能在未来的推特上使用 python。

在下一章中，我们将研究从另一种类型的数据(图表)中提取特征，以便就在社交媒体上关注谁提出建议。**