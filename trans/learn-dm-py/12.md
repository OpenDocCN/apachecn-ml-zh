# 使用大数据

数据量正以指数级的速度增长。今天的系统正在生成和记录关于客户行为、分布式系统、网络分析、传感器以及更多来源的信息。在当前移动数据大趋势推动当前增长的同时，下一件大事——**物联网(IoT)**——将进一步提升增速。

这对数据挖掘意味着一种新的思维方式。运行时间高的复杂算法需要改进或丢弃，而可以处理更多样本的更简单的算法越来越流行使用。例如，虽然支持向量机是很好的分类器，但是一些变体很难在非常大的数据集上使用。相比之下，逻辑回归等更简单的算法在这些场景中更容易管理。

这种复杂性与分布性的问题只是深度神经网络如此流行的原因之一。你可以使用 DNNs 创建非常复杂的模型，但是你也可以很容易地将训练它们的工作量分配到多台计算机上。

在本章中，我们将研究以下内容:

*   大数据挑战和应用
*   MapReduce 范例
*   Hadoop MapReduce
*   mrjob，一个在亚马逊 AWS 基础设施上运行 MapReduce 程序的 Python 库

# 大数据

是什么让大数据与众不同？大多数大数据支持者谈论大数据的四个方面:

*   **卷**:我们生成和存储的数据量正在以越来越快的速度增长，对未来的预测一般只建议进一步增长。今天的几十亿字节大小的硬盘将在几年后变成十亿字节大小的硬盘，网络吞吐量也将增加。信噪比可能相当困难，重要数据会在堆积如山的非重要数据中丢失。
*   **速度**:与体积相关的同时，数据的速度也在增加。现代汽车有数百个传感器，这些传感器将数据传输到计算机中，来自这些传感器的信息需要在亚秒级进行分析，才能操作汽车。这不仅仅是在海量数据中寻找答案的问题；这些答案往往需要尽快得出。在某些情况下，我们也没有足够的磁盘空间来存储数据，这意味着我们还需要决定保留什么数据以供以后分析。
*   **多样性**:列定义清晰的好数据集只是我们目前拥有的数据集的一小部分。考虑一篇可能包含文本、照片、用户提及、喜欢、评论、视频、地理信息和其他领域的社交媒体帖子。简单地忽略这些数据中不适合您的模型的部分会导致信息丢失，但是集成这些信息本身可能非常困难。
*   **准确性**:随着数据量的增加，很难确定数据是否被正确收集——是否过时、有噪音、包含异常值——或者一般来说是否有用。当人类无法可靠地验证数据时，很难信任数据。外部数据集也越来越多地被合并到内部数据集，这给数据的准确性带来了更多麻烦。

这四个主要的 Vs(其他人提出了额外的 Vs)概述了为什么大数据不同于仅仅*大量数据*。在这种规模下，处理数据的工程问题往往更加困难，更不用说分析了。虽然有很多蛇油推销员夸大了特定产品分析大数据的能力，但很难否认工程挑战和大数据分析的潜力。

到目前为止，我们在书中使用的算法将数据集加载到内存中，然后在内存版本中工作。这在计算速度方面带来了很大的好处(因为使用计算机内存比使用硬盘快)，因为根据内存中的数据进行计算比在使用之前加载样本要快得多。此外，内存中的数据允许我们多次迭代数据，从而改进我们的机器学习模型。

在大数据中，我们无法将数据加载到内存中。从许多方面来说，这是一个很好的定义，说明问题是否是大数据——如果数据可以放在计算机的内存中，那么你就不是在处理大数据问题。

When looking at the data you create, such as log data from your company's internal applications, it might be tempting to simply throw it all into a file, unstructured, and use big-data concepts later to analyze it. It is best not to do this; instead, you should use structured formats for your own datasets. The reason is that the four Vs we just outlined are actually *problems* that need to be solved to perform data analysis, not goals to strive for!

# 大数据的应用

公共和私营部门都有许多大数据使用案例。

人们使用基于大数据的系统最常见的体验是在互联网搜索中，比如谷歌。要运行这些系统，需要在几分之一秒内对数十亿个网站进行搜索。做一个基本的基于文本的搜索不足以解决这样的问题。简单地存储所有这些网站的文本是一个大问题。为了处理查询，需要专门为此应用程序创建和实现新的数据结构和数据挖掘方法。

大数据也用于许多其他科学实验，如大型强子对撞机，下面是其中的一部分。它绵延超过 27 公里，包含 1.5 亿个传感器，每秒监测数亿次粒子碰撞。这个实验的数据非常庞大，经过一个过滤过程，每天产生 25pb 的数据(如果不使用过滤，每年将有 1.5 亿 Pb 的数据)。对如此庞大的数据进行分析，让人们对我们的世界有了惊人的了解，但这一直是一个重大的工程和分析挑战。

![](images/B06162_12_01.jpg)

政府也越来越多地使用大数据来跟踪人口、企业和与国家相关的其他方面。跟踪数百万人和数十亿次互动(如商业交易或医疗支出)导致许多政府机构需要大数据分析。

交通管理是世界各地许多政府的特别关注点，他们正在使用数百万个传感器跟踪交通，以确定哪些道路最拥堵，并预测新道路对交通水平的影响。这些管理系统将在不久的将来与自动驾驶汽车的数据相连接，从而实时获得更多关于交通状况的数据。利用这些数据的城市会发现他们的交通更加自由。

大型零售组织正在使用大数据来改善客户体验和降低成本。这包括预测客户需求以获得正确的库存水平，向客户追加销售他们可能喜欢购买的产品，以及跟踪交易以寻找趋势、模式和潜在的欺诈。自动创造伟大预测的公司可以以更低的成本获得更高的销售额。

其他大型企业也在利用大数据来自动化其业务的各个方面并改进其产品。这包括利用分析来预测其行业的未来趋势，并跟踪外部竞争对手。大型企业还使用分析来管理自己的员工——跟踪员工以寻找员工可能离开公司的迹象，以便在他们离开之前进行干预。

信息安全部门也在利用大数据，通过监控网络流量来寻找大型网络中的恶意软件感染。这可能包括寻找奇怪的流量模式、恶意软件传播的证据以及其他奇怪的东西。高级持久威胁(APTs)是另一个问题，有动机的攻击者会将他们的代码隐藏在大型网络中，以窃取信息或在很长一段时间内造成损害。寻找 APTs 通常是对许多计算机进行法医检查的一个例子，这项任务对于人类来说需要太长时间才能有效地完成。分析有助于自动化和分析这些法医图像，以发现感染。

大数据正被越来越多的行业和应用所使用，这一趋势可能只会持续下去。

# MapReduce

对大数据进行数据挖掘和一般计算有许多概念。其中最流行的是 MapReduce 模型，它可以用于任意大数据集上的一般计算。

MapReduce 起源于谷歌，它是在考虑分布式计算的情况下开发的。它还引入了容错和可伸缩性改进。MapReduce 最初的*研究发表于 2004 年，从那以后有成千上万的项目、实现和应用程序使用它。*

 *虽然这个概念类似于许多以前的概念，但 MapReduce 已经成为大数据分析的主要内容。

MapReduce 作业有两个主要阶段。

1.  第一个是 Map，我们通过它获取一个函数和一个项目列表，并将该函数应用于每个项目。换句话说，我们将每个项目作为函数的输入，并存储该函数调用的结果:

![](images/B06162_12_02-e1493116032553.jpg)

2.  第二步是 Reduce，我们从地图步骤中获取结果，并使用函数将它们组合起来。对于统计学来说，这可能就像把所有的数字加在一起一样简单。这个场景中的 reduce 函数是一个 add 函数，它将取前面的总和，并添加新的结果:

![](images/B06162_12_03-e1493116048271.jpg)

在这两个步骤之后，我们将转换我们的数据，并将其简化为最终结果。

地图缩减作业可以有许多迭代，其中一些仅是地图作业，一些仅是缩减作业，一些具有地图和缩减步骤的迭代。现在让我们看一些更具体的例子，首先使用内置的 python 函数，然后使用特定的工具进行 MapReduce 作业。

# MapReduce 背后的直觉

MapReduce 有两个主要步骤:`Map`步骤和`Reduce`步骤。这些都是建立在将函数映射到列表并减少结果的函数编程概念之上的。为了解释这个概念，我们将开发代码来迭代一个列表，并产生这些列表中所有数字的总和。

MapReduce 范例中还有`shuffle`和`combine`步骤，我们稍后会看到。

首先，“映射”步骤采用一个函数，并将其应用于列表中的每个元素。返回的结果是一个大小相同的列表，函数的结果应用于每个元素。

要打开新的 Jupyter 笔记本，首先创建一个列表，每个子列表中都有数字:

```
a = [[1,2,1], [3,2], [4,9,1,0,2]]

```

接下来，我们可以使用求和函数执行`map`。该步骤将对 *a* 的每个元素应用求和函数:

```
sums = map(sum, a)

```

而`sums`是一个生成器(实际值直到我们要求时才计算)，前面的步骤大约等于下面的代码:

```
sums = []
for sublist in a:
    results = sum(sublist)
    sums.append(results)

```

`reduce`步骤稍微复杂一点。它包括将一个函数应用于返回结果的每个元素，应用于某个起始值。我们从一个初始值开始，然后对这个初始值和第一个值应用一个给定的函数。然后，我们将给定的函数应用于结果和下一个值，以此类推

我们首先创建一个函数，将两个数字相加。

```
def add(a, b): 
    return a + b

```

然后我们执行缩减。`reduce`的签名是:`reduce(function, sequence, initial)`，其中函数在每个步骤应用于序列。在第一步中，初始值被用作列表的第一个值，而不是第一个元素:

```
from functools import reduce 
print(reduce(add, sums, 0))

```

结果 25 是 sums 列表中每个值的总和，因此是原始数组中每个元素的总和。

前面的代码类似于下面的代码:

```
initial = 0 
current_result = initial 
for element in sums: 
    current_result = add(current_result, element)

```

在这个简单的例子中，如果不使用 MapReduce 范式，我们的代码将会大大简化，但是真正的收益来自于分配计算。例如，如果我们有一百万个子列表，并且每个子列表包含一百万个元素，我们可以将这个计算分布在许多计算机上。

为此，我们通过分割数据来分配`map`步骤。对于我们列表中的每一个元素，我们将它连同我们的功能描述一起发送给计算机。然后，这台计算机将结果返回给我们的主计算机(主机)。

然后，主机将结果发送到计算机进行`reduce`步骤。在我们的一百万个子列表的例子中，我们将向不同的计算机发送一百万个作业(同一台计算机在完成我们的第一个作业后可能会被重用)。返回的结果将只是一个包含一百万个数字的列表，然后我们计算这些数字的总和。

结果是，没有一台计算机需要存储超过一百万个数字，尽管我们的原始数据中有一万亿个数字。

# 字数统计的例子

MapReduce 的任何*实际*实现都比仅仅使用一个`map`和`reduce`步骤要复杂一点。这两个步骤都是使用键调用的，这允许分离数据和跟踪值。

The map function takes a key-value pair and returns a list of *key/value* pairs. The keys for the input and output don't necessarily relate to each other.

例如，对于执行字数统计的 MapReduce 程序，输入键可能是示例文档的 ID 值，而输出键可能是给定的单词。输入值是文档的文本，输出值是每个单词的频率。我们分割文档以获得单词，然后产生每个单词，计数成对。这里的单词是关键，计数是 MapReduce 术语中的值:

```
from collections import defaultdict
def map_word_count(document_id, document):
    counts = defaultdict(int) 
    for word in document.split(): 
        counts[word] += 1
    for word in counts: 
        yield (word, counts[word])

```

Have a really, really big dataset? You can just do `yield (word, 1)` when you come across a new word and then combine the ones in the shuffle step rather than count within the map step. Where you place it depends on your dataset size, per-document size, network capacity, and a whole range of factors. Big data is a big engineering problem and to get the maximum performance out of a system, you'll need to model how data will flow throughout the algorithm.

通过使用单词作为键，我们可以执行随机播放步骤，对每个键的所有值进行分组:

```
def shuffle_words(results):
    records = defaultdict(list)
    for results in results_generators: 
        for word, count in results: 
            records[word].append(count)
    for word in records: 
        yield (word, records[word])

```

最后一步是 reduce 步骤，它获取一个键值对(在本例中，该值始终是一个列表)，并生成一个键值对。在我们的示例中，关键字是单词，输入列表是洗牌步骤中产生的计数列表，输出值是计数的总和:

```
def reduce_counts(word, list_of_counts): 
    return (word, sum(list_of_counts))

```

为了看到这一点，我们可以使用 scikit-learn 中提供的 20 个新闻组数据集。这个数据集不是大数据，但是我们可以在这里看到正在运行的概念:

```
from sklearn.datasets import fetch_20newsgroups 
dataset = fetch_20newsgroups(subset='train') 
documents = dataset.data

```

然后我们应用我们的地图步骤。我们在这里使用枚举来自动为我们生成文档标识。虽然这些键在本应用程序中并不重要，但在其他应用程序中却很重要:

```
map_results = map(map_word_count, enumerate(documents))

```

这里的实际结果只是一个发生器；没有实际计数。也就是说，它是一个发出(单词，计数)对的生成器。

接下来，我们执行洗牌步骤来对这些字数进行排序:

```
shuffle_results = shuffle_words(map_results)

```

本质上，这是一个 MapReduce 作业；然而，它只在一个线程上运行，这意味着我们没有从 MapReduce 数据格式中获得任何好处。在下一节中，我们将开始使用 Hadoop，一个 MapReduce 的开源提供者，开始获得这种范式的好处。

# Hadoop MapReduce

Hadoop 是一组来自 Apache 的开源工具，包括 MapReduce 的实现。在许多情况下，它是许多人使用的事实上的实现。该项目由 Apache 集团管理(他们负责著名的同名网络服务器)。

Hadoop 生态系统相当复杂，有大量的工具。我们将使用的主要组件是 Hadoop MapReduce。Hadoop 中包含的其他处理大数据的工具如下:

*   **Hadoop 分布式文件系统(HDFS)** :这是一个可以在多台计算机上存储文件的文件系统，目标是在提供高带宽的同时，对硬件故障具有鲁棒性。
*   **纱**:这是一种调度应用和管理计算机集群的方法。
*   **Pig:** 这是 MapReduce 的高级编程语言。Hadoop MapReduce 是用 Java 实现的，Pig 位于 Java 实现之上，允许您用其他语言编写程序——包括 Python。
*   **Hive:** 用于管理数据仓库和执行查询。
*   **HBase** :这是谷歌 BigTable 的一个实现，一个分布式数据库。

这些工具都解决了在进行大数据实验(包括数据分析)时出现的不同问题。

还有非基于 Hadoop 的 MapReduce 实现，以及其他具有类似目标的项目。此外，许多云提供商都有基于 MapReduce 的系统。

# 应用 MapReduce

在本应用程序中，我们将根据作家对不同词语的使用来预测他们的性别。我们将使用朴素贝叶斯方法，在 MapReduce 中进行训练。最终的模型不需要 MapReduce，尽管我们可以使用 Map 步骤来这样做——也就是说，对列表中的每个文档运行预测模型。这是 MapReduce 中用于数据挖掘的常见 Map 操作，Reduce 步骤只是组织预测列表，以便可以将它们追溯到原始文档。

我们将使用亚马逊的基础设施来运行我们的应用程序，使我们能够利用他们的计算资源。

# 获取数据

我们将要使用的数据是一组博客文章，这些文章被标注了年龄、性别、行业(也就是工作)以及，有趣的是，星座。这个数据是 2004 年 8 月从[http://blogger.com](http://blogger.com)收集的，在 60 多万个帖子里有 1.4 亿多字。每个博客都可能是由一个人写的*，并投入了一些工作来验证这一点(尽管，我们永远无法真正确定)。帖子也与发布日期相匹配，这使得它成为一个非常丰富的数据集。*

 *要获取数据，请前往[http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm](http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm)并点击下载语料库。从那里，解压文件到你的电脑上的一个目录。

数据集由一个博客组织成一个文件，文件名表示类。例如，其中一个文件名如下:

```
1005545.male.25.Engineering.Sagittarius.xml

```

文件名由句点分隔，字段如下:

*   **博主 ID** :这是一个简单的 ID 值，用来组织身份。
*   **性别**:这个不是男就是女，所有的博客都标识为这两个选项之一(这个数据集中不包含其他选项)。
*   **年龄**:给出了确切的年龄，但故意存在一些差距。目前的年龄在(包括)13-17 岁、23-27 岁和 33-48 岁之间。差距的原因是允许将博客分成有差距的年龄范围，因为很难将 18 岁的人的作品和 19 岁的人的作品分开，而且年龄本身可能有点过时，无论如何都需要更新到 19 岁。
*   **工业**:包括科学、工程、艺术和房地产在内的 40 个不同行业之一。此外，还包括一个未知行业的 indUnk。
*   **星座**:这是 12 个星象星座之一。

所有的值都是自我报告的，这意味着标签可能有错误或不一致，但被认为是最可靠的——如果人们想以这些方式保护他们的隐私，他们可以选择不设置值。

单个文件采用伪 XML 格式，包含一个`<Blog>`标签，然后是一系列`<post>`标签。每个`<post>`标签也由一个`<date>`标签进行。虽然我们可以将其解析为 XML，但逐行解析要简单得多，因为文件并不完全是格式良好的 XML，存在一些错误(主要是编码问题)。为了读取文件中的帖子，我们可以使用循环来遍历这些行。

我们设置了一个测试文件名，这样我们就可以看到它在运行:

```
import os 
filename = os.path.join(os.path.expanduser("~"), "Data", "blogs", "1005545.male.25.Engineering.Sagittarius.xml")

```

首先，我们创建一个列表，让我们存储每个帖子:

```
all_posts = []

```

然后，我们打开文件阅读:

```
with open(filename) as inf:
    post_start = False
    post = []
    for line in inf: 
        line = line.strip()
        if line == "<post>":
            # Found a new post
            post_start = True 
        elif line == "</post>":
            # End of the current post, append to our list of posts and start a new one
            post_start = False
            all_posts.append("n".join(post))
            post = []
        elif post_start:
            # In a current post, add the line to the text of the post
            post.append(line)

```

如果我们不在当前的帖子里，我们就忽略这条线。

然后我们可以抓取每个帖子的文本:

```
print(all_posts[0])

```

我们还可以找到这位作者创建了多少帖子:

```
print(len(all_posts))

```

# 朴素贝叶斯预测

我们现在将使用 mrjob 实现朴素贝叶斯算法，允许它处理我们的数据集。从技术上来说，我们的版本将是大多数朴素贝叶斯实现的简化版本，没有许多您期望的平滑小值的特性。

# mrjob 包

**mrjob** 包允许我们创建可以在亚马逊基础设施上轻松计算的 MapReduce 作业。虽然 mrjob 听起来像是对 Men 先生系列儿童书籍的刻意添加，但它代表着**地图缩小作业**。

You can install mrjob using the following: `pip install ``mrjob`
I had to install the filechunkio package separately using `conda install -c conda-forge filechunkio`, but this will depend on your system setup. There are other Anaconda channels for installing mrjob, check them with:
`anaconda search -t conda mrjob`

本质上，mrjob 提供了大多数 MapReduce 作业所需的标准功能。它最惊人的功能是，你可以编写相同的代码，在你的本地机器上进行测试(不需要像 Hadoop 那样的繁重基础设施)，然后推送到亚马逊的 EMR 服务或另一台 Hadoop 服务器上。

这使得测试代码变得非常容易，尽管它不能神奇地将一个大问题变小——任何局部测试都使用数据集的子集，而不是整个大数据集。相反，mrjob 给了你一个框架，你可以用一个小问题进行测试，并且更有信心解决方案将扩展到一个更大的问题，分布在不同的系统上。

# 提取博客文章

我们首先要创建一个 MapReduce 程序，从每个博客文件中提取每个帖子，并将它们存储为单独的条目。由于我们对帖子作者的性别感兴趣，我们也将提取它并将其存储在帖子中。

We can't do this in a Jupyter Notebook, so instead open a Python IDE for development. If you don't have a Python IDE you can use a text editor. I recommend PyCharm, although it has a larger learning curve and it is probably a bit heavy for just this chapter's code.

至少，我建议使用一个带有语法突出显示和变量名基本完成的 IDE(最后一个有助于轻松找到代码中的错别字)。

If you still can't find an IDE you like, you can write the code in an IPython Notebook and then click on File| Download As| Python. Save this file to a directory and run it as we outlined in [Chapter 11](01.html), *Classifying Objects in Images using Deep Learning*.

为了做到这一点，我们将需要`os`和`re`库，因为我们将获得环境变量，我们还将使用一个正则表达式进行单词分离:

```
import os 
import re

```

然后，我们导入 MRJob 类，它将从我们的 MapReduce 作业中继承:

```
from mrjob.job import MRJob

```

然后我们创建一个子类化 MRJob 的新类。我们将像以前一样使用类似的循环从文件中提取博客文章。我们接下来将定义的映射函数将在每一行工作，这意味着我们必须在映射函数之外跟踪不同的帖子。为此，我们制作`post_start`和 post 类变量，而不是函数内部的变量。然后我们定义我们的映射函数——从一个文件中取一行作为输入，生成博客文章。这些行保证从相同的每个作业文件中订购。这允许我们使用上面的类变量来记录当前的帖子数据:

```
class ExtractPosts(MRJob):
    post_start = False 
    post = []

    def mapper(self, key, line):
        filename = os.environ["map_input_file"]
        # split the filename to get the gender (which is the second token)
        gender = filename.split(".")[1]
        line = line.strip()
        if line == "<post>":
            self.post_start = True
        elif line == "</post>":
            self.post_start = False
            yield gender, repr("n".join(self.post))
            self.post = []
        elif self.post_start:
            self.post.append(line)

```

我们没有像前面那样将帖子存储在列表中，而是让出了它们。这允许 mrjob 跟踪输出。我们提供了性别和职位，这样我们就可以记录下每条记录匹配的性别。这个函数的其余部分的定义方式与我们上面的循环相同。

最后，在函数和类之外，我们设置脚本在从命令行调用时运行这个 MapReduce 作业:

```
if __name__ == '__main__': 
 ExtractPosts.run()

```

现在，我们可以使用以下 shell 命令运行这个 MapReduce 作业。

```
$ python extract_posts.py <your_data_folder>/blogs/51* 
 --output-dir=<your_data_folder>/blogposts --no-output

```

Just a reminder that you don't need to enter the $ on the above line - this just indicates this is a command run from the command line, and not in a Jupyter Notebook.

第一个参数`<your_data_folder>/blogs/51*`(只需记住将`<your_data_folder>`更改为您的数据文件夹的完整路径)获得数据样本(所有文件都以 51 开头，这只是 11 个文档)。然后，我们将输出目录设置到一个新文件夹中，并将其放入数据文件夹中，并指定不输出流数据。如果没有最后一个选项，输出数据会在我们运行时显示在命令行上——这对我们没有太大帮助，而且会大大降低计算机的速度。

运行脚本，很快每个博客文章将被提取并存储在我们的输出文件夹中。这个脚本只在本地计算机上的一个线程上运行，所以我们根本没有得到加速，但是我们知道代码在运行。

我们现在可以在输出文件夹中查看结果。创建了一堆文件，每个文件在单独的一行包含每个博客文章，前面是博客作者的性别。

# 训练朴素贝叶斯

现在我们已经提取了博客文章，我们可以在上面训练我们的朴素贝叶斯模型。直觉是，我们记录了一个单词被特定性别书写的概率，并将这些值记录在我们的模型中。为了对一个新样本进行分类，我们将概率相乘，找到最可能的性别。

这段代码的目的是输出一个文件，列出语料库中的每个单词，以及每个性别的单词频率。输出文件如下所示:

```
"'ailleurs" {"female": 0.003205128205128205}
"'air" {"female": 0.003205128205128205}
"'an" {"male": 0.0030581039755351682, "female": 0.004273504273504274}
"'angoisse" {"female": 0.003205128205128205}
"'apprendra" {"male": 0.0013047113868622459, "female": 0.0014172668603481887}
"'attendent" {"female": 0.00641025641025641}
"'autistic" {"male": 0.002150537634408602}
"'auto" {"female": 0.003205128205128205}
"'avais" {"female": 0.00641025641025641}
"'avait" {"female": 0.004273504273504274}
"'behind" {"male": 0.0024390243902439024} 
"'bout" {"female": 0.002034152292059272}

```

第一个值是单词，第二个值是一个字典，将性别映射到该性别作品中该单词的出现频率。

在 Python IDE 或文本编辑器中打开一个新文件。我们将再次需要`os`和`re`库，以及来自`mrjob`的`NumPy`和`MRJob`。我们还需要`itemgetter`，因为我们要整理字典:

```
import os 
import re 
import numpy as np 
from mrjob.job import MRJob 
from operator import itemgetter

```

我们还需要`MRStep`，它概述了 MapReduce 作业中的一个步骤。我们之前的工作只有一个步骤，它被定义为映射函数，然后是归约函数。这项工作将有多个步骤，我们映射，减少，然后再次映射和减少。直觉与我们在前面章节中使用的管道相同，其中一个步骤的输出是下一个步骤的输入:

```
from mrjob.step import MRStep

```

然后我们创建我们的单词搜索正则表达式并编译它，允许我们找到单词边界。这种类型的正则表达式比我们在前面几章中使用的简单拆分要强大得多，但是如果您正在寻找更精确的单词拆分器，我建议使用 NLTK 或 Spacey，就像我们在[第 6 章](05.html) <q>中使用的那样，使用朴素贝叶斯</q>的社交媒体 Insight:

```
word_search_re = re.compile(r"[w']+") 

```

我们为我们的培训定义了一个新的班级。我将首先提供整个类作为一个代码块，然后我们将回到每个部分来回顾它的功能:

```
class NaiveBayesTrainer(MRJob):

    def steps(self):
    return [
            MRStep(mapper=self.extract_words_mapping,
                   reducer=self.reducer_count_words),
            MRStep(reducer=self.compare_words_reducer),
    ]

    def extract_words_mapping(self, key, value):
        tokens = value.split()
        gender = eval(tokens[0])
        blog_post = eval(" ".join(tokens[1:]))
        all_words = word_search_re.findall(blog_post)
        all_words = [word.lower() for word in all_words]
        for word in all_words:
            # Occurence probability
            yield (gender, word), 1\. / len(all_words)

    def reducer_count_words(self, key, counts):
        s = sum(counts)
        gender, word = key #.split(":")
        yield word, (gender, s)

    def compare_words_reducer(self, word, values):
        per_gender = {}
        for value in values:
            gender, s = value
            per_gender[gender] = s
            yield word, per_gender

    def ratio_mapper(self, word, value):
        counts = dict(value)
        sum_of_counts = float(np.mean(counts.values()))
        maximum_score = max(counts.items(), key=itemgetter(1))
        current_ratio = maximum_score[1] / sum_of_counts
        yield None, (word, sum_of_counts, value)

    def sorter_reducer(self, key, values):
        ranked_list = sorted(values, key=itemgetter(1), reverse=True)
        n_printed = 0
        for word, sum_of_counts, scores in ranked_list:
            if n_printed < 20:
                print((n_printed + 1), word, scores)
            n_printed += 1
        yield word, dict(scores)

```

让我们看一下这段代码的各个部分，一步一个脚印:

```
class NaiveBayesTrainer(MRJob):

```

我们定义了 MapReduce 作业的步骤。有两个步骤:

第一步将提取单词出现概率。第二步将比较两种性别，并将每种性别的概率输出到我们的输出文件中。在每个 MRStep 中，我们定义了映射器和缩减器函数，它们是这个 NaiveBayesTrainer 类中的类函数(接下来我们将编写这些函数):

```
    def steps(self):
        return [
            MRStep(mapper=self.extract_words_mapping,
                   reducer=self.reducer_count_words),
            MRStep(reducer=self.compare_words_reducer),
        ]

```

第一个函数是第一步的映射函数。这个函数的目标是获取每个博客文章，获取该文章中的所有单词，然后记录事件。我们想要单词的频率，所以我们将返回`1 / len(all_words)`，这允许我们稍后对频率的值求和。这里的计算并不完全正确——我们还需要对文档的数量进行归一化。然而，在这个数据集中，类的大小是相同的，所以我们可以方便地忽略这一点，而对我们的最终版本几乎没有影响。

我们还输出了文章作者的性别，因为我们稍后将需要它:

```
    def extract_words_mapping(self, key, value):
        tokens = value.split()
        gender = eval(tokens[0])
        blog_post = eval(" ".join(tokens[1:]))
        all_words = word_search_re.findall(blog_post)
        all_words = [word.lower() for word in all_words]
        for word in all_words:
            # Occurence probability
            yield (gender, word), 1\. / len(all_words)

```

We used `eval` in the preceding code to simplify the parsing of the blog posts from the file, for this example. This is not recommended. Instead, use a format such as JSON to properly store and parse the data from the files. A malicious user with access to the dataset can insert code into these tokens and have that code run on your server.

在第一步的缩减器中，我们将每个性别和单词对的频率相加。我们还将关键字更改为单词，而不是组合，因为这允许我们在使用最终训练好的模型时按单词进行搜索(尽管，我们仍然需要输出性别供以后使用)；

```
    def reducer_count_words(self, key, counts):
        s = sum(counts)
        gender, word = key #.split(":")
        yield word, (gender, s)

```

最后一步不需要映射函数，这就是为什么我们没有添加映射函数。数据将作为一种身份映射器直接传递。然而，缩减器将结合给定单词下每个性别的频率，然后输出单词和频率字典。

这为我们提供了实现朴素贝叶斯所需的信息:

```
    def compare_words_reducer(self, word, values):
        per_gender = {}
        for value in values:
            gender, s = value
            per_gender[gender] = s
            yield word, per_gender

```

最后，当文件作为脚本运行时，我们设置代码来运行这个模型。我们需要将此代码添加到文件中:

```
if __name__ == '__main__': 
 NaiveBayesTrainer.run()

```

然后我们可以运行这个脚本。这个脚本的输入是前一个后提取器脚本的输出(如果你愿意的话，我们实际上可以将它们作为不同的步骤放在同一个 MapReduce 作业中)；

```
$ python nb_train.py <your_data_folder>/blogposts/ 
 --output-dir=<your_data_folder>/models/ --no-output

```

输出目录是一个文件夹，它将存储一个包含这个 MapReduce 作业输出的文件，这将是我们运行朴素贝叶斯分类器所需的概率。

# 把它们放在一起

我们现在可以使用这些概率来运行朴素贝叶斯分类器。我们将在 Jupyter Notebook 中进行此操作，尽管此处理本身可以转移到 mrjob 包中进行大规模执行。

首先，看一下在上一次 MapReduce 作业中指定的`models`文件夹。如果输出不止一个文件，我们可以通过在`models`目录中使用命令行函数将它们相互附加来合并文件:

```
cat * > model.txt

```

如果您这样做，您将需要用`model.txt`作为模型文件名来更新以下代码。

回到我们的笔记本，我们首先导入我们的脚本所需的一些标准导入:

```
import os 
import re
import numpy as np 
from collections import defaultdict 
from operator import itemgetter

```

我们再次重新定义了我们的单词搜索正则表达式——如果你在一个真实的应用程序中这样做，我建议集中功能。在训练和测试中，以相同的方式提取单词非常重要:

```
word_search_re = re.compile(r"[w']+")

```

Next, we create the function that loads our model from a given filename. The model parameters will take the form of a dictionary of dictionaries, where the first key is a word, and the inner dictionary maps each gender to a probability. We use `defaultdicts`, which will return zero if a value isn't present;

```
def load_model(model_filename):
    model = defaultdict(lambda: defaultdict(float))
    with open(model_filename) as inf: 
        for line in inf:
            word, values = line.split(maxsplit=1) 
            word = eval(word) 
            values = eval(values)
            model[word] = values
    return model

```

该行被分成两个部分，由空格分隔。第一个是单词本身，第二个是概率字典。对于每一个，我们在它们上运行`eval`来获得实际值，该值是在前面的代码中使用`repr`存储的。

接下来，我们加载我们的实际模型。您可能需要更改模型文件名—它将在最后一个 MapReduce 作业的输出目录中；

```
model_filename = os.path.join(os.path.expanduser("~"), "models", "part-00000") 
model = load_model(model_filename)

```

举个例子，我们可以看到男性和女性在单词 *i* (在 MapReduce 作业中所有单词都变成小写)的用法上的差异:

```
model["i"]["male"], model["i"]["female"]

```

接下来，我们创建一个可以使用这个模型进行预测的函数。在这个例子中，我们不会使用 scikit-learn 接口，而是创建一个函数。我们的函数以模型和文档为参数，返回最可能的性别:

```
def nb_predict(model, document):
    probabilities = defaultdict(lambda : 1)
    words = word_search_re.findall(document)
    for word in set(words): 
        probabilities["male"] += np.log(model[word].get("male", 1e-15)) 
        probabilities["female"] += np.log(model[word].get("female", 1e-15))
        most_likely_genders = sorted(probabilities.items(), key=itemgetter(1), reverse=True) 
    return most_likely_genders[0][0]

```

需要注意的是，我们使用`np.log`来计算概率。朴素贝叶斯模型中的概率通常很小。许多统计值都需要乘以小值，这可能会导致下溢错误，因为计算机的精度不够好，只能使整个值为 0。在这种情况下，它会导致两性的可能性都为零，从而导致不正确的预测。

为了解决这个问题，我们使用对数概率。对于两个值 a 和 b， *log(a× b)* 等于 *log(a) + log(b)* 。小概率的对数是负值，但相对较大。例如，log(0.00001)约为-11.5。这意味着，我们可以将对数概率相加，并以同样的方式比较这些值，而不是将实际概率相乘并冒下溢错误的风险(数字越大，表示可能性越大)。

If you want to obtain probabilities back from the log probabilities, make sure to undo the log operation by using e to the power of the value you are interested in. To revert -11.5 into the probability, take e<sup>-11.5</sup>, which equals 0.00001 (approximately).

使用对数概率的一个问题是它们不能很好地处理零值(尽管乘以零概率也不行)。这是因为日志(0)未定义。在朴素贝叶斯的一些实现中，在所有计数中添加 1 来消除这种情况，但是还有其他方法来解决这个问题。这是平滑值的一种简单形式。在我们的代码中，如果我们给定的性别没有看到这个词，我们只返回一个非常小的值。

Adding one to all counts above is a form of smoothing. Another option is to initialise to a very small value, such as 10<sup>-16</sup> - as long as its not exactly 0!

回到我们的预测函数，我们可以通过从数据集复制一篇文章来测试这一点:

```
new_post = """ Every day should be a half day. Took the afternoon off to hit the dentist, and while I was out I managed to get my oil changed, too. Remember that business with my car dealership this winter? Well, consider this the epilogue. The friendly fellas at the Valvoline Instant Oil Change on Snelling were nice enough to notice that my dipstick was broken, and the metal piece was too far down in its little dipstick tube to pull out. Looks like I'm going to need a magnet. Damn you, Kline Nissan, daaaaaaammmnnn yooouuuu.... Today I let my boss know that I've submitted my Corps application. The news has been greeted by everyone in the company with a level of enthusiasm that really floors me. The back deck has finally been cleared off by the construction company working on the place. This company, for anyone who's interested, consists mainly of one guy who spends his days cursing at his crew of Spanish-speaking laborers. Construction of my deck began around the time Nixon was getting out of office.
"""

```

然后我们用下面的代码进行预测:

```
nb_predict(model, new_post)

```

由此得出的预测*男性*对于本例是正确的。当然，我们从不在单个样本上测试模型。我们使用从 51 开始的文件来训练这个模型。样本不多，所以我们不能期望太高的准确度。

我们应该做的第一件事是训练更多的样本。我们将测试任何以 6 或 7 开头的文件，并对其余文件进行训练。

在命令行和博客文件夹所在的数据文件夹(`cd <your_data_folder>`)中，在新文件夹中创建博客文件夹的副本。

为我们的训练集制作一个文件夹:

```
mkdir blogs_train

```

将任何以 6 或 7 开头的文件从训练集中移到测试集中:

```
cp blogs/4* blogs_train/ 
cp blogs/8* blogs_train/

```

然后，为我们的测试集创建一个文件夹:

```
mkdir blogs_test

```

将任何以 6 或 7 开头的文件从训练集中移到测试集中:

```
cp blogs/6* blogs_test/ 
cp blogs/7* blogs_test/

```

我们将对训练集中的所有文件重新运行博客抽取。然而，这是一个比我们的系统更适合云基础设施的大型计算。因此，我们现在将解析工作转移到亚马逊的基础设施。

像以前一样，在命令行上运行以下命令。唯一不同的是，我们在不同文件夹的输入文件上训练。在运行以下代码之前，请删除博客文章和模型文件夹中的所有文件:

```
$ python extract_posts.py ~/Data/blogs_train --output-dir=/home/bob/Data/blogposts_train --no-output

```

接下来是我们的朴素贝叶斯模型的训练。这里的代码将花费相当长的时间来运行。很多很多个小时。你可能想跳过本地运行这个，除非你有一个真正强大的系统！如果你确实想跳过，前往下一部分。

```
$ python nb_train.py ~/Data/blogposts_train/ --output-dir=/home/bob/models/ --no-output

```

我们将在测试集中的任何博客文件上进行测试。为了拿到文件，我们需要提取它们。我们将使用`extract_posts.py` MapReduce 作业，但是将文件存储在单独的文件夹中:

```
python extract_posts.py ~/Data/blogs_test --output-dir=/home/bob/Data/blogposts_test --no-output

```

回到 Jupyter 笔记本中，我们列出了所有输出的测试文件:

```
testing_folder = os.path.join(os.path.expanduser("~"), "Data", "blogposts_testing") 
testing_filenames = [] 
for filename in os.listdir(testing_folder): 
    testing_filenames.append(os.path.join(testing_folder, filename))

```

对于这些文件中的每一个，我们提取性别和文档，然后调用预测函数。我们在生成器中这样做，因为有很多文档，我们不想使用太多内存。生成器生成实际性别和预测性别:

```
def nb_predict_many(model, input_filename): 
    with open(input_filename) as inf: # remove leading and trailing whitespace 
    for line in inf: 
        tokens = line.split() 
        actual_gender = eval(tokens[0]) 
        blog_post = eval(" ".join(tokens[1:])) 
        yield actual_gender, nb_predict(model, blog_post)

```

然后，我们在整个数据集中记录预测和实际性别。我们的预测不是男性就是女性。为了使用 scikit-learn 中的`f1_score`函数，我们需要将它们转换成 1 和 0。为了做到这一点，如果性别是男性，我们记录一个 0，如果是女性，记录一个 1。为此，我们使用布尔测试，看看性别是否是女性。然后，我们使用 NumPy 将这些布尔值转换为`int`:

```
y_true = []
y_pred = [] 
for actual_gender, predicted_gender in nb_predict_many(model, testing_filenames[0]):                    
    y_true.append(actual_gender == "female")   
    y_pred.append(predicted_gender == "female") 
    y_true = np.array(y_true, dtype='int') 
    y_pred = np.array(y_pred, dtype='int')

```

现在，我们使用 scikit-learn 中的 F1 分数来测试该结果的质量:

```
from sklearn.metrics import f1_score 
print("f1={:.4f}".format(f1_score(y_true, y_pred, pos_label=None)))

```

0.78 的结果相当合理。我们可能可以通过使用更多的数据来改进这一点，但要做到这一点，我们需要转向能够处理它的更强大的基础架构。

# 亚马逊电子病历基础设施培训

我们将使用亚马逊的**弹性地图缩减** ( **EMR** )基础设施来运行我们的解析和模型构建工作。

为了做到这一点，我们首先需要在亚马逊的存储云中创建一个桶。为此，请在您的网络浏览器中打开亚马逊 S3 控制台，方法是转到[http://console.aws.amazon.com/s3](http://console.aws.amazon.com/s3)，然后单击创建桶。记住水桶的名字，因为我们以后会用到它。

右键单击新存储桶，然后选择属性。然后，更改权限，授予每个人完全访问权限。一般来说，这不是一个好的安全做法，我建议您在完成本章后更改访问权限。您可以使用亚马逊服务中的高级权限来授予您的脚本访问权限，并防止第三方查看您的数据。

左键单击存储桶将其打开，然后单击创建文件夹。命名文件夹 blogs_train。我们将把我们的训练数据上传到这个文件夹，以便在云上处理。

在您的计算机上，我们将使用亚马逊的 AWS 命令行界面，这是一个在亚马逊云上进行处理的命令行界面。

要安装它，请使用以下命令:

```
sudo pip install awscli

```

按照[http://docs . AWS . Amazon . com/CLI/latest/user guide/CLI-chap-get-setup . html](http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-set-up.html)中的说明设置该程序的凭据。

我们现在想把数据上传到我们的新桶中。首先，我们要创建我们的数据集，它是所有不以 6 或 7 开头的博客。有更多优雅的方式来完成这个复制，但没有一个是跨平台的，足以推荐。相反，只需复制所有文件，然后从训练数据集中删除以 6 或 7 开头的文件:

```
cp -R ~/Data/blogs ~/Data/blogs_train_large 
rm ~/Data/blogs_train_large/blogs/6* 
rm ~/Data/blogs_train_large/blogs/7*

```

接下来，将数据上传到你的亚马逊 S3 桶。请注意，这将需要一些时间，并使用相当多的上传数据(几百兆字节)。对于那些网速较慢的人来说，在网速较快的地方这样做可能是值得的；

```
aws s3 cp ~/Data/blogs_train_large/ s3://ch12/blogs_train_large --recursive --exclude "*" 
--include "*.xml"

```

我们将使用 mrjob 连接到亚马逊的 EMR(弹性地图缩减)——它为我们处理整个事情；这样做只需要我们的证书。按照[https://pythonhosted.org/mrjob/guides/emr-quickstart.html](https://pythonhosted.org/mrjob/guides/emr-quickstart.html)的说明，使用您的亚马逊凭证设置 mrjob。

完成后，我们稍微改变了我们的 mrjob 运行，在亚马逊 EMR 上运行。我们只是告诉 mrjob 使用-r 开关使用 emr，然后将 s3 容器设置为输入和输出目录。即使这将在亚马逊的基础设施上运行，它仍然需要相当长的时间来运行，因为 mrjob 的默认设置使用一台低功耗的计算机。

```
$ python extract_posts.py -r emr s3://ch12gender/blogs_train_large/ 
--output-dir=s3://ch12/blogposts_train/ --no-output 
$ python nb_train.py -r emr s3://ch12/blogposts_train/ --output-dir=s3://ch12/model/ --o-output

```

You will be charged for the usage of both S3 and EMR. This will only be a few dollars, but keep this in mind if you are going to keep running the jobs or doing other jobs on bigger datasets. I ran a very large number of jobs and was charged about $20 all up. Running just these few should be less than $4\. However, you can check your balance and set up pricing alerts, by going to [https://console.aws.amazon.com/billing/home](https://console.aws.amazon.com/billing/home)

blogposts_train 和 model 文件夹没有必要存在——它们将由 EMR 创建。事实上，如果它们存在，你会得到一个错误。如果要重新运行，只需将这些文件夹的名称更改为新名称，但请记住将两个命令更改为相同的名称(也就是说，第一个命令的输出目录是第二个命令的输入目录)。

If you are getting impatient, you can always stop the first job after a while and just use the training data gathered so far. I recommend leaving the job for an absolute minimum of 15 minutes and probably at least an hour. You can't stop the second job and get good results though; the second job will probably take about two to three times as long as the first job did.

如果你有能力购买更先进的硬件，mrjob 支持在亚马逊的基础设施上创建集群，还能使用更强大的计算硬件。通过在命令行指定类型和数量，可以在一组计算机上运行作业。例如，要使用 16 台 c1 .中型计算机提取文本，请运行以下命令:

```
$ python extract_posts.py -r emr s3://chapter12/blogs_train_large/blogs/ --output-dir=s3://chapter12/blogposts_train/ --no-output  --instance-type c1.medium --num-core-instances 16

```

In addition, you can create clusters separately and reattach jobs to those clusters. See mrjob's documentation at [https://pythonhosted.org/mrjob/guides/emr-advanced.html](https://pythonhosted.org/mrjob/guides/emr-advanced.html) for more information on this process. Keep in mind that more advanced options become an interaction between advanced features of mrjob and advanced features of Amazon's AWS infrastrucutre, meaning you will need to investigate both technologies to get high-powered processing. Keep in mind that if you run more instances of more powerful hardware, you will be charged more in turn.

现在，您可以返回 s3 控制台，从桶中下载输出模型。保存在本地，我们可以回到我们的 Jupyter 笔记本，使用新的模式。我们在这里重新输入代码，只突出显示不同之处，只是为了更新到我们的新模型:

```
ws_model_filename = os.path.join(os.path.expanduser("~"), "models", "aws_model")
aws_model = load_model(aws_model_filename) 
y_true = [] 
y_pred = [] 
for actual_gender, predicted_gender in nb_predict_many(aws_model, testing_filenames[0]):
    y_true.append(actual_gender == "female") 
    y_pred.append(predicted_gender == "female") 
y_true = np.array(y_true, dtype='int') 
y_pred = np.array(y_pred, dtype='int') 
print("f1={:.4f}".format(f1_score(y_true, y_pred, pos_label=None)))

```

额外数据的结果更好，为 0.81。

If everything went as planned, you may want to remove the bucket from Amazon S3—you will be charged for the storage.

# 摘要

在本章中，我们研究了在大数据上运行作业。按照大多数标准，我们的数据集实际上非常小——只有几百兆字节。许多工业数据集要大得多，因此需要额外的处理能力来执行计算。此外，我们使用的算法可以针对不同的任务进行优化，以进一步提高可扩展性。

我们的方法从博客文章中提取词频，以预测文档作者的性别。我们在 mrjob 中使用基于 MapReduce 的项目提取了博客和词频。提取这些信息后，我们就可以执行朴素贝叶斯式的计算来预测新文档的性别。

我们只是触及了 MapReduce 的表面，我们甚至没有充分利用它在这个应用程序中的潜力。要进一步学习这些课程，请将预测功能转换为 MapReduce 作业。也就是说，您在 MapReduce 上训练模型以获得模型，然后使用 MapReduce 运行模型以获得预测列表。通过在 MapReduce 中进行评估来扩展这一点，最终结果将简单地以 F1 分数的形式返回！

我们可以使用 mrjob 库在本地进行测试，然后自动设置并使用亚马逊的 EMR 云基础设施。您可以使用其他云基础设施，甚至定制的亚马逊 EMR 集群来运行这些 MapReduce 作业，但是要让它们运行起来，还需要进行更多的修补。**