# 数据挖掘入门

我们正在以人类历史上前所未有的规模收集关于我们世界的信息。随着这一趋势，我们现在越来越重视日常生活中对这些信息的使用。我们现在期望我们的计算机将网页翻译成其他语言，高精度地预测天气，推荐我们喜欢的书籍，并诊断我们的健康问题。无论是在应用广度还是在功效方面，这些期望都将在未来增长。**数据挖掘**是一种我们可以用来训练计算机用数据做出决策的方法论，它构成了当今许多高科技系统的主干。

**Python** 编程语言正在迅速普及，这是有原因的。它给了程序员灵活性，它有许多模块来执行不同的任务，Python 代码通常比任何其他语言都更易读和简洁。有大量活跃的研究人员、实践者和初学者使用 Python 进行数据挖掘。

在本章中，我们将介绍使用 Python 进行数据挖掘。我们将涵盖以下主题

*   什么是数据挖掘，我们可以在哪里使用它？
*   设置基于 Python 的环境来执行数据挖掘
*   亲和力分析的一个例子，根据购买习惯推荐产品
*   (经典)分类问题的一个例子，根据测量值预测植物种类

# 介绍数据挖掘

数据挖掘为计算机提供了一种学习如何利用数据做出决策的方法。这个决定可能是预测明天的天气，阻止垃圾邮件进入你的收件箱，检测网站的语言，或者在约会网站上寻找新的浪漫。数据挖掘有许多不同的应用，新的应用一直在被发现。

Data mining is part algorithm design, statistics, engineering, optimization, and computer science. However, combined with these *base* skills in the area, we also need to apply **domain knowledge (expert knowledge)**of the area we are applying the data mining. Domain knowledge is critical for going from good results to great results. Applying data mining effectively usually requires this domain-specific knowledge to be integrated with the algorithms.

大多数数据挖掘应用程序都使用相同的**高级**视图，其中一个模型从一些数据中学习并应用于其他数据，尽管细节经常会有很大的变化。

数据挖掘应用程序包括创建数据集和调整算法，如以下步骤所述

1.  我们从创建数据集开始数据挖掘过程，描述现实世界的一个方面。数据集由以下两个方面组成:

*   **样本**:这些是现实世界中的物体，比如一本书、一张照片、一只动物、一个人或者任何其他物体。在其他命名约定中，示例也被称为观察、记录或行。
*   **特征**:这些是我们数据集中样本的描述或测量。特征可以是长度、特定单词的出现频率、动物的腿数、它被创造的日期等等。特性也被称为变量、列、属性或协变量，这也是其他命名约定的一部分。

2.  下一步是调整数据挖掘算法。每个数据挖掘算法都有参数，或者在算法中，或者由用户提供。这种调整允许算法学习如何对数据做出决策。

举个简单的例子，我们可能希望计算机能够将人归类为*矮*或*高*。我们首先收集我们的数据集，其中包括不同人的身高以及他们被认为是矮还是高:

| **人** | **高度** | **矮还是高？** |
| one | 155 厘米 | 短的 |
| Two | 165 厘米 | 短的 |
| three | 175 厘米 | 高的 |
| four | 185 厘米 | 高的 |

如上所述，下一步涉及调整我们算法的参数。作为简单的算法；如果身高大于 *x* ，则人高。否则，它们是短的。然后，我们的训练算法将查看数据并决定 *x* 的良好值。对于前面的数据，这个阈值的合理值是 170 厘米。算法认为身高超过 170 厘米的人是高的。按照这个标准，其他任何人都被认为是矮的。这样，我们的算法就可以对新数据进行分类，例如身高 167 厘米的人，尽管我们以前可能从未见过有这些测量值的人。

在前面的数据中，我们有一个明显的特征类型。我们想知道人们是矮还是高，所以我们收集了他们的身高。这种特征工程是数据挖掘中的一个关键问题。在后面的章节中，我们将讨论选择要在数据集中收集的良好要素的方法。最终，这一步通常需要一些专家领域的知识，或者至少需要一些反复试验。

在本书中，我们将通过 Python 介绍数据挖掘。在某些情况下，我们选择清晰的代码和工作流，而不是执行每项任务的最佳方式。这种清晰性有时包括跳过一些可以提高算法速度或效率的细节。

# 使用 python 和 jupiter 笔记本

在本节中，我们将介绍 Python 的安装以及本书大部分内容将使用的环境，即 **Jupyter** 笔记本。此外，我们将安装 **NumPy** 模块，我们将在第一组示例中使用该模块。

The Jupyter Notebook was, until very recently, called the IPython Notebook. You'll notice the term in web searches for the project. Jupyter is the new name, representing a broadening of the project beyond using just Python.

# 安装 Python

Python 编程语言是一种奇妙的、通用的、易于使用的语言。

对于这本书，我们将使用 Python 3.5，它可以从 Python 组织的网站[https://www.python.org/downloads/](https://www.python.org/downloads/)上获得。不过我建议你用 Anaconda 安装 Python，可以在[https://www.continuum.io/downloads](https://www.continuum.io/downloads)官网下载。

There will be two major versions to choose from, Python 3.5 and Python 2.7\. Remember to download and install Python 3.5, which is the version tested throughout this book. Follow the installation instructions on that website for your system. If you have a strong reason to learn version 2 of Python, then do so by downloading the Python 2.7 version. Keep in mind that some code may not work as in the book, and some workarounds may be needed.

在这本书里，我假设你对编程和 Python 本身有一些了解。完成这本书不需要成为 Python 专家，尽管良好的知识水平会有所帮助。我不会在本书中解释一般的代码结构和语法，除非它不同于被认为是*正常的* python 编码实践。

如果您没有任何编程经验，我建议您从 Packt Publishing 处购买*学习 Python* 的书籍，或者在[www.diveintopython3.net](http://www.diveintopython3.net)在线购买*深入 Python* 的书籍

Python 组织还为那些对 Python 不熟悉的人维护了两个在线教程的列表:

*   对于想通过 Python 语言学习编程的非程序员:

[https://wiki.python.org/moin/BeginnersGuide/NonProgrammers](https://wiki.python.org/moin/BeginnersGuide/NonProgrammers)T2】

*   对于已经知道如何编程，但需要专门学习 Python 的程序员:

[https://wiki.python.org/moin/BeginnersGuide/Programmers](https://wiki.python.org/moin/BeginnersGuide/Programmers)
Windows 用户将需要从命令行设置一个环境变量来使用 Python，其他系统通常会立即执行。我们按照以下步骤进行设置

1.  首先，找到您在计算机上安装 Python 3 的位置；默认位置为`C:\Python35`。
2.  接下来，将该命令输入命令行(cmd 程序):将环境设置为`PYTHONPATH=%PYTHONPATH%;C:\Python35`。

Remember to change the `C:\Python35` if your installation of Python is in a different folder.

一旦您的系统上运行了 Python，您应该能够打开命令提示符，并可以运行以下代码来确保它安装正确。

```py
    $ python
    Python 3.5.1 (default, Apr 11 2014, 13:05:11)
    [GCC 4.8.2] on Linux
    Type "help", "copyright", "credits" or "license" for more 
      information.
    >>> print("Hello, world!")
Hello, world!
    >>> exit()

```

请注意，我们将使用美元符号($)来表示您在终端中键入的命令(在 Windows 上也称为 shell 或`cmd`)。您不需要键入该字符(或重新键入任何已经出现在屏幕上的内容)。只需键入该行的其余部分，然后按回车键。

运行上述`"Hello, world!"`示例后，退出程序，继续安装更高级的环境来运行 Python 代码，即 Jupyter Notebook。

Python 3.5 will include a program called **pip**, which is a package manager that helps to install new libraries on your system. You can verify that `pip` is working on your system by running the `$ pip freeze` command, which tells you which packages you have installed on your system. Anaconda also installs their package manager, `conda`, that you can use. If unsure, use `conda` first, use `pip` only if that fails.

# 安装 Jupyter 笔记本

**Jupyter** 是一个 Python 开发平台，包含了一些运行 Python 的工具和环境，比标准解释器有更多的特性。它包含强大的 Jupyter 笔记本，允许你在网络浏览器中编写程序。它还格式化您的代码，显示输出，并允许您注释您的脚本。它是探索数据集的一个很好的工具，我们将把它作为本书代码的主要环境。

要在您的计算机上安装 Jupyter 笔记本，您可以在命令行提示符下键入以下内容(不要键入 Python):

```py
    $ conda install jupyter notebook

```

安装它不需要管理员权限，因为 Anaconda 会将包保存在用户的目录中。

安装 Jupyter 笔记本后，您可以通过以下方式启动它:

```py
    $ jupyter notebook

```

运行此命令将完成两件事。首先，它将创建一个 Jupyter Notebook 实例——后端——它将在您刚刚使用的命令提示符下运行。其次，它将启动您的网络浏览器并连接到此实例，允许您创建一个新的笔记本。它看起来像下面的截图(在那里你需要用你当前的工作目录替换`/home/bob`):

![](images/image_01_001.png)

要停止 Jupyter 笔记本运行，请打开运行该实例的命令提示符(您之前用来运行`jupyter notebook  `命令的提示符)。然后，按 *Ctrl* + *C* ，系统会提示`Shutdown this notebook server (y/[n])?`。键入 *y* 并按*进入*，Jupyter 笔记本将关闭。

# 安装 sci kit-学习

`scikit-learn`包是一个机器学习库，用 Python 编写(但也包含其他语言的代码)。它包含许多算法、数据集、实用程序和框架来执行机器学习。Scikit-learnis 建立在科学 python 栈的基础上，包括用于速度的库，如`NumPy`和`SciPy`。Scikit-learn 在许多情况下是快速且可扩展的，对从初学者到高级研究用户的所有技能范围都很有用。我们将在[第 2 章](01.html)、*用 scikit-learn 估计器分类中介绍 scikit-learn 的更多细节。*

要安装`scikit-learn`，可以使用 Python 3 自带的`conda`实用程序，如果还没有的话，Python 3 还会安装`NumPy`和`SciPy`库。打开具有管理员/超级用户权限的终端，输入以下命令:

```py
    $ conda install scikit-learn

```

像 Ubuntu 或红帽这样的主要 Linux 发行版的用户可能希望从他们的包管理器安装正式的包。

Not all distributions have the latest versions of scikit-learn, so check the version before installing it. The minimum version needed for this book is 0.14\. My recommendation for this book is to use Anaconda to manage this for you, rather than installing using your system's package manager.

希望通过编译源代码来安装最新版本的用户，或者查看更详细的安装说明的用户，可以前往[http://scikit-learn.org/stable/install.html](http://scikit-learn.org/stable/install.html)并参考关于安装 scikit-learn 的官方文档。

# 一个简单的相似性分析示例

在这一节中，我们跳到第一个例子。数据挖掘的一个常见用例是通过询问购买产品的客户是否也想要另一个类似的产品来提高销售额。你可以通过亲和力分析来进行这种分析，亲和力分析是研究事物何时一起存在，即。相互关联。

重复统计学课上教过的现在臭名昭著的一句话，*相关性不是因果关系*。这个短语意味着亲和分析的结果不能给出原因。在我们的下一个示例中，我们对产品购买执行相似性分析。结果表明，产品是一起购买的，但并不是购买一种产品会导致购买另一种产品。例如，当确定如何使用结果来影响业务流程时，这种区别非常重要。

# 什么是亲和力分析？

相似性分析是一种给出样本(对象)之间相似性的数据挖掘类型。这可能是以下两者之间的相似之处:

*   **用户**在网站上，提供各种服务或定向广告
*   **物品**出售给那些用户，提供推荐的电影或产品
*   **人类基因**，寻找拥有相同祖先的人

我们可以用几种方法来衡量亲和力。例如，我们可以记录两种产品一起购买的频率。我们还可以记录一个人购买对象 1 和他们购买对象 2 时的陈述准确性。衡量相似性的其他方法包括计算样本之间的相似性，我们将在后面的章节中介绍。

# 产品推荐

将传统业务转移到网上(如商业)的一个问题是，过去由人类完成的任务需要自动化，以便在线业务能够扩展并与现有的自动化业务竞争。这方面的一个例子是追加销售，或者向已经购买的顾客出售额外的商品。通过数据挖掘的自动化产品推荐是电子商务革命背后的驱动力之一，这场革命每年将数十亿美元转化为收入。

在这个例子中，我们将关注一个基本的产品推荐服务。我们基于以下想法设计这一点:当两个项目在历史上一起购买时，它们在未来更有可能一起购买。这种想法是许多产品推荐服务背后的原因，无论是在线还是线下业务。

对于这种类型的产品推荐算法，一个非常简单的算法是简单地找到用户已经带来一个项目的任何历史案例，并推荐历史用户带来的其他项目。实际上，像这样简单的算法可以做得很好，至少比选择随机项目推荐要好。然而，它们可以显著改进，这就是数据挖掘的用武之地。

为了简化编码，我们一次只考虑两个项目。举个例子，人们可能在超市同时买面包和牛奶。在这个早期的例子中，我们希望找到表单的简单规则:

*如果一个人购买了产品 X，那么他们很可能会购买产品 Y*

涉及多个项目的更复杂的规则将不会被涵盖，例如购买香肠和汉堡的人更有可能购买番茄酱。

# 用 NumPy 加载数据集

数据集可以从随书提供的代码包中下载，也可以从官方 GitHub 资源库中下载，网址为:
[https://GitHub . com/data pipeline ou/learnidataminingwithtypsy 2](https://github.com/dataPipelineAU/LearningDataMiningWithPython2)
下载此文件并保存在您的计算机上，注意数据集的路径。最简单的方法是将它放在运行代码的目录中，但是我们可以从您计算机上的任何位置加载数据集。

对于此示例，我建议您在计算机上创建一个新文件夹来存储数据集和代码。从这里，打开你的 Jupyter 笔记本，导航到这个文件夹，并创建一个新的笔记本。

我们将在本例中使用的数据集是一个 NumPy 二维数组，这是本书其余部分中大多数示例的基础格式。该数组看起来像一个表，行代表不同的样本，列代表不同的特征。

单元格代表特定样本的特定特征的值。为了说明，我们可以用以下代码加载数据集:

```py
import numpy as np 
dataset_filename = "affinity_dataset.txt" 
X = np.loadtxt(dataset_filename)

```

将前面的代码输入(Jupyter)笔记本的第一个单元格。然后，您可以通过按 Shift + Enter 来运行代码(这也将为下一段代码添加一个新的单元格)。代码运行后，第一个单元格左侧的方括号将被分配一个递增的数字，让您知道这个单元格已经完成。第一个单元格应该如下所示:

![](images/B06162_1-1.png)

For code that will take more time to run, an asterisk will be placed here to denote that this code is either running or scheduled to run. This asterisk will be replaced by a number when the code has completed running (including if the code completes because it failed).

这个数据集有 100 个样本和五个特征，我们将需要知道这些，以备后面的代码使用。让我们使用以下代码提取这些值:

```py
n_samples, n_features = X.shape

```

如果您选择将数据集存储在除 Jupyter 笔记本所在目录之外的某个位置，则需要将`dataset_filename`值更改为新位置。

接下来，我们可以显示数据集的一些行来了解数据。在下一个单元格中输入以下代码行并运行它，以打印数据集的前五行:

```py
print(X[:5])

```

结果将显示在列出的前五笔交易中购买了哪些商品:

```py
[[ 0\.  1\.  0\.  0\.  0.] 
 [ 1\.  1\.  0\.  0\.  0.] 
 [ 0\.  0\.  1\.  0\.  1.] 
 [ 1\.  1\.  0\.  0\.  0.] 
 [ 0\.  0\.  1\.  1\.  1.]]

```

# 下载示例代码

您可以从您在[http://www.packtpub.com](http://www.packtpub.com/)的账户下载您购买的所有 Packt Publishing 书籍的示例代码文件。如果您在其他地方购买了这本书，您可以访问[http://www.packtpub.com/support](http://www.packtpub.com/support)并注册，以便将文件直接通过电子邮件发送给您。我还设置了一个 GitHub 存储库，其中包含代码的实时版本，以及新的修复、更新等等。您可以在此处的知识库中检索代码和数据集:[https://github . com/data pipelineu/learningdataminingwithtypsy 2](https://github.com/dataPipelineAU/LearningDataMiningWithPython2)

您可以通过一次查看每行(水平线)来读取数据集。第一行`(0, 1, 0, 0, 0)`显示在第一笔交易中购买的物品。每列(垂直行)代表每个项目。它们分别是面包、牛奶、奶酪、苹果和香蕉。因此，在第一笔交易中，这个人买了奶酪、苹果和香蕉，但没有买面包或牛奶。在新单元格中添加以下行，以便我们将这些特征数字转换为实际单词:

```py
features = ["bread", "milk", "cheese", "apples", "bananas"]

```

这些特性中的每一个都包含二进制值，只说明项目是否被购买，而不说明购买了多少。A *1* 表示*至少购买了 1 件*这种类型的物品，而 a *0* 表示该物品绝对没有购买。对于真实世界的数据集，需要使用精确的数字或更大的阈值。

# 实现规则的简单排序

我们希望找到类型为*的规则，如果一个人购买了产品 X，那么他们很可能会购买产品 y。*我们可以通过简单地找到两个产品一起购买的所有情况，很容易地在我们的数据集中创建所有规则的列表。然而，我们需要一种方法来确定好的规则和坏的规则，允许我们选择特定的产品来推荐。

我们可以用很多方法来评估这种类型的规则，我们将重点关注两个:**支持**和**信心。**

Support 是规则在数据集中出现的次数，它是通过简单地计算规则对其有效的样本数来计算的。它有时可以通过除以规则前提有效的总次数来规范化，但是我们将简单地计算这个实现的总数。

The **premise** is the requirements for a rule to be considered active. The **conclusion** is the output of the rule. For the example *if a person buys an apple, they also buy a banana*, the rule is only valid if the premise happens - a person buys an apple. The rule's conclusion then states that the person will buy a banana.

虽然支持度衡量规则存在的频率，但置信度衡量规则在使用时的准确性。您可以通过确定前提适用时规则适用的次数百分比来计算。我们首先计算一个规则应用于我们的数据的次数，然后除以前提(T0)出现的样本数。

举个例子，我们将计算规则*的支持度和置信度，如果一个人买苹果，他们也买香蕉。*

如下例所示，我们可以通过检查`sample[3]`的值来判断是否有人在交易中购买了苹果，其中我们将一个样本分配给矩阵的一行:

```py
sample = X[2]

```

同样，我们可以通过查看`sample[4]`的值是否等于 1(以此类推)来检查香蕉是否是在交易中购买的。我们现在可以计算我们的规则在数据集中存在的次数，并由此计算置信度和支持度。

现在我们需要为数据库中的所有规则计算这些统计数据。我们将通过为*有效规则*和*无效规则*创建字典来实现这一点。这本字典的关键是一个元组(前提和结论)。我们将存储索引，而不是实际的功能名称。因此，我们将存储(3 和 4)来表示先前的规则*如果一个人买苹果，他们也会买香蕉。*如果给出前提和结论，则认为规则有效。而如果给出了前提，但没有给出结论，则认为该规则对该样本无效。

以下步骤将帮助我们计算所有可能规则的可信度和支持度:

1.  我们首先建立一些字典来存储结果。我们将为此使用`defaultdict`，如果访问了一个尚不存在的键，它将设置一个默认值。我们记录有效规则、无效规则以及每个前提出现的次数:

```py
from collections import defaultdict 
valid_rules = defaultdict(int) 
invalid_rules = defaultdict(int) 
num_occurences = defaultdict(int)

```

2.  接下来，我们在一个大循环中计算这些值。我们迭代数据集中的每个样本，然后循环每个特征作为前提。当再次循环每个特征作为可能的结论时，将关系前提映射到结论。如果样本中包含购买前提和结论的人，我们在`valid_rules`中记录。如果他们没有购买结论产品，我们将此记录在`invalid_rules`中。
3.  对于 X 中的样本:

```py
for sample in X:
    for premise in range(n_features):
    if sample[premise] == 0: continue
# Record that the premise was bought in another transaction
    num_occurences[premise] += 1
    for conclusion in range(n_features):
    if premise == conclusion: 
# It makes little sense to
    measure if X -> X.
    continue
    if sample[conclusion] == 1:
# This person also bought the conclusion item
    valid_rules[(premise, conclusion)] += 1

```

如果前提对这个样本有效(它有一个 *1* 的值)，那么我们记录这个并检查我们规则的每个结论。我们跳过任何与前提相同的结论——这会给我们一些规则，比如:*如果一个人买苹果，那么他们就会买苹果*，这显然对我们没有多大帮助。

我们现在已经完成了计算必要的统计数据，现在可以计算每个规则的*支持*和*信心*。和以前一样，支持只是我们的`valid_rules`值:

```py
support = valid_rules

```

我们可以用同样的方法计算置信度，但是我们必须循环每个规则来计算:

```py
confidence = defaultdict(float)
for premise, conclusion in valid_rules.keys():
    rule = (premise, conclusion)
    confidence[rule] = valid_rules[rule] / num_occurences [premise]

```

我们现在有一部字典，它对每条规则都有支持和信心。我们可以创建一个函数，以可读的格式打印出规则。规则的签名采用前提和结论索引、我们刚刚计算的支持度和置信度字典，以及告诉我们`features`的含义的特征数组。然后我们打印出这个规则的`Support`和`Confidence`:

```py
for premise, conclusion in confidence:
    premise_name = features[premise]
    conclusion_name = features[conclusion]
    print("Rule: If a person buys {0} they will also 
          buy{1}".format(premise_name, conclusion_name))
    print(" - Confidence: {0:.3f}".format
          (confidence[(premise,conclusion)]))
    print(" - Support: {0}".format(support
                                   [(premise, 
                                     conclusion)]))
    print("")

```

我们可以通过以下方式调用它来测试代码——随意尝试不同的前提和结论:

```py
for premise, conclusion in confidence:
    premise_name = features[premise]
    conclusion_name = features[conclusion]
    print("Rule: If a person buys {0} they will also 
          buy{1}".format(premise_name, conclusion_name))
    print(" - Confidence: {0:.3f}".format
          (confidence[(premise,conclusion)]))
    print(" - Support: {0}".format(support
                                   [(premise, 
                                     conclusion)]))
    print("")

```

# 排名寻找最佳规则

现在我们可以计算所有规则的支持度和置信度，我们希望能够找到*最好的*规则。为此，我们执行排名，并打印具有最高值的那些。为了支持和信心，我们可以这样做。

为了找到支持度最高的规则，我们首先对支持度字典进行排序。默认情况下，词典不支持排序；`items()`函数给我们一个包含字典中数据的列表。我们可以使用`itemgetter`类作为关键字对这个列表进行排序，这允许对像这样的嵌套列表进行排序。使用`itemgetter(1)`允许我们根据值进行排序。`Setting reverse=True`先给我们最高的数值:

```py
from operator import itemgetter 
sorted_support = sorted(support.items(), key=itemgetter(1), reverse=True)

```

然后我们可以打印出前五条规则:

```py
sorted_confidence = sorted(confidence.items(), key=itemgetter(1),
                           reverse=True)
for index in range(5):
    print("Rule #{0}".format(index + 1))
    premise, conclusion = sorted_confidence[index][0]
    print_rule(premise, conclusion, support, confidence, features)

```

结果如下所示:

```py
Rule #1 
Rule: If a person buys bananas they will also buy milk 
 - Support: 27 
 - Confidence: 0.474 
Rule #2 
Rule: If a person buys milk they will also buy bananas 
 - Support: 27 
 - Confidence: 0.519 
Rule #3 
Rule: If a person buys bananas they will also buy apples 
 - Support: 27 
 - Confidence: 0.474 
Rule #4 
Rule: If a person buys apples they will also buy bananas 
 - Support: 27 
 - Confidence: 0.628 
Rule #5 
Rule: If a person buys apples they will also buy cheese 
 - Support: 22 
 - Confidence: 0.512

```

同样，我们也可以基于信心打印顶级规则。首先，计算排序后的置信度列表，然后使用与之前相同的方法打印出来。

```py
sorted_confidence = sorted(confidence.items(), key=itemgetter(1),
                           reverse=True)
for index in range(5):
    print("Rule #{0}".format(index + 1))
    premise, conclusion = sorted_confidence[index][0]
    print_rule(premise, conclusion, support, confidence, features)

```

两个规则都在列表的顶部附近。第一个是*如果一个人买苹果，他们也会买奶酪*，第二个是*如果一个人买奶酪，他们也会买香蕉*。商店经理可以使用这样的规则来组织他们的商店。例如，如果苹果本周打折，在附近摆放奶酪。同样，将两种香蕉与奶酪同时出售也没有什么意义，因为近 66%购买奶酪的人可能会购买香蕉——我们的出售不会增加香蕉的购买量。

Jupyter Notebook will display graphs inline, right in the notebook. Sometimes, however, this is not always configured by default. To configure Jupyter Notebook to display graphs inline, use the following line of code: `%matplotlib inline`

我们可以使用名为`matplotlib`的库来可视化结果。

我们将从一个简单的折线图开始，按照置信度的顺序显示规则的置信度值。`matplotlib`让这个变得简单——我们只要传入数字，它就会画出一个简单但有效的图:

```py
from matplotlib import pyplot as plt 
plt.plot([confidence[rule[0]] for rule in sorted_confidence])

```

![](images/image_01_003.png)

使用前面的图表，我们可以看到前五个规则有相当大的信心，但在那之后，功效下降得相当快。利用这些信息，我们可能决定只使用前五条规则来推动业务决策。最终，像这样的探索技术，结果取决于用户。

数据挖掘在这样的例子中有很大的探索能力。一个人可以使用数据挖掘技术来探索其数据集中的关系，以找到新的见解。在下一节中，我们将数据挖掘用于不同的目的:预测和分类。

# 一个简单的分类例子

在相似性分析示例中，我们在数据集中寻找不同变量之间的相关性。在分类中，我们有一个我们感兴趣的单一变量，我们称之为**类**(也称为目标)。在前面的例子中，如果我们对如何让人们购买更多的苹果感兴趣，我们将探索与苹果相关的规则，并使用这些规则来指导我们的决策。

# 什么是分类？

分类是数据挖掘在实际应用和研究中最大的用途之一。和以前一样，我们有一组代表我们感兴趣分类的对象或事物的样本。我们还有一个新的数组，类值。这些类值给了我们样本的分类。一些例子如下:

*   通过测量来确定植物的种类。这里的类值是:*这是哪个物种？*
*   确定图像是否包含狗。班级会是:*这个形象里有狗吗？*
*   根据特定测试的结果确定患者是否患有癌症。问题是:*这个病人有癌症吗？*

虽然前面的许多例子都是二元(是/否)问题，但它们并不一定是二元的，就像本节中的植物物种分类一样。

The goal of classification applications is to train a model on a set of samples with known classes and then apply that model to new unseen samples with unknown classes. For example, we want to train a spam classifier on my past e-mails, which I have labeled as spam or not spam. I then want to use that classifier to determine whether my next email is spam, without me needing to classify it myself.

# 加载和准备数据集

我们将在这个例子中使用的数据集是著名的植物分类 Iris 数据库。在这个数据集中，我们有 150 个植物样本，每个样本有四个测量值:**萼片长度**、**萼片宽度**、**花瓣长度**和**花瓣宽度**(均以厘米为单位)。这个经典数据集(1936 年首次使用！)是数据挖掘的经典数据集之一。有三类:**濑户鸢尾**、**彩叶鸢尾**、**海滨鸢尾**。目的是通过检查样本的测量值来确定样本是哪种类型的植物。

`scikit-learn`库内置了这个数据集，使得数据集的加载变得简单:

```py
from sklearn.datasets import load_iris 
dataset = load_iris() 
X = dataset.data 
y = dataset.target

```

也可以`print(dataset.DESCR)`查看数据集的轮廓，包括要素的一些细节。

该数据集中的要素是连续值，这意味着它们可以取任意范围的值。测量是这种类型特征的一个很好的例子，其中测量可以取值 1、1.2 或 1.25 等等。连续特征的另一个方面是，彼此接近的特征值表示相似性。萼片长 1.2 厘米的植物就像萼片宽 1.25 厘米的植物。

与之相反的是分类特征。这些特征虽然经常被表示为数字，但不能以同样的方式进行比较。在虹膜数据集中，类别值是分类特征的一个例子。类别 0 代表 Iris Setosa1 类代表云芝，2 类代表弗吉尼亚鸢尾。编号并不意味着鸢尾花更像云芝，而不是弗吉尼亚鸢尾花——尽管类别值更相似。这里的数字代表类别。我们能说的就是类别是相同还是不同。

还有其他类型的功能，我们将在后面的章节中介绍。这些包括像素强度、词频和 n-gram 分析。

虽然此数据集中的要素是连续的，但我们将在此示例中使用的算法需要分类要素。将连续特征转化为分类特征是一个称为离散化的过程。

一个简单的离散化算法是选择一些阈值，任何低于这个阈值的值都被赋予一个值 0。同时，任何高于这个值的都被赋予值 1。对于我们的阈值，我们将计算该特征的平均值。首先，我们计算每个特征的平均值:

```py
attribute_means = X.mean(axis=0)

```

这段代码的结果将是一个长度为 4 的数组，这是我们拥有的特征数量。第一个值是第一个特征的平均值，依此类推。接下来，我们使用它将数据集从具有连续特征的数据集转换为具有离散分类特征的数据集:

```py
assert attribute_means.shape == (n_features,)
X_d = np.array(X >= attribute_means, dtype='int')

```

我们将使用这个新的`X_d`数据集(对于 *X 离散化的*)进行我们的**训练和测试**，而不是原始数据集( *X* )。

# 实现 OneR 算法

**OneR** 是一个简单的算法，通过寻找特征值最频繁的类别来简单地预测样本的类别。 **OneR** 是 *One Rule* 的简写，表示我们通过选择性能最好的特征，只使用单一规则进行分类。虽然后来的一些算法要复杂得多，但这种简单的算法已经在一些真实数据集上显示出良好的性能。

该算法从迭代每个特征的每个值开始。对于该值，计算具有该特征值的每个类的样本数。记录最常见的特征值类别以及该预测的误差。

例如，如果一个特征有两个值， *0* 和 *1* ，我们首先检查所有具有值 *0* 的样本。对于这个值，我们可能有 20 个在 *A 类*，60 个在 *B 类*，还有 20 个在 *C 类*。这个值最常见的类是 *B* ，有 40 个实例有不同的类。该特征值的预测为 *B* ，误差为 40，因为有 40 个样本与预测的类别不同。然后，我们对该特征的值 *1* 执行相同的过程，然后对所有其他特征值组合执行相同的过程。

一旦计算出这些组合，我们通过对该特征的所有值的误差求和来计算每个特征的误差。选择总误差最小的特征作为*一个规则*，然后用于对其他实例进行分类。

在代码中，我们将首先创建一个函数来计算特定特征值的类预测和误差。我们有两个必要的导入，`defaultdict`和`itemgetter`，我们在前面的代码中使用了它们:

```py
from collections import defaultdict 
from operator import itemgetter

```

接下来，我们创建函数定义，它需要数据集、类、我们感兴趣的特征的索引以及我们正在计算的值。它在每个样本上循环，并计算每个特征值对应于特定类别的次数。然后，我们为当前要素/值对选择最常见的类别:

```py
def train_feature_value(X, y_true, feature, value):
# Create a simple dictionary to count how frequency they give certain
predictions
 class_counts = defaultdict(int)
# Iterate through each sample and count the frequency of each
class/value pair
 for sample, y in zip(X, y_true):
    if sample[feature] == value: 
        class_counts[y] += 1
# Now get the best one by sorting (highest first) and choosing the
first item
sorted_class_counts = sorted(class_counts.items(), key=itemgetter(1),
                             reverse=True)
most_frequent_class = sorted_class_counts[0][0]
 # The error is the number of samples that do not classify as the most
frequent class
 # *and* have the feature value.
    n_samples = X.shape[1]
    error = sum([class_count for class_value, class_count in
                 class_counts.items()
 if class_value != most_frequent_class])
    return most_frequent_class, error

```

作为最后一步，我们还计算了这个规则的误差。在`OneR`算法中，任何具有该特征值的样本都将被预测为最频繁类别。因此，我们通过对其他类(不是最频繁的类)的计数求和来计算误差。这些表示导致错误或不正确分类的训练样本。

有了这个函数，我们现在可以通过循环该特征的所有值，对误差求和，并记录每个值的预测类来计算整个特征的误差。

该函数需要我们感兴趣的数据集、类和要素索引。然后，它遍历不同的值，并找到最准确的特征值用于该特定特征，如`OneR`中的规则:

```py
def train(X, y_true, feature): 
    # Check that variable is a valid number 
    n_samples, n_features = X.shape 
    assert 0 <= feature < n_features 
    # Get all of the unique values that this variable has 
    values = set(X[:,feature]) 
    # Stores the predictors array that is returned 
    predictors = dict() 
    errors = [] 
    for current_value in values: 
        most_frequent_class, error = train_feature_value
        (X, y_true, feature, current_value) 
        predictors[current_value] = most_frequent_class 
        errors.append(error) 
    # Compute the total error of using this feature to classify on 
    total_error = sum(errors) 
    return predictors, total_error

```

让我们更详细地看一下这个函数。

在一些初始测试之后，我们找到给定特性的所有唯一值。下一行中的索引查看给定功能的整个列，并将其作为数组返回。然后，我们使用 set 函数仅查找唯一值:

```py
    values = set(X[:,feature_index])

```

接下来，我们创建字典来存储预测值。该字典将特征值作为关键字，分类作为值。键为 1.5 且值为 2 的条目意味着，当要素的值设置为 1.5 时，将其归类为属于类别 2。我们还创建了一个列表，存储每个特征值的错误:

```py
predictors = {} 
    errors = []

```

作为该函数的主要部分，我们迭代该特征的所有唯一值，并使用我们先前定义的`train_feature_value`函数来查找给定特征值的最频繁类别和误差。我们如前所述存储结果:

最后，我们计算这个规则的总误差，并返回预测值和这个值:

```py
total_error = sum(errors)
return predictors, total_error

```

# 测试算法

当我们评估前面部分的相似性分析算法时，我们的目标是探索当前数据集。有了这个分类，我们的问题就不一样了。我们想建立一个模型，通过将以前看不到的样本与我们所知道的问题进行比较，来对它们进行分类。

为此，我们将机器学习工作流程分为两个阶段:培训和测试。在训练中，我们获取数据集的一部分并创建我们的模型。在测试中，我们应用该模型并评估它在数据集上的工作效率。由于我们的目标是创建一个可以对以前看不到的样本进行分类的模型，因此我们不能使用测试数据来训练模型。如果我们这样做，我们就有**过度拟合**的风险。

过拟合是创建一个模型的问题，该模型对我们的训练数据集进行了很好的分类，但在新样本上表现不佳。解决方法很简单:永远不要用训练数据来测试你的算法。这个简单的规则有一些复杂的变体，我们将在后面的章节中介绍；但是，目前，我们可以通过简单地将我们的数据集分成两个小数据集来评估我们的`OneR`实现:一个训练数据集和一个测试数据集。本节给出了该工作流程。

`scikit-learn`库包含一个将数据分割成训练和测试组件的功能:

```py
from sklearn.cross_validation import train_test_split

```

该函数将按照给定的比率(默认情况下使用数据集的 25%进行测试)将数据集分成两个子数据集。它是随机进行的，这提高了算法在真实世界环境(我们期望数据来自随机分布)中执行预期的信心:

```py
Xd_train, Xd_test, y_train, y_test = train_test_split(X_d, y, 
    random_state=14)

```

我们现在有两个较小的数据集:`Xd_train`包含我们用于训练的数据，`Xd_test`包含我们用于测试的数据。`y_train`和`y_test`给出了这些数据集对应的类值。

我们还指定了一个`random_state`。设置随机状态将在每次输入相同的值时给出相同的分割。它将*看起来*是随机的，但是使用的算法是确定性的，输出将是一致的。对于这本书，我建议将随机状态设置为与我相同的值，因为它会给你与我相同的结果，让你验证你的结果。要获得每次运行时都会改变的真正随机的结果，请将`random_state`设置为`None`。

接下来，我们计算数据集所有特征的预测值。请记住，在此过程中只使用培训数据。我们迭代数据集中的所有要素，并使用我们之前定义的函数来训练预测器和计算误差:

```py
all_predictors = {} 
errors = {} 
for feature_index in range(Xd_train.shape[1]): 
    predictors, total_error = train(Xd_train,
                                    y_train,
                                    feature_index) 
    all_predictors[feature_index] = predictors 
    errors[feature_index] = total_error

```

接下来，我们通过找到误差最小的特征来找到用作我们的*一个规则*的最佳特征:

```py
best_feature, best_error = sorted(errors.items(), key=itemgetter(1))[0]

```

然后，我们通过存储最佳特征的预测值来创建我们的`model`:

```py
model = {'feature': best_feature,
         'predictor': all_predictors[best_feature]}

```

我们的模型是一个字典，它告诉我们哪个特征用于我们的*一个规则*以及基于它所具有的值做出的预测。给定这个模型，我们可以通过找到特定特征的值并使用适当的预测器来预测以前看不到的样本的类别。下面的代码对给定的示例执行此操作:

```py
variable = model['feature'] 
predictor = model['predictor'] 
prediction = predictor[int(sample[variable])]

```

通常我们希望一次预测几个新的样本，这可以使用下面的函数来实现。它只是使用上面的代码，但是迭代一个数据集中的所有样本，获得每个样本的预测:

```py
def predict(X_test, model):
variable = model['feature']
predictor = model['predictor']
y_predicted = np.array([predictor
                        [int(sample[variable])] for sample
                        in X_test])
return y_predicted

```

对于我们的`testing`数据集，我们通过调用以下函数获得预测:

```py
y_predicted = predict(Xd_test, model)

```

然后，我们可以通过将其与已知类别进行比较来计算其准确性:

```py
accuracy = np.mean(y_predicted == y_test) * 100 
print("The test accuracy is {:.1f}%".format(accuracy))

```

这个算法给出了 65.8%的准确率，对于单个规则来说还不错！

# 摘要

在本章中，我们介绍了使用 Python 进行数据挖掘。如果您可以运行本节中的代码(请注意，完整的代码可以在提供的代码包中找到)，那么您的计算机就可以完成本书其余部分的大部分内容。其他 Python 库将在后面的章节中介绍，以执行更专业的任务。

我们使用 Jupyter Notebook 来运行我们的代码，它允许我们立即查看一小部分代码的结果。Jupyter 笔记本是一个有用的工具，将在整本书中使用。

我们引入了一个简单的相似性分析，寻找一起购买的产品。这种类型的探索性分析提供了对业务流程、环境或场景的洞察。来自这些类型分析的信息可以帮助业务流程，找到下一个重大的医学突破，或者创造下一个人工智能。

此外，在本章中，有一个使用`OneR`算法的简单分类示例。这个简单的算法只是找到最佳特征，并预测在训练数据集中最常具有该值的类。

为了扩展本章的结果，考虑如何实现一个可以考虑多个特征/值对的`OneR`变体。尝试实现你的新算法并对其进行评估。请记住在单独的数据集上对训练数据测试您的算法。否则，您将面临数据过度拟合的风险。

在接下来的几章中，我们将扩展分类和相似性分析的概念。我们还将在 scikit-learn 包中引入分类器，并使用它们进行机器学习，而不是自己编写算法。