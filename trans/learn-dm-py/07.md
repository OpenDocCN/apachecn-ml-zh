# 使用图挖掘遵循建议

图形可以用来表示各种各样的现象。在线社交网络尤其如此，还有**物联网** ( **物联网**)。图形挖掘是一项大生意，像脸书这样的网站都在运行对图形进行的数据分析实验。

社交媒体网站建立在参与的基础上。没有活跃的新闻源或有趣的朋友跟随的用户，不参与网站。相比之下，拥有更多有趣朋友的用户和*关注者*参与度更高，看到的广告也更多。这为网站带来了更大的收入来源。

在本章中，我们将了解如何定义图的相似性，以及如何在数据挖掘上下文中使用它们。同样，这是基于一个现象模型。我们看一些基本的图概念，像子图和连通分支。这就引出了聚类分析的研究，我们在[第 10 章](09.html)<q>*聚类新闻文章中对此进行了更深入的探讨。*</q>

本章涵盖的主题包括:

*   聚类数据以寻找模式
*   从以前的实验中加载数据集
*   从推特获取关注者信息
*   创建图形和网络
*   寻找用于聚类分析的子图

# 正在加载数据集

在本章中，我们的任务是基于共享连接在在线社交网络上推荐用户。我们的逻辑是，如果两个用户有相同的朋友，他们就高度相似，值得互相推荐。我们希望我们的建议具有高价值。我们只能在它变得乏味之前推荐这么多人，因此我们需要找到吸引用户的推荐。

为此，我们使用前一章的歧义消除模型，只找到谈论 *Python 作为编程语言*的用户。在本章中，我们使用一个数据挖掘实验的结果作为另一个数据挖掘实验的输入。一旦我们选择了 Python 程序员，我们就可以利用他们的友谊来寻找彼此高度相似的用户群。两个用户之间的相似性将由他们有多少共同的朋友来定义。我们的直觉是，两个人有越多的共同朋友，两个人就越有可能成为朋友(因此应该在我们的社交媒体平台上)。

我们将使用上一章介绍的应用编程接口从推特创建一个小的社交图。我们寻找的数据是对类似主题(同样是 Python 编程语言)感兴趣的用户子集，以及他们所有朋友(他们关注的人)的列表。有了这些数据，我们将根据两个用户有多少共同的朋友来检查他们有多相似。

There are many other online social networks apart from Twitter. The reason we have chosen Twitter for this experiment is that their API makes it quite easy to get this sort of information. The information is available from other sites, such as Facebook, LinkedIn, and Instagram, as well. However, getting this information is more difficult.

要开始收集数据，请建立一个新的 Jupyter 笔记本和一个`twitter`连接的实例，就像我们在上一章中所做的那样。您可以重复使用上一章中的应用程序信息或创建新的应用程序信息:

```
import twitter
consumer_key = "<Your Consumer Key Here>"
consumer_secret = "<Your Consumer Secret Here>"
access_token = "<Your Access Token Here>"
access_token_secret = "<Your Access Token Secret Here>"
authorization = twitter.OAuth(access_token, 
access_token_secret, consumer_key, consumer_secret)
t = twitter.Twitter(auth=authorization, retry=True)

```

另外，设置文件名。您将希望在本实验中使用与您在[第 6 章](05.html)、*社交媒体洞察中使用的文件夹不同的文件夹，使用朴素贝叶斯*，确保您不会覆盖之前的数据集！

```
import os 
data_folder = os.path.join(os.path.expanduser("~"), "Data", "twitter")
output_filename = os.path.join(data_folder, "python_tweets.json")

```

接下来，我们需要一个用户列表。我们会像上一章一样搜索推文，寻找那些提到`python`这个词的人。首先，创建两个列表来存储推文的文本和相应的用户。我们稍后将需要用户标识，所以我们现在创建一个字典映射。代码如下:

```
original_users = [] 
tweets = []
user_ids = {}

```

我们现在将搜索 python 这个词，就像我们在上一章中所做的那样，并遍历搜索结果，只保存带有文本的推文(按照上一章的要求):

```
search_results = t.search.tweets(q="python", count=100)['statuses']
for tweet in search_results:
    if 'text' in tweet:
        original_users.append(tweet['user']['screen_name']) 
        user_ids[tweet['user']['screen_name']] = tweet['user']['id']
        tweets.append(tweet['text'])

```

运行这段代码将获得大约 100 条推文，在某些情况下可能会少一点。不过，并不是所有的都与编程语言相关。我们将通过使用上一章中训练的模型来解决这个问题。

# 使用现有模型进行分类

正如我们在上一章中了解到的，并非所有提到 python 这个词的推文都与编程语言有关。为此，我们将使用上一章中使用的分类器来获取基于编程语言的推文。我们的分类器并不完美，但它将导致比单独搜索更好的专门化。

在这种情况下，我们只对那些在推特上谈论编程语言 Python 的用户感兴趣。我们将使用上一章中的分类器来确定哪些推文与编程语言相关。从那里，我们将只选择那些在推特上谈论编程语言的用户。

为了完成这部分更广泛的实验，我们首先需要保存上一章的模型。打开我们在上一章制作的 Jupyter 笔记本，我们在其中构建和训练了分类器。

If you have closed it, then the Jupyter Notebook won't remember what you did, and you will need to run the cells again. To do this, click on the Cell menu on the Notebook and choose Run All.

计算完所有单元格后，选择最后一个空白单元格。如果您的笔记本末尾没有空白单元格，请选择最后一个单元格，选择“插入”菜单，然后选择“在下方插入单元格”选项。

我们将使用`joblib`库保存我们的模型并加载它。

`joblib` is included with the `scikit-learn` package as a built-in external package. No extra installation step needed! This library has tools for saving and loading models, and also for simple parallel processing - which is used in `scikit-learn` quite a lot.

首先，导入库并为我们的模型创建一个输出文件名(确保目录存在，否则不会创建它们)。我已经将这个模型存储在我的`Models`目录中，但是你可以选择将它们存储在其他地方。代码如下:

```
from sklearn.externals import joblib
output_filename = os.path.join(os.path.expanduser("~"), "Models", "twitter", "python_context.pkl")

```

接下来，我们使用`joblib`中的`dump`函数，它的工作方式与`json`库中同名的版本非常相似。我们传递模型本身和输出文件名:

```
joblib.dump(model, output_filename)

```

运行这段代码会将我们的模型保存到给定的文件名。接下来，回到您在上一小节中创建的新 Jupyter 笔记本，并加载此模型。

您需要通过复制以下代码在此笔记本中再次设置模型的文件名:

```
model_filename = os.path.join(os.path.expanduser("~"), "Models", "twitter", "python_context.pkl")

```

确保文件名是您之前保存模型时使用的文件名。接下来，我们需要重新创建我们的 BagOfWords 类，因为它是一个定制的类，不能由 joblib 直接加载。只需从上一章的代码中复制整个 BagOfWords 类，包括它的依赖项:

```
import spacy
from sklearn.base import TransformerMixin

# Create a spaCy parser
nlp = spacy.load('en')

class BagOfWords(TransformerMixin):
    def fit(self, X, y=None):
        return self

    def transform(self, X):
        results = []
        for document in X:
            row = {}
            for word in list(nlp(document, tag=False, parse=False, entity=False)):
                if len(word.text.strip()): # Ignore words that are just whitespace
                    row[word.text] = True
                    results.append(row)
        return results

```

In production, you would need to develop your custom transformers in separate, centralized files, and import them into the Notebook instead. This little hack simplifies the workflow, but feel free to experiment with centralizing important code by creating a library of common functionality.

现在加载模型只需要调用`joblib`的`load`功能:

```
from sklearn.externals import joblib
context_classifier = joblib.load(model_filename)

```

我们的 context_classifier 的工作原理与我们在[第 6 章](05.html)<q>*使用朴素贝叶斯的社交媒体洞察*中看到的笔记本模型对象完全一样，它是一个 Pipeline 的实例，具有与之前相同的三个步骤(`BagOfWords`、`DictVectorizer`)和一个`BernoulliNB`分类器。在这个模型上调用 predict 函数可以预测我们的推文是否与编程语言相关。代码如下:</q>

```
y_pred = context_classifier.predict(tweets)

```

如果带有推文的*与编程语言相关，则`y_pred`中带有*项的*将为 1，否则将为 0。从这里，我们可以获得相关的推文和相关用户:*

```
relevant_tweets = [tweets[i] for i in range(len(tweets)) if y_pred[i] == 1]
relevant_users = [original_users[i] for i in range(len(tweets)) if y_pred[i] == 1]

```

使用我的数据，这涉及到 46 个相关用户。比我们之前的 100 条推文/用户少了一点，但现在我们有了建立社交网络的基础。我们总是可以添加更多的数据来获得更多的用户，但是 40+的用户将足以作为第一遍浏览这一章。我建议回来，添加更多的数据，并再次运行代码，看看你会得到什么结果。

# 从推特获取关注者信息

有了最初的一组用户，我们现在需要获得这些用户的朋友。朋友是用户关注的人。这个的 API 叫做 friends/ids，有好有坏。好消息是，它在一次 API 调用中最多返回 5000 个朋友 id。坏消息是，你每 15 分钟只能打 15 个电话，这意味着每个用户至少需要 1 分钟才能获得所有关注者——如果他们有 5000 多个朋友(这种情况发生的频率可能比你想象的要高)，那么关注者就会更多。

该代码类似于我们之前的应用编程接口使用代码(获取推文)。我们将把它打包成一个函数，因为我们将在接下来的两节中使用这段代码。我们的函数接受推特用户的身份值，并返回他们的朋友。虽然这可能会让一些人感到惊讶，但许多推特用户有 5000 多名朋友。由于这一点，我们将需要使用推特的分页功能，它允许推特通过单独的应用编程接口调用返回多页数据。当你向推特询问信息时，它会给你你的信息和一个光标，这是一个整数，推特会用它来跟踪你的请求。如果没有更多信息，此光标为 0；否则，您可以使用提供的光标来获取下一页结果。传递这个光标让 twitter 继续你的查询，返回下一组数据给你。

在函数中，当这个游标不等于 0 时，我们继续循环(因为，当它等于 0 时，没有更多的数据要收集)。然后，我们对用户的关注者执行请求，并将他们添加到我们的列表中。我们在 try block 中这样做，因为可能会发生我们可以处理的错误。追随者的 id 存储在结果字典的 id 键中。获得这些信息后，我们更新光标。它将在循环的下一次迭代中使用。最后，我们检查一下我们是否有超过一万个朋友。如果是这样，我们就打破这个循环。代码如下:

```
import time

def get_friends(t, user_id):
    friends = []
    cursor = -1
    while cursor != 0: 
        try:
            results = t.friends.ids(user_id= user_id, cursor=cursor, count=5000)
            friends.extend([friend for friend in results['ids']])
            cursor = results['next_cursor'] 
            if len(friends) >= 10000:
                break
        except TypeError as e:
            if results is None:
                print("You probably reached your API limit, waiting for 5 minutes")
                sys.stdout.flush() 
                time.sleep(5*60) # 5 minute wait 
            else: 
                # Some other error happened, so raise the error as normal
                raise e
        except twitter.TwitterHTTPError as e:
            print(e)
            break
        finally:
            # Break regardless -- this stops us going over our API limit
            time.sleep(60)

```

It is worth inserting a warning here. We are dealing with data from the Internet, which means weird things can and do happen regularly. A problem I ran into when developing this code was that some users have many, many, many thousands of friends. As a fix for this issue, we will put a failsafe here, exiting the function if we reach more than 10,000 users. If you want to collect the full dataset, you can remove these lines, but beware that it may get stuck on a particular user for a very long time.

上面的大部分功能都是错误处理，因为在处理外部 API 时会有很多错误！

最有可能发生的错误是我们不小心达到了我们的应用编程接口限制(虽然我们有一个睡眠来停止它，但是如果您在这个睡眠结束之前停止并运行您的代码，它可能会发生)。在这种情况下，结果是`None`，我们的代码将以`TypeError`失败。在这种情况下，我们等待 5 分钟，然后再次尝试，希望我们已经到达了下一个 15 分钟的窗口。此时可能会出现另一个`TypeError`。如果其中一个有，我们就提出来，需要单独处理。

第二个可能发生的错误发生在 Twitter 端，比如要求一个不存在的用户或者其他一些基于数据的错误，导致`TwitterHTTPError`(这是一个类似于 HTTP 404 错误的概念)。在这种情况下，不要再尝试这个用户，只需返回我们获得的任何关注者(在这种情况下，很可能是 0)。

最后，推特只让我们每 15 分钟询问 15 次跟随者信息，所以我们会等 1 分钟再继续。我们在 finally 块中这样做，这样即使出现错误，它也会发生。

# 构建网络

现在我们要建立我们的用户网络，如果两个用户互相跟随，用户就会被链接起来。建立这个网络的目的是给我们一个数据结构，我们可以用它将用户列表分成组。然后，我们可以从这些组中相互推荐同一组中的人。从我们的原始用户开始，我们将为他们每个人获取朋友，并将其存储在字典中。使用这个概念，我们可以从最初的一组用户向外扩展图表。

从我们的原始用户开始，我们将获取他们每个人的朋友，并将其存储在字典中(从我们的`*user_id*`字典中获取用户标识后):

```
friends = {} 
for screen_name in relevant_users:
    user_id = user_ids[screen_name]
    friends[user_id] = get_friends(t, user_id)

```

接下来，我们将删除任何没有朋友的用户。对于这些用户，我们真的不能这样做推荐。相反，我们可能不得不看他们的内容或关注他们的人。但是，我们将把它排除在本章的范围之外，所以让我们删除这些用户。代码如下:

```
friends = {user_id:friends[user_id] 
           for user_id in friends
           if len(friends[user_id]) > 0}

```

根据您最初的搜索结果，我们现在有 30 到 50 个用户。我们现在要把这个数字增加到 150。下面的代码将花费相当长的时间来运行——考虑到 API 的限制，我们每分钟只能为一个用户获取一次好友。简单的数学会告诉我们，150 个用户需要 150 分钟，这至少是 2 小时 30 分钟。考虑到我们将花费在获取这些数据上的时间，确保我们只获得好用户是值得的。

然而，怎样才能成为一个好用户呢？鉴于我们将基于共享连接进行推荐，我们将基于共享连接搜索用户。我们将获得现有用户的朋友，从那些与现有用户联系更好的用户开始。为此，我们会记录用户出现在我们朋友列表中的所有时间。在考虑您的采样策略时，值得考虑应用程序的目标。为此，获得许多相似的用户使得推荐能够更有规律地适用。

要做到这一点，我们只需遍历我们所有的朋友列表，然后在每次有朋友出现时进行计数。

```
from collections import defaultdict
def count_friends(friends): 
    friend_count = defaultdict(int)
    for friend_list in friends.values(): 
        for friend in friend_list:
            friend_count[friend] += 1 
    return friend_count

```

计算我们当前的朋友数量，然后我们可以从我们的样本中获得最有联系的人(也就是说，我们现有列表中的大多数朋友)。代码如下:

```
friend_count = count_friends(friends)
from operator import itemgetter
best_friends = sorted(friend_count, key=friend_count.get, reverse=True)

```

从这里，我们建立了一个循环，一直持续到我们有 150 个用户的朋友。然后，我们遍历所有最好的朋友(按照拥有他们作为朋友的人数顺序进行)，直到找到一个我们还没有检查过的用户。然后我们获取该用户的朋友并更新`friends`计数。最后，我们计算出谁是我们列表中还没有的最有联系的用户:

```
while len(friends) < 150:
    for user_id, count in best_friends:
        if user_id in friends:
            # Already have this user, move to next one
            continue
        friends[user_id] = get_friends(t, user_id) 
        for friend in friends[user_id]: 
            friend_count[friend] += 1
        best_friends = sorted(friend_count.items(), key=itemgetter(1), reverse=True)
        break

```

然后代码将循环并继续，直到我们达到 150 个用户。

You may want to set these values lower, such as 40 or 50 users (or even just skip this bit of code temporarily). Then, complete the chapter's code and get a feel for how the results work. After that, reset the number of users in this loop to 150, leave the code to run for a few hours, and then come back and rerun the later code.

考虑到收集这些数据可能需要近 3 个小时，保存这些数据是个好主意，以防我们不得不关闭电脑。使用`json`库，我们可以轻松地将朋友词典保存到一个文件中:

```
import json
friends_filename = os.path.join(data_folder, "python_friends.json")
with open(friends_filename, 'w') as outf: 
    json.dump(friends, outf)

```

如果需要加载文件，使用`json.load`功能:

```
with open(friends_filename) as inf:
    friends = json.load(inf)

```

# 创建图表

在我们实验的这一点上，我们有一个用户和他们朋友的列表。这给了我们一个图表，其中一些用户是其他用户的朋友(尽管不一定是相反的)。

一个**图**是一组节点和边。节点通常是感兴趣的对象——在这种情况下，它们是我们的用户。这个初始图中的边表示用户 A 是用户 b 的朋友。我们称之为**有向图**，因为节点的顺序很重要。仅仅因为用户 A 是用户 B 的朋友，这并不意味着用户 B 是用户 A 的朋友。下面的示例网络显示了这一点，还有一个用户 C 是用户 B 的朋友，也是用户 B 的朋友:

![](images/B06162_07_01.png)

在 python 中，处理图形(包括创建、可视化和计算)的最佳库之一被称为 **NetworkX** 。

Once again, you can use Anaconda to install NetworkX: `conda install networkx`

首先，我们使用网络创建一个有向图。按照惯例，在导入 NetworkX 时，我们使用缩写 nx(尽管这不是必需的)。代码如下:

```
import networkx as nx 
G = nx.DiGraph()

```

我们将只可视化我们的关键用户，而不是所有的朋友(因为有成千上万的这样的用户，很难可视化)。我们获取主要用户，然后将他们作为节点添加到我们的图表中:

```
main_users = friends.keys() 
G.add_nodes_from(main_users)

```

接下来，我们设置边缘。如果第二个用户是第一个用户的朋友，我们创建从一个用户到另一个用户的边。为此，我们遍历给定用户的所有朋友。我们确保朋友是我们的主要用户之一(因为我们目前对可视化其他用户不感兴趣)，如果他们感兴趣，我们会添加边缘。

```
for user_id in friends:
    for friend in friends[user_id]:
        if str(friend) in main_users: 
            G.add_edge(user_id, friend) 

```

我们现在可以使用 network 的 draw 函数来可视化网络，该函数使用 matplotlib。为了在笔记本中获得图像，我们使用 matplotlib 上的 inline 函数，然后调用 draw 函数。代码如下:

```
 %matplotlib inline 
 nx.draw(G)

```

结果有点难以理解；它们只显示节点的环，很难计算出数据集的任何细节。完全不是一个好形象:

![](images/B06162_07_02.png)

我们可以通过使用 pyplot 来处理图形的创建，使图形更好一点，这是由 NetworkX 用来绘制图形的。导入`pyplot,`创建一个更大的图形，然后调用 NetworkX 的`draw`函数来增加图像的大小:

```
from matplotlib import pyplot as plt
plt.figure(3,figsize=(20,20))
nx.draw(G, alpha=0.1, edge_color='b')

```

通过放大图形并增加透明度，现在可以看到图形的外观轮廓:

![](images/B06162_07_03.png)

在我的图表中，有一大群用户彼此高度关联，而大多数其他用户根本没有多少联系。如你所见，它在中间连接得非常好！

这实际上是我们选择新用户的方法的一个特性——我们选择那些已经在我们的图中很好地链接在一起的用户，所以他们很可能会让这个群体变得更大。对于社交网络，一般来说，用户的连接数遵循幂律。一小部分用户有很多连接，其他人只有很少的连接。图形的形状经常被描述为有一条*长尾巴*。

通过放大部分图表，你可以开始看到结构。像这样可视化和分析图形是很难的——我们将在下一节看到一些工具来简化这个过程。

# 创建相似度图

这个实验的最后一步是根据用户分享的朋友数量来推荐用户。如前所述，我们的逻辑是，如果两个用户有相同的朋友，他们就高度相似。我们可以在此基础上向其他用户推荐一个。

因此，我们将利用我们现有的图(它有与友谊相关的边)并根据它的信息创建一个新的图。节点仍然是用户，但是边将是**加权边**。加权边只是具有权重属性的边。逻辑是，较高的权重比较低的权重表示两个节点之间更相似。这取决于上下文。如果权重代表距离，那么较低的权重表示更相似。

对于我们的应用程序，权重将是由该边连接的两个用户的相似度(基于他们共享的朋友数量)。这个图还具有不定向的性质。这是由于我们的相似度计算，其中用户 A 与用户 B 的相似度与用户 B 与用户 A 的相似度相同。

Other similarity measurements are directed. An example is ratio of similar users, which is the number of friends in common divided by the user's total number of friends. In this case, you would need a directed graph.

像这样计算两个列表之间的相似度有很多方法。例如，我们可以计算出这两个人有多少共同的朋友。然而，对于朋友多的人来说，这个标准总是会更高。相反，我们可以通过除以两者拥有的不同朋友的总数来使其正常化。这被称为 **Jaccard 相似度**。

Jaccard 相似度始终介于 0 和 1 之间，表示两者的重叠百分比。正如我们在[第 2 章](06.html)、*用 scikit-learn estimates*进行分类时所看到的，规范化是数据挖掘练习的重要部分，通常是一件好事。有些边缘情况下，您不会标准化数据，但默认情况下会先标准化。

为了计算 Jaccard 相似性，我们将两组从动件的交集除以两者的并集。这些是设置操作，我们有列表，所以我们需要先将好友列表转换为设置。代码如下:

```
friends = {user: set(friends[user]) for user in friends}

```

然后我们创建一个函数来计算两组好友列表的相似度。代码如下:

```
def compute_similarity(friends1, friends2):
    return len(friends1 & friends2) / (len(friends1 | friends2)  + 1e-6)

```

We add 1e-6 (or 0.000001) to the similarity above to ensure we never get a division by zero error, in cases where neither user has any friends. It is small enough to not really affect our results, but big enough to be more than zero.

从这里，我们可以创建用户之间相似度的加权图。我们将在本章的剩余部分中大量使用这个，所以我们将创建一个函数来执行这个操作。让我们看看阈值参数:

```
def create_graph(followers, threshold=0): 
    G = nx.Graph()
    for user1 in friends.keys(): 
        for user2 in friends.keys(): 
            if user1 == user2:
                continue
            weight = compute_similarity(friends[user1], friends[user2])
            if weight >= threshold:
                G.add_node(user1) 
                G.add_node(user2)
                G.add_edge(user1, user2, weight=weight)
    return G

```

我们现在可以通过调用这个函数来创建一个图。我们从没有阈值开始，这意味着所有链接都被创建。代码如下:

```
G = create_graph(friends)

```

结果是一个非常强的连通图——所有节点都有边，尽管其中许多节点的权重为 0。我们将通过绘制线宽相对于边的权重的图表来查看边的权重，较粗的线表示较高的权重。

由于节点的数量，为了更清楚地了解连接，将图形放大是有意义的:

```
plt.figure(figsize=(10,10))

```

我们将使用权重绘制边，因此我们需要先绘制节点。基于特定的标准，网络使用布局来确定节点和边的位置。可视化网络是一个非常困难的问题，尤其是随着节点数量的增长。可视化网络的技术有很多种，但它们的工作程度在很大程度上取决于您的数据集、个人偏好和可视化的目标。我发现 spring_layout 工作得很好，但是其他选项，如 circular_layout(如果没有其他功能，这是一个很好的默认值)、random_layout、shell_layout 和 spectral _ layout 也存在，并且在其他选项可能会失败的地方也有使用。

Visit [http://networkx.lanl.gov/reference/drawing.html](http://networkx.lanl.gov/reference/drawing.html)  for more details on layouts in NetworkX. Although it adds some complexity, the `draw_graphviz` option works quite well and is worth investigating for better visualizations. It is well worth considering in real-world uses.

让我们使用`spring_layout`进行可视化:

```
pos = nx.spring_layout(G)

```

使用我们的`pos`布局，我们可以定位节点:

```
nx.draw_networkx_nodes(G, pos)

```

接下来，我们绘制边缘。为了获得权重，我们迭代图中的边(以特定的顺序)并收集权重:

```
edgewidth = [ d['weight'] for (u,v,d) in G.edges(data=True)]

```

然后我们绘制边缘:

```
nx.draw_networkx_edges(G, pos, width=edgewidth)

```

结果将取决于您的数据，但它通常会显示一个图表，其中一大组节点连接非常紧密，而少数节点与网络的其余部分连接不良。

![](images/B06162_07_04.png)

这个图与之前的图的区别在于，边根据我们的相似性度量来确定节点之间的相似性，而不是一个节点是否是另一个节点的朋友(尽管两者之间有相似之处！).我们现在可以开始从这个图中提取信息，以便提出我们的建议。

# 寻找子图

从我们的相似性函数中，我们可以简单地对每个用户的结果进行排名，返回最相似的用户作为推荐——就像我们对产品推荐所做的那样。这是可行的，并且确实是执行这种类型分析的一种方式。

相反，我们可能希望找到彼此相似的用户群。我们可以建议这些用户成立一个小组，创建针对这一细分市场的广告，或者甚至只使用这些集群来自己做推荐。找到这些相似用户的聚类是一项叫做**聚类分析**的任务。

Cluster analysis is a difficult task, with complications that classification tasks do not typically have. For example, evaluating classification results is relatively easy - we compare our results to the ground truth (from our training set) and see what percentage we got right. With cluster analysis, though, there isn't typically a ground truth. Evaluation usually comes down to seeing if the clusters make sense, based on some preconceived notion we have of what the cluster should look like.

聚类分析的另一个复杂之处是，模型不能根据预期的学习结果进行训练——它必须使用基于聚类数学模型的近似，而不是用户希望从分析中获得的结果。

由于这些问题，聚类分析更多的是一种探索性工具，而不是一种预测工具。一些研究和应用使用聚类进行分析，但其作为预测模型的有效性取决于分析师选择参数并找到看起来正确的图形，而不是特定的评估指标。

# 连接的组件

聚类最简单的方法之一是在图中找到**连接的组件**。连通分支是图中通过边连接的一组节点。并非所有节点都需要相互连接才能成为连接的组件。然而，要使两个节点位于同一个连接的组件中，需要有一种方法通过沿边缘移动来使*从该连接的组件中的一个节点移动到另一个节点。*

Connected components do not consider edge weights when being computed; they only check for the presence of an edge. For that reason, the code that follows will remove any edge with a low weight.

NetworkX 有一个计算连接组件的功能，我们可以在图中调用它。首先，我们使用`create_graph`函数创建一个新的图，但是这次我们通过了一个 0.1 的阈值，只获得那些权重至少为 0.1 的边，表示两个节点用户之间共有 10%的关注者:

```
G = create_graph(friends, 0.1)

```

然后，我们使用网络来查找图中的连接组件:

```
sub_graphs = nx.connected_component_subgraphs(G)

```

为了了解图形的大小，我们可以遍历这些组并打印出一些基本信息:

```
for i, sub_graph in enumerate(sub_graphs):
    n_nodes = len(sub_graph.nodes()) 
    print("Subgraph {0} has {1} nodes".format(i, n_nodes))

```

结果将告诉您每个连接的组件有多大。我的结果有一个由 62 个用户组成的大型子图，以及许多由十几个或更少用户组成的小型子图。

我们可以改变**阈值**来改变连接的组件。这是因为阈值越高，连接节点的边越少，因此连接的组件越少，数量越多。我们可以通过以更高的阈值运行前面的代码来看到这一点:

```
G = create_graph(friends, 0.25) 
sub_graphs = nx.connected_component_subgraphs(G) 
for i, sub_graph in enumerate(sub_graphs): 
    n_nodes = len(sub_graph.nodes()) 
    print("Subgraph {0} has {1} nodes".format(i, n_nodes))

```

前面的代码给了我们更小的子图和更多的子图。我最大的集群被分成至少三部分，没有一个集群的用户超过 10 人。下图显示了一个示例集群，还显示了该集群中的连接。请注意，因为它是一个连接的组件，所以从该组件中的节点到图中的其他节点没有边(至少，阈值设置为 0.25)。

我们可以绘制整个图表，用不同的颜色显示每个连接的组件。由于这些相互连接的组件没有相互连接，因此将它们绘制在一张图表上实际上没有什么意义。这是因为节点和组件的定位是任意的，会混淆可视化。相反，我们可以在单独的子图形上分别绘制每一个。

在新单元格中，获取已连接的组件以及已连接组件的计数:

```
sub_graphs = nx.connected_component_subgraphs(G) 
n_subgraphs = nx.number_connected_components(G)

```

`sub_graphs` is a generator, not a list of the connected components. For this reason, use `nx.number_connected_components` to find out how many connected components there are; don't use `len`, as it doesn't work due to the way that NetworkX stores this information. This is why we need to recompute the connected components here.

创建一个新的 pyplot 图形，并留出足够的空间来显示所有连接的组件。出于这个原因，我们允许图形随着连接组件的数量而增大。

接下来，迭代每个连接的组件，并为每个组件添加一个子图。add _ subscript 的参数是子场景的行数、列数和我们感兴趣的子场景的索引。我的可视化使用三列，但是您可以尝试其他值而不是三列(只要记住更改这两个值):

```
fig = plt.figure(figsize=(20, (n_subgraphs * 3)))
for i, sub_graph in enumerate(sub_graphs): 
    ax = fig.add_subplot(int(n_subgraphs / 3) + 1, 3, i + 1)
    ax.get_xaxis().set_visible(False) 
    ax.get_yaxis().set_visible(False)
    pos = nx.spring_layout(G) 
    nx.draw_networkx_nodes(G, pos, sub_graph.nodes(), ax=ax, node_size=500) 
    nx.draw_networkx_edges(G, pos, sub_graph.edges(), ax=ax)

```

结果可视化了每个连接的组件，让我们了解每个组件中的节点数量以及它们之间的连接程度。

![](images/B06162_07_05.png)

If you are not seeing anything on your graphs, try rerunning the line:
`sub_graphs = nx.connected_component_subgraphs(G)`
The `sub_graphs` object is a generator and is "consumed" after being used.

# 优化标准

我们寻找这些连接组件的算法依赖于**阈值**参数，该参数决定了是否将边添加到图中。反过来，这直接决定了我们发现了多少相连的组件以及它们有多大。从这里开始，我们可能要决定使用哪个*最佳*阈值。这是一个非常主观的问题，没有确定的答案。这是任何聚类分析任务的主要问题。

但是，我们可以确定我们认为好的解决方案应该是什么样子，并根据这个想法定义一个度量标准。一般来说，我们通常需要一种解决方案，其中:

*   同一聚类中的样本(连接的组件)彼此高度*相似*
*   不同聚类中的样本彼此高度*不同*

**轮廓系数**是量化这些点的度量。给定一个样本，我们定义轮廓系数如下:

![](images/B06162_07_06.png)

其中 *a* 是**聚类内距离**或到样本聚类中其他样本的平均距离， <q>b</q> 是**聚类间距离**或到*次最近的*聚类中其他样本的平均距离。

To compute the overall Silhouette Coefficient, we take the mean of the Silhouette Coefficients for each sample. A clustering that provides a Silhouette Coefficient close to the maximum of 1 has clusters that have samples all similar to each other, and these clusters are very spread apart. Values near 0 indicate that the clusters all overlap and there is little distinction between clusters. Values close to the minimum of -1 indicate that samples are probably in the wrong cluster, that is, they would be better off in other clusters.

使用这个度量，我们希望找到一个解决方案(即阈值的值)，通过改变阈值参数来最大化剪影系数。为此，我们创建了一个以阈值为参数的函数，并计算轮廓系数。

然后，我们将其传递到 SciPy 的**优化**模块，该模块包含`minimize`函数，该函数用于通过更改其中一个参数来找到函数的最小值。虽然我们对最大化轮廓系数感兴趣，但 SciPy 没有最大化功能。相反，我们最小化剪影的倒数(这基本上是一回事)。

scikit-learn 库具有计算轮廓系数`sklearn.metrics.silhouette_score`的功能；但是，它没有修复 SciPy 最小化函数所需的函数格式。最小化函数要求变量参数在第一位(在我们的例子中是阈值)，任何参数在其后。在我们的例子中，我们需要通过朋友字典作为参数来计算图表。

The Silhouette Coefficient is not defined unless there are at least two nodes (in order for distance to be computed at all). In this case, we define the problem scope as invalid. There are a few ways to handle this, but the easiest is to return a very poor score. In our case, the minimum value that the Silhouette Coefficient can take is -1, and we will return -99 to indicate an invalid problem. Any valid solution will score higher than this. 

下面的函数结合了所有这些问题，为我们提供了一个函数，该函数接受一个阈值和一个朋友列表，并计算剪影系数。它通过使用 NetworkX 的`to_scipy_sparse_matrix`函数从图形中构建矩阵来实现这一点。

```
import numpy as np
from sklearn.metrics import silhouette_score

def compute_silhouette(threshold, friends):
    G = create_graph(friends, threshold=threshold) 
    if len(G.nodes()) < 2:
        return -99
    sub_graphs = nx.connected_component_subgraphs(G)

    if not (2 <= nx.number_connected_components() < len(G.nodes()) - 1): 
        return -99

    label_dict = {}
    for i, sub_graph in enumerate(sub_graphs): 
        for node in sub_graph.nodes(): 
            label_dict[node] = i

    labels = np.array([label_dict[node] for node in G.nodes()])
    X = nx.to_scipy_sparse_matrix(G).todense()
    X = 1 - X
    return silhouette_score(X, labels, metric='precomputed')

```

For evaluating sparse datasets, I recommend that you look into V-Measure or Adjusted Mutual Information. These are both implemented in scikit-learn, but they have very different parameters for performing their evaluation.

在撰写本文时，scikit-learn 中的剪影系数实现不支持稀疏矩阵。为此，我们需要调用`todense`函数。通常，这是一个坏主意——通常使用稀疏矩阵，因为数据通常不应该是密集格式。在这种情况下，它会很好，因为我们的数据集相对较小；但是，对于较大的数据集，不要尝试这种方法。

We have two forms of inversion happening here. The first is taking the inverse of the similarity to compute a distance function; this is needed, as the Silhouette Coefficient only accepts distances. The second is the inverting of the Silhouette Coefficient score so that we can minimize with SciPy's optimize module.

最后，我们创建将最小化的函数。这个函数是`compute_silhouette`函数的逆函数，因为我们希望分数越低越好。我们可以在我们的`compute_silhouette`功能中做到这一点——我在这里将它们分开，以阐明所涉及的不同步骤。

```
def inverted_silhouette(threshold, friends):
    return -compute_silhouette(threshold, friends)

```

此函数从原始函数创建一个新函数。当调用新函数时，所有相同的参数和关键字都传递给原始函数并返回返回值，除了这个返回值在返回之前被否定。

现在我们可以进行实际的优化了。我们在我们定义的反转`compute_silhouette`函数上调用最小化函数:

```
from scipy.optimize import minimize
result = minimize(inverted_silhouette, 0.1, args=(friends,))

```

This function will take quite a while to run. Our graph creation function isn't that fast, nor is the function that computes the Silhouette Coefficient. Decreasing the `maxiter` parameter's value will result in fewer iterations being performed, but we run the risk of finding a suboptimal solution.

运行这个函数，我得到了一个 0.135 的阈值，它返回 10 个组件。最小化函数返回的分数是-0.192。然而，我们必须记住，我们否定了这个价值。这意味着我们的分数实际上是 0.192。该值为正，这表明集群倾向于更好地分离(这是一件好事)。我们可以运行其他模型，并检查它是否导致更好的分数，这意味着集群被更好地分离。

我们可以使用这个结果来推荐用户——如果一个用户在一个特定的连接组件中，那么我们可以推荐同一个组件中的其他用户。这个建议遵循了我们使用 Jaccard 相似性来寻找用户之间的良好连接，我们使用连接的组件来将它们分成集群，以及我们使用优化技术来寻找这个设置中的最佳模型。

但是，大量用户可能根本没有连接，所以我们将使用不同的算法为他们找到集群。我们将在[第 10 章](09.html) *中看到聚类分析的其他方法，聚类新闻文章*。

# 摘要

在这一章中，我们研究了来自社交网络的图表以及如何对它们进行聚类分析。我们还研究了如何使用我们在[第 6 章](05.html)<q>*中创建的分类模型，使用朴素贝叶斯* <q>从 scikit-learn 中保存和加载模型。</q></q>

我们从社交网络推特上创建了一个朋友图。然后，我们根据两个用户的朋友来检查他们有多相似。拥有更多共同朋友的用户被认为更相似，尽管我们通过考虑他们拥有的朋友总数来规范化这一点。这是一种基于相似用户推断知识(如年龄或一般讨论话题)的常用方法。我们可以用这个逻辑向其他人推荐用户——如果他们追随用户 X，而用户 Y 与用户 X 相似，他们很可能会喜欢用户 Y。这在很多方面类似于我们前面章节中以事务为导向的相似性。

这个分析的目的是推荐用户，我们对聚类分析的使用允许我们找到相似用户的聚类。为此，我们在基于此相似性度量创建的加权图上找到了连接的组件。我们使用网络包来创建图形，使用我们的图形，并找到这些连接的组件。

然后我们使用了剪影系数，这是一个衡量聚类解决方案好坏的指标。根据簇内和簇间距离的概念，较高的分数表示较好的聚类。SciPy 的优化模块用于寻找最大化该值的解决方案。

在这一章中，我们看到了一些对立的行动。相似性是两个对象之间的度量，其中较高的值表示这些对象之间更相似。相比之下，距离是一种度量，其中较低的值表示更相似。我们看到的另一个对比是损失函数，分数越低被认为越好(也就是说，我们损失越少)。它的对立面是分数函数，分数越高被认为越好。

为了扩展本章的工作，检查 scikit-learn 中的 V-measure 和调整后的互信息分数。这些取代了本章中使用的轮廓系数。通过最大化这些指标得到的聚类是否比轮廓系数的聚类更好？进一步，你怎么知道？通常，聚类分析的问题是你不能客观地判断，可能会使用人为干预来选择最佳选项。

在下一章中，我们将看到如何从另一种新的数据类型——图像中提取特征。我们将讨论如何使用神经网络来识别图像中的数字，并开发一个程序来自动击败验证码图像。