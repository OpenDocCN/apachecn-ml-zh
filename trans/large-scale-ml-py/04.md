# 第四章。神经网络与深度学习

在这一章中，我们将讲述人工智能和机器学习中最令人兴奋的领域之一:深度学习。本章将介绍有效应用深度学习所必需的最重要的概念。我们将在本章中讨论的主题如下:

*   本质神经网络理论
*   在图形处理器或中央处理器上运行神经网络
*   神经网络的参数调整
*   H2O 大规模深度学习
*   自动编码器深度学习(预处理)

深度学习产生于发展神经网络的人工智能领域。严格来说，任何大的神经网络都可以认为是*深度学习*。然而，深度架构的最新发展需要的不仅仅是建立大型神经网络。深度体系结构与普通多层网络的区别在于，深度体系结构由多个预处理和无监督步骤组成，这些步骤检测数据中的潜在维度，这些维度随后将被馈送到网络的其他阶段。关于深度学习要知道的最重要的一点就是通过这些深度架构来学习和转化新的特性，以提高整体的学习精度。因此，当前一代深度学习方法和其他机器学习方法之间的一个重要区别是，通过深度学习，特征工程的任务在一定程度上是自动化的。如果这些概念听起来很抽象，不要太担心，它们将在本章后面与实际例子一起阐明。这些深度学习方法引入了新的复杂性，这使得有效地应用它们非常具有挑战性。

最大的挑战是它们在训练、计算时间和参数调整方面的困难。本章将讨论这些困难的解决办法。

在过去的十年中，深度学习的有趣应用可以在计算机视觉、自然语言处理和音频处理中找到，例如由脸书的一个研究小组创建的脸书深度人脸项目，该研究小组部分由著名的深度学习学者扬·勒坤领导。深度人脸旨在从数字图像中提取和识别人脸。谷歌有自己的项目 **DeepMind** ，由杰弗里·辛顿领导。谷歌最近推出了 **TensorFlow** ，这是一个提供深度学习应用的开源库，将在下一章详细介绍。

在我们开始释放通过图灵测试和数学竞赛的自主智能代理之前，让我们稍微后退一步，从头开始。

# 神经网络架构

现在让我们关注神经网络是如何组织的，从它们的架构和一些定义开始。

学习流一路向前传递到输出的网络称为**前馈神经网络**。

一个基本的前馈神经网络可以很容易地用网络图来描述，如下图所示:

![The neural network architecture](graphics/B05135_04_01.jpg)

在网络图中，可以看到这个架构由输入层、隐藏层和输出层组成。输入层包含特征向量(其中每个观察都有 *n* 特征)，输出层在分类的情况下由每类输出向量的独立单元组成，在回归的情况下由单个数值向量组成。

单元之间的连接强度通过稍后传递给激活函数的权重来表示。激活函数的目标是将其输入转换为输出，使二进制决策更加可分。

这些激活函数最好是可微的，这样就可以用来学习。

广泛使用的激活功能是 **sigmoid** 和 **tanh** ，最近更是**整流线性单元** ( **ReLU** )获得了牵引力。让我们比较一下最重要的激活函数，以便了解它们的优缺点。注意，我们提到了功能的输出范围和活动范围。输出范围只是函数本身的实际输出。然而，活动范围稍微复杂一些；这是梯度在最终权重更新中方差最大的范围。这意味着在该范围之外，梯度接近于零，并且不会增加学习期间的参数更新。这个接近于零的梯度的问题也被称为**消失梯度问题**，并通过 ReLU 激活功能来解决，该功能此时是较大神经网络最流行的激活:

![The neural network architecture](graphics/B05135_04_02.jpg)

需要注意的是，需要将特征缩放至所选激活功能的*激活范围*。大多数最新的包都将此作为标准的预处理过程，因此您不需要自己做:

![The neural network architecture](graphics/B05135_04_27.jpg)

Sigmoid 函数通常用于数学上的便利，因为它们的导数非常容易计算，我们将使用它来计算训练算法中的权重更新:

![The neural network architecture](graphics/B05135_04_28.jpg)

有趣的是，tanh 和逻辑 sigmoid 函数是线性相关的，并且 tanh 可以被视为 sigmoid 函数的重新缩放版本，因此它的范围在`-1`和`1`之间。

![The neural network architecture](graphics/B05135_04_29.jpg)

该功能是更深层架构的最佳选择。它可以看作是一个斜坡函数，其范围位于`0`以上到无穷大。你可以看到，它比 sigmoid 函数更容易计算。这个函数的最大好处是它绕过了消失梯度问题。如果 ReLU 是深度学习项目中的一个选项，请使用它。

**用于分类的 Softmax】**

到目前为止，我们已经看到激活函数将值乘以权重向量后，在一定范围内进行转换。我们还需要在提供平衡类或概率输出(对数似然值)之前转换最后一个隐藏层的输出。

这将把前一层的输出转换成概率值，从而可以进行最终的类别预测。每当输出明显小于所有值的最大值时，这种情况下的幂运算将返回一个接近零的值；这样差异就被放大了:

![The neural network architecture](graphics/B05135_04_40.jpg)

**正向传播**

现在我们已经了解了激活函数和网络的最终输出，让我们看看输入特征是如何通过网络提供最终的预测的。具有大量单元和连接的计算看起来可能是一项复杂的任务，但幸运的是，神经网络的前馈过程归结为一系列向量计算:

![The neural network architecture](graphics/B05135_04_26.jpg)

通过执行以下步骤，我们得出最终预测:

1.  用第一层和第二层之间的权重对输入执行点积，并用激活函数转换结果。
2.  用第二和第三层之间的权重对第一隐藏层的输出执行点积。这些结果随后用第二隐藏层的每个单元上的激活函数进行转换。
3.  最后，我们通过将向量乘以激活函数(softmax 用于分类)来得出我们的预测。

我们可以将网络中的每一层视为一个向量，并应用简单的向量乘法。更正式地说，这看起来如下:![The neural network architecture](graphics/B05135_04_30.jpg)

=层 *x* 的权重向量

*b1* 和 *b2* 为偏差单位

*f* =激活功能

![The neural network architecture](graphics/B05135_04_31.jpg)

![The neural network architecture](graphics/B05135_04_32.jpg)

![The neural network architecture](graphics/B05135_04_33.jpg)

![The neural network architecture](graphics/B05135_04_34.jpg)

### 注

请注意，此示例基于单一隐藏层网络架构。

让我们对一个神经网络执行一个简单的前馈传递，该神经网络具有两个带有基本 NumPy 的隐藏层。我们将`softmax`功能应用于最终输出:

```py
import numpy as np
import math
b1=0 #bias unit 1
b2=0 #bias unit 2

def sigmoid(x):      # sigmoid function
    return 1 /(1+(math.e**-x))

def softmax(x):     #softmax function
    l_exp = np.exp(x)
    sm = l_exp/np.sum(l_exp, axis=0)
    return sm

# input dataset with 3 features
X = np.array([  [.35,.21,.33],
    [.2,.4,.3],
    [.4,.34,.5],
    [.18,.21,16] ])
len_X = len(X) # training set size
input_dim = 3 # input layer dimensionality
output_dim = 1 # output layer dimensionality
hidden_units=4

np.random.seed(22)
# create random weight vectors
theta0 = 2*np.random.random((input_dim, hidden_units))
theta1 = 2*np.random.random((hidden_units, output_dim))

# forward propagation pass
d1 = X.dot(theta0)+b1
l1=sigmoid(d1)
l2 = l1.dot(theta1)+b2
#let's apply softmax to the output of the final layer
output=softmax(l2)
```

### 注

请注意，偏置单元使函数能够上下移动，有助于更接近目标值。每个隐藏层由一个偏置单元组成。

**反向传播**

通过我们简单的前馈示例，我们已经迈出了训练模型的第一步。神经网络的训练非常类似于我们在其他机器学习算法中看到的梯度下降方法。也就是说，我们升级模型的参数，以便找到误差函数的全局最小值。神经网络的一个重要区别是，我们现在必须处理网络中的多个单元，我们需要独立训练这些单元。我们使用成本函数的偏导数来实现这一点，并计算当我们将特定参数向量改变一定量(学习速率)时，误差曲线下降了多少。我们从最接近输出的层开始，计算相对于损失函数导数的梯度。如果有隐藏层，我们移动到第二个隐藏层并更新权重，直到到达前馈网络中的第一层。

反向传播的核心思想与其他机器学习算法非常相似，重要的复杂性在于我们要处理多个层和单元。我们已经看到，网络中的每一层都由权重向量![The neural network architecture](graphics/B05135_04_30.jpg) ij 表示。那么，我们如何解决这个问题呢？我们不得不独立训练大量的重量，这似乎令人生畏。然而，非常方便的是，我们可以使用矢量化运算。就像我们对向前传球所做的一样，我们计算梯度并更新应用于权重向量的权重(![The neural network architecture](graphics/B05135_04_30.jpg) ij)。

我们可以总结反向传播算法中的以下步骤:

1.  前馈传递:我们随机初始化权重向量，并将输入与后续权重向量相乘，得到最终输出。
2.  Calculate the error: We calculate the error/loss of the output of the feedforward step.

    随机初始化权重向量。

3.  反向传播到最后一个隐藏层(相对于输出)。我们计算这个误差的梯度，并朝着梯度的方向改变权重。我们通过将权重向量![The neural network architecture](graphics/B05135_04_30.jpg) j 乘以所执行的梯度来实现这一点。
4.  Update the weights till the stopping criterion is reached (minimum error or number of training rounds):

    ![The neural network architecture](graphics/B05135_04_41.jpg)

我们现在已经讨论了任意两层神经网络的前馈通路；让我们在 NumPy 中将 SGD 的反向传播应用到上一个示例中使用的相同输入。特别注意我们如何升级重量参数:

```py
import numpy as np
import math
def sigmoid(x):      # sigmoid function
    return 1 /(1+(math.e**-x))

def deriv_sigmoid(y): #the derivative of the sigmoid function
    return y * (1.0 - y)   

alpha=.1    #this is the learning rate
X = np.array([  [.35,.21,.33],
    [.2,.4,.3],
    [.4,.34,.5],
    [.18,.21,16] ])
y = np.array([[0],
        [1],
        [1],
        [0]])
np.random.seed(1)
#We randomly initialize the layers
theta0 = 2*np.random.random((3,4)) - 1
theta1 = 2*np.random.random((4,1)) - 1

for iter in range(205000): #here we specify the amount of training rounds.
    # Feedforward the input like we did in the previous exercise
    input_layer = X
    l1 = sigmoid(np.dot(input_layer,theta0))
    l2 = sigmoid(np.dot(l1,theta1))

    # Calculate error 
    l2_error = y - l2

    if (iter% 1000) == 0:
        print "Neuralnet accuracy:" + str(np.mean(1-(np.abs(l2_error))))

    # Calculate the gradients in vectorized form 
    # Softmax and bias units are left out for instructional simplicity
    l2_delta = alpha*(l2_error*deriv_sigmoid(l2))
    l1_error = l2_delta.dot(theta1.T)
    l1_delta = alpha*(l1_error * deriv_sigmoid(l1))

    theta1 += l1.T.dot(l2_delta)
    theta0 += input_layer.T.dot(l1_delta)
```

现在看看如何随着每次通过网络而提高的准确度:

```py
Neuralnet accuracy:0.983345051044
Neuralnet accuracy:0.983404936523
Neuralnet accuracy:0.983464255273
Neuralnet accuracy:0.983523015841
Neuralnet accuracy:0.983581226603
Neuralnet accuracy:0.983638895759
Neuralnet accuracy:0.983696031345
Neuralnet accuracy:0.983752641234
Neuralnet accuracy:0.983808733139
Neuralnet accuracy:0.98386431462
Neuralnet accuracy:0.983919393086
Neuralnet accuracy:0.983973975799
Neuralnet accuracy:0.984028069878
Neuralnet accuracy:0.984081682304
Neuralnet accuracy:0.984134819919
```

**反向传播的常见问题**

神经网络的一个常见问题是，在反向传播优化过程中，梯度会陷入局部最小值。当误差最小化被欺骗为看到一个最小值(图像中的点 **S** )时，这种情况就会发生，在该点处，通过峰值 **S** 实际上只是一个局部凸起:

![The neural network architecture](graphics/B05135_04_03.jpg)

另一个常见的问题是当梯度下降错过全局最小值时，这有时会导致令人惊讶的低性能模型。这个问题被称为**超调**。

通过在模型超调时选择较低的学习速率或者在陷入局部极小值时选择较高的学习速率，可以解决这两个问题。有时这种调整仍然没有带来令人满意的快速收敛。最近，已经找到了一系列解决方案来缓解这些问题。我们刚刚介绍的对普通的`SGDalgorithms`进行微调的学习算法已经开发出来了。理解它们很重要，这样你就可以为任何给定的任务选择正确的任务。让我们更详细地介绍这些学习算法。

**小批量反向传播**

批处理梯度下降使用整个数据集计算梯度，但是反向传播 SGD 也可以使用所谓的**小批处理**，其中大小为 *k* (批处理)的数据集样本用于更新学习参数。每次更新之间的误差不规则量可以用小批量平滑掉，这样可以避免陷入和超过局部最小值。在大多数神经网络包中，我们可以改变算法的批量大小(我们将在后面看到)。根据训练示例的数量，10 到 300 之间的批量可能会有所帮助。

**动量训练**

动量是一种将先前权重更新的一部分添加到当前权重更新的方法:

![The neural network architecture](graphics/B05135_04_42.jpg)

这里，先前权重更新的一部分被添加到当前权重更新中。高动量参数可以帮助加快收敛速度，更快地达到全局最小值。看公式，可以看到一个 *v* 参数。这相当于以学习速率![The neural network architecture](graphics/B05135_04_35.jpg)更新梯度的速度。理解这一点的一个简单方法是，当梯度在多个实例中不断指向同一方向时，收敛速度随着向最小值的每一步而增加。这也消除了梯度之间一定幅度的不规则性。大多数包都有这个动量参数(我们将在后面的例子中看到)。当我们将该参数设置得太高时，我们必须记住，存在超过全局最小值的风险。另一方面，当我们将动量参数设置得太低时，系数可能会陷入局部极小值，也会减慢学习速度。动量系数的理想设置通常在. 5 和. 99 范围内。

**内斯特罗夫气势**

内斯特罗夫动量是经典动量的更新和改进版本。除了经典的动量训练外，它还会在梯度的方向*向前看*。换句话说，内斯特罗夫动量从 x 到 y 简单地移动了一步，并在这个方向上再移动一点，这样 *x* 到 *y* 就变成了 *x* 到 *{y (v1 +1)}* 在前一点给定的方向上。我将省略技术细节，但请记住，在收敛性方面，它始终优于正常的动量训练。如果内斯特罗夫势头有选择，那就利用它。

**自适应梯度(ADAGRAD)**

ADAGRAD 提供了特定于功能的学习率，利用了以前升级的信息:

![The neural network architecture](graphics/B05135_04_43.jpg)

ADAGRAD 根据来自该参数先前迭代梯度的信息更新每个参数的学习速率。这是通过将每个项除以其先前梯度平方和的平方根来实现的。这使得学习率随着时间的推移而降低，因为每次迭代的平方和将继续增加。降低学习速率的优点是大大降低了超过全局最小值的风险。

**弹性反向传播(RPROP)**

RPROP 是一种自适应方法，它不查看历史信息，而仅查看训练实例上偏导数的符号，并相应地更新权重。

![The neural network architecture](graphics/B05135_04_04.jpg)

快速反向传播学习的直接自适应方法:RPROP 算法。马丁·里德米勒 1993

RPROP 是一种自适应方法，它不查看历史信息，而仅查看训练实例上偏导数的符号，并且相应地更新权重。仔细检查前面的图像，我们可以看到，一旦误差的偏导数改变其符号( *> 0* 或 *< 0* )，梯度开始向相反的方向移动，导致对超调的全局最小校正。然而，如果这个符号没有任何变化，就会朝着全局最小值迈出更大的步伐。许多文章已经证明了 RPROP 相对于 ADAGRAD 的优越性，但在实践中，这并没有得到一致的证实。另一件需要记住的重要事情是，RPROP 在迷你批次中无法正常工作。

rmsprep

RMSProp 是一种自适应学习方法，不会降低学习速度:

![The neural network architecture](graphics/B05135_04_44.jpg)

RMSProp 也是一种自适应学习方法，它利用了动量学习和 ADAGRAD 的思想，重要的补充是它避免了学习速率随时间的收缩。使用这种技术，收缩由梯度平均值上的指数衰减函数控制。

以下是梯度下降优化算法的列表:

<colgroup><col style="text-align: left"> <col style="text-align: left"> <col style="text-align: left"> <col style="text-align: left"></colgroup> 
|   | 

应用程序

 | 

常见问题

 | 

实用技巧

 |
| --- | --- | --- | --- |
| **常规 SGD** | 广泛适用 | 过冲，陷入局部最小值 | 与动量和小批量一起使用 |
| adagrad | 较小的数据集< 10k | 缓慢收敛 | 使用. 01 到. 1 之间的学习率。广泛适用。处理稀疏数据 |
| **丙肝** | 大于 10k 的较大数据集 | 不适用于小批量 | 尽可能使用 RMSProp |
| rmsprep | 大于 10k 的较大数据集 | 对宽而浅的网无效 | 对于宽稀疏数据特别有用 |

## 神经网络学习什么以及如何学习

既然我们已经对各种形式的反向传播有了基本的了解，是时候解决神经网络项目中最困难的任务了:我们如何选择正确的架构？神经网络的一个关键能力是架构内的权重可以将输入转换到非线性特征空间，从而解决非线性分类(决策边界)和回归问题。让我们做一个简单而有见地的练习，在`neurolab`包中演示这个想法。我们只会用`neurolab`进行短暂的锻炼；对于可扩展学习问题，我们将提出其他方法。

首先，用`pip`安装`neurolab`包装。

从终端安装`neurolab`:

```py
> $pip install neurolab

```

通过这个例子，我们将使用`numpy`生成一个简单的非线性余弦函数，并训练一个神经网络来从一个变量预测余弦函数。我们将设置几个神经网络架构，以了解每个架构预测余弦目标变量的能力:

```py
import neurolab as nl
import numpy as np
from sklearn import preprocessing
import matplotlib.pyplot as plt
plt.style.use('ggplot')
# Create train samples
x = np.linspace(-10,10, 60)
y = np.cos(x) * 0.9
size = len(x)
x_train = x.reshape(size,1)
y_train = y.reshape(size,1)

# Create network with 4 layers and random initialized
# just experiment with the amount of layers

d=[[1,1],[45,1],[45,45,1],[45,45,45,1]]
for i in range(4):
    net = nl.net.newff([[-10, 10]],d[i])
    train_net=nl.train.train_gd(net, x_train, y_train, epochs=1000, show=100)
    outp=net.sim(x_train)
# Plot results (dual plot with error curve and predicted values)
    import matplotlib.pyplot 
    plt.subplot(2, 1, 1)
    plt.plot(train_net)
    plt.xlabel('Epochs')
    plt.ylabel('squared error')
    x2 = np.linspace(-10.0,10.0,150)
    y2 = net.sim(x2.reshape(x2.size,1)).reshape(x2.size)
    y3 = outp.reshape(size)
    plt.subplot(2, 1, 2)

    plt.suptitle([i ,'hidden layers'])
    plt.plot(x2, y2, '-',x , y, '.', x, y3, 'p')
    plt.legend(['y predicted', 'y_target'])
    plt.show()
```

现在仔细观察误差曲线如何表现，以及当我们给神经网络增加更多层时，预测值如何开始接近目标值。

通过零隐藏层，神经网络通过目标值投射一条直线。误差曲线在拟合不良的情况下迅速降至最小值:

![What and how neural networks learn](graphics/B05135_04_05.jpg)

有了一个隐藏层，网络开始接近目标输出。观察误差曲线有多不规则:

![What and how neural networks learn](graphics/B05135_04_06.jpg)

通过两个隐藏层，神经网络更接近目标值。误差曲线下降更快，不规则性更小:

![What and how neural networks learn](graphics/B05135_04_07.jpg)

一个几乎完美的符合三个隐藏层。误差曲线下降得更快(在 **220** 迭代左右)。

![What and how neural networks learn](graphics/B05135_04_08.jpg)

上图中的橙色线是误差如何随每个时期下降的可视化图(整个训练集)。它向我们表明，我们需要通过训练集的一定次数才能达到全局最小值。如果您更仔细地检查这个误差曲线，您会发现误差曲线在每个体系结构中的表现是不同的。下图(虚线)显示了预测值如何开始接近目标值。由于没有隐藏层，神经网络无法检测非线性函数，但一旦我们添加隐藏层，网络就会开始学习非线性函数和日益复杂的函数。事实上，神经网络可以学习任何可能的功能。这种学习每一个可能函数的能力被称为**通用近似定理**。我们可以通过向神经网络添加隐藏的神经元(单元和层)来修改这种近似。然而，我们确实需要谨慎，不要过量食用；增加大量的层和单元将导致训练数据的记忆，而不是拟合可推广的函数。通常，网络中的层数太多会影响预测的准确性。

## 选择合适的架构

正如我们已经看到的，可能的神经网络结构的组合空间几乎是无限的。那么如何提前知道哪种架构会适合我们的项目呢？我们需要某种启发式或经验法则来为特定任务设计架构。在最后一节中，我们使用了一个只有一个输出和一个特性的简单示例。然而，我们称之为*深度学习*的最近一波神经网络架构非常复杂，能够为任何给定任务构建正确的神经网络架构至关重要。正如我们之前提到的，典型的神经网络由输入层、一个或多个隐藏层和输出层组成。让我们详细看看架构的每一层，这样我们就可以有一种为任何给定任务设置正确架构的感觉。

### 输入层

当我们提到输入层时，我们基本上是在谈论将被用作神经网络的输入的特征。所需的预处理步骤高度依赖于数据的形状和内容。如果我们有在不同尺度上测量的特征，我们需要重新缩放和归一化数据。在我们有大量特征的情况下，像主成分分析或奇异值分解这样的降维技术将变得值得推荐。

在学习之前，可以对输入应用以下预处理技术:

*   标准化、缩放和异常值检测
*   降维(奇异值分解和因子分析)
*   预处理(自动编码器和玻尔兹曼机器)

我们将在接下来的例子中介绍这些方法。

### 隐藏层

我们如何选择隐藏层中的单位数量？我们在网络中增加了多少隐藏层？我们在前面的例子中已经看到，没有隐藏层的神经网络不能学习非线性函数(无论是在回归的曲线拟合中还是在分类的决策边界中)。因此，如果有一个非线性模式或决策边界要投影，我们将需要隐藏层。说到选择隐藏层中的单位数量，我们一般希望隐藏层中的单位数量少于输入层中的单位数量，而单位数量多于输出单位数量:

*   优选地，比输入特征的数量更少的隐藏单元
*   超过输出单位数量的单位(分类类别)

有时，当目标函数的形状非常复杂时，就会出现异常。在我们添加的单位多于输入尺寸的情况下，我们添加特征空间的**扩展**。具有这种层的网络通常被称为**广域网**。

复杂网络可以学习更复杂的函数，但这并不意味着我们可以简单地继续堆叠层。建议控制层数，因为层数过多会导致过拟合、较高的 CPU 负载甚至是欠拟合的问题。通常一到四个隐藏层就足够了。

### 型式

最好使用一到四层作为起点。

### 输出层

每个神经网络都有一个输出层，就像输入层一样，高度依赖于所讨论的数据的结构。对于分类，我们一般会使用`softmax`功能。在这种情况下，我们应该使用与我们预测的类别数量相同的单位数量。

## 作用中的神经网络

让我们获得一些训练神经网络进行分类的实践经验。我们将使用 sknn，用于千层面和派尔恩 2 的 Scikit-learn 包装纸。您可以在[https://github.com/aigamedev/scikit-neuralnetwork/](https://github.com/aigamedev/scikit-neuralnetwork/)了解更多关于套餐的信息。

我们将使用这个工具，因为它的实用和 Pythonic 接口。这是对像 Keras 这样更复杂的框架的很好的介绍。

sknn 库可以在中央处理器或图形处理器上运行，无论您喜欢哪个。请注意，如果您选择使用图形处理器，sknn 将运行在:

```py
For CPU (most stable) :
# Use the GPU in 32-bit mode, from sknn.platform import gpu32

from sknn.platform import cpu32, threading
# Use the CPU in 64-bit mode.from sknn.platform import cpu64

from sknn.platform import cpu64, threading

GPU:
# Use the GPU in 32-bit mode,
from sknn.platform import gpu32
# Use the CPU in 64-bit mode.
from sknn.platform import cpu64
```

## 【sknn 的并行化

我们可以通过以下方式利用并行处理，但是这个有一个警告。它不是最稳定的方法:

```py
from sknn.platform import cpu64, threading
```

我们可以指定 Scikit-学会利用特定数量的线程:

```py
from sknn.platform import cpu64, threads2 #any desired amount of threads
```

当您指定了适当数量的线程后，您可以通过在交叉验证中实现`n_jobs=nthreads`来并行化您的代码。

既然我们已经涵盖了最重要的概念并准备好了我们的环境，让我们实现一个神经网络。

对于这个例子，我们将使用方便但相当枯燥的 Iris 数据集。

之后，我们将以标准化和缩放的形式应用预处理，并开始构建我们的模型:

```py
import numpy as np
from sklearn.datasets import load_iris
from sknn.mlp import Classifier, Layer
from sklearn import preprocessing
from sklearn.cross_validation import train_test_split
from sklearn import cross_validation
from sklearn import datasets

# import the familiar Iris data-set
iris = datasets.load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris.data,
iris.target, test_size=0.2, random_state=0)
```

这里，我们对输入进行预处理、归一化和缩放:

```py
X_trainn = preprocessing.normalize(X_train, norm='l2')
X_testn = preprocessing.normalize(X_test, norm='l2')

X_trainn = preprocessing.scale(X_trainn)
X_testn = preprocessing.scale(X_testn)
```

让我们设置我们的神经网络结构和参数。让我们从一个具有两个层的神经网络开始。在`Layer`部分，我们单独指定每一层的设置。(我们将在 Tensorflow 和 Keras 中再次看到这种方法。)Iris 数据集由四个特征组成，但因为在这种特殊情况下*宽的*神经网络工作得相当好，我们将在每个隐藏层中使用 13 个单元。请注意，sknn 默认应用 SGD:

```py
clf = Classifier(
    layers=[
    Layer("Rectifier", units=13),
    Layer("Rectifier", units=13),
    Layer("Softmax")],    learning_rate=0.001,
    n_iter=200)

model1=clf.fit(X_trainn, y_train)
y_hat=clf.predict(X_testn)
scores = cross_validation.cross_val_score(clf, X_trainn, y_train, cv=5)
print 'train mean accuracy %s' % np.mean(scores)
print 'vanilla sgd test %s' % accuracy_score(y_hat,y_test)

OUTPUT:]
train sgd mean accuracy 0.949909090909
sgd test 0.933333333333
```

在训练中取得了不错的成绩，但我们可能会做得更好。

我们讨论了内斯特罗夫动量如何将长度缩短到全球最小值；让我们用`nesterov`来运行这个算法，看看是否能提高精度和改善收敛性:

```py
clf = Classifier(
    layers=[
    Layer("Rectifier", units=13),   
    Layer("Rectifier", units=13),   
    Layer("Softmax")],        learning_rate=0.001,learning_rule='nesterov',random_state=101,
    n_iter=1000)

model1=clf.fit(X_trainn, y_train)
y_hat=clf.predict(X_testn)
scores = cross_validation.cross_val_score(clf, X_trainn, y_train, cv=5)
print 'Nesterov train mean accuracy %s' % np.mean(scores)
print 'Nesterov  test %s' % accuracy_score(y_hat,y_test)

OUTPUT]
Nesterov train mean accuracy 0.966575757576
Nesterov  test 0.966666666667
```

我们的模型是改进的，在这种情况下有内斯特罗夫动量。

# 神经网络和正则化

即使我们在上一个例子中没有过度训练我们的模型，也有必要考虑神经网络的正则化策略。我们可以将正则化应用于神经网络的三种最广泛使用的方法如下:

*   **L1** 和 **L2** 正则化，权重衰减作为正则化强度的参数
*   **Dropout** means that deactivating units within the neural network at random can force other units in the network to take over

    ![Neural networks and regularization](graphics/B05135_04_09.jpg)

    在左边，我们看到一个应用了丢包的架构，随机去激活网络中的单元。在右边，我们看到一个普通的神经网络(标有 X)。

*   **求平均值**或集合多个神经网络(每个神经网络具有不同的设置)

让我们尝试这种模式的辍学，看看是否可行:

```py
clf = Classifier(
    layers=[
    Layer("Rectifier", units=13),
    Layer("Rectifier", units=13),
    Layer("Softmax")],
    learning_rate=0.01,
    n_iter=2000,
    learning_rule='nesterov',
    regularize='dropout', #here we specify dropout
    dropout_rate=.1,#dropout fraction of neural units in entire network
    random_state=0)
model1=clf.fit(X_trainn, y_train)

scores = cross_validation.cross_val_score(clf, X_trainn, y_train, cv=5)
print np.mean(scores)
y_hat=clf.predict(X_testn)
print accuracy_score(y_hat,y_test)

OUTPUT]
dropout train score 0.933151515152
dropout test score 0.866666666667
```

在这种情况下，辍学并没有带来令人满意的结果，所以我们应该把它完全排除在外。也可以用其他方法随意试验。只需更改`learning_rule`参数，看看它对整体精度有何影响。可以尝试的车型有`sgd`、`momentum`、`nesterov`、`adagrad`、`rmsprop`。从这个例子中，您已经了解到内斯特罗夫动量可以提高整体精度。在这种情况下，`dropout`不是最佳的正则化方法，并且对模型性能有害。考虑到这大量的参数都相互作用并产生不可预测的结果，我们确实需要一种调优方法。这正是我们下一节要做的。

# 神经网络与超参数优化

由于神经网络和深度学习模型的参数空间如此之广，优化是一项艰巨的任务并且计算非常昂贵。错误的神经网络架构可能会导致失败。只有当我们应用正确的参数并为我们的问题选择正确的架构时，这些模型才能准确。不幸的是，提供调整方法的应用程序很少。我们发现目前最佳的参数调整方法是**随机搜索**，这是一种随机迭代参数空间的算法，节省了计算资源。sknn 库是唯一有这个选项的库。让我们以葡萄酒质量数据集为基础，通过下面的示例来浏览参数调整方法。

在这个例子中，我们首先加载葡萄酒数据集。然后我们对数据进行转换，从那里我们根据选择的参数调整模型。请注意，该数据集有 13 个要素；我们指定每层中的单位在 4 到 20 之间。在这种情况下，我们不使用小批量；数据集太小了:

```py
import numpy as np
import scipy as sp 
import pandas as pd
from sklearn.grid_search import RandomizedSearchCV
from sklearn.grid_search import GridSearchCV, RandomizedSearchCV
from scipy import stats
from sklearn.cross_validation import train_test_split
from sknn.mlp import  Layer, Regressor, Classifier as skClassifier

# Load data
df = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv ' , sep = ';')
X = df.drop('quality' , 1).values # drop target variable

y1 = df['quality'].values # original target variable
y = y1 <= 5 # new target variable: is the rating <= 5?

# Split the data into a test set and a training set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print X_train.shape

max_net = skClassifier(layers= [Layer("Rectifier",units=10),
                                       Layer("Rectifier",units=10),
                                       Layer("Rectifier",units=10),
                                       Layer("Softmax")])
params={'learning_rate': sp.stats.uniform(0.001, 0.05,.1),
'hidden0__units': sp.stats.randint(4, 20),
'hidden0__type': ["Rectifier"],
'hidden1__units': sp.stats.randint(4, 20),
'hidden1__type': ["Rectifier"],
'hidden2__units': sp.stats.randint(4, 20),
'hidden2__type': ["Rectifier"],
'batch_size':sp.stats.randint(10,1000),
'learning_rule':["adagrad","rmsprop","sgd"]}
max_net2 = RandomizedSearchCV(max_net,param_distributions=params,n_iter=25,cv=3,scoring='accuracy',verbose=100,n_jobs=1,\
                             pre_dispatch=None)
model_tuning=max_net2.fit(X_train,y_train)

print "best score %s" % model_tuning.best_score_
print "best parameters %s" % model_tuning.best_params_

OUTPUT:]
[CV]  hidden0__units=11, learning_rate=0.100932183167, hidden2__units=4, hidden2__type=Rectifier, batch_size=30, hidden1__units=11, learning_rule=adagrad, hidden1__type=Rectifier, hidden0__type=Rectifier, score=0.655914 -   3.0s
[Parallel(n_jobs=1)]: Done  74 tasks       | elapsed:  3.0min
[CV] hidden0__units=11, learning_rate=0.100932183167, hidden2__units=4, hidden2__type=Rectifier, batch_size=30, hidden1__units=11, learning_rule=adagrad, hidden1__type=Rectifier, hidden0__type=Rectifier 
[CV]  hidden0__units=11, learning_rate=0.100932183167, hidden2__units=4, hidden2__type=Rectifier, batch_size=30, hidden1__units=11, learning_rule=adagrad, hidden1__type=Rectifier, hidden0__type=Rectifier, score=0.750000 -   3.3s
[Parallel(n_jobs=1)]: Done  75 tasks       | elapsed:  3.0min
[Parallel(n_jobs=1)]: Done  75 out of  75 | elapsed:  3.0min finished
best score 0.721366278222

best parameters {'hidden0__units': 14, 'learning_rate': 0.03202394348494512, 'hidden2__units': 19, 'hidden2__type': 'Rectifier', 'batch_size': 30, 'hidden1__units': 17, 'learning_rule': 'adagrad', 'hidden1__type': 'Rectifier', 'hidden0__type': 'Rectifier'}
```

### 注

警告:由于参数空间是随机搜索的，结果可能不一致。

我们可以看到我们模型的最佳参数是，最重要的是，第一层包含 14 个单位，第二层包含 17 个单位，第三层包含 19 个单位。这是一个相当复杂的架构，我们可能永远无法自己推导出来，这证明了超参数优化的重要性。

# 神经网络和决策边界

我们在上一节已经介绍过，通过向神经网络添加隐藏单元，我们可以更接近地逼近目标函数。然而，我们还没有将其应用于分类问题。为此，我们将生成具有非线性目标值的数据，并查看一旦我们向架构中添加隐藏单元，决策面将如何变化。让我们看看普适近似定理在起作用！首先，让我们生成一些具有两个特征的非线性可分离数据，设置我们的神经网络体系结构，看看我们的决策边界如何随着每个体系结构而变化:

```py
%matplotlib inline
from sknn.mlp import Classifier, Layer
from sklearn import preprocessing
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from itertools import product

X,y= datasets.make_moons(n_samples=500, noise=.2, random_state=222)
from sklearn.datasets import make_blobs

net1 = Classifier(
   layers=[
       Layer("Softmax")],random_state=222,
   learning_rate=0.01,
   n_iter=100)
net2 = Classifier(
   layers=[
       Layer("Rectifier", units=4),
       Layer("Softmax")],random_state=12,
   learning_rate=0.01,
   n_iter=100)
net3 =Classifier(
   layers=[
       Layer("Rectifier", units=4),
       Layer("Rectifier", units=4),
       Layer("Softmax")],random_state=22,
   learning_rate=0.01,
   n_iter=100)
net4 =Classifier(
   layers=[
       Layer("Rectifier", units=4),
       Layer("Rectifier", units=4),
       Layer("Rectifier", units=4),
       Layer("Rectifier", units=4),
       Layer("Rectifier", units=4),
       Layer("Rectifier", units=4),
       Layer("Softmax")],random_state=62,
   learning_rate=0.01,
   n_iter=100)

net1.fit(X, y)
net2.fit(X, y)
net3.fit(X, y)
net4.fit(X, y)

# Plotting decision regions
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),
                    np.arange(y_min, y_max, 0.1))

f, arxxx = plt.subplots(2, 2, sharey='row',sharex='col', figsize=(8, 8))
plt.suptitle('Neural Network - Decision Boundary')
for idx, clf, ti in zip(product([0, 1], [0, 1]),
                       [net1, net2, net3,net4],
                       ['0 hidden layer', '1 hidden layer',
                        '2 hidden layers','6 hidden layers']):

   Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
   Z = Z.reshape(xx.shape)

   arxxx[idx[0], idx[1]].contourf(xx, yy, Z, alpha=0.5)
   arxxx[idx[0], idx[1]].scatter(X[:, 0], X[:, 1], c=y, alpha=0.5)
   arxxx[idx[0], idx[1]].set_title(ti)

plt.show()
```

![Neural networks and decision boundaries](graphics/B05135_04_10.jpg)

在这张截图中，我们可以看到，随着我们向神经网络添加隐藏层，我们可以越来越多地学习复杂的决策边界。一个有趣的边注是，具有两层的网络产生了最准确的预测。

### 注

请注意，结果可能在运行之间有所不同。

# 与 H2O 进行大规模深度学习

在前几节中，我们介绍了在本地计算机上运行的神经网络和深度架构，我们发现神经网络已经高度矢量化，但计算成本仍然很高。如果我们想使该算法在台式计算机上更具可扩展性，除了利用 Anano 和 GPU 计算之外，我们无能为力。因此，如果我们想更彻底地扩展深度学习算法，我们需要找到一种工具，可以在内核外运行算法，而不是在本地 CPU/GPU 上运行。此时此刻，H2O 是唯一能够快速运行深度学习算法的开源核心外平台。也是跨平台的；除了 Python，还有针对 R、Scala 和 Java 的 API。

H2O 是在一个基于 Java 的平台上编译的，该平台是为广泛的数据科学相关任务开发的，例如数据处理和机器学习。H2O 在内存中的分布式并行 CPU 上运行，因此数据将存储在 H2O 集群中。到目前为止，H2O 平台已经应用了**通用线性模型**(**【GLM】**)、随机森林、**梯度增强机器** ( **GBM** )、K 均值、朴素贝叶斯、主成分分析、主成分回归，当然，我们本章的主要重点是深度学习。

太好了，现在我们准备进行第一次 H2O 核外分析。

让我们启动 H2O 实例，并在 H2O 的分布式内存系统中加载一个文件:

```py
import sys  
sys.prefix = "/usr/local"
import h2o  

h2o.init(start_h2o=True)  

Type this to get interesting information about the specifications of your cluster.
Look at the memory that is allowed and the number of cores.

h2o.cluster_info()
```

这看起来或多或少类似于以下内容(试验和系统之间可能会有细微差异):

```py
OUTPUT:]

Java Version: java version "1.8.0_60"
Java(TM) SE Runtime Environment (build 1.8.0_60-b27)
Java HotSpot(TM) 64-Bit Server VM (build 25.60-b23, mixed mode)

Starting H2O JVM and connecting: .................. Connection successful!
------------------------------  ---------------------------------------
H2O cluster uptime:             2 seconds 346 milliseconds
H2O cluster version:            3.8.2.3
H2O cluster name:               H2O_started_from_python**********nzb520
H2O cluster total nodes:        1
H2O cluster total free memory:  3.56 GB
H2O cluster total cores:        8
H2O cluster allowed cores:      8
H2O cluster healthy:            True
H2O Connection ip:              1**.***.***.***
H2O Connection port:            54321
H2O Connection proxy:
Python Version:                 2.7.10
------------------------------  ---------------------------------------
------------------------------  ---------------------------------------
H2O cluster uptime:             2 seconds 484 milliseconds
H2O cluster version:            3.8.2.3
H2O cluster name:               H2O_started_from_python_quandbee_nzb520
H2O cluster total nodes:        1
H2O cluster total free memory:  3.56 GB
H2O cluster total cores:        8
H2O cluster allowed cores:      8
H2O cluster healthy:            True
H2O Connection ip:              1**.***.***.***
H2O Connection port:            54321
H2O Connection proxy:
Python Version:                 2.7.10
------------------------------  ---------------------------------------
Sucessfully closed the H2O Session.
Successfully stopped H2O JVM started by the h2o python module.
```

## 与 H2O 进行大规模深度学习

在 H2O 深度学习中，我们将用来训练的数据集是著名的 MNIST 数据集。它由 28×28 手写数字图像的像素强度组成。训练集合有 70，000 个训练项目，784 个特征，以及包含目标标签*数字*的每个记录的标签。

现在，我们对在 H2O 管理数据更加放心了，让我们执行一个深入的学习示例。

在 H2O，我们不需要转换或标准化输入数据；它是内部自动标准化的。每个特征被变换到 N(0，1)空间。

让我们将著名的手写数字图像数据集 MNIST 从亚马逊服务器导入到 H2O 集群:

```py
import h2o
h2o.init(start_h2o=True) 
train_url ="https://h2o-public-test-data.s3.amazonaws.com/bigdata/laptop/mnist/train.csv.gz"
test_url="https://h2o-public-test-data.s3.amazonaws.com/bigdata/laptop/mnist/test.csv.gz"

train=h2o.import_file(train_url)
test=h2o.import_file(test_url)

train.describe()
test.describe()

y='C785'
x=train.names[0:784]
train[y]=train[y].asfactor()
test[y]=test[y].asfactor()

from h2o.estimators.deeplearning import H2ODeepLearningEstimator

model_cv=H2ODeepLearningEstimator(distribution='multinomial'
                                 ,activation='RectifierWithDropout',hidden=[32,32,32],
                                        input_dropout_ratio=.2,
                                        sparse=True,
                                        l1=.0005,
                                            epochs=5)
```

这个打印模型的输出将提供很多详细的信息。您将看到的第一个表格如下。这提供了关于神经网络架构的所有细节。您可以看到，我们使用了一个输入维数为 717 的神经网络，该网络有三个隐藏层(每个隐藏层由 32 个单元组成)，softmax 激活应用于输出层，ReLU 应用于隐藏层之间:

```py
model_cv.train(x=x,y=y,training_frame=train,nfolds=3)
print model_cv

OUTPUT]
```

![Large scale deep learning with H2O](graphics/B05135_04_12.jpg)

如果你想要一个简短的模型性能概述，这是一个非常实用的方法。

在下表中，最有趣的指标是每个文件夹的训练分类错误和验证分类错误。如果您想要验证您的模型，您可以很容易地比较这些:

```py
    print model_cv.scoring_history()
```

![Large scale deep learning with H2O](graphics/B05135_04_13.jpg)

我们在 MNIST 数据集上的训练分类误差 **.096430** 和精度在. 907 范围内是相当好的；这几乎和 Yann LeCun 的卷积神经网络提交一样好。

H2O 还提供了一种获取验证指标的便捷方法。我们可以通过将验证数据框传递给交叉验证函数来实现这一点:

```py
model_cv.train(x=x,y=y,training_frame=train,validation_frame=test,nfolds=3)
print model_cv 
```

![Large scale deep learning with H2O](graphics/B05135_04_14.jpg)

在这种情况下，我们可以很容易地将**训练 _ 分类 _ 错误** ( `.089`)与我们的**验证 _ 分类 _ 错误** ( `.0954`)进行比较。

也许我们可以提高我们的分数；让我们使用超参数优化模型。

## H2O 的网格研究

考虑到我们之前的模型表现相当好，我们将把我们的调优工作集中在我们网络的架构上。H2O 的 gridsearch 功能与 Scikit-learn 的随机化搜索非常相似；也就是说，它不是搜索整个参数空间，而是遍历随机的参数列表。首先，我们将设置一个传递给 gridsearch 函数的参数列表。H2O 将在参数搜索中为我们提供每个模型的输出和相应的分数:

```py
hidden_opt = [[18,18],[32,32],[32,32,32],[100,100,100]]
# l1_opt = [s/1e6 for s in range(1,1001)]

# hyper_parameters = {"hidden":hidden_opt, "l1":l1_opt}
hyper_parameters = {"hidden":hidden_opt}

#important: here we specify the search parameters
#be careful with these, training time can explode (see max_models)
search_c = {"strategy":"RandomDiscrete",

"max_models":10, "max_runtime_secs":100,

"seed":222}

from h2o.grid.grid_search import H2OGridSearch

model_grid = H2OGridSearch(H2ODeepLearningEstimator, hyper_params=hyper_parameters)

#We have added a validation set to the gridsearch method in order to have a better #estimate of the model performance.

model_grid.train(x=x, y=y, distribution="multinomial", epochs=1000, training_frame=train, validation_frame=test,
    score_interval=2, stopping_rounds=3, stopping_tolerance=0.05,search_criteria=search_c)

print model_grid

# Grid Search Results for H2ODeepLearningEstimator:

OUTPUT]

deeplearning Grid Build Progress: [##################################################] 100%
    hidden  \
0    [100, 100, 100]   
1       [32, 32, 32]   
2           [32, 32]   
3           [18, 18]   

    model_ids   logloss  
0  Grid_DeepLearning_py_1_model_python_1464790287811_3_model_3  0.148162  ←------
1  Grid_DeepLearning_py_1_model_python_1464790287811_3_model_2  0.173675  
2  Grid_DeepLearning_py_1_model_python_1464790287811_3_model_1  0.212246  
3  Grid_DeepLearning_py_1_model_python_1464790287811_3_model_0  0.227706  
```

我们可以看到，我们最好的架构是一个三层结构，每层 100 个单元。我们还可以清楚地看到【gridsearch 甚至在上大幅增加了训练时间，T1 是一个强大的计算集群，就像 H2O 运行的集群一样。因此，即使在 H2O，我们也应该谨慎使用 gridsearch，并对模型中解析的参数保持保守。

现在，让我们在继续之前关闭 H2O 实例:

```py
h2o.shutdown(prompt=False)
```

# 深度学习和无监督预处理

在这一节中，我们将介绍深度学习中最重要的概念:如何通过无监督预训练来提高学习。通过无监督的预处理，我们使用神经网络来发现数据中的潜在特征和因素，以便稍后传递给神经网络。这种方法具有强大的训练网络的能力来学习其他机器学习方法不能学习的任务，没有手工特征。我们将进入细节，并介绍一个新的强大的库。

# 用茶氨酸进行深度学习

Scikit-learn 的神经网络应用对于参数调整特别有意思。不幸的是，它对于无监督神经网络应用的能力是有限的。对于下一个主题，我们深入更复杂的学习方法，我们需要另一个库。在这一章中，我们将集中讨论茶氨酸。我们喜欢茶氨酸是因为的易用性和稳定性；这是得克萨斯大学的利夫·约翰逊开发的一个非常流畅且维护良好的软件包。建立一个神经网络结构的工作原理与 sklearn 非常相似；也就是说，我们实例化一个学习目标(分类或回归)，指定层，并训练它。更多信息，可登陆[http://theanets.readthedocs.org/en/stable/](http://theanets.readthedocs.org/en/stable/)。

你所要做的就是用`pip`安装天线:

```py
$ pip install theanets
```

由于茶氨酸是建立在茶氨酸之上的，你也需要正确安装茶氨酸。让我们运行一个基本的神经网络模型来看看茶氨酸是如何工作的。与 Scikit-learn 的相似之处显而易见。请注意，我们在本例中使用动量，默认情况下，在示例中使用 softmax，因此我们不必指定它:

```py
import climate # This package provides the reporting of iterations 
from sklearn.metrics import confusion_matrix
import numpy as np
from sklearn import datasets
from sklearn.cross_validation import train_test_split
from sklearn.metrics import mean_squared_error
import theanets
import theano
import numpy as np
import matplotlib.pyplot as plt
import climate
from sklearn.cross_validation import train_test_split
import theanets
from sklearn.metrics import confusion_matrix
from sklearn import preprocessing
from sklearn.metrics import accuracy_score
from sklearn import datasets
climate.enable_default_logging()

digits = datasets.load_digits()
digits = datasets.load_digits()
X = np.asarray(digits.data, 'float32')

Y = digits.target

Y=np.array(Y, dtype=np.int32)
#X = (X - np.min(X, 0)) / (np.max(X, 0) + 0.0001)  # 0-1 scaling

X_train, X_test, y_train, y_test = train_test_split(X, Y,
                                                    test_size=0.2,
                                                    random_state=0)

# Build a classifier model with 64 inputs, 1 hidden layer with 100 units  and 10 outputs.
net = theanets.Classifier([64,100,10])

# Train the model using Resilient backpropagation and momentum.
net.train([X_train,y_train], algo='sgd', learning_rate=.001, momentum=0.9,patience=0,
validate_every=N,
min_improvement=0.8)

# Show confusion matrices on the training/validation splits.
print(confusion_matrix(y_test, net.predict(X_test)))
print (accuracy_score(y_test, net.predict(X_test)))

OUTPUT ]

[[27  0  0  0  0  0  0  0  0  0]
 [ 0 32  0  0  0  1  0  0  0  2]
 [ 0  1 34  0  0  0  0  1  0  0]
 [ 0  0  0 29  0  0  0  0  0  0]
 [ 0  0  0  0 29  0  0  1  0  0]
 [ 0  0  0  0  0 38  0  0  0  2]
 [ 0  1  0  0  0  0 43  0  0  0]
 [ 0  0  0  0  1  0  0 38  0  0]
 [ 0  2  1  0  0  0  0  0 36  0]
 [ 0  0  0  0  0  1  0  0  0 40]]
0.961111111111
```

# 自动编码器和无监督学习

到目前为止，我们讨论了多层和多种参数优化的神经网络。我们常说的深度学习这一代神经网络能力更强；它能够自动学习新特性，因此只需要很少的特性工程和领域专业知识。这些特征是通过无监督的方法在未标记的数据上创建的，这些数据随后被输入到神经网络的后续层。这种方法被称为(无监督)**预处理**。这种方法已经被证明在图像识别、语言学习甚至普通的机器学习项目中非常成功。近年来最重要和占主导地位的技术被称为**去噪自动编码器**和基于玻尔兹曼技术的算法。**玻尔兹曼机器**，曾经是**深度信念网络** ( **DBN** )的构建模块，最近在深度学习社区中失宠了，因为它们被证明很难训练和优化。因此，我们将只关注自动编码器。让我们以可管理的小步骤来讨论这个重要的话题。

## 自动编码器

我们试图找到一个函数( *F* )，有一个输出作为它的输入，误差最小*F(x)∞' x*。该功能通常被称为**身份功能**，我们尝试对其进行优化，使 *x* 尽可能接近 *'x* 。 *x* 与 *'x* 之间的差异称为**重构误差**。

让我们看一个简单的单层体系结构来直观地了解发生了什么。我们将看到这些架构非常灵活，需要仔细调整:

![Autoencoders](graphics/B05135_04_15.jpg)

单层自动编码器架构

重要的是要理解，当我们在隐藏层中的单位少于输入空间时，我们会强制权重来压缩输入数据。

在本例中，我们有一个包含五个要素的数据集。中间是包含三个单位( *Wij* )的隐藏层。这些单位具有与我们在神经网络中看到的权重向量相同的属性；也就是说，它们由可以用反向传播训练的权重组成。利用隐藏层的输出，我们通过与我们在神经网络中看到的相同的前馈向量操作获得作为输出的特征表示。

计算向量 *'x* 的过程与我们通过计算各层权重向量的点积所看到的正向传播非常相似:

*W*=重量

![Autoencoders](graphics/B05135_04_37.jpg)

![Autoencoders](graphics/B05135_04_38.jpg)

重建误差可以用平方误差或交叉熵的形式来测量，这在许多其他方法中已经看到。在这种情况下， *ŷ* 代表重构输出， *y* 代表真实输入:

![Autoencoders](graphics/B05135_04_39.jpg)

一个重要的概念是，在只有一个隐藏层的情况下，自动编码器模型捕获的数据中的维度近似于**主成分分析** ( **主成分分析**)的结果。然而，如果涉及到非线性，自动编码器的行为会大不相同。自动编码器将检测主成分分析永远无法检测到的不同潜在因素。既然我们对自动编码器的架构有了更多的了解，也知道了如何根据其同一性近似计算误差，那么让我们看看这些**稀疏度参数**，我们用它们来压缩输入。

你可能会问:为什么我们甚至需要这个稀缺参数？我们就不能运行算法找到身份函数然后继续前进吗？

不幸的是，事情没那么简单。有些情况下，身份函数几乎完美地投射了输入，但仍然无法提取输入特征的潜在维度。在这种情况下，该函数只是存储输入数据，而不是提取有意义的特征。我们可以做两件事。首先，我们故意给信号添加噪声(**去噪自动编码器**)，其次，我们引入稀疏性参数，强制停用弱激活单元。让我们首先看看稀疏性是如何工作的。

我们讨论了生物神经元的激活阈值；如果一个神经元的电位接近 1，我们可以认为它是*活动的*，如果它的输出值接近 0，我们可以认为它是*不活动的*。我们可以通过提高激活阈值来限制神经元在大部分时间不活动。我们通过降低每个神经元/单位的平均激活概率来做到这一点。查看以下公式，我们可以了解如何最小化激活阈值:

![Autoencoders](graphics/B05135_04_45.jpg)

![Autoencoders](graphics/P.jpg) *<sub> j </sub>* :隐藏层每个神经元的平均激活阈值。

*ρ* :我们预先指定的网络期望激活阈值。在大多数情况下，该值设置为. 05。

*a* :隐藏图层的权重向量。

在这里，我们看到了一个优化的机会，通过对![Autoencoders](graphics/P.jpg)*<sub>【j】</sub>*和 *ρ* 之间的错误率进行惩罚。

在本章中，我们不会太担心这个优化目标的技术细节。在大多数包中，我们可以使用一个非常简单的指令来做到这一点(我们将在下一个例子中看到)。需要理解的最重要的一点是，对于自动编码器，我们有两个主要的学习目标:通过优化身份函数来最小化输入向量 x 和输出向量' x 之间的误差，以及最小化网络中每个神经元的期望激活阈值和平均激活之间的差异。

我们可以强制自动编码器检测潜在特征的第二种方法是在模型中引入噪声；这就是名字**去噪自动编码器**的由来。这个想法是通过破坏输入的 T4，我们迫使自动编码器学习更健壮的 T5 数据表示。在接下来的例子中，我们将简单地把高斯噪声引入自动编码器模型。

**利用堆叠去噪自动编码器进行真正深度的学习–分类预处理**

通过这个练习，你将把自己从众多谈论深度学习的人和少数真正做到的人中区分开来！现在，我们将对著名的 MNIST 数据集的迷你版本应用自动编码器，它可以方便地从 Scikit-learn 中加载。数据集由 28 x 28 个手写数字图像的像素强度组成。训练集有 1，797 个训练项目，64 个特征，每个记录的标签包含目标标签*数字*，从 0 到 9。所以我们有 64 个特征，目标变量由 10 个类组成(数字从 0-9)来预测。

首先，我们训练稀疏度为. 9 的堆叠去噪自动编码器模型，并检查重建误差。我们将使用深度学习研究论文的结果作为设置的指南。更多信息可以阅读本文([http://arxiv.org/pdf/1312.5663.pdf](http://arxiv.org/pdf/1312.5663.pdf))。然而，我们有一些限制，因为这些类型的模型计算量巨大。因此，对于这个自动编码器，我们使用五层 ReLU 激活，并将数据从 64 个特征压缩到 45 个特征:

```py
model = theanets.Autoencoder([64,(45,'relu'),(45,'relu'),(45,'relu'),(45,'relu'),(45,'relu'),64])
dAE_model=model.train([X_train],algo='rmsprop',input_noise=0.1,hidden_l1=.001,sparsity=0.9,num_updates=1000)
X_dAE=model.encode(X_train)
X_dAE=np.asarray(X_dAE, 'float32')
:OUTPUT:
I 2016-04-20 05:13:37 downhill.base:232 RMSProp 2639 loss=0.660185 err=0.645118
I 2016-04-20 05:13:37 downhill.base:232 RMSProp 2640 loss=0.660031 err=0.644968
I 2016-04-20 05:13:37 downhill.base:232 validation 264 loss=0.660188 err=0.645123
I 2016-04-20 05:13:37 downhill.base:414 patience elapsed!
I 2016-04-20 05:13:37 theanets.graph:447 building computation graph
I 2016-04-20 05:13:37 theanets.losses:67 using loss: 1.0 * MeanSquaredError (output out:out)
I 2016-04-20 05:13:37 theanets.graph:551 compiling feed_forward function
```

现在我们有了自动编码器的输出，它是我们用一组新的压缩特征创建的。让我们仔细看看这个新数据集:

```py
X_dAE.shape
Output: (1437, 45)
```

在这里，我们实际上可以看到，我们已经将数据从 64 个特征压缩到 45 个特征。新的数据集不那么稀疏(意味着更少的零)并且在数字上更加连续。既然我们已经从自动编码器获得了预训练数据，我们可以对其应用深度神经网络进行监督学习:

```py
#By default, hidden layers use the relu transfer function so we don't need to specify #them. Relu is the best option for auto-encoders.
# Theanets classifier also uses softmax by default so we don't need to specify them.
net = theanets.Classifier(layers=(45,45,45,10))
autoe=net.train([X_dAE, y_train], algo='rmsprop',learning_rate=.0001,batch_size=110,min_improvement=.0001,momentum=.9,
nesterov=True,num_updates=1000)
## Enjoy the rare pleasure of 100% accuracy on the training set.
OUTPUT:
I 2016-04-19 10:33:07 downhill.base:232 RMSProp 14074 loss=0.000000 err=0.000000 acc=1.000000
I 2016-04-19 10:33:07 downhill.base:232 RMSProp 14075 loss=0.000000 err=0.000000 acc=1.000000
I 2016-04-19 10:33:07 downhill.base:232 RMSProp 14076 loss=0.000000 err=0.000000 acc=1.000000
```

在我们对测试集预测这个神经网络之前，将我们已经训练的自动编码器模型应用于测试集是很重要的:

```py
dAE_model=model.train([X_test],algo='rmsprop',input_noise=0.1,hidden_l1=.001,sparsity=0.9,num_updates=100)
X_dAE2=model.encode(X_test)
X_dAE2=np.asarray(X_dAE2, 'float32')
```

现在让我们检查一下测试集的性能:

```py
final=net.predict(X_dAE2)
from sklearn.metrics import accuracy_score
print accuracy_score(final,y_test)
OUTPUT: 0.972222222222
```

我们可以看到，具有自动编码特征 (.9722)的模型的最终精度优于没有它的模型(. 9611)。

# 总结

在这一章中，我们研究了深度学习背后最重要的概念以及可扩展的解决方案。

我们通过学习如何为任何给定的任务构建正确的体系结构来消除一些黑箱现象，并通过前向传播和反向传播的机制来工作。更新神经网络的权重是一项艰巨的任务，常规的随机梯度下降会导致陷入全局最小值或超调。动量、ADAGRAD、RPROP 和 RMSProp 等更复杂的算法可以提供解决方案。尽管神经网络比其他机器学习方法更难训练，但它们具有转换特征表示的能力，并且可以学习任何给定的函数(通用近似定理)。我们还和 H2O 一起投入到大规模的深度学习中，甚至利用参数优化这个非常热门的话题进行深度学习。

使用自动编码器的无监督预训练可以提高任何给定深度网络的准确性，我们通过一个茶氨酸框架内的实际例子来实现这一点。

在这一章中，我们主要使用构建在 Anano 框架之上的包。在下一章中，我们将介绍基于新的开源框架 Tensorflow 构建的包的深度学习技术。