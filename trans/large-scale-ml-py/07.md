# 第七章。大规模无监督学习

在前几章中，问题的焦点是预测一个变量，它可能是一个数字、类或类别。在这一章中，我们将改变方法，并尝试在规模上创建新的特征和变量，希望比已经包含在观察矩阵中的特征和变量更好地用于我们的预测目的。我们将首先介绍无监督方法，并举例说明其中三种能够扩展到大数据的方法:

*   **主成分分析** ( **主成分分析**)，一种减少特征数量的有效方法
*   **K-means** ，一种可扩展的聚类算法
*   **潜在狄利克雷分配** ( **LDA** )，一种能够从一系列文本文档中提取主题的非常有效的算法

# 无监督方法

无监督学习是机器学习的一个分支，其算法从没有明确标签(无标签数据)的数据中揭示推理。这种技术的目标是提取隐藏的模式，并将相似的数据分组。

在这些算法中，每个观察的未知感兴趣参数(例如组成员和主题组成)通常被建模为潜在变量(或一系列隐藏变量)，隐藏在不能直接观察的观察变量系统中，而只能从系统的过去和现在的输出中推导出来。通常，系统的输出包含噪声，这使得操作更加困难。

在常见问题中，无监督方法主要用于两种情况:

*   使用标记数据集提取要由分类器/回归器向下处理到处理链的附加特征。通过附加功能的增强，它们可能会表现得更好。
*   用有标签或无标签的数据集来提取一些关于数据结构的信息。这类算法通常在建模的**探索性数据分析** ( **EDA** )阶段使用。

首先，在开始演示之前，让我们沿着笔记本中的章节导入必要的模块:

```
In : import matplotlib
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import pylab
%matplotlib inline
import matplotlib.cm as cm
import copy
import tempfile
import os
```

# 特征分解–主成分分析

主成分分析是一种常用的算法，用于分解输入信号的维度，只保留*主成分*的维度。从数学角度来看，主成分分析对观测矩阵进行正交变换，输出一组线性不相关变量，称为主成分。输出变量形成一个基集，其中每个分量都与其他分量正交。此外，可以对输出分量进行排序(以便仅使用主分量)，因为第一个分量包含输入数据集的最大可能方差，第二个分量与第一个分量正交(根据定义)，包含残差信号的最大可能方差，第三个分量与前两个分量正交，并且基于残差方差，以此类推。

主成分分析的一般变换可以表示为空间的投影。如果只从变换基中提取主成分，输出空间将比输入空间具有更小的维度。数学上，它可以表达如下:

![Feature decomposition – PCA](graphics/B05135_07_17.jpg)

这里， *X* 是维度 *N* 训练集的泛点， *T* 是来自 PCA 的变换矩阵，![Feature decomposition – PCA](graphics/B05135_07_19.jpg)是输出向量。请注意，符号表示该矩阵方程中的点积。从实用的角度来看，还要注意的是 *X* 的所有特性在做这个操作之前都必须以零为中心。

现在让我们从一个实际的例子开始；后面，我们将深入讲解数学 PCA。在本例中，我们将创建一个由两个点组成的虚拟数据集，一个点在(-5，0)中居中，另一个点在(5，5)中居中。让我们使用主成分分析来转换数据集，并将输出与输入进行比较。在这个简单的例子中，我们将使用所有的特征，也就是说，我们将不执行特征约简:

```
In:from sklearn.datasets.samples_generator import make_blobs
from sklearn.decomposition import PCA

X, y = make_blobs(n_samples=1000, random_state=101, \
centers=[[-5, 0], [5, 5]])
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)
pca_comp = pca.components_.T

test_point = np.matrix([5, -2])
test_point_pca = pca.transform(test_point)

plt.subplot(1, 2, 1)
plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='none')
plt.quiver(0, 0, pca_comp[:,0], pca_comp[:,1], width=0.02, \
            scale=5, color='orange')
plt.plot(test_point[0, 0], test_point[0, 1], 'o')
plt.title('Input dataset')

plt.subplot(1, 2, 2)
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, edgecolors='none')
plt.plot(test_point_pca[0, 0], test_point_pca[0, 1], 'o')
plt.title('After "lossless" PCA')

plt.show()
```

![Feature decomposition – PCA](graphics/B05135_07_01.jpg)

如您所见，输出比原始要素的空间更有条理，如果下一个任务是分类，它将只需要数据集的一个要素，节省了几乎 50%的空间和所需的计算。在图像中，您可以清楚地看到 PCA 的核心:它只是输入数据集到左侧图像中以橙色绘制的变换基础的投影。你不确定吗？让我们测试一下:

```
In:print "The blue point is in", test_point[0, :]
print "After the transformation is in", test_point_pca[0, :]
print "Since (X-MEAN) * PCA_MATRIX = ", np.dot(test_point - \
pca.mean_, pca_comp)

Out:The blue point is in [[ 5 -2]]
After the transformation is in [-2.34969911 -6.2575445 ]
Since (X-MEAN) * PCA_MATRIX =  [[-2.34969911 -6.2575445 ]
```

现在，我们来挖掘核心问题:如何从训练集中生成 T？它应该包含正交向量，向量应该根据它们可以解释的方差量(即观测矩阵携带的能量或信息)进行排序。已经实现了很多解决方案，但是最常见的实现是基于**奇异值分解** ( **奇异值分解**)。

奇异值分解是一种将任意矩阵 *M* 分解成三个具有特殊性质的矩阵![Feature decomposition – PCA](graphics/B05135_07_20.jpg)的技术，这三个矩阵的乘法又返回了 *M* :

![Feature decomposition – PCA](graphics/B05135_07_21.jpg)

具体来说，给定 *M* ，一个由 *m* 行和 *n* 列组成的矩阵，等价的结果元素如下:

*   *U* 是矩阵 *m x m* (正方形矩阵)，它是酉的，它的列构成了正交的基。同样，它们被命名为左奇异向量，或者输入奇异向量，它们是矩阵乘积![Feature decomposition – PCA](graphics/B05135_07_22.jpg)的特征向量。
*   ![Feature decomposition – PCA](graphics/B05135_07_24.jpg)是一个矩阵 *m x n* ，它的对角线上只有非零元素。这些值被称为奇异值，都是非负的，并且都是![Feature decomposition – PCA](graphics/B05135_07_22.jpg)和![Feature decomposition – PCA](graphics/B05135_07_23.jpg)的特征值。
*   w 是酉矩阵 *n x n* (方阵)，它的列构成一个正交基，它们被命名为右(或输出)奇异向量。同样，它们是矩阵乘积![Feature decomposition – PCA](graphics/B05135_07_23.jpg)的特征向量。

为什么需要这样？解决方案非常简单:主成分分析的目标是尝试并估计输入数据集方差较大的方向。为此，我们首先需要去除每个特征的均值，然后对协方差矩阵![Feature decomposition – PCA](graphics/B05135_07_25.jpg)进行运算。

假设，通过用 SVD 分解矩阵 *X* ，我们得到了矩阵 *W* 的列，它们是协方差的主成分(即，我们要寻找的矩阵 *T* )，![Feature decomposition – PCA](graphics/B05135_07_24.jpg)的对角线包含由主成分解释的方差， *U* 的列是主成分。这就是为什么主成分分析总是用奇异值分解来完成。

现在我们来看一个真实的例子。让我们在 Iris 数据集上进行测试，提取前两个主成分(即从一个由四个特征组成的数据集传递到一个由两个特征组成的数据集):

```
In:from sklearn import datasets

iris = datasets.load_iris()
X = iris.data
y = iris.target

print "Iris dataset contains", X.shape[1], "features"

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

print "After PCA, it contains", X_pca.shape[1], "features"
print "The variance is [% of original]:", \
        sum(pca.explained_variance_ratio_)

plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, edgecolors='none')
plt.title('First 2 principal components of Iris dataset')

plt.show()

Out:Iris dataset contains 4 features
After PCA, it contains 2 features
The variance is [% of original]: 0.977631775025
```

![Feature decomposition – PCA](graphics/B05135_07_02.jpg)

这是对过程输出的分析:

*   解释的方差几乎是来自输入的原始方差的 98%。特征数量减少了一半，但只有 2%的信息不在输出中，希望只是噪音。
*   从视觉观察来看，组成 Iris 数据集的不同类似乎是相互分离的。这意味着，在这样一个约简集上工作的分类器在准确性方面将具有相当的性能，但是训练和运行预测将更快。

作为第二点的证明，让我们现在尝试训练和测试两个分类器，一个使用原始数据集，另一个使用约简集，并打印它们的准确性:

```
In:from sklearn.linear_model import SGDClassifier
from sklearn.cross_validation import train_test_split
from sklearn.metrics import accuracy_score

def test_classification_accuracy(X_in, y_in):
    X_train, X_test, y_train, y_test = \
        train_test_split(X_in, y_in, random_state=101, \
        train_size=0.50)

    clf = SGDClassifier('log', random_state=101)
clf.fit(X_train, y_train)

    return accuracy_score(y_test, clf.predict(X_test))

print "SGDClassifier accuracy on Iris set:", \
            test_classification_accuracy(X, y)
print "SGDClassifier accuracy on Iris set after PCA (2 components):", \
            test_classification_accuracy(X_pca, y)

Out:SGDClassifier accuracy on Iris set: 0.586666666667
SGDClassifier accuracy on Iris set after PCA (2 components): 0.72
```

如您所见，这种技术不仅降低了学习者在链中的复杂性和空间，而且有助于实现泛化(完全像 Ridge 或 Lasso 正则化)。

现在，如果您不确定输出中应该有多少组件，通常根据经验，选择能够解释至少 90%(或 95%)输入差异的最小数量。从经验来看，这样的选择通常会确保只有噪音被切断。

到目前为止，一切似乎都很完美:我们找到了一个减少特征数量的很好的解决方案，构建了一些具有很高预测能力的特征，我们也有一个经验法则来猜测它们的正确数量。现在让我们检查一下这个解决方案的可伸缩性:我们正在研究当观察和特征的数量增加时，它是如何伸缩的。首先要注意的是，主成分分析的*核心*部分——奇异值分解算法不是随机的；因此，它需要整个矩阵，以便能够提取其主成分。现在，让我们看看在一些具有越来越多的特征和观察的合成数据集上，主成分分析在实践中是如何可扩展的。我们将执行完全(无损)分解(实例化对象 PCA 时的参数是`None`，因为要求较少数量的特征不会影响性能(这只是对 SVD 的输出矩阵进行切片的问题)。

在下面的代码中，我们首先创建包含 10，000 个点和 20，50，100，250，1，000 和 2，500 个要由 PCA 处理的特征的矩阵。然后，我们创建具有 100 个特征以及 1、5、10、25、50 和 100，000 个要用主成分分析处理的观察值的矩阵:

```
In:import time

def check_scalability(test_pca):
    pylab.rcParams['figure.figsize'] = (10, 4)

    # FEATURES
    n_points = 10000
    n_features = [20, 50, 100, 250, 500, 1000, 2500]
    time_results = []

    for n_feature in n_features:
        X, _ = make_blobs(n_points, n_features=n_feature, \
random_state=101)

        pca = copy.deepcopy(test_pca)
        tik = time.time()
        pca.fit(X)
        time_results.append(time.time()-tik)

    plt.subplot(1, 2, 1)
    plt.plot(n_features, time_results, 'o--')
    plt.title('Feature scalability')
    plt.xlabel('Num. of features')
    plt.ylabel('Training time [s]')

    # OBSERVATIONS
    n_features = 100
    n_observations = [1000, 5000, 10000, 25000, 50000, 100000]
    time_results = []

    for n_points in n_observations:
        X, _ = make_blobs(n_points, n_features=n_features, \
random_state=101)
        pca = copy.deepcopy(test_pca)
        tik = time.time()
        pca.fit(X)
        time_results.append(time.time()-tik)

    plt.subplot(1, 2, 2)
    plt.plot(n_observations, time_results, 'o--')
    plt.title('Observations scalability')
    plt.xlabel('Num. of training observations')
    plt.ylabel('Training time [s]')

    plt.show()

check_scalability(PCA(None))

Out:
```

![Feature decomposition – PCA](graphics/B05135_07_03.jpg)

可以清楚地看到，基于 SVD 的 PCA 是不可伸缩的:如果特征数量线性增加，训练算法所需的时间就会呈指数级增加。此外，处理具有几百个观察值的矩阵所需的时间变得太高，并且(图中未显示)内存消耗使得该问题对于家用计算机(具有 16gb 或更少的内存)不可行。似乎很明显，基于奇异值分解的主成分分析不是大数据的解决方案；幸运的是，近年来引入了许多变通方法。在接下来的几节中，你会发现它们的简短介绍。

## 随机主成分分析

这种技术的正确名称应该是基于随机化奇异值分解的*主成分分析*，但它已经以**随机化主成分分析**的名称而流行。随机化背后的核心思想是所有主成分的冗余；事实上，如果方法的目标是降维，那么应该期望在输出中只需要几个向量(K 个主向量)。通过关注寻找最佳 K 个主向量的问题，该算法的规模更大。注意，在这个算法中，*K*——要输出的主成分个数——是一个关键参数:设置太大，性能不会比 PCA 好；将它设置得太低，用结果向量解释的方差将太低。

如同在 PCA 中一样，我们希望找到包含观测值 *X* 的矩阵的近似值，例如![Randomized PCA](graphics/B05135_07_26.jpg)；我们还希望矩阵 *Q* 具有 *K* 正交列(它们将被称为主成分)。有了奇异值分解，我们现在可以计算*小*矩阵![Randomized PCA](graphics/B05135_07_27.jpg)的分解。正如我们所证明的，这不会花很长时间。作为![Randomized PCA](graphics/B05135_07_28.jpg)，通过取![Randomized PCA](graphics/B05135_07_29.jpg)，我们现在有了基于低秩奇异值分解![Randomized PCA](graphics/B05135_07_30.jpg)的 *X* 的截断近似。

从数学上看，这似乎很完美，但仍有两点缺失:随机化有什么作用？如何得到矩阵 *Q* ？两个问题都在这里回答:提取高斯随机矩阵![Randomized PCA](graphics/B05135_07_31.jpg)，计算 *Y* 为![Randomized PCA](graphics/B05135_07_32.jpg)。然后 *Y* 进行 QR 分解，创建![Randomized PCA](graphics/B05135_07_33.jpg)，这里是 *Q* ，我们要找的 *K* 正交列的矩阵。

这个分解下面的数学相当重，幸运的是一切都已经在 Scikit-learn 中实现了，所以你不需要去弄清楚如何处理高斯随机变量等等。让我们先看看用随机化主成分分析计算完全(无损)分解时，它的性能有多差:

```
In:from sklearn.decomposition import RandomizedPCA
check_scalability(RandomizedPCA(None))
```

![Randomized PCA](graphics/B05135_07_04.jpg)

表现比经典 PCA差；在事实上，当要求一组简化的组件时，这种转换非常有效。现在来看看 *K=20* 时的表现:

```
In:check_scalability(RandomizedPCA(20))
```

![Randomized PCA](graphics/B05135_07_05.jpg)

不出所料，计算非常快；在不到一秒钟的时间内，该算法能够执行最复杂的因子分解。

检查结果和算法，我们仍然注意到一些奇怪的事情:训练数据集 *X* 必须都适合内存才能被分解，即使在随机化主成分分析的情况下。是否有一个在线版本的主成分分析能够增量拟合主向量，而无需将整个数据集存储在内存中？是的，有——增量主成分分析。

## 增量主成分分析

增量主成分分析，或称小批量主成分分析，是主成分分析的在线版本。算法的核心非常简单:将该批数据初步拆分为具有相同数量观测值的小批量。(唯一的限制是每个小批量的观察数量应该大于特征的数量。)然后，将第一个小批量居中(去除平均值)，并执行其奇异值分解，存储主成分。然后，当下一个小批量进入流程时，它首先居中，然后与从上一个小批量中提取的主要成分堆叠在一起(它们作为附加观察值插入)。现在，执行另一个奇异值分解，并用新的主分量覆盖主分量。这个过程一直持续到最后一个小批量:对于每个小批量，首先是对中，然后是堆叠，最后是奇异值分解。这样做，而不是一个*大*奇异值分解，我们执行的*小*奇异值分解和小批量的数量一样多。

正如你所理解的，这种技术并没有优于随机化的主成分分析，但它的目标是在不适合内存的数据集上需要主成分分析时提供一种解决方案(或唯一的解决方案)。增量 PCA 不是为了赢得速度挑战而运行，而是为了限制内存消耗；内存使用在整个训练过程中是恒定的，可以通过设置小批量来调整。根据经验，内存占用量与小批量大小的平方大致相同。

作为一个代码示例，现在让我们检查增量 PCA 如何处理大数据集，在我们的示例中，该数据集由 1000 万个观察值和 100 个特征组成。以前的算法都不能做到这一点，除非你想让你的计算机崩溃(或者见证内存和存储磁盘之间的大量交换)。使用增量主成分分析，这样的任务就变成了小菜一碟，而且考虑到所有因素，这个过程并不那么慢(请注意，我们正在进行完全无损的分解，消耗了稳定的内存量):

```
In:from sklearn.decomposition import IncrementalPCA

X, _ = make_blobs(100000, n_features=100, random_state=101)
pca = IncrementalPCA(None, batch_size=1000)

tik = time.time()
for i in range(100):
    pca.partial_fit(X)
print "PCA on 10M points run with constant memory usage in ", \
    time.time() - tik, "seconds"

Out:PCA on 10M points run with constant memory usage in  155.642718077 seconds
```

## 稀疏主成分分析

稀疏主成分分析的运行方式不同于以前的算法；它不是使用应用在协方差矩阵上的奇异值分解来操作特征约简(居中后)，而是对该矩阵进行类似特征选择的操作，找到最佳重建数据的稀疏分量集。与套索正则化一样，稀疏度可以通过对系数的惩罚(或约束)来控制。

关于主成分分析，稀疏主成分分析不能保证得到的分量是正交的，但是结果更容易解释，因为主向量实际上是输入数据集的一部分。此外，它在特征数量方面是可伸缩的:如果当特征数量变大时(假设超过 1000 个)，PCA 及其可伸缩版本被卡住，稀疏 PCA 在速度方面仍然是一个最佳解决方案，这要归功于解决 Lasso 问题的内部方法，通常基于 Lars 或坐标下降。(记住套索试图最小化系数的 L1 范数。)此外，当特征的数量大于观察的数量时，例如，一些图像数据集，这很好。

现在让我们看看它如何在包含 10，000 个要素的 25，000 个观测数据集中工作。对于这个例子，我们使用的是`SparsePCA`算法的小批量版本，它确保了恒定的内存使用，并且能够处理大规模数据集，最终大于可用内存(注意，批量版本被命名为`SparsePCA`，但不支持在线训练):

```
In:from sklearn.decomposition import MiniBatchSparsePCA

X, _ = make_blobs(25000, n_features=10000, random_state=101)

tik = time.time()
pca = MiniBatchSparsePCA(20, method='cd', random_state=101, \
                    n_iter=1000)
pca.fit(X)
print "SparsePCA on matrix", X.shape, "done in ", time.time() - \
        tik, "seconds"

Out:
SparsePCA on matrix (25000, 10000) done in  41.7692570686 seconds
```

在大约 40 秒内，`SparsePCA`能够使用恒定的内存量产生解决方案。

# 与 H2O 进行主成分分析

我们也可以使用 H2O 提供的 PCA 实现。(我们在之前的章节已经看到了 H2O，并在书中提到了它。)

有了 H2O，我们首先需要用`init`方法打开服务器。然后，我们将数据集转储到一个文件中(准确地说，是一个 CSV 文件)，最后运行 PCA 分析。最后一步，我们关闭服务器。

我们正在一些迄今为止最大的数据集上尝试这种实现——一个包含 10 万个观测值和 100 个要素，另一个包含 10K 观测值和 2，500 个要素:

```
In: import h2o
from h2o.transforms.decomposition import H2OPCA
h2o.init(max_mem_size_GB=4)

def testH2O_pca(nrows, ncols, k=20):
    temp_file = tempfile.NamedTemporaryFile().name
    X, _ = make_blobs(nrows, n_features=ncols, random_state=101)
np.savetxt(temp_file, np.c_[X], delimiter=",")
    del X

pca = H2OPCA(k=k, transform="NONE", pca_method="Power")
    tik = time.time()
    pca.train(x=range(100), \
training_frame=h2o.import_file(temp_file))

    print "H2OPCA on matrix ", (nrows, ncols), \
" done in ", time.time() - tik, "seconds"
os.remove(temp_file)

testH2O_pca(100000, 100)
testH2O_pca(10000, 2500)
h2o.shutdown(prompt=False)

Out:[...]
H2OPCA on matrix  (100000, 100) done in  12.9560530186 seconds
[...]
H2OPCA on matrix  (10000, 2500) done in  10.1429388523 seconds
```

正如你所看到的，在这两种情况下，H2O 确实表现得非常快，与 Scikit-learn 相当(如果不是超越的话)。

# 聚类–K 均值

K-means 是一种无监督算法，它以相等的方差创建 K 个不相交的点簇，最小化失真(也称为惯性)。

仅给定一个参数 K，代表要创建的聚类数，K-means 算法创建 K 组点 S <sub>1</sub> 、S <sub>2</sub> 、…、S <sub>K</sub> ，每个点由其质心表示:C <sub>1</sub> 、C <sub>2</sub> 、…、C <sub>K</sub> 。通用形心，C <sub>i</sub> ，仅仅是与簇 Si 相关联的点的样本的平均值，以便最小化簇内距离。系统的输出如下:

1.  聚类 S <sub>1</sub> 、S <sub>2</sub> 、…、S <sub>K</sub> 的组成，即组成与聚类号 1、2、…、K 相关联的训练集的点的集合
2.  每个簇的质心，C <sub>1</sub> ，C <sub>2</sub> ，…，C <sub>K</sub> 。质心可用于未来的关联。
3.  The distortion introduced by the clustering, computed as follows:

    ![Clustering – K-means](graphics/B05135_07_34.jpg)

这个等式表示在 K-means 算法中本质上完成的优化:质心被选择为最小化簇内失真，即每个输入点和该点所关联的簇的质心之间的距离的欧几里德范数之和。换句话说，该算法试图拟合最佳矢量量化。

K-means 算法的训练阶段也被称为劳埃德算法，以最早提出该算法的斯图尔特·劳埃德的名字命名。这是一种迭代算法，由两个阶段组成，一遍又一遍地迭代，直到收敛(失真达到最小)。它是广义的**期望最大化** ( **EM** )算法的变体，作为第一步，为分数的**期望** ( **E** )创建函数，而**最大化** ( **M** )步骤计算使分数最大化的参数。(注意，在这个公式中，我们试图实现相反的结果，即失真最小化。)这是它的公式:

*   The expectation step: In this step, the points in the training set are assigned to the closest centroid:

    ![Clustering – K-means](graphics/B05135_07_35.jpg)

    这一步也被称为赋值或矢量量化。

*   The maximization step: The centroid of each cluster is moved to the middle of the cluster by averaging the points composing it:

    ![Clustering – K-means](graphics/B05135_07_36.jpg)

    此步骤也称为更新步骤。

执行这两个步骤直到收敛(点在它们的簇中是稳定的)，或者直到算法达到预设的迭代次数。请注意，对于每个合成，失真不会在整个训练阶段增加(不同于基于随机梯度下降的方法)；因此，在这个算法中，迭代次数越多，结果越好。

现在让我们看看它在虚拟二维数据集上的样子。我们首先创建一组 1000 个点，集中在相对于原点对称的四个位置。每个集群，每个构造，都有相同的差异:

```
In:import matplotlib
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline

In:from sklearn.datasets.samples_generator import make_blobs

centers = [[1, 1], [1, -1], [-1, -1], [-1, 1]]
X, y = make_blobs(n_samples=1000, centers=centers, 
                  cluster_std=0.5, random_state=101)
```

现在让我们绘制数据集。为了让事情变得更简单，我们将使用不同的颜色对集群进行着色:

```
In:plt.scatter(X[:,0], X[:,1], c=y, edgecolors='none', alpha=0.9)
plt.show()
```

![Clustering – K-means](graphics/B05135_07_06.jpg)

现在让我们运行 K-means 并检查每次迭代中发生了什么。为此，我们将迭代停止到 1、2、3 和 4 次迭代，并绘制点及其相关聚类(颜色编码)以及质心、失真(在标题中)和决策边界(也称为沃罗诺伊单元)。质心的初始选择是随机的，即在训练的期望阶段，在第一次迭代中选择四个训练点作为质心:

```
In:pylab.rcParams['figure.figsize'] = (10.0, 8.0)

from sklearn.cluster import KMeans

for n_iter in range(1, 5):

    cls = KMeans(n_clusters=4, max_iter=n_iter, n_init=1,
                 init='random', random_state=101)
    cls.fit(X)

    # Plot the voronoi cells

    plt.subplot(2, 2, n_iter)
    h=0.02
    xx, yy = np.meshgrid(np.arange(-3, 3, h), np.arange(-3, 3, h))
    Z = cls.predict(np.c_[xx.ravel(), \
        yy.ravel()]).reshape(xx.shape)
    plt.imshow(Z, interpolation='nearest', cmap=plt.cm.Accent, \
               extent=(xx.min(), xx.max(), yy.min(), yy.max()), \
               aspect='auto', origin='lower')

    plt.scatter(X[:,0], X[:,1], c=cls.labels_, \
edgecolors='none', alpha=0.7)
    plt.scatter(cls.cluster_centers_[:,0], \
                cls.cluster_centers_[:,1], \
                marker='x', color='r', s=100, linewidths=4)
    plt.title("iter=%s, distortion=%s" %(n_iter, \
                int(cls.inertia_)))

plt.show()
```

![Clustering – K-means](graphics/B05135_07_07.jpg)

可以看到，随着迭代次数的增加，失真越来越小。对于这个虚拟数据集，似乎通过几次迭代(五次迭代)，我们已经达到了收敛。

## 初始化方法

求 K 均值中失真的全局最小值是一个 NP 难问题；此外，与随机梯度下降的完全一样，该方法容易收敛到局部极小值，尤其是在维数较高的情况下。为了避免此类行为并限制最大迭代次数，您可以使用以下对策:

*   使用不同的初始条件多次运行该算法。在 Scikit-learn 中，`KMeans`类有`n_init`参数，该参数控制用不同的质心种子运行 K-means 算法的次数。最后，选择确保较低失真的模型。如果有多个可用的内核，可以通过将`n_jobs`参数设置为需要剥离的作业数量来并行运行该过程。请注意，内存消耗与并行作业的数量呈线性关系。
*   比起随机选择训练点，更喜欢 k-means++初始化(默认为`KMeans`类)。K-means++初始化选择彼此之间相距*的点；这应该确保质心能够在空间的均匀子空间中形成簇。也证明了这个事实保证了*更有可能*找到最优解。*

 *## K-均值假设

K-means 依赖于假设每个聚类都有一个(超)球形，也就是说它没有拉长的形状(像箭头一样)，所有的聚类内部都有相同的方差，并且它们的大小相当(或者非常远)。

所有这些假设都可以通过强大的特征预处理步骤来保证；主成分分析、核主成分分析、特征归一化和采样可以是一个很好的开始。

现在让我们看看当不满足 K 均值背后的假设时会发生什么:

```
In:pylab.rcParams['figure.figsize'] = (5.0, 10.0)
from sklearn.datasets import make_moons

# Oblong/elongated sets
X, _ = make_moons(n_samples=1000, noise=0.1, random_state=101)
cls = KMeans(n_clusters=2, random_state=101)
y_pred = cls.fit_predict(X)

plt.subplot(3, 1, 1)
plt.scatter(X[:, 0], X[:, 1], c=y_pred, edgecolors='none')
plt.scatter(cls.cluster_centers_[:,0], cls.cluster_centers_[:,1], 
                marker='x', color='r', s=100, linewidths=4)
plt.title("Elongated clusters")

# Different variance between clusters
centers = [[-1, -1], [0, 0], [1, 1]]
X, _ = make_blobs(n_samples=1000, cluster_std=[0.1, 0.4, 0.1],
                  centers=centers, random_state=101)
cls = KMeans(n_clusters=3, random_state=101)
y_pred = cls.fit_predict(X)

plt.subplot(3, 1, 2)
plt.scatter(X[:, 0], X[:, 1], c=y_pred, edgecolors='none')
plt.scatter(cls.cluster_centers_[:,0], cls.cluster_centers_[:,1], 
                marker='x', color='r', s=100, linewidths=4)
plt.title("Unequal Variance between clusters")

# Unevenly sized blobs
centers = [[-1, -1], [1, 1]]
centers.extend([[0,0]]*20)
X, _ = make_blobs(n_samples=1000, centers=centers, 
                  cluster_std=0.28, random_state=101)
cls = KMeans(n_clusters=3, random_state=101)
y_pred = cls.fit_predict(X)

plt.subplot(3, 1, 3)
plt.scatter(X[:, 0], X[:, 1], c=y_pred, edgecolors='none')
plt.scatter(cls.cluster_centers_[:,0], cls.cluster_centers_[:,1], 
                marker='x', color='r', s=100, linewidths=4)
plt.title("Unevenly Sized Blobs")

plt.show()
```

![K-means assumptions](graphics/B05135_07_08.jpg)

在前面所有的例子中，聚类操作并不完美，输出了错误且不稳定的结果。

到目前为止，我们已经假设确切知道哪个是确切的 K，即我们期望在聚类操作中使用的聚类数量。实际上，在现实问题中，这并不总是正确的。我们经常使用无监督学习方法来发现数据的底层结构，包括组成数据集的聚类数量。让我们看看当我们试图在一个简单的虚拟数据集上用一个错误的 K 运行 K-means 时会发生什么；我们将尝试较低的 K 和较高的 K:

```
In:pylab.rcParams['figure.figsize'] = (10.0, 4.0)
X, _ = make_blobs(n_samples=1000, centers=3, random_state=101)

for K in [2, 3, 4]:
    cls = KMeans(n_clusters=K, random_state=101)
    y_pred = cls.fit_predict(X)

    plt.subplot(1, 3, K-1)
    plt.title("K-means, K=%s" % K)
    plt.scatter(X[:, 0], X[:, 1], c=y_pred, edgecolors='none')
    plt.scatter(cls.cluster_centers_[:,0], cls.cluster_centers_[:,1], 
                marker='x', color='r', s=100, linewidths=4)

plt.show()
```

![K-means assumptions](graphics/B05135_07_09.jpg)

如您所见，如果没有猜中正确的 K，即使对于这个简单的虚拟数据集，结果也是大错特错的。在下一节中，我们将解释一些最佳选择 k 的技巧。

## 最佳钾的选择

如果满足 K 均值背后的假设，有几种方法可以检测最佳 K。其中一些是基于输出的交叉验证和度量；它们可以用在所有的聚类方法上，但是只有当一个基本事实可用时(它们被称为监督度量)。其他一些是基于聚类算法的内在参数，并且可以通过存在或不存在基础事实(也称为无监督度量)来独立使用。不幸的是，它们都不能确保找到正确结果的 100%准确性。

监督度量需要一个基本事实(包含集合中的真实关联)，并且它们通常与网格搜索分析相结合来理解最佳 k。这些度量中的一些是从等价的分类中导出的，但是它们允许有不同数量的无序集合作为预测标签。我们要看的第一个叫做**同质化**；正如你所料，它给出了预测的簇中有多少只包含一个类的点的度量。这是一种基于熵的度量，是分类精度的聚类等价。它是 0(最差)和 1(最好)之间的度量界限；它的数学公式如下:

![Selection of the best K](graphics/B05135_07_37.jpg)

这里， *H(C|K)* 是给定所提出的聚类分配的类分布的条件熵， *H(C)* 是类的熵。*当聚类没有提供新信息时，H(C|K)* 最大，等于*H(C)*；当每个集群只包含一个类的成员时，它为零。

与之相连的是分类的精确度和召回率，还有**完整性**分数:它给出了一个度量，关于一个类的所有成员被分配到同一个聚类的程度。甚至这一个也在 0(最差)和 1(最好)之间，它的数学公式深深基于熵:

![Selection of the best K](graphics/B05135_07_38.jpg)

这里， *H(K|C)* 是给定类的建议聚类分布的条件熵， *H(K)* 是聚类的熵。

最后，相当于分类任务的 f1 分数，V-测度是同质性和完备性的调和平均值:

![Selection of the best K](graphics/B05135_07_39.jpg)

让我们回到第一个数据集(四个对称的有噪声的聚类)，并尝试看看这些分数是如何操作的，以及它们是否能够突出最佳 K 来使用:

```
In:pylab.rcParams['figure.figsize'] = (6.0, 4.0)
from sklearn.metrics import homogeneity_completeness_v_measure

centers = [[1, 1], [1, -1], [-1, -1], [-1, 1]]
X, y = make_blobs(n_samples=1000, centers=centers, 
                  cluster_std=0.5, random_state=101)

Ks = range(2, 10)
HCVs = []
for K in Ks:
    y_pred = KMeans(n_clusters=K, random_state=101).fit_predict(X)
    HCVs.append(homogeneity_completeness_v_measure(y, y_pred))

plt.plot(Ks, [el[0] for el in HCVs], 'r', label='Homogeneity')
plt.plot(Ks, [el[1] for el in HCVs], 'g', label='Completeness')
plt.plot(Ks, [el[2] for el in HCVs], 'b', label='V measure')
plt.ylim([0, 1])
plt.legend(loc=4)
plt.show()
```

![Selection of the best K](graphics/B05135_07_10.jpg)

在剧情上，最初( *K < 4* )完整性高，但同质性低；对于 *K > 4* 来说，则相反:同质性高，但完备性低。在这两种情况下，V 值都很低。相反，对于 *K=4* ，所有度量都达到了最大值，这表明这是集群数量 *K* 的最佳值。

除了这些被监督的度量之外，还有其他被命名为无监督的度量，它们不需要一个基本的事实，只是基于学习者本身。

在这个部分，我们首先要看到的是**肘击法**，应用于扭曲。这很简单，不需要任何数学:你只需要画出许多 K-means 模型不同 K 的失真，然后选择一个增加 K 不会在解中引入*低得多的*失真的模型。在 Python 中，这很容易实现:

```
In:Ks = range(2, 10)
Ds = []
for K in Ks:
    cls = KMeans(n_clusters=K, random_state=101)
    cls.fit(X)
    Ds.append(cls.inertia_)

plt.plot(Ks, Ds, 'o-')
plt.xlabel("Value of K")
plt.ylabel("Distortion")
plt.show()
```

![Selection of the best K](graphics/B05135_07_11.jpg)

如你所料，失真下降到 *K=4* ，然后慢慢下降。这里，最好的 K 是 4。

我们将看到的另一个无监督的度量是**轮廓**。它更复杂，但也比之前的试探法更强大。在很高的层次上，它衡量一个观测值与指定的聚类有多接近(相似)，它与附近聚类的数据匹配有多松散(不相似)。剪影得分为 1 表示所有数据都在最佳聚类中，而-1 表示完全错误的聚类结果。由于 Scikit-learn 实现，使用 Python 代码获得这样的度量非常容易:

```
In:from sklearn.metrics import silhouette_score

Ks = range(2, 10)
Ds = []
for K in Ks:
    cls = KMeans(n_clusters=K, random_state=101)
    Ds.append(silhouette_score(X, cls.fit_predict(X)))

plt.plot(Ks, Ds, 'o-')
plt.xlabel("Value of K")
plt.ylabel("Silhouette score")
plt.show()
```

![Selection of the best K](graphics/B05135_07_12.jpg)

即使在这种情况下，我们也得出了相同的结论:K 的最佳值是 4，因为轮廓分数随着 K 的越来越高而越来越低。

## 换算 K 均值-小批量

现在让我们测试一下 K 均值的可伸缩性。从 UCI 的网站上，我们为这个任务选择了一个合适的数据集:美国 1990 年人口普查数据。这个数据集包含近 250 万个观察值和 68 个分类(但已经是数字编码的)属性。没有丢失数据，文件是 CSV 格式。每个观察都包含个人的 ID(在聚类之前要删除)和其他关于性别、收入、婚姻状况、工作等的信息。

### 注

更多关于数据集的信息可以在[http://archive . ics . UCI . edu/ml/datasets/US+Census+Data+% 281990% 29](http://archive.ics.uci.edu/ml/datasets/US+Census+Data+%281990%29)或 Meek，Thiesson 和 Heckerman (2001)在《机器学习研究杂志》上发表的题为*应用于聚类的学习曲线方法*的论文中找到。

首先，您必须下载包含数据集的文件，并将其存储在临时目录中。请注意，它的大小为 345 兆字节，因此在慢速连接上下载可能需要很长时间:

```
In:import urllib
import os.path

url = "http://archive.ics.uci.edu/ml/machine-learning-databases/census1990-mld/USCensus1990.data.txt"
census_csv_file = "/tmp/USCensus1990.data.txt"

import os.path
if not os.path.exists(census_csv_file):
    testfile = urllib.URLopener()
    testfile.retrieve(url, census_csv_file)
```

现在，让我们运行一些测试，记录训练 K 均值学习者所需的时间，K 等于 4、8 和 12，数据集包含 20K、200K 和 0.5M 个观察值。由于我们不想让机器的内存饱和，因此我们将只读取前 500K 行，并删除包含用户标识符的列。最后，让我们为完整的绩效评估绘制培训时间图:

```
In:piece_of_dataset = pd.read_csv(census_csv_file, iterator=True).get_chunk(500000).drop('caseid', axis=1).as_matrix()

time_results = {4: [], 8:[], 12:[]}
dataset_sizes = [20000, 200000, 500000]

for dataset_size in dataset_sizes:
    print "Dataset size:", dataset_size
    X = piece_of_dataset[:dataset_size,:]

    for K in [4, 8, 12]:
        print "K:", K
        cls = KMeans(K, random_state=101)
        timeit = %timeit -o -n1 -r1 cls.fit(X)

        time_results[K].append(timeit.best)

plt.plot(dataset_sizes, time_results[4], 'r', label='K=4')
plt.plot(dataset_sizes, time_results[8], 'g', label='K=8')
plt.plot(dataset_sizes, time_results[12], 'b', label='K=12')

plt.xlabel("Training set size")
plt.ylabel("Training time")
plt.legend(loc=0)
plt.show()

Out:Dataset size: 20000
K: 4
1 loops, best of 1: 478 ms per loop
K: 8
1 loops, best of 1: 1.22 s per loop
K: 12
1 loops, best of 1: 1.76 s per loop
Dataset size: 200000
K: 4
1 loops, best of 1: 6.35 s per loop
K: 8
1 loops, best of 1: 10.5 s per loop
K: 12
1 loops, best of 1: 17.7 s per loop
Dataset size: 500000
K: 4
1 loops, best of 1: 13.4 s per loop
K: 8
1 loops, best of 1: 48.6 s per loop
K: 12
1 loops, best of 1: 1min 5s per loop
```

![Scaling K-means – mini-batch](graphics/B05135_07_13.jpg)

很明显，给定图和实际时间，训练时间随着 K 和训练集大小线性增加，但是对于大的 K 和训练大小，这种关系变成非线性。对许多 Ks 的整个训练集进行详尽的搜索似乎是不可扩展的。

幸运的是，有一个基于小批量的在线版 K-means，已经在 Scikit-learn 中实现并命名为`MiniBatchKMeans`。让我们在前一个单元格最慢的情况下尝试一下，也就是 *K=12* 。使用经典的 K-means，对 500，000 个样本(约占整个数据集的 20%)的训练花费了一分多钟；让我们看看在线小批量版本的性能，将批量大小设置为 1000，并从数据集导入 50000 个观察值的组块。作为输出，我们绘制了训练时间与训练阶段已经通过的组块数量的关系图:

```
In:from sklearn.cluster import MiniBatchKMeans
import time

cls = MiniBatchKMeans(12, batch_size=1000, random_state=101)
ts = []

tik = time.time()
for chunk in pd.read_csv(census_csv_file, chunksize=50000):
    cls.partial_fit(chunk.drop('caseid', axis=1))
    ts.append(time.time()-tik)

plt.plot(range(len(ts)), ts)
plt.xlabel('Training batches')
plt.ylabel('time [s]')

plt.show()
```

![Scaling K-means – mini-batch](graphics/B05135_07_14.jpg)

每个数据块的训练时间是线性的，在将近 20 秒的时间内对全部 250 万个观察数据集执行聚类。有了这个实现，我们可以运行一个完整的搜索来选择最佳的 K 使用肘方法对失真。让我们做一个网格搜索，K 从 4 到 12，并绘制失真:

```
In:Ks = list(range(4, 13))
ds = []

for K in Ks:
    cls = MiniBatchKMeans(K, batch_size=1000, random_state=101)

    for chunk in pd.read_csv(census_csv_file, chunksize=50000):
        cls.partial_fit(chunk.drop('caseid', axis=1))
    ds.append(cls.inertia_)

plt.plot(Ks, ds)
plt.xlabel('Value of K')
plt.ylabel('Distortion')

plt.show()

Out:
```

![Scaling K-means – mini-batch](graphics/B05135_07_15.jpg)

从图中来看，肘似乎与 K=8 对应。除了价值之外，我们想指出的是，由于批量实现，在不到几分钟的时间内，我们已经能够在一个大数据集上执行这种大规模操作；因此，如果数据集越来越大，请记住永远不要使用普通的 K 均值。

# K-表示与 H2O

在这里，我们将 H2O 的 K-means 实现与 Scikit-learn 进行比较。更具体地说，我们将使用在 H2O 可用的 K-means 的对象，运行小批量实验。设置类似于*主成分分析与 H2O* 部分所示，实验与前一部分相同:

```
In:import h2o
from h2o.estimators.kmeans import H2OKMeansEstimator
h2o.init(max_mem_size_GB=4)

def testH2O_kmeans(X, k):

    temp_file = tempfile.NamedTemporaryFile().name
    np.savetxt(temp_file, np.c_[X], delimiter=",")

    cls = H2OKMeansEstimator(k=k, standardize=True)
    blobdata = h2o.import_file(temp_file) 

    tik = time.time()
    cls.train(x=range(blobdata.ncol), training_frame=blobdata)
    fit_time = time.time() - tik

    os.remove(temp_file)

    return fit_time

piece_of_dataset = pd.read_csv(census_csv_file, iterator=True).get_chunk(500000).drop('caseid', axis=1).as_matrix()
time_results = {4: [], 8:[], 12:[]}
dataset_sizes = [20000, 200000, 500000]

for dataset_size in dataset_sizes:
    print "Dataset size:", dataset_size
    X = piece_of_dataset[:dataset_size,:]

    for K in [4, 8, 12]:
        print "K:", K
        fit_time = testH2O_kmeans(X, K)
        time_results[K].append(fit_time)

plt.plot(dataset_sizes, time_results[4], 'r', label='K=4')
plt.plot(dataset_sizes, time_results[8], 'g', label='K=8')
plt.plot(dataset_sizes, time_results[12], 'b', label='K=12')

plt.xlabel("Training set size")
plt.ylabel("Training time")
plt.legend(loc=0)
plt.show()

testH2O_kmeans(100000, 100)

h2o.shutdown(prompt=False)

Out:
```

![K-means with H2O](graphics/B05135_07_16.jpg)

得益于 H2O 架构，其 K-means 的实现非常快，并且可扩展，能够在不到 30 秒的时间内对所有选定的 K 执行 500K 点数据集的聚类。

# LDA

**LDA** 代表**潜在狄利克雷分配**，是分析文本文档集合的常用技术之一。

### 注

线性判别分析是另一种技术使用的首字母缩略词，它是一种有监督的分类方法。注意 LDA 是如何使用的，因为这两种算法之间没有联系。

对线性判别分析的完整数学解释需要概率建模的知识，这超出了本实用书的范围。相反，在这里，我们将为您提供模型背后最重要的直觉，以及如何在大规模数据集上实际应用该模型。

首先，LDA 用于数据科学的一个分支，称为文本挖掘，其重点是帮助学习者理解自然语言，例如，基于文本示例。具体来说，LDA 属于主题建模算法的范畴，因为它试图对文档中包含的主题进行建模。理想情况下，例如，LDA 能够理解一份文件是关于金融、政治还是宗教。然而，与分类器不同，它还能够量化文档中主题的存在。例如，让我们想想罗琳的《哈利·波特》小说。一个分类器将能够评估它的类别(奇幻小说)；相反，LDA 能够理解其中有多少喜剧、戏剧、神秘、浪漫和冒险。而且，LDA 不需要任何标签；这是一种无监督的方法，并在内部构建输出类别或主题及其组成(即由组成主题的词集给出)。

在处理过程中，线性判别分析建立了每个文档的主题模型和每个主题的单词模型，建模为狄利克雷分布。尽管复杂性很高，但由于类似蒙特卡罗的迭代核心函数，输出稳定结果所需的处理时间并没有那么长。

LDA 模型很容易理解:每个文档被建模为主题的分布，每个主题被建模为单词的分布。分布假设具有狄利克雷先验(具有不同的参数，因为每个主题的字数通常不同于每个文档的主题数)。由于吉布斯采样，分布不应该被直接采样，而是迭代地获得它的精确近似。使用变分贝叶斯技术可以获得类似的结果，其中近似是用期望最大化方法生成的。

最终的线性判别分析模型是生成的(就像隐马尔可夫模型、朴素贝叶斯和受限玻尔兹曼机器一样)，因此每个变量都可以被模拟和观察。

现在让我们看看它是如何在一个真实的数据集上工作的——20 新闻组数据集。它由 20 个新闻组中交换的电子邮件组成。让我们首先加载它，从回复的电子邮件中删除电子邮件的页眉、页脚和引号:

```
In:from sklearn.datasets import fetch_20newsgroups
documents = fetch_20newsgroups(remove=('headers', 'footers', \
            'quotes'), random_state=101).data
```

检查数据集的大小(即有多少个文档)，并打印其中一个文档，以查看一个文档实际由什么组成:

```
In:len(documents)

Out:11314

In:document_num = 9960
print documents[document_num]

Out:Help!!!

I have an ADB graphicsd tablet which I want to connect to my
Quadra 950\. Unfortunately, the 950 has only one ADB port and
it seems I would have to give up my mouse.

Please, can someone help me? I want to use the tablet as well as
the mouse (and the keyboard of course!!!).

Thanks in advance.
```

举个例子，一个人正在平板电脑上为他的视频插座寻求帮助。

现在，我们导入运行 LDA 所需的 Python 包。Gensim 包是最好的包之一，正如您将在本节末尾看到的，它也是非常可扩展的:

```
In:import gensim
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS
from nltk.stem import WordNetLemmatizer, SnowballStemmer

np.random.seed(101)
```

作为第一步，我们应该清理文本。一些步骤是必要的，这是典型的任何 NLP 文本处理:

1.  标记化是将文本拆分成句子，将句子拆分成单词。最后，单词是低级的。此时，标点符号(和重音符号)被删除。
2.  由少于三个字符组成的单词将被删除。(这一步删除了大多数首字母缩略词、表情符号和连词。)
3.  英语*停止词*列表中出现的词被删除。这个列表中的单词非常常见，没有预测能力(如，an，so，then，have，等等)。
4.  然后标记被引理化；第三人称的单词变成第一人称，过去和将来时态的动词变成现在(比如 goes、goes、goes 都变成了 go)。
5.  最后，词干去除了词形变化，将单词还原为词根(例如，鞋变成鞋)。

在下面这段代码中，我们将完全这样做:尽可能地清理文本，并列出构成每个文本的单词。在单元格的末尾，我们可以看到该操作如何改变之前看到的文档:

```
In:lm = WordNetLemmatizer()
stemmer = SnowballStemmer("english")

def lem_stem(text):
    return stemmer.stem(lm.lemmatize(text, pos='v'))

def tokenize_lemmatize(text):
    return [lem_stem(token)
            for token in gensim.utils.simple_preprocess(text) 
            if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3]

print tokenize_lemmatize(documents[document_num])

Out:[u'help', u'graphicsd', u'tablet', u'want', u'connect', u'quadra', u'unfortun', u'port', u'mous', u'help', u'want', u'tablet', u'mous', u'keyboard', u'cours', u'thank', u'advanc']
```

现在，作为下一步，让我们操作所有文档的清理步骤。在这之后，我们必须建立一个字典，包含一个单词在训练集中出现的次数。由于有了 Gensim 包，该操作非常简单:

```
In:processed_docs = [tokenize(doc) for doc in documents]
word_count_dict = gensim.corpora.Dictionary(processed_docs)
```

现在，当我们想要构建一个通用的快速解决方案时，让我们去掉所有非常罕见和非常常见的单词。例如，我们可以过滤掉所有出现少于 20 次(总共)且不超过 20%的文档中的单词:

```
In:word_count_dict.filter_extremes(no_below=20, no_above=0.2)
```

作为下一步，使用这样一组缩减的单词，我们现在为每个文档构建单词包模型；也就是说，对于每个文档，我们创建一个字典来报告单词的数量和出现次数:

```
In:bag_of_words_corpus = [word_count_dict.doc2bow(pdoc) \
for pdoc in processed_docs]
```

例如，让我们来看一下前面文档的单词包模型:

```
In:bow_doc1 = bag_of_words_corpus[document_num]

for i in range(len(bow_doc1)):
    print "Word {} (\"{}\") appears {} time[s]" \
.format(bow_doc1[i][0], \
word_count_dict[bow_doc1[i][0]], bow_doc1[i][1])

Out:Word 178 ("want") appears 2 time[s]
Word 250 ("keyboard") appears 1 time[s]
Word 833 ("unfortun") appears 1 time[s]
Word 1037 ("port") appears 1 time[s]
Word 1142 ("help") appears 2 time[s]
Word 1543 ("quadra") appears 1 time[s]
Word 2006 ("advanc") appears 1 time[s]
Word 2124 ("cours") appears 1 time[s]
Word 2391 ("thank") appears 1 time[s]
Word 2898 ("mous") appears 2 time[s]
Word 3313 ("connect") appears 1 time[s]
```

现在，我们已经到达了算法的核心部分:运行 LDA。至于我们的决定，让我们要求 12 个主题(有 20 种不同的时事通讯，但有些是相似的):

```
In:lda_model = gensim.models.LdaMulticore(bag_of_words_corpus, num_topics=10, id2word=word_count_dict, passes=50)
```

### 注

如果你得到这样一个代码的错误，试着用`gensim.models.LdaModel`类而不是`gensim.models.LdaMulticore`来处理这个版本。

现在我们打印题目作文，即每个题目中出现的单词及其相对权重:

```
In:for idx, topic in lda_model.print_topics(-1):
    print "Topic:{} Word composition:{}".format(idx, topic)
    print

Out:
Topic:0 Word composition:0.015*imag + 0.014*version + 0.013*avail + 0.013*includ + 0.013*softwar + 0.012*file + 0.011*graphic + 0.010*program + 0.010*data + 0.009*format

Topic:1 Word composition:0.040*window + 0.030*file + 0.018*program + 0.014*problem + 0.011*widget + 0.011*applic + 0.010*server + 0.010*entri + 0.009*display + 0.009*error

Topic:2 Word composition:0.011*peopl + 0.010*mean + 0.010*question + 0.009*believ + 0.009*exist + 0.008*encrypt + 0.008*point + 0.008*reason + 0.008*post + 0.007*thing

Topic:3 Word composition:0.010*caus + 0.009*good + 0.009*test + 0.009*bike + 0.008*problem + 0.008*effect + 0.008*differ + 0.008*engin + 0.007*time + 0.006*high

Topic:4 Word composition:0.018*state + 0.017*govern + 0.015*right + 0.010*weapon + 0.010*crime + 0.009*peopl + 0.009*protect + 0.008*legal + 0.008*control + 0.008*drug

Topic:5 Word composition:0.017*christian + 0.016*armenian + 0.013*jesus + 0.012*peopl + 0.008*say + 0.008*church + 0.007*bibl + 0.007*come + 0.006*live + 0.006*book

Topic:6 Word composition:0.018*go + 0.015*time + 0.013*say + 0.012*peopl + 0.012*come + 0.012*thing + 0.011*want + 0.010*good + 0.009*look + 0.009*tell

Topic:7 Word composition:0.012*presid + 0.009*state + 0.008*peopl + 0.008*work + 0.008*govern + 0.007*year + 0.007*israel + 0.007*say + 0.006*american + 0.006*isra

Topic:8 Word composition:0.022*thank + 0.020*card + 0.015*work + 0.013*need + 0.013*price + 0.012*driver + 0.010*sell + 0.010*help + 0.010*mail + 0.010*look

Topic:9 Word composition:0.019*space + 0.011*inform + 0.011*univers + 0.010*mail + 0.009*launch + 0.008*list + 0.008*post + 0.008*anonym + 0.008*research + 0.008*send

Topic:10 Word composition:0.044*game + 0.031*team + 0.027*play + 0.022*year + 0.020*player + 0.016*season + 0.015*hockey + 0.014*leagu + 0.011*score + 0.010*goal

Topic:11 Word composition:0.075*drive + 0.030*disk + 0.028*control + 0.028*scsi + 0.020*power + 0.020*hard + 0.018*wire + 0.015*cabl + 0.013*instal + 0.012*connect
```

不幸的是，LDA 没有为每个主题提供名称；我们应该根据我们对算法结果的解释，自己手动完成。仔细检查了作文之后，我们可以把发现的题目命名如下:

<colgroup><col style="text-align: left"> <col style="text-align: left"></colgroup> 
| 

主题

 | 

名字

 |
| --- | --- |
| Zero | 软件 |
| one | 应用程序 |
| Two | 论证 |
| three | 运输 |
| four | 政府 |
| five | 宗教 |
| six | 人员行动 |
| seven | 中东 |
| eight | 个人电脑设备 |
| nine | 空间 |
| Ten | 比赛 |
| Eleven | 驱动 |

现在，让我们试着理解前面的文档中表示了哪些主题及其权重:

```
In:
for index, score in sorted( \
lda_model[bag_of_words_corpus[document_num]], \
key=lambda tup: -1*tup[1]):
    print "Score: {}\t Topic: {}".format(score, lda_model.print_topic(index, 10))
Out:Score: 0.938887758964     Topic: 0.022*thank + 0.020*card + 0.015*work + 0.013*need + 0.013*price + 0.012*driver + 0.010*sell + 0.010*help + 0.010*mail + 0.010*look
```

最高分与主题 *PC 设备*相关。基于我们之前对文献集的了解，主题提取似乎表现得相当不错。

现在，让我们从整体上评估模型。困惑(或其对数)为我们提供了一个度量标准，以了解 LDA 在训练数据集上的表现:

```
In:print "Log perplexity of the model is", lda_model.log_perplexity(bag_of_words_corpus)

Out:Log perplexity of the model is -7.2985188569
```

在这种情况下，困惑度是 2-7.298，并且它与 LDA 模型能够生成测试集中的文档的(日志)可能性有关，给定这些文档的主题分布。困惑度越低，模型越好，因为这基本上意味着模型可以很好地重新生成文本。

现在，让我们尝试在一个看不见的文档上使用这个模型。为了简单起见，文档只包含了句子，*高尔夫还是网球？哪项运动最好玩？*:

```
In:unseen_document = "Golf or tennis? Which is the best sport to play?"

bow_vector = word_count_dict.doc2bow(\
tokenize_lemmatize(unseen_document))
for index, score in sorted(lda_model[bow_vector], \
key=lambda tup: -1*tup[1]):
    print "Score: {}\t Topic: {}".format(score, \
                lda_model.print_topic(index, 5))

Out:Score: 0.610691655136     Topic: 0.044*game + 0.031*team + 0.027*play + 0.022*year + 0.020*player

Score: 0.222640440339     Topic: 0.018*state + 0.017*govern + 0.015*right + 0.010*weapon + 0.010*crime
```

不出所料，得分较高的话题是关于“游戏”的话题，其次是得分相对较小的话题。

LDA 如何随着语料库的大小而缩放？幸运的是，非常好；该算法是迭代的，允许在线学习，类似于小批量学习。在线流程的关键是`LdaModel`(或`LdaMulticore`)提供的`.update()`方法。

我们将在由前 1000 个文档组成的原始语料库的子集上进行这个测试，并且我们将使用 50、100、200 和 500 个文档的批次来更新我们的 LDA 模型。对于每一个更新模型的小批量，我们将记录时间并将它们绘制在图表上:

```
In:small_corpus = bag_of_words_corpus[:1000]
batch_times = {}

for batch_size in [50, 100, 200, 500]:
    print "batch_size =", batch_size
    tik0 = time.time()
    lda_model = gensim.models.LdaModel(num_topics=12, \
                id2word=word_count_dict)
    batch_times[batch_size] = []

    for i in range(0, len(small_corpus), batch_size):
        lda_model.update(small_corpus[i:i+batch_size], \
update_every=25, \
passes=1+500/batch_size)
        batch_times[batch_size].append(time.time() - tik0)

Out:batch_size = 50
batch_size = 100
batch_size = 200
batch_size = 500
```

注意，我们已经在模型更新中设置了`update_every`和`passes`参数。这对于使模型在每次迭代时收敛并且不返回不收敛的模型是必要的。注意 500 是试探性选择的；如果您将其设置得更低，您将会收到 Gensim 关于模型不收敛的许多警告。

现在我们来绘制结果:

```
In:plt.plot(range(50, 1001, 50), batch_times[50], 'g', \
label='size 50')
plt.plot(range(100, 1001, 100), batch_times[100], 'b', \
label='size 100')
plt.plot(range(200, 1001, 200), batch_times[200], 'k', \
label='size 200')
plt.plot(range(500, 1001, 500), batch_times[500], 'r', \
label='size 500')

plt.xlabel("Training set size")
plt.ylabel("Training time")
plt.xlim([0, 1000])
plt.legend(loc=0)
plt.show()

Out:
```

![LDA](graphics/B05135_07_18.jpg)

批次越大，训练越快。(请记住，更新模型时，大批量需要较少的通过次数。)另一方面，批次越大，存储和处理语料库所需的内存量就越大。得益于小批量更新方法，LDA 能够扩展到处理数百万个文档的语料库。事实上，Gensim 包提供的实现能够在家用计算机上在几个小时内扩展和处理整个维基百科。如果你有足够的勇气亲自尝试，下面是完成任务的完整说明，由包的作者提供:

[https://radimrehurek . com/genim/wiki . html](https://radimrehurek.com/gensim/wiki.html)

## 缩放 LDA–内存、中央处理器和机器

gensim非常灵活，用于处理大型文本语料库；事实上，该库无需任何修改或额外下载即可扩展:

1.  用 CPU 的数量，允许在单个节点上并行处理(用类，如第一个例子所示)。
2.  随着观察次数的增加，允许基于小批量的在线学习。这可以通过`LdaModel`和`LdaMulticore`中可用的`update`方法来实现(如前例所示)。
3.  在集群上运行它，在集群中的节点之间分配工作负载，这要归功于 Python 库 Pyro4 和`models.lda_dispatcher`(作为调度器)和`models.lda_worker`(作为工作进程)对象，这两个对象都由 Gensim 提供。

除了经典的 LDA 算法，Gensim 还提供了其分层版本，命名为**分层狄利克雷处理** ( **HDP** )。使用这种算法，主题遵循多级结构，使用户能够更好地理解复杂的语料库(也就是说，一些文档是通用的，而一些特定于某个主题)。该模块相当新，截至 2015 年底，其可扩展性不如经典的 LDA。

# 总结

在本章中，我们介绍了三种流行的无监督学习者，它们能够扩展以应对大数据。第一种是主成分分析，它能够通过创建包含大部分方差的特征(即主特征)来减少特征的数量。K-means 是一种聚类算法，能够将相似的点组合在一起，并将它们与质心相关联。LDA 是对文本数据进行主题建模的有力方法，即对每个文档的主题和主题中出现的单词进行联合建模。

在下一章中，我们将介绍一些先进的和非常新的机器学习方法，它们仍然不是主流的一部分，对于小数据集来说自然很棒，但也适合处理大规模的机器学习。*