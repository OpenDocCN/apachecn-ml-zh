# 第六章。分级回归树

在这一章中，我们将集中讨论分类和回归树的可伸缩方法。将涵盖以下主题:

*   Scikit-learn 中快速随机森林应用的提示和技巧
*   加性随机森林模型和二次抽样
*   梯度升压
*   XGBoost 和流方法
*   非常快的 GBM 和 H2O 的随机森林

决策树的目的是学习一系列决策规则，根据训练数据推断目标标签。使用递归算法，该过程从树根开始，并在杂质最少的特征上分割数据。目前，最广泛应用的可扩展的基于树的应用程序是基于 CART 的。 **CART** 是**分类回归树**的缩写，由布瑞曼、弗里德曼·斯通和奥尔森于 1984 年引入。CART 在两个方面不同于其他决策树模型(如 ID3、C4.5/C5.0、CHAID 和 MARS)。首先，CART 适用于分类和回归问题。其次，它构建二叉树(在每次分裂时，产生一个二进制分裂)。这使得 CART 树能够对给定的特征进行递归操作，并以贪婪的方式对杂质形式的误差度量进行优化。这些二叉树以及可扩展的解决方案是本章的重点。

让我们仔细看看这些树是如何建造的。我们可以看到决策树是一个有节点的图，从上到下传递信息。树中的每个决策都是通过对类(布尔型)或连续变量(阈值)进行二进制拆分而做出的，从而得到最终的预测。

树是通过以下过程构建和学习的:

*   递归地寻找将目标标签从根节点分割到终端节点的最佳变量。这是通过我们根据目标结果最小化的每个特征的杂质来衡量的。在本章中，相关的杂质测度是基尼杂质和交叉熵。

**基尼杂质**

![Classification and Regression Trees at Scale](graphics/B05135_06_04.jpg)

基尼不纯度是一个度量标准，用于衡量目标类( *k* )的概率 *pi* 之间的差异，因此概率值在目标类上的平均分布会导致较高的基尼不纯度。

**交叉熵**

![Classification and Regression Trees at Scale](graphics/B05135_06_05.jpg)

通过交叉熵，我们可以看到错误分类的对数概率。这两个指标都被证明产生了非常相似的结果。然而，基尼杂质在计算上更有效，因为它不需要计算日志。

我们这样做，直到满足停止标准。这个标准大致意味着两件事:第一，添加新变量不再改善目标结果，第二，达到最大树深度或树复杂度阈值。请注意，具有许多节点的非常深且复杂的树很容易导致过度拟合。为了防止这种情况，我们通常通过限制树的深度来修剪树。

为了获得对这个过程如何工作的直觉，让我们用 Scikit 构建一个决策树——用 graphviz 学习和可视化它。首先，创建一个玩具数据集，看看我们能否根据智商(数字)、年龄(数字)、年收入(数字)、企业主(布尔)和大学学位(布尔)来预测谁是吸烟者，谁不是。您需要从[http://www.graphviz.org](http://www.graphviz.org)下载软件，以便加载我们将使用 Scikit-learn 创建的`tree.dot`文件的可视化:

```
import numpy as np
from sklearn import tree
iq=[90,110,100,140,110,100]
age=[42,20,50,40,70,50]
anincome=[40,20,46,28,100,20]
businessowner=[0,1,0,1,0,0]
univdegree=[0,1,0,1,0,0]
smoking=[1,0,0,1,1,0]
ids=np.column_stack((iq, age, anincome,businessowner,univdegree))
names=['iq','age','income','univdegree']
dt = tree.DecisionTreeClassifier(random_state=99)
dt.fit(ids,smoking)
dt.predict(ids)
tree.export_graphviz(dt,out_file='tree2.dot',feature_names=names,label=all,max_depth=5,class_names=True)
```

现在可以在工作目录中找到`tree.dot`文件。找到该文件后，您可以使用 graphviz 软件打开它:

![Classification and Regression Trees at Scale](graphics/B05135_06_14.jpg)

*   根节点(收入):这是表示具有最高信息增益和最低杂质(基尼系数=.5)的特征的起始节点
*   内部节点(年龄和智商):这是根节点和终端之间的每个节点。父节点将决策规则传递给接收端—子节点(左和右)
*   终端节点(叶节点):由树结构划分的目标标签

**树深**是从根节点到终端节点的边数。在这种情况下，我们的树深度为 3。

我们现在可以看到生成的树产生的所有二进制拆分。在根节点的顶部，我们可以看到收入低于 24k 的人不是吸烟者(收入< 24)。我们还可以在每个节点上看到相应的基尼杂质(. 5)。没有剩余的子节点，因为决定是最终的。路径仅仅到此结束，因为它完全划分了目标类。然而，在收入的正确子节点(年龄)中，树分支出来。在这里，如果年龄小于等于 46，那么这个人不是吸烟者，但是年龄大于 46，智商低于 105，这个人就是吸烟者。同样重要的是，我们创建的几个不属于树的特性——学位和企业所有者。这是因为树中的变量能够对没有它们的目标标签进行分类。这些省略的特征根本无助于降低树的杂质水平。

单个树有其缺点，因为它们容易过度覆盖，因此不能很好地推广到看不见的数据。这些技术的当前一代是用集成方法训练的，在集成方法中，单棵树被聚合成更强大的模型。这些 CART 集成技术是最常用的机器学习方法之一，因为它们的准确性、易用性和处理异构数据的能力。这些技术已成功应用于最近的数据科学竞赛，如卡格尔杯和 KDD 杯。由于分类和回归树的集成方法目前是人工智能和数据科学领域的标准，CART 集成方法的可扩展解决方案将是本章的主要主题。

一般来说，我们可以辨别两类用于 CART 模型的集成方法，即打包和增强。让我们通过这些概念来更好地理解合奏的形成过程。

# 引导聚合

**装袋**是**自举聚合**的缩写。自举技术起源于分析师不得不处理数据稀缺的背景。使用这种统计方法，当统计分布不能先验地计算出来时，使用子样本来估计总体参数。自举的目标是为群体参数提供更稳健的估计，其中通过随机二次抽样和替换将更多可变性引入较小的数据集。通常，引导遵循以下基本步骤:

1.  从给定数据集中随机抽取一批大小为 *x* 的样本进行替换。
2.  从每个样本中计算一个度量或参数来估计总体参数。
3.  汇总结果。

近年来，自举方法也被用于机器学习模型的参数。当分类器提供高度多样化的决策边界时，集成是最有效的。这种集成的多样性可以通过其底层模型和这些模型所基于的数据的多样性来实现。树非常适合分类器之间的这种多样性，因为树的结构可以是高度可变的。然而，最流行的集成方法是使用不同的训练数据集来训练单个分类器。通常，这样的数据集是通过二次采样技术获得的，例如自举和打包。一切都始于这样一个想法，即通过利用更多的数据，我们可以减少估计的方差。如果手头不可能有更多的数据，重新采样可以提供显著的改进，因为它允许在训练样本的许多版本上重新训练算法。这就是打包的想法；使用 bagging，我们通过聚集(例如，平均)许多重采样的结果，将最初的 bootstrap 思想更进一步，以便得到最终的预测，在该预测中，由于样本内过拟合引起的误差被相互平滑。

当我们将打包等集成技术应用于树模型时，我们会在原始数据集的每个单独的自举样本(或使用采样进行二次采样，无需替换)上构建多个树，然后聚合结果(通常通过算术、几何平均或投票)。

以这种方式，标准打包算法将如下所示:

1.  从整个数据集(*【S1】*、 *S2* 、… *Sn* )中抽取一个 *n* 个大小为 *K* 的随机样本进行替换。
2.  在( *S1* 、 *S2* 、… *锡*上种植不同的树木。
3.  根据新数据计算样本( *S1* 、 *S2* 、… *锡*)的预测值，并汇总其结果。

CART 模型从打包方法中受益匪浅，因为它引入了随机性和多样性。

# 随机林和极随机林

除了装袋，基于训练示例，我们还可以根据特征抽取随机子样本。这样的方法被称为随机子空间。随机子空间对于高维数据(具有大量特征的数据)特别有用，它是我们称之为随机森林的方法的基础。在撰写本文时，随机森林是最受欢迎的机器学习算法，因为它易于使用，对杂乱的数据具有鲁棒性，并且可并行化。它进入了各种各样的应用程序，如定位应用程序、游戏和医疗保健应用的筛选方法。例如，Xbox Kinect 使用随机森林模型进行运动检测。考虑到随机森林算法基于 bagging 方法，该算法相对简单:

1.  从可用样本中抽取尺寸为 *N* 的 *m* 样本。
2.  在每个节点分割处，使用特征集 *G* (无替换)的不同部分，在每个子集( *S1* 、 *S2* 、… *锡*)上独立构建树。
3.  最小化节点分裂的误差(基于基尼指数或熵度量)。
4.  让每棵树进行预测并汇总结果，使用投票进行分类，使用平均进行回归。

由于 bagging 依赖于多个子样本，因此它是并行化的绝佳选择，其中每个 CPU 单元都专用于计算单独的模型。这样，我们可以利用多核的广泛可用性来加快学习速度。作为这种扩展策略的一个限制，我们必须意识到 Python 是单线程的，我们将不得不复制许多 Python 实例，每个实例都复制我们必须使用的示例中的内存空间。因此，我们需要有大量可用的内存来适应训练矩阵和进程数量。如果可用的内存不够，设置在我们的计算机上同时运行的并行树计算的数量将无助于缩放算法。在这种情况下，CPU 使用率和 RAM 内存是重要的瓶颈。

随机森林模型很容易用于机器学习，因为它们不需要大量的超参数调整就能很好地执行。感兴趣的最重要参数是对模型性能影响最大的*树木数量*和树木深度(树木深度)。当对这两个超参数进行操作时，它会导致精度/性能之间的权衡，即更多的树和更多的深度会导致更高的计算负载。作为实践者，我们的经验建议不要将*树数量*的值设置得太高，因为最终，该模型的性能将达到一个稳定水平，并且在添加更多树时不会再提高，而只会导致对 CPU 内核的征税。在这样的考虑下，尽管带有默认参数的随机森林模型在开箱即用时表现良好，但我们仍然可以通过调整树的数量来提高其性能。有关随机林的超参数的概述，请参见下表。

随机森林装袋的最重要参数:

*   `n_estimators`:模型中树的数量
*   `max_features`:用于树构造的特征数量
*   `min_sample_leaf`:如果终端节点包含的样本少于最小值，则删除节点分裂
*   `max_depth`:我们从根节点到终端节点自上而下传递的节点数
*   `criterion`:计算最佳分割的方法(基尼或熵)
*   `min_samples_split`:分割一个内部节点所需的最小样本数

Scikit-learn 提供了大量强大的 CART 集成应用程序，其中一些计算效率很高。说到随机森林，有一个经常被忽视的算法叫做额外树，更广为人知的是**极随机森林**。当谈到 CPU 效率时，额外的树可以提供比常规随机森林相当大的加速——有时甚至是十倍。

在下表中，您可以看到每种方法的计算速度。一旦我们增加样本量，额外树的速度会大大加快，差异会更明显:

<colgroup><col style="text-align: left"> <col style="text-align: left"> <col style="text-align: left"></colgroup> 
| 

样品化

 | 

提取物

 | 

随机森林

 |
| --- | --- | --- |
| One hundred thousand | 25.9 秒 | 164 s |
| Fifty thousand | 9.95 秒 | 35.1 秒 |
| ten thousand | 2.11 秒 | 6.3 秒 |

用 50 个特征和 100 个估计量训练模型，用于额外的树和随机森林。在这个例子中，我们使用了一个具有 16GB 内存的四核 MacBook Pro。我们以秒为单位测量训练时间。

在这一段中，我们将使用极端森林来代替 Scikit-learn 中的香草随机森林方法。所以你可能会问:它们有什么不同？区别并不明显复杂。在随机森林中，节点分割决策规则基于每次迭代中随机选择的特征的最佳得分。在极度随机化的森林中，对随机子集中的每个要素生成随机分割(因此不需要计算来寻找每个要素的最佳分割)，然后选择最佳评分阈值。这种方法带来了一些有利的特性，因为该方法导致获得具有较低方差的模型，即使每个单独的树生长直到在终端节点中具有最大可能的精度。随着更多的随机性被添加到分支分裂中，树学习器产生错误，这些错误因此在集合中的树之间不太相关。这将导致集成中更多的不相关估计，并且根据学习问题(毕竟没有免费的午餐)，导致比标准随机森林集成更低的泛化误差。然而，实际上，规则随机森林模型可以提供稍高的精度。

给定如此有趣的学习属性，通过更有效的节点分裂计算和同样的可用于随机森林的并行性，如果我们想要加速核心内学习，我们认为极随机化树是集成树算法范围内的优秀候选。

要阅读极其随机化的森林算法的详细描述，您可以阅读以下启动一切的文章:

页（page 的缩写）厄恩斯特和魏汉高，*极随机化树*，机器学习，63(1)，3-42，2006。本文可在[https://www . semanticschool . org/paper/extreme-随机化-trees-Geurts-Ernst/336 a 165 c 17 c 9 c 56160d 332 b 9 F4 a2 b 403 fccbdbfb/pdf](https://www.semanticscholar.org/paper/Extremely-randomized-trees-Geurts-Ernst/336a165c17c9c56160d332b9f4a2b403fccbdbfb/pdf)免费获取。

作为如何扩展核心树集成的一个例子，我们将运行一个例子，其中我们将一个有效的随机森林方法应用于信用数据。该数据集用于预测信用卡客户的违约率。数据由 18 个特征和 30，000 个训练示例组成。由于我们需要导入一个 XLS 格式的文件，您将需要安装`xlrd`包，我们可以通过在命令行终端中键入以下内容来实现:

```
$ pip install xlrd
import pandas as pd
import numpy as np
import os
import xlrd
import urllib
#set your path here
os.chdir('/your-path-here')

url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls'
filename='creditdefault.xls'
urllib.urlretrieve(url, filename)

target = 'default payment next month'
data = pd.read_excel('creditdefault.xls', skiprows=1)

target = 'default payment next month'
y = np.asarray(data[target])
features = data.columns.drop(['ID', target])
X = np.asarray(data[features])

from sklearn.ensemble import ExtraTreesClassifier
from sklearn.cross_validation import cross_val_score
from sklearn.datasets import make_classification
from sklearn.cross_validation import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.30, random_state=101)

clf = ExtraTreesClassifier(n_estimators=500, random_state=101)
clf.fit(X_train,y_train)
scores = cross_val_score(clf, X_train, y_train, cv=3,scoring='accuracy', n_jobs=-1)
print "ExtraTreesClassifier -> cross validation accuracy: mean = %0.3f std = %0.3f" % (np.mean(scores), np.std(scores))

Output]

ExtraTreesClassifier -> cross validation accuracy: mean = 0.812 std = 0.003

```

现在我们已经对训练集的准确性有了一些基本的估计，让我们看看它在测试集上的表现。在这种情况下，我们希望监控假阳性和假阴性，并检查目标变量上的类不平衡:

```
y_pred=clf.predict(X_test)
from sklearn.metrics import confusion_matrix
confusionMatrix = confusion_matrix(y_test, y_pred)
print confusionMatrix
from sklearn.metrics import accuracy_score
accuracy_score(y_test, y_pred)

OUTPUT:
[[6610  448]
[1238  704]]

Our overall test accuracy:
0.81266666666666665
```

有趣的是，测试集的准确性等于我们的训练结果。由于我们的基线模型只是使用默认设置，因此我们可以尝试通过调整超参数来提高性能，这是一项计算成本很高的任务。最近，计算效率更高的超参数优化方法得到了发展，我们将在下一节中介绍。

# 随机搜索快速参数优化

您可能已经熟悉了 Scikit-learn 的 gridsearch 功能。这是一个很好的工具，但是当它涉及到大文件时，它可以极大地增加训练时间，这取决于参数空间。对于极端随机森林，我们可以使用名为**随机搜索**的替代参数搜索方法来加快参数调整的计算时间。公共网格搜索通过系统测试所有可能的超参数设置组合来消耗 CPU 和内存，随机搜索随机选择超参数组合。当 gridsearch 测试超过 30 种组合时(对于较小的搜索空间，gridsearch 仍然具有竞争力)，这种方法可以显著提高计算速度。可实现的增益与我们从随机森林切换到极随机森林时看到的相同(根据硬件规格、超参数空间和数据集大小，可以考虑 2 到 10 倍的增益)。

我们可以指定由`n_iter`参数随机评估的超参数设置的数量:

```
from sklearn.grid_search import GridSearchCV, RandomizedSearchCV

param_dist = {"max_depth": [1,3, 7,8,12,None],
    "max_features": [8,9,10,11,16,22],
    "min_samples_split": [8,10,11,14,16,19],
    "min_samples_leaf": [1,2,3,4,5,6,7],
    "bootstrap": [True, False]}

#here we specify the search settings, we use only 25 random parameter 
#valuations but we manage to keep training times in check.
rsearch = RandomizedSearchCV(clf, param_distributions=param_dist,
    n_iter=25)  

rsearch.fit(X_train,y_train)
rsearch.grid_scores_

bestclf=rsearch.best_estimator_
print bestclf
```

在这里，我们可以看到我们模型的最佳参数设置列表。

我们现在可以使用这个模型对我们的测试集进行预测:

```
OUTPUT:
ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',
    max_depth=12, max_features=11, max_leaf_nodes=None,
    min_samples_leaf=4, min_samples_split=10,
    min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=1,
    oob_score=False, random_state=101, verbose=0, warm_start=False)

y_pred=bestclf.predict(X_test)
confusionMatrix = confusion_matrix(y_test, y_pred)
print confusionMatrix
accuracy=accuracy_score(y_test, y_pred)
print accuracy

OUT
[[6733  325]
[1244  698]]

Out[152]:
0.82566666666666666
```

我们设法在可管理的训练时间范围内提高模型的性能，同时提高精度。

## 极度随机化的树和大数据集

到目前为止，由于随机森林的特定特征及其更有效的替代物，极度随机化森林，我们已经研究了利用多核处理器和随机化来扩大规模的解决方案。但是，如果您必须处理一个不适合内存或对 CPU 要求过高的大型数据集，您可能希望尝试一种核外解决方案。集成的核心外方法的最佳解决方案是 H2O 提供的解决方案，这将在本章后面详细介绍。然而，我们可以运用另一个实用技巧，让随机森林或额外的树在大规模数据集上平稳运行。第二好的解决方案是在数据的子样本上训练模型，然后在不同的数据子样本上集成每个模型的结果(毕竟，我们只需要对结果进行平均或分组)。在[第 3 章](03.html "Chapter 3. Fast SVM Implementations")、*快速学习支持向量机*中，我们已经介绍了储层采样的概念，处理数据流上的采样。在本章中，我们将再次使用采样，诉诸于更多的采样算法选择。首先，让我们安装一个名为 subsample 的非常方便的工具，由 Paul Butler([https://github.com/paulgb/subsample](https://github.com/paulgb/subsample))开发，这是一个命令行工具，用于从一个大的、以换行符分隔的数据集(通常是一个类似 CSV 的文件)中采样数据。该工具提供了快速简便的采样方法，例如储层采样。

如[第 3 章](03.html "Chapter 3. Fast SVM Implementations")、*快速学习支持向量机*所示，储层采样是一种采样算法，有助于从一条河流中采样固定大小的样本。概念上很简单(我们已经在第 3 章中看到了公式)，它只需要简单地传递数据来产生一个样本，该样本将存储在磁盘上的一个新文件中。(我们在第 3 章中的脚本将它存储在内存中。)

在下一个示例中，我们将使用这个子样本工具和一种方法来集成在这些子样本上训练的模型。

概括地说，在本节中，我们将执行以下操作:

1.  创建我们的数据集，并将其分为测试和训练数据。
2.  绘制训练数据的子样本，并将它们作为单独的文件保存在硬盘上。
3.  加载这些子样本，并在其上训练极其随机的森林模型。
4.  汇总模型。
5.  检查结果。

让我们用`pip`安装这个子示例工具:

```
$pip install subsample

```

在命令行中，设置包含要采样的文件的工作目录:

```
$ cd /yourpath-here

```

此时，使用`cd`命令，您可以指定您的工作目录，您将需要在其中存储我们将在下一步创建的文件。

我们通过以下方式做到这一点:

```
from sklearn.datasets import fetch_covtype
import numpy as np
from sklearn.cross_validation import train_test_split
dataset = fetch_covtype(random_state=111, shuffle=True)
dataset = fetch_covtype()
X, y = dataset.data, dataset.target
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=0)
del(X,y)
covtrain=np.c_[X_train,y_train]
covtest=np.c_[X_test,y_test]
np.savetxt('covtrain.csv', covtrain, delimiter=",")
np.savetxt('covtest.csv', covtest, delimiter=",")
```

现在，我们已经将数据集划分为测试集和训练集，让我们对训练集进行二次采样，以便获得可以在内存中上传的数据块。考虑到完整训练数据集的大小为 30，000 个示例，我们将对三个较小的数据集进行二次采样，每个数据集由 10，000 个项目组成。如果您的计算机配备了小于 2GB 的内存，您可能会发现将初始训练集分割成更小的文件更容易管理，尽管您将获得的建模结果可能与我们基于三个子样本的示例不同。通常，子样本中的例子越少，模型的偏差就越大。当进行二次抽样时，我们实际上是在权衡处理更易管理的数据量的优势和估计偏差的增加:

```
$ subsample --reservoir -n 10000 covtrain.csv > cov1.csv

$ subsample --reservoir -n 10000 covtrain.csv > cov2.csv

$ subsample --reservoir -n 10000 covtrain.csv>cov3.csv
```

现在，您可以在命令行中指定的文件夹中找到这些子集。

现在确保在 IDE 或笔记本中设置了相同的路径。

让我们一个接一个地加载样本，并在样本上训练一个随机森林模型。

为了稍后将它们组合起来进行最终预测，请注意，我们保持一种逐行的方法，以便您可以密切关注连续的步骤。

为了使这些示例成功，请确保您使用的是 IDE 或 Jupyter 笔记本中设置的相同路径:

```
import os
os.chdir('/your-path-here')
```

此时，我们已经准备好开始从数据中学习，我们可以将样本逐个加载到内存中，并在这些样本上训练一组树:

```
import numpy as np
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.cross_validation import cross_val_score
from sklearn.cross_validation import train_test_split
import pandas as pd
import os
```

在报告验证分数之后，代码将在所有数据块上继续训练我们的模型，一次一个。当我们从不同的数据分区、一个数据块接一个数据块分别学习时，我们使用`warm_start=Trueparameter`和`set_params`方法初始化集成学习器(在本例中为`ExtraTreeClassifier`),其中从之前的训练会话中递增地添加树，因为 fit 方法被多次调用:

```
#here we load sample 1
df = pd.read_csv('/yourpath/cov1.csv')
y=df[df.columns[54]]
X=df[df.columns[0:54]]

clf1=ExtraTreesClassifier(n_estimators=100, random_state=101,warm_start=True)
clf1.fit(X,y)
scores = cross_val_score(clf1, X, y, cv=3,scoring='accuracy', n_jobs=-1)
print "ExtraTreesClassifier -> cross validation accuracy: mean = %0.3f std = %0.3f" % (np.mean(scores), np.std(scores))
print scores
print 'amount of trees in the model: %s' % len(clf1.estimators_)

#sample 2
df = pd.read_csv('/yourpath/cov2.csv')
y=df[df.columns[54]]
X=df[df.columns[0:54]]

clf1.set_params(n_estimators=150, random_state=101,warm_start=True)
clf1.fit(X,y)
scores = cross_val_score(clf1, X, y, cv=3,scoring='accuracy', n_jobs=-1)
print "ExtraTreesClassifier after params -> cross validation accuracy: mean = %0.3f std = %0.3f" % (np.mean(scores), np.std(scores))
print scores
print 'amount of trees in the model: %s' % len(clf1.estimators_)

#sample 3
df = pd.read_csv('/yourpath/cov3.csv')
y=df[df.columns[54]]
X=df[df.columns[0:54]]
clf1.set_params(n_estimators=200, random_state=101,warm_start=True)
clf1.fit(X,y)
scores = cross_val_score(clf1, X, y, cv=3,scoring='accuracy', n_jobs=-1)
print "ExtraTreesClassifier after params -> cross validation accuracy: mean = %0.3f std = %0.3f" % (np.mean(scores), np.std(scores))
print scores
print 'amount of trees in the model: %s' % len(clf1.estimators_)

# Now let's predict our combined model on the test set and check our score.

df = pd.read_csv('/yourpath/covtest.csv')
X=df[df.columns[0:54]]
y=df[df.columns[54]]
pred2=clf1.predict(X)
scores = cross_val_score(clf1, X, y, cv=3,scoring='accuracy', n_jobs=-1)
print "final test score %r" % np.mean(scores)

OUTPUT:]
ExtraTreesClassifier -> cross validation accuracy: mean = 0.803 std = 0.003
[ 0.805997    0.79964007  0.8021021 ]
amount of trees in the model: 100
ExtraTreesClassifier after params -> cross validation accuracy: mean = 0.798 std = 0.003
[ 0.80155875  0.79651861  0.79465626]
amount of trees in the model: 150
ExtraTreesClassifier after params -> cross validation accuracy: mean = 0.798 std = 0.006
[ 0.8005997   0.78974205  0.8033033 ]
amount of trees in the model: 200
final test score 0.92185447181058278
```

### 注

警告:这个方法看起来不像皮托尼克，但是非常有效。

我们现在已经提高了最终预测的分数；在测试集上，我们从大约 0.8 的精确度到 0.922 的精确度。这是因为我们有一个最终的组合模型，包含了前面三个随机森林模型的所有树信息。在代码的输出中，您还可以注意到添加到初始模型中的树的数量。

从这里开始，您可能希望在更大的数据集上尝试这样的方法，利用更多的子样本，或者对其中一个子样本应用随机搜索以进行更好的调整。

# 推车和助推

我们从装袋开始这一章；现在我们将使用 boosting 来完成我们的概述，boosting 是一种不同的集成方法。就像装袋一样，boosting 既可以用于回归也可以用于分类，并且最近为了更高的准确性而盖过了随机森林。

作为一个优化过程，boosting 基于我们在其他方法中看到的随机梯度下降原理，即通过根据梯度最小化误差来优化模型。迄今为止最常见的助推方法是 **AdaBoost** 和梯度助推(GBM 和最近的 XGBoost)。AdaBoost 算法归结为最小化那些预测略有错误的情况的误差，以便更难分类的情况得到更多的关注。最近，AdaBoost 失宠了，因为发现其他助推方法通常更准确。

在本章中，我们将介绍 Python 用户迄今为止可用的两种最有效的增强算法:**Scikit-learn 包中的梯度增强机器** ( **GBM** )和T5】极限梯度增强 ( **XGBoost** )。由于 GBM 本质上是顺序的，该算法很难并行化，因此比随机森林更难扩展，但是一些技巧可以做到这一点。一些加速算法的提示和技巧将会被一个不错的 H2O 内存不足解决方案所覆盖。

## 梯度增压机

正如我们在前面几节中看到的，随机森林和极端树是高效的算法，两者都可以用最少的努力很好地工作。虽然 GBM 被认为是一种更精确的方法，但它并不容易使用，并且总是需要调整它的许多超参数才能获得最佳结果。另一方面，随机森林只需要考虑几个参数(主要是树的深度和树的数量)，就可以表现得相当好。另一个要注意的是过度训练。随机森林对过度训练的敏感度低于 GBM。所以，有了 GBM，我们还需要考虑正则化策略。最重要的是，随机森林更容易执行并行操作，因为 GBM 是顺序的，因此计算速度较慢。

在本章中，我们将在 Scikit-learn 中应用 GBM，看看名为 XGBoost 的下一代树提升算法，并在 H2O 上实现更大规模的提升。

我们在 Scikit-learn 和 H2O 中使用的 GBM 算法基于两个重要的概念:**加法展开**和通过**最速下降**算法的梯度优化。前者的一般思想是生成一系列相对简单的树(弱学习者)，其中每个连续的树都是沿着一个梯度添加的。让我们假设我们有 *M* 树来聚合集合中的最终预测。每次迭代 *fk* 中的树现在是模型中所有可能的树(![Gradient Boosting Machines](graphics/B05135_06_09.jpg))的更广阔空间的一部分(在 Scikit-learn 中，该参数更好地称为`n_estimators`):

![Gradient Boosting Machines](graphics/B05135_06_15.jpg)

加法扩展将以分阶段的方式向以前的树添加新树:

![Gradient Boosting Machines](graphics/B05135_06_10.jpg)

我们的梯度增强系综的预测只是所有先前的树和新添加的树![Gradient Boosting Machines](graphics/B05135_06_11.jpg)的预测的总和，更正式地导致以下结果:

![Gradient Boosting Machines](graphics/B05135_06_16.jpg)

GBM 算法的第二个重要但相当棘手的部分是通过**最速下降**进行梯度优化。这意味着我们在加法模型中加入了越来越强大的树。这是通过对新树应用梯度优化来实现的。由于没有像传统学习算法那样的参数，我们如何用树执行梯度更新？首先，我们参数化树；我们通过沿着梯度递归升级节点分割值来实现这一点，其中节点由向量表示。这样，最陡的下降方向是损失函数的负梯度，节点分裂将被升级和学习，导致:

*   ![Gradient Boosting Machines](graphics/B05135_06_12.jpg):收缩参数(在本文中也称为学习率)，随着更多树的加入，该参数将导致集成学习缓慢
*   ![Gradient Boosting Machines](graphics/B05135_06_13.jpg):梯度升级参数，也称为步长

因此，每片叶子的预测分数就是新树的最终分数，它就是每片叶子的总和:

![Gradient Boosting Machines](graphics/B05135_06_17.jpg)

因此，总结来说，GBM 的工作原理是逐渐增加沿着梯度学习的更精确的树。

现在我们已经理解了核心概念，让我们运行一个 GBM 示例，看看最重要的参数。对于 GBM 来说，这些参数特别重要，因为当我们将树的数量设置得太高时，我们必然会成倍地消耗计算资源。所以要小心这些参数。Scikit-learn 的 GBM 应用程序中的大多数参数与我们在上一段中介绍的随机森林中的参数相同。我们需要考虑三个需要特别注意的参数。

### 最大深度

与随机森林相反，随机森林在将树结构构建到最大扩展时表现更好(从而构建和集合具有高方差的预测因子)，GBM 倾向于更好地处理较小的树(从而利用具有较高偏差的预测因子，即弱学习者)。使用较小的决策树或仅使用树桩(只有一个分支的决策树)可以减少训练时间，在执行速度和较大偏差之间进行权衡(因为较小的树很难截取数据中更复杂的关系)。

### 学习率

也称为收缩![learning_rate](graphics/B05135_06_12.jpg)，这是一个与梯度下降优化和相关的参数，每个树将如何为集成做出贡献。该参数的较小值可以改善训练过程中的优化，尽管这将需要更多的估计器来收敛，从而需要更多的计算时间。由于它会影响集合中每棵树的权重，较小的值意味着每棵树对优化过程的贡献很小，在找到好的解决方案之前，您需要更多的树。因此，在为性能优化该参数时，我们应该避免可能导致次优模型的过大值；我们还必须避免使用太低的值，因为这将严重影响计算时间(集成需要更多的树来收敛到一个解)。根据我们的经验，一个好的起点是使用范围在< 0.1 和>001 之间的学习率。

### 子样本

让我们回忆一下装袋和粘贴的原理，我们随机抽取样本并在那些样本上构建树。如果我们对 GBM 应用子采样，我们会随机化树结构，防止过度拟合，减少内存负载，甚至有时会提高精度。我们也可以将这个过程应用于 GBM，使它更随机，从而利用打包的优势。我们可以通过将子样本参数设置为. 5 来随机化 GBM 中的树结构。

### 更快的 GBM，带热启动

该参数允许在每次迭代添加到前一次迭代后存储新的树信息，而不需要生成新的树。这样，我们可以节省内存，并大大加快计算时间。

使用 Scikit-learn 中提供的 GBM，我们可以采取两种措施来提高内存和 CPU 效率:

*   (半)增量学习的热启动
*   我们可以在交叉验证期间使用并行处理

让我们运行一个 GBM 分类示例，其中我们使用了来自 UCI 机器学习库的垃圾邮件数据集。我们将首先加载数据，对其进行预处理，并查看每个特征的可变重要性:

```
import pandas
import urllib2
import urllib2
from sklearn import ensemble
columnNames1_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names'
columnNames1 = [
   line.strip().split(':')[0]
   for line in urllib2.urlopen(columnNames1_url).readlines()[33:]]

columnNames1
n = 0
for i in columnNames1:
   columnNames1[n] = i.replace('word_freq_','')
   n += 1
print columnNames1

spamdata = pandas.read_csv(
   'https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data',
   header=None, names=(columnNames1 + ['spam'])
)

X = spamdata.values[:,:57]
y=spamdata['spam']

spamdata.head()

import numpy as np
from sklearn import cross_validation
from sklearn.metrics import classification_report
from sklearn.cross_validation import cross_val_score
from sklearn.cross_validation import cross_val_predict
from sklearn.cross_validation import train_test_split
from sklearn.metrics  import recall_score, f1_score
from sklearn.cross_validation import cross_val_predict
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.ensemble import GradientBoostingClassifier

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=22)

clf = ensemble.GradientBoostingClassifier(n_estimators=300,random_state=222,max_depth=16,learning_rate=.1,subsample=.5)
scores=clf.fit(X_train,y_train)
scores2 = cross_val_score(clf, X_train, y_train, cv=3, scoring='accuracy',n_jobs=-1)
print scores2.mean()

y_pred = cross_val_predict(clf, X_test, y_test, cv=10)
print 'validation accuracy %s' % accuracy_score(y_test, y_pred)

OUTPUT:]
validation accuracy 0.928312816799

confusionMatrix = confusion_matrix(y_test, y_pred)
print confusionMatrix

from sklearn.metrics import accuracy_score
accuracy_score(y_test, y_pred)

clf.feature_importances_

def featureImp_order(clf, X, k=5):
    return X[:,clf.feature_importances_.argsort()[::-1][:k]]
newX = featureImp_order(clf,X,2)
print newX

# let's order the features in amount of importance

print sorted(zip(map(lambda x: round(x, 4), clf.feature_importances_), columnNames1),
    reverse=True)
OUTPUT]

0.945030177548
    precision    recall  f1-score   support

    0       0.93      0.96      0.94       835
    1       0.93      0.88      0.91       546

avg / total       0.93      0.93      0.93      1381

[[799  36]
 [ 63 483]]

Feature importance:

[(0.2262, 'char_freq_;'), 
(0.0945, 'report'), 
(0.0637, 'capital_run_length_average'), 
(0.0467, 'you'), 
(0.0461, 'capital_run_length_total')
(0.0403, 'business')
(0.0397, 'char_freq_!')
(0.0333, 'will')
(0.0295, 'capital_run_length_longest')
(0.0275, 'your')
(0.0259, '000')
(0.0257, 'char_freq_(')
(0.0235, 'char_freq_$')
(0.0207, 'internet')
```

我们可以看到这个字符`;`在垃圾邮件的分类上是最有区别的。

### 注

可变重要性向我们展示了每个特征的拆分在多大程度上减少了树中所有拆分的相对杂质。

#### 暖机加速 warm _ start

遗憾的是，Scikit-learn 中没有针对 GBM 的并行处理。只有交叉验证和 gridsearch 可以并行化。那么，我们能做些什么来加快速度呢？我们看到 GBM 的工作原理是加法扩展，其中树是递增添加的。我们可以在 Scikit-learn 中用`warm-start`参数利用这个想法。我们可以用 Scikit-learn 的 GBM 功能对此进行建模，方法是使用方便的 for 循环逐步构建树模型。因此，让我们用相同的数据集来做这件事，并检查它提供的计算优势:

```
gbc = GradientBoostingClassifier(warm_start=True, learning_rate=.05, max_depth=20,random_state=0)
for n_estimators in range(1, 1500, 100):
    gbc.set_params(n_estimators=n_estimators)
    gbc.fit(X_train, y_train) 
y_pred = gbc.predict(X_test)
print(classification_report(y_test, y_pred))
print(gbc.set_params)
OUTPUT:
 precision    recall  f1-score   support

    0       0.93      0.95      0.94       835
    1       0.92      0.89      0.91       546

avg / total       0.93      0.93      0.93      1381

<bound method GradientBoostingClassifier.set_params of GradientBoostingClassifier(init=None, learning_rate=0.05, loss='deviance',
    max_depth=20, max_features=None, max_leaf_nodes=None,
    min_samples_leaf=1, min_samples_split=2,
    min_weight_fraction_leaf=0.0, n_estimators=1401,
    presort='auto', random_state=0, subsample=1.0, verbose=0,
    warm_start=True)>
```

建议特别注意树的设置输出(`n_estimators=1401`)。你可以看到我们用的树的大小是 1401。当我们将其与类似的 GBM 模型进行比较时，这个小技巧帮助我们减少了大量的训练时间(想想一半甚至更少)，我们会同时用 1401 棵树进行训练。请注意，我们可以将此方法用于随机森林和极端随机森林。然而，我们发现这对于 GBM 特别有用。

让我们看看显示常规 GBM 的训练时间和我们的`warm_start`方法的图。计算速度相当快，精度保持相对不变:

![Speeding up GBM with warm_start](graphics/B05135_06_18.jpg)

### 训练和存储 GBM 模型

有没有想过同时在三台电脑上训练一个模型？还是在 EC2 实例上训练 GBM 模型？可能会有这样一种情况:你训练一个模型，并希望存储它，以便以后再次使用。当你要等两天才能完成一轮完整的训练时，我们不想再次经历这个过程。在一个案例中，您已经在亚马逊 EC2 实例上的云中训练了一个模型，您可以存储该模型，并在以后使用 Scikit-learn 的`joblib`在另一台计算机上重用它。因此，让我们走完这个过程，因为 Scikit-learn 为我们提供了一个方便的工具来管理它。

让我们导入正确的库，并为文件位置设置目录:

```
import errno    
import os
#set your path here
path='/yourpath/clfs'
clfm=os.makedirs(path)
os.chdir(path)
```

现在让我们将模型导出到硬盘上的指定位置:

```
from sklearn.externals import joblib
joblib.dump( gbc,'clf_gbc.pkl')
```

现在，我们可以加载模型并将其重新用于其他目的:

```
model_clone = joblib.load('clf_gbc.pkl')
zpred=model_clone.predict(X_test)
print zpred
```

# XGBoost

我们刚刚讨论过，当使用 Scikit-learn 的 GBM 时，没有并行处理的选项，这正是 XGBoost 的用武之地。在 GBM 的基础上，XGBoost 引入了更具可扩展性的方法，在单台机器上利用多线程，在多台服务器的集群上利用并行处理(使用分片)。XGBoost 相对于 GBM 最重要的改进在于后者管理稀疏数据的能力。XGBoost 自动接受稀疏数据作为输入，而不在内存中存储零值。XGBoost 的第二个好处在于在分支树时计算最佳节点分割值的方式，这种方法被称为分位数草图。这种方法通过加权算法对数据进行转换，以便根据一定的准确度对候选拆分进行排序。更多信息请阅读 http://arxiv.org/pdf/1603.02754v3.pdf 的文章。

XGBoost 代表 Extreme Gradient Boosting，这是一种开源的梯度 Boosting 算法，在数据科学竞赛中获得了大量的人气，比如 Kaggle([https://www.kaggle.com/](https://www.kaggle.com/))和 KDD 杯 2015。(如我们在[第 1 章](01.html "Chapter 1. First Steps to Scalability")、*可扩展性第一步*中所述，代码可在 https://github.com/dmlc/XGBoost[的 GitHub 上获得。)正如](https://github.com/dmlc/XGBoost)作者(陈天奎、佟赫和卡洛斯·盖斯特林)在他们的算法 XGBoost 上写的论文中所报告的那样，在 2015 年在 Kaggle 上举行的 29 场挑战中，17 场获胜的解决方案将 XGBoost 作为独立的或多种模型的某种集成的一部分。在他们的论文 *XGBoost:一个可扩展的树促进系统*(可以在[http://learningsys.org/papers/LearningSys_2015_paper_32.pdf](http://learningsys.org/papers/LearningSys_2015_paper_32.pdf)找到)中，作者报告说，在最近的 2015 年 KDD 杯中，XGBoost 被每个进入比赛前十的队伍使用。除了在准确性和计算效率方面的成功表现之外，我们在本书中主要关注的是可伸缩性，从不同的角度来看，XGBoost 确实是一个可伸缩的解决方案。XGBoost 是新一代 GBM 算法，对初始树 boost GBM 算法进行了重要调整。XGBoost 提供并行处理；该算法提供的可伸缩性是由于其作者开发了许多新的调整和添加:

*   一种接受稀疏数据的算法，可以利用稀疏矩阵，节省内存(不需要密集矩阵)和计算时间(零值以特殊方式处理)
*   一种近似树学习(加权分位数草图)，具有类似的结果，但比经典的对可能的分支切割的完整探索花费的时间少得多
*   单台机器上的并行计算(在搜索最佳分割的阶段使用多线程)和多台机器上类似的分布式计算
*   利用名为“列块”的数据存储解决方案，在单台机器上进行核外计算，该解决方案按列在磁盘上排列数据，从而按照优化算法(适用于列向量)的预期，通过从磁盘中提取数据来节省时间

从实用的角度来看，XGBoost 的特性大多与 GBM 的参数相同。XGBoost 也很有能力处理丢失的数据。基于标准决策树的其他树集成要求首先使用一个非标度值(例如一个大的负数)来估算缺失数据，以便开发一个适当的树分支来处理缺失值。取而代之的是，XGBoost 首先拟合所有非缺失值，并在为变量创建分支后，决定哪个分支更适合缺失值，以最小化预测误差。这种方法导致树更紧凑，有效的插补策略导致更强的预测能力。

最重要的 XGBoost 参数如下:

*   `eta`(默认值=0.3):这相当于 Scikit-learn 的 GBM 中的学习速率
*   `min_child_weight`(默认值=1):较高的值可防止过拟合和树的复杂性
*   `max_depth`(默认值=6):这是树中的交互次数
*   `subsample`(默认值=1):这是我们在每次迭代中获取的训练数据样本的一小部分
*   `colsample_bytree`(默认值=1):这是每次迭代中特征的分数
*   `lambda`(默认值=1):这是L2 正则化(布尔型)
*   `seed`(默认值=0):这是Scikit-learn 的`random_state`参数的等价物，允许跨多个测试和不同机器的学习过程的再现性

现在我们知道了 XGBoost 最重要的参数，让我们在用于 GBM 的相同数据集上运行一个 XGBoost 示例，并使用相同的参数设置(尽可能多)。XGBoost 使用起来没有 Scikit-learn 包简单。因此，我们将提供一些基本示例，您可以将其用作更复杂模型的起点。在我们深入研究 XGBoost 应用程序之前，让我们将其与垃圾数据集中`sklearn`中的 GBM 方法进行比较；我们已经将数据加载到内存中:

```
import xgboost as xgb
import numpy as np
from sklearn.metrics import classification_report
from sklearn import cross_validation

clf = xgb.XGBClassifier(n_estimators=100,max_depth=8,
    learning_rate=.1,subsample=.5)

clf1 = GradientBoostingClassifier(n_estimators=100,max_depth=8,
    learning_rate=.1,subsample=.5)

%timeit xgm=clf.fit(X_train,y_train)
%timeit gbmf=clf1.fit(X_train,y_train)

y_pred = xgm.predict(X_test)
y_pred2 = gbmf.predict(X_test)

print 'XGBoost results %r' % (classification_report(y_test, y_pred))
print 'gbm results %r' % (classification_report(y_test, y_pred2))

OUTPUT:
1 loop, best of 3: 1.71 s per loop
1 loop, best of 3: 2.91 s per loop
XGBoost results '             precision    recall  f1-score   support\n\n          0       0.95      0.97      0.96       835\n          1       0.95      0.93      0.94       546\n\navg / total       0.95      0.95      0.95      1381\n'
gbm results '             precision    recall  f1-score   support\n\n          0       0.95      0.97      0.96       835\n          1       0.95      0.92      0.93       546\n\navg / total       0.95      0.95      0.95      1381\n
```

我们可以清楚地看到，尽管我们没有对 xboost 使用并行化，但 xboost 比 GBM 快得多(1.71s 对 2.91s)。后来，当我们应用内存外流时，当我们对 XGBoost 使用并行化和核外方法时，我们甚至可以达到更大的加速。在某些情况下，XGBoost 模型比 GBM 具有更高的精度，但是(几乎)从来没有相反的结果。

## XGBoost 回归

增强方法通常用于分类，但对于回归任务也非常有效。由于回归经常被忽略，让我们运行一个回归示例，并遍历关键问题。让我们用 gridsearch 在加州住宅区安装一个助推模型。加州房屋数据集最近被添加到 Scikit-learn 中，这为我们节省了一些预处理步骤:

```
import numpy as np
import scipy.sparse
import xgboost as xgb
import os
import pandas as pd
from sklearn.cross_validation import train_test_split
import numpy as np
from sklearn.datasets import fetch_california_housing
from sklearn.metrics import mean_squared_error
pd=fetch_california_housing()

#because the y  variable is highly skewed we apply the log transformation 
y=np.log(pd.target)
X_train, X_test, y_train, y_test = train_test_split(pd.data,
    y,
    test_size=0.15,
    random_state=111)
names = pd.feature_names
print names

import xgboost as xgb
from xgboost.sklearn import XGBClassifier
from sklearn.grid_search import GridSearchCV

clf=xgb.XGBRegressor(gamma=0,objective= "reg:linear",nthread=-1)

clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print 'score before gridsearch %r' % mean_squared_error(y_test, y_pred)

params = {
 'max_depth':[4,6,8],
 'n_estimators':[1000],
'min_child_weight':range(1,3),
'learning_rate':[.1,.01,.001],
'colsample_bytree':[.8,.9,1]
,'gamma':[0,1]}

#with the parameter nthread we specify XGBoost for parallelisation 
cvx = xgb.XGBRegressor(objective= "reg:linear",nthread=-1)
clf=GridSearchCV(estimator=cvx,param_grid=params,n_jobs=-1,scoring='mean_absolute_error',verbose=True)

clf.fit(X_train,y_train)
print clf.best_params_
y_pred = clf.predict(X_test)
print 'score after gridsearch %r' %mean_squared_error(y_test, y_pred)

#Your output might look a little different based on your hardware.

OUTPUT
['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']
score before gridsearch 0.07110580252173157
Fitting 3 folds for each of 108 candidates, totalling 324 fits
[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.9min
[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed: 11.3min
[Parallel(n_jobs=-1)]: Done 324 out of 324 | elapsed: 22.3min finished
{'colsample_bytree': 0.8, 'learning_rate': 0.1, 'min_child_weight': 1, 'n_estimators': 1000, 'max_depth': 8, 'gamma': 0}
score after gridsearch 0.049878294113796254
```

通过 gridsearch，我们已经能够大大提高我们的分数；你可以看到我们 gridsearch 的最佳参数。你可以在 sklearn 中看到它与常规助推方法的相似之处。但是，默认情况下，XGBoost 会在所有可用内核上并行化该算法。您可以通过将`n_estimators`参数增加到 2500 或 3000 左右来提高模型的性能。然而，我们发现，对于计算机功能较弱的读者来说，培训时间会有点太长。

### XGBoost 和可变重要性

XGBoost 有一些非常实用的内置功能来绘制变量的重要性。首先，相对于手头的模型，有一个方便的特征选择工具。您可能知道，可变重要性是基于树结构中每个特征的相对影响。它为特征选择和洞察预测模型的本质提供了实用的方法。让我们看看如何用 XGBoost 绘制重要性:

```
import numpy as np
import os
from matplotlib import pylab as plt
# %matplotlib inline   <- this only works in jupyter notebook

#our best parameter set 
# {'colsample_bytree': 1, 'learning_rate': 0.1, 'min_child_weight': 1, 'n_estimators': 500, #'max_depth': 8, 'gamma': 0}

params={'objective': "reg:linear",
        'eval_metric': 'rmse',
        'eta': 0.1,
       'max_depth':8,
       'min_samples_leaf':4,
        'subsample':.5,
        'gamma':0
       }

dm = xgb.DMatrix(X_train, label=y_train,
                 feature_names=names)
regbgb = xgb.train(params, dm, num_boost_round=100)
np.random.seed(1)
regbgb.get_fscore()

regbgb.feature_names
regbgb.get_fscore()
xgb.plot_importance(regbgb,color='magenta',title='california-housing|variable importance')
```

![XGBoost and variable importance](graphics/B05135_06_19.jpg)

应谨慎使用要素重要性(对于 GBM 和随机森林也是如此)。特征重要性度量标准完全基于用特定模型的参数训练的特定模型上构建的树结构。这意味着如果我们改变模型的参数，重要性度量和一些排名也会改变。因此，重要的是要注意，对于任何重要性度量，它们都不应该被认为是跨模型的通用变量结论。

## xboost 流式处理大型数据集

就准确性/性能权衡而言，这是最好的桌面解决方案。我们看到，在之前的随机森林示例中，我们需要执行二次采样，以防止我们的主内存过载。

XGBoost 的一个经常被忽视的功能是通过内存传输数据的方法。这个方法以分阶段的方式通过主内存解析数据，随后被解析成 XGBoost 模型训练。这种方法是在大数据集上训练模型的先决条件，而大数据集是不可能放入主存的。使用 XGBoost 进行流式处理仅适用于 LIBSVM 文件，这意味着我们首先必须将数据集解析为 LIBSVM 格式，并将其导入为 XGBoost 保留的内存缓存中。另外需要注意的是，我们使用不同的方法来实例化 XGBoost 模型。XGBoost 的类似 Scikit-learn 的接口只适用于常规的 NumPy 对象。让我们看看这是如何工作的。

首先，我们需要加载 LIBSVM 格式的数据集，并将其拆分为训练集和测试集，然后我们继续进行预处理和训练。不幸的是，使用这个 XGBoost 方法无法使用 gridsearch 进行参数调整。如果要调优参数，我们需要将 LIBSVM 文件转换成 Numpy 对象，这样会将数据从内存缓存转储到主内存。不幸的是，这是不可扩展的，因此如果您想要对大型数据集执行调优，我建议使用我们之前介绍的储层采样工具，并对子样本进行调优:

```
import urllib
from sklearn.datasets import dump_svmlight_file
from sklearn.datasets import load_svmlight_file
trainfile = urllib.URLopener()
trainfile.retrieve("http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/poker.bz2", "pokertrain.bz2")
X,y = load_svmlight_file('pokertrain.bz2')
dump_svmlight_file(X, y,'pokertrain', zero_based=True,query_id=None, multilabel=False)
testfile = urllib.URLopener()
testfile.retrieve("http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/poker.t.bz2", "pokertest.bz2")
X,y = load_svmlight_file('pokertest.bz2')
dump_svmlight_file(X, y,'pokertest', zero_based=True,query_id=None, multilabel=False)
del(X,y)
from sklearn.metrics import classification_report
import numpy as np
import xgboost as xgb
dtrain = xgb.DMatrix('/yourpath/pokertrain#dtrain.cache')
dtest = xgb.DMatrix('/yourpath/pokertest#dtestin.cache')

# For parallelisation it is better to instruct "nthread" to match the exact amount of cpu cores you want #to use.
param = {'max_depth':8,'objective':'multi:softmax','nthread':2,'num_class':10,'verbose':True}
num_round=100
watchlist = [(dtest,'eval'), (dtrain,'train')]
bst = xgb.train(param, dtrain, num_round,watchlist)
print bst
OUTPUT:
[89]    eval-merror:0.228659    train-merror:0.016913
[90]    eval-merror:0.228599    train-merror:0.015954
[91]    eval-merror:0.227671    train-merror:0.015354
[92]    eval-merror:0.227777    train-merror:0.014914
[93]    eval-merror:0.226247    train-merror:0.013355
[94]    eval-merror:0.225397    train-merror:0.012155
[95]    eval-merror:0.224070    train-merror:0.011875
[96]    eval-merror:0.222421    train-merror:0.010676
[97]    eval-merror:0.221881    train-merror:0.010116
[98]    eval-merror:0.221922    train-merror:0.009676
[99]    eval-merror:0.221733    train-merror:0.009316
```

我们真的可以从内存 XGBoost 中体验到巨大的加速。如果我们使用内存版本，我们将需要更多的训练时间。在本例中，我们已经将测试集作为一轮验证包含在*观察列表*中。然而，如果我们想预测未知数据的值，我们可以简单地使用与 Scikit-learn 和 XGBoost 中任何其他模型相同的预测程序:

```
bst.predict(dtest)
OUTPUT:
array([ 0.,  0.,  1., ...,  0.,  0.,  1.], dtype=float32)
```

## XGBoost 模型持久性

在前一章中，我们介绍了如何将 GBMmodel 存储到磁盘，以便以后导入并用于预测。XGBoost 提供同样的功能。让我们看看如何存储和导入模型:

```
import pickle
bst.save_model('xgb.model')
```

现在，您可以从之前指定的目录中导入保存的模型:

```
imported_model = xgb.Booster(model_file='xgb.model')
```

太好了，现在你可以用这个模型来预测:

```
imported_model.predict(dtest)
OUTPUT array([ 9.,  9.,  9., ...,  1.,  1.,  1.], dtype=float32)
```

# 带 H2O 的堆外推车

到目前为止，我们只处理 CART 机型的桌面解决方案。在[第 4 章](04.html "Chapter 4. Neural Networks and Deep Learning")、*神经网络和深度学习*中，我们介绍了 H2O 的深度记忆外学习，它提供了一种强大的可扩展方法。幸运的是，H2O 还利用其强大的并行 Hadoop 生态系统提供了树集成方法。由于我们在前面的章节中已经详细介绍了 GBM 和随机森林，让我们马上开始吧。在本练习中，我们将使用之前使用的垃圾邮件数据集。

## H2O 的随机森林和网格研究

让我们用 gridsearch 超参数优化实现一个随机森林。在本节中，我们首先从网址源加载垃圾邮件数据集:

```
import pandas as pd
import numpy as np
import os
import xlrd
import urllib
import h2o

#set your path here
os.chdir('/yourpath/')

url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data'
filename='spamdata.data'
urllib.urlretrieve(url, filename)
```

现在我们已经加载了数据，我们可以初始化 H2O 会话:

```
h2o.init(max_mem_size_GB = 2)

OUTPUT:
```

![Random forest and gridsearch on H2O](graphics/B05135_06_20.jpg)

这里，我们对数据进行预处理，将数据分为训练集、验证集和测试集。我们用 H2O 函数(`.split_frame`)来做这件事。还要注意我们将目标向量`C58`转换为因子变量的重要步骤:

```
spamdata = h2o.import_file(os.path.realpath("/yourpath/"))
spamdata['C58']=spamdata['C58'].asfactor()
train, valid, test= spamdata.split_frame([0.6,.2], seed=1234)
spam_X = spamdata.col_names[:-1]    
spam_Y = spamdata.col_names[-1]
```

在这一部分，我们将设置我们将使用 gridsearch 优化的参数。首先，我们将模型中棵树的数量设置为单个值 300。使用 gridsearch 迭代的参数如下:

*   `max_depth`:树的最大深度
*   `balance_classes`:每一次迭代都为目标结果使用平衡类
*   `sample_rate`:这是每次迭代采样的行数的分数

现在，让我们将这些参数传递到 Python 列表中，以便在我们的 H2O 网格搜索模型中使用:

```
hyper_parameters={'ntrees':[300], 'max_depth':[3,6,10,12,50],'balance_classes':['True','False'],'sample_rate':[.5,.6,.8,.9]}
grid_search = H2OGridSearch(H2ORandomForestEstimator, hyper_params=hyper_parameters)
grid_search.train(x=spam_X, y=spam_Y,training_frame=train)
print 'this is the optimum solution for hyper parameters search %s' % grid_search.show()
OUTPUT:
```

![Random forest and gridsearch on H2O](graphics/B05135_06_21.jpg)

在所有可能的组合中，行采样率为. 9、树深度为 50、平衡类的模型产生最高的精度。现在，让我们训练一个新的随机森林模型，该模型具有我们的网格搜索得到的最佳参数，并预测测试集的结果:

```
final = H2ORandomForestEstimator(ntrees=300, max_depth=50,balance_classes=True,sample_rate=.9)
final.train(x=spam_X, y=spam_Y,training_frame=train)
print final.predict(test)
```

H2O预测的最终输出产生一个数组，第一列包含实际预测的类别，第二列包含每个目标标签的类别概率:

```
OUTPUT:
```

![Random forest and gridsearch on H2O](graphics/B05135_06_22.jpg)

## H2O 随机梯度推进与网格研究

我们在前面的例子中已经看到，大多数情况下，一个调整良好的 GBM 模型比随机的森林表现更好。所以现在让我们在 H2O 进行一次 T2 网格搜索，看看我们是否能提高 T3 的分数。在本次会议中，我们将介绍与 H2O 随机森林模型相同的随机二次抽样方法。基于 Jerome Friedman 1999 年的文章([https://statweb.stanford.edu/~jhf/ftp/stobst.pdf](https://statweb.stanford.edu/~jhf/ftp/stobst.pdf))，介绍了一种名为**随机梯度推进**的方法。添加到模型中的这种随机性利用了随机二次抽样，而无需在每次树迭代时从数据中进行替换，这被认为是为了防止过度拟合并提高整体精度。在这个例子中，我们通过在每次迭代中引入基于特征的随机子采样，进一步利用了随机性的思想。

这种随机子采样特征的方法也被称为**随机子空间方法**，我们已经在本章的*随机森林和极随机森林*部分看到过。我们通过`col_sample_rate`参数实现这一点。总而言之，在这个 GBM 模型中，我们将对以下参数执行 gridsearch 优化:

*   `max_depth`:最大树深
*   `sample_rate`:每次迭代使用的行数的分数
*   `col_sample_rate`:每次迭代使用的特征的分数

我们使用与上一节完全相同的垃圾邮件数据集，因此我们可以直接进入该数据集:

```
hyper_parameters={'ntrees':[300],'max_depth':[12,30,50],'sample_rate':[.5,.7,1],'col_sample_rate':[.9,1],
'learn_rate':[.01,.1,.3],}
grid_search = H2OGridSearch(H2OGradientBoostingEstimator, hyper_params=hyper_parameters)
grid_search.train(x=spam_X, y=spam_Y, training_frame=train)
print 'this is the optimum solution for hyper parameters search %s' % grid_search.show()
```

![Stochastic gradient boosting and gridsearch on H2O](graphics/B05135_06_23.jpg)

```
gbm Grid Build Progress: [##################################################] 100%
```

gridsearch 输出的上半部分显示，我们应该使用异常高的. 3 学习率、0 . 9 的列采样率和 30 的最大树深度。基于行的随机子采样没有提高性能，但是基于分数为. 9 的特征的子采样在这种情况下非常有效。现在让我们训练一个新的 GBM 模型，其最佳参数来自我们的网格搜索优化，并预测测试集的结果:

```
spam_gbm2 = H2OGradientBoostingEstimator(
  ntrees=300,
  learn_rate=0.3,
  max_depth=30,
  sample_rate=1,
  col_sample_rate=0.9,
  score_each_iteration=True,
  seed=2000000
)
spam_gbm2.train(spam_X, spam_Y, training_frame=train, validation_frame=valid)

confusion_matrix = spam_gbm2.confusion_matrix(metrics="accuracy")
print confusion_matrix
OUTPUT:
```

![Stochastic gradient boosting and gridsearch on H2O](graphics/B05135_06_24.jpg)

这为模型的性能提供了有趣的诊断，例如`accuracy`、`rmse`、`logloss`和`AUC`。但是，它的输出太大，这里不包括。查看 IPython 笔记本的输出，了解完整的输出。

您可以通过以下方式利用这一点:

```
print spam_gbm2.score_history()
```

当然，最终的预测可以通过以下方式实现:

```
print spam_gbm2.predict(test)
```

太好了，我们已经能够将模型的准确性提高到接近 100%。如您所见，在 H2O，您在建模和管理数据方面可能不太灵活，但是的处理速度和的准确性是无与伦比的。要结束此会话，您可以执行以下操作:

```
h2o.shutdown(prompt=False)
```

# 总结

我们看到，使用集成例程训练的 CART 方法在预测准确性方面非常强大。然而，它们在计算上可能很昂贵，我们已经在 sklearn 的应用程序中介绍了一些加速它们的技术。我们注意到，如果使用得当，使用随机搜索调整的极端随机森林可以加快 10 倍的速度。然而，对于 GBM 来说，sklearn 中没有实现并行化，而这正是 XGBoost 的用武之地。

XGBoost 附带了一个有效的并行增强算法，可以很好地加速算法。当我们使用更大的文件(> 100k 个训练示例)时，有一种核心外的方法可以确保我们在训练模型时不会过载主内存。

速度和记忆力最大的进步可以从 H2O 身上找到；我们看到了强大的调优能力以及令人印象深刻的训练速度。