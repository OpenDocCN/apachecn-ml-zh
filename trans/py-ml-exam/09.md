# nine

# 用文本分析技术挖掘 20 个新闻组数据集

在前几章中，我们介绍了一系列基本的机器学习概念和监督学习算法。从这一章开始，作为我们学习旅程的第二步，我们将详细介绍几种重要的无监督学习算法和技术。为了让我们的旅程更有趣，我们将从一个**自然语言处理** ( **NLP** )问题开始——探索新闻组数据。您将获得处理文本数据的实践经验，尤其是如何将单词和短语转换为机器可读的值，以及如何清理意义不大的单词。我们还将以无监督的学习方式将文本数据映射到二维空间中，从而实现文本数据的可视化。

我们将详细讨论以下每个主题:

*   自然语言处理基础和应用
*   浏览 Python NLP 库
*   标记化、词干化和引理化
*   获取和浏览新闻组数据
*   使用`seaborn`和`matplotlib`进行数据可视化
*   **包字** ( **弓**)模型
*   文本预处理
*   降维
*   用于文本可视化的 SNE 和 SNE

# 计算机如何理解语言——自然语言处理

在*第一章*、*机器学习入门和 Python* 中，我提到机器学习驱动的程序或计算机擅长通过处理和处理数据来发现事件模式。当数据结构良好或定义良好时，例如在微软 Excel 电子表格表或关系数据库表中，很明显为什么机器学习比人类更擅长处理它。电脑读取这类数据的方式和人类一样，比如`revenue: 5,000,000`为收入 500 万，`age: 30`为年龄 30；然后，计算机处理各种数据并以比人类更快的方式产生见解。然而，当数据是非结构化的，例如人类交流的单词、新闻文章或某人的法语演讲时，计算机似乎无法像人类那样理解单词(目前还不能)。

## 什么是 NLP？

世界上有很多关于单词或原始文本的信息，或者广义地说，有很多关于自然语言的信息。这个指的是人类用来相互交流的任何语言。自然语言可以采取各种形式，包括但不限于以下形式:

*   文本，如网页、短信、电子邮件和菜单
*   音频，如语音和 Siri 命令
*   标志和手势
*   许多其他形式，如歌曲、乐谱和莫尔斯电码

列表是无穷无尽的，我们无时无刻都被自然语言包围着(没错，就在你读这本书的时候)。考虑到这种非结构化数据(自然语言数据)的重要性，我们必须有方法让计算机理解自然语言并进行推理，并从中提取数据。配备 NLP 贯穿技术的程序已经可以在某些领域做很多事情，这已经看起来很神奇了！

自然语言处理是机器学习的一个重要分支，处理机器(计算机)和人类(自然)语言之间的交互。自然语言处理任务的数据可以是不同的形式，例如，来自社交媒体帖子、网页、甚至医疗处方的文本，或者来自语音邮件的音频、控制系统的命令，甚至是最喜欢的歌曲或电影。如今，自然语言处理广泛地涉及到我们的日常生活:没有机器翻译，我们就无法生存；自动生成天气预报脚本；我们发现语音搜索很方便；我们很快得到一个问题的答案(比如加拿大的人口是多少)得益于智能问答系统；语音转文本技术帮助有特殊需求的人。

## 自然语言处理的历史

如果机器能够像人类一样理解语言，我们认为它们是智能的。1950 年，著名数学家艾伦·图灵在一篇文章*中提出了计算机器和智能*，将测试作为机器智能的一个标准。现在的叫做**图灵测试**([https://plato.stanford.edu/entries/turing-test/](https://plato.stanford.edu/entries/turing-test/))，它的目标是检验一台计算机是否能够充分理解语言，从而愚弄人类，让他们认为这台机器是另一个人。到目前为止，没有一台计算机通过图灵测试，这可能对你来说并不奇怪，但 20 世纪 50 年代被认为是自然语言处理历史的开端。

理解语言可能很难，但自动将文本从一种语言翻译成另一种语言会更容易吗？在我的第一门编程课程中，实验室手册中有粗粒度机器翻译的算法。这种类型的翻译包括查字典和用新语言生成文本。一个更实际可行的方法是收集已经被人类翻译的文本，并在这些文本上训练一个计算机程序。1954 年，在乔治城 IBM 实验中([https://en . Wikipedia . org/wiki/George town % E2 % 80% 93 IBM _ experience](https://en.wikipedia.org/wiki/Georgetown%E2%80%93IBM_experiment))，科学家声称机器翻译将在三到五年内得到解决。不幸的是，一个能够击败人类专家翻译的机器翻译系统还不存在。但是自从引入深度学习以来，机器翻译已经有了很大的发展，并在某些领域取得了令人难以置信的成就，例如，社交媒体(脸书开源的神经机器翻译系统、[https://ai.facebook.com/tools/translate/](https://ai.facebook.com/tools/translate/))、实时对话(Skype、SwiftKey Keyboard 和谷歌 Pixel Buds)以及基于图像的翻译，如谷歌翻译。

会话代理是自然语言处理中的另一个热门话题。计算机能够与我们对话这一事实已经改变了商业运作的方式。2016 年，**微软的 AI chatbot** 、**Tay**([https://blogs . Microsoft . com/blog/2016/03/25/learning-tays-introduction/](https://blogs.microsoft.com/blog/2016/03/25/learning-tays-introduction/))被释放出来模仿一个十几岁的小女孩，在推特上与用户实时对话。她从用户在推特上发布和评论的所有东西中学会了如何说话。然而，她被巨魔的推文淹没了，并自动了解到他们的不良行为，并开始在她的订阅源上输出不合适的东西。她最终在 24 小时内被解雇了。

## 自然语言处理应用

还有几个任务试图以计算机程序更容易操作的方式组织知识和概念。我们组织和表示概念的方式被称为**本体论**。本体定义概念和概念之间的关系。比如我们可以有一个所谓的三重，比如(“python”、“语言”、“is-a”)代表两个概念之间的关系，比如 *Python 是一种语言*。

与前面的案例相比，自然语言处理的一个重要用例是**词性标注** ( **词性标注** ) **标注**。PoS 是一个语法词类，如名词或动词。词性标注试图为句子或较大文档中的每个单词确定合适的标签。下表给出了英语 PoS 的示例:

<colgroup><col> <col></colgroup> 
| 词性 | 例子 |
| 名词 | 大卫，机器 |
| 代词 | 他们，她 |
| 形容词 | 太棒了，太棒了 |
| 动词 | 读，写 |
| 副词 | 非常非常 |
| 介词 | 出去，在 |
| 结合 | 但是 |
| 感叹词 | 唷，哎呀 |
| 文章 | 一个 |

表 9.1: PoS 示例

有多种真实世界的自然语言处理应用程序涉及监督学习，例如前面提到的 PoS 标签。一个典型的例子是识别新闻情绪，在二元情况下可以是积极的或消极的，在多类分类中可以是积极的、中立的或消极的。新闻情绪分析为股市交易提供了一个重要的信号。

我们很容易想到的另一个例子是新闻主题分类，其中类可能是互斥的，也可能不是互斥的。在我们刚刚讨论的新闻组示例中，类是互斥的(尽管略有重叠)，例如技术、体育和宗教。然而，意识到一篇新闻文章可以偶尔被分配多个类别(多标签分类)是很好的。例如，一篇关于奥运会的文章，如果有意想不到的政治参与，可能会被贴上体育和政治的标签。

最后，一个可能意想不到的有趣的应用是**命名实体识别** ( **NER** )。命名实体是确定类别的短语，如人名、公司名、地理位置、日期和时间、数量和货币价值。NER 是信息提取的重要子任务，以寻找和识别这样的实体。例如，我们可以用下面的一句话来引导 NER:由著名科技企业家埃隆·马斯克(Elon Musk)[Person]创建的总部位于加州[Location]的公司 SpaceX[Organization]宣布，将在 2020 年[Date]制造下一代 9[Quantum]-米直径运载火箭和宇宙飞船，用于首次轨道飞行。

在下一章中，我们将讨论如何将无监督学习(包括聚类和主题建模)应用于文本数据。我们将在本章接下来的章节中从介绍 NLP 基础知识开始。

# 参观流行的自然语言处理库，学习自然语言处理基础知识

现在，我们已经介绍了自然语言处理的真实应用程序的一个简短列表，我们将浏览 Python 自然语言处理库的基本堆栈。如前所述，这些包处理各种各样的自然语言处理任务，包括情感分析、文本分类和 NER。

## 安装著名的 NLP 库

Python 中最著名的NLP库包括**自然语言工具包**(**NLTK**)**spaCy****Gensim**和 **TextBlob。**sci kit-learn库也有令人印象深刻的 NLP 相关特性。让我们更详细地看看它们:

*   `nltk`: This library ([http://www.nltk.org/](http://www.nltk.org/)) was originally developed for educational purposes and is now being widely used in industry as well. It is said that you can't talk about NLP without mentioning NLTK. It is one of the most famous and leading platforms for building Python-based NLP applications. You can install it simply by running the following command line in the terminal:

    ```
    sudo pip install -U nltk 
    ```

    如果您正在使用`conda`，请执行以下命令行:

    ```
    conda install nltk 
    ```

*   `spaCy`: This library ([https://spacy.io/](https://spacy.io/)) is a more powerful toolkit in the industry than NLTK. This is mainly for two reasons: one, `spaCy` is written in Cython, which is much more memory-optimized (now you can see where the `Cy` in `spaCy` comes from) and excels in NLP tasks; second, `spaCy` uses state-of-the-art algorithms for core NLP problems, such as **convolutional neural network** (**CNN**) models for tagging and NER. However, it could seem advanced for beginners. In case you're interested, here are the installation instructions.

    在终端中运行以下命令行:

    ```
    pip install -U spacy 
    ```

    对于`conda`，执行以下命令行:

    ```
    conda install -c conda-forge spacy 
    ```

*   `Gensim`: This library ([https://radimrehurek.com/gensim/](https://radimrehurek.com/gensim/)), developed by Radim Rehurek, has been gaining popularity over recent years. It was initially designed in 2008 to generate a list of similar articles given an article, hence the name of this library (`generate similar`—> `Gensim`). It was later drastically improved by Radim Rehurek in terms of its efficiency and scalability. Again, you can easily install it via `pip` by running the following command line:

    ```
    pip install --upgrade gensim 
    ```

    在`conda`的情况下，可以在终端执行以下命令行:

    ```
    conda install -c conda-forge gensim 
    ```

您应该确保依赖项 NumPy 和 SciPy 在`gensim`之前已经安装。

*   `TextBlob`: This library ([https://textblob.readthedocs.io/en/dev/](https://textblob.readthedocs.io/en/dev/)) is a relatively new one built on top of NLTK. It simplifies NLP and text analysis with easy-to-use built-in functions and methods, as well as wrappers around common tasks. We can install `TextBlob` by running the following command line in the terminal:

    ```
    pip install -U textblob 
    ```

    `TextBlob`有一些 NLTK(目前)没有的有用功能，比如拼写检查和纠正、语言检测和翻译。

## 全集

截至 2018 年，NLTK 拥有超过 100 个大型且结构良好的文本数据集集合，在 NLP 中被称为**语料库**。语料库可以用作检查单词出现的词典，也可以用作模型学习和验证的训练池。一些有用的和有趣的语料库包括网络文本语料库、推特样本、莎士比亚语料库、情感极性、姓名语料库(它包含流行姓名的列表，我们将很快探索)、WordNet 和路透社基准语料库。完整的名单可以在[http://www.nltk.org/nltk_data](http://www.nltk.org/nltk_data)找到。在使用这些语料库资源之前，我们需要首先通过在 Python 解释器中运行以下代码来下载它们:

```
>>> import nltk
>>> nltk.download() 
```

将弹出一个新窗口，询问您要下载哪些集合(下图中的**集合**选项卡)或语料库(下图中的**语料库**选项卡)，以及数据保存在哪里:

<figure class="mediaobject">![](../Images/B16326_09_01.png)</figure>

图 9.1:NLTK 安装中的集合选项卡

安装整个热门软件包是快速的解决方案，因为它包含了您当前学习和未来研究所需的所有重要语料库。安装一个特定的语料库，如下面的截图所示，也是可以的:

<figure class="mediaobject">![](../Images/B16326_09_02.png)</figure>

图 9.2:NLTK 安装中的“语料库”选项卡

一旦你想探索的包或语料库安装好了，你现在可以看一下**人名**语料库(确保`names`语料库安装好了)。

首先，导入`names`语料库:

```
>>> from nltk.corpus import names 
```

我们可以查看列表中的第一个`10`名称:

```
>>> print(names.words()[:10])
['Abagael', 'Abagail', 'Abbe', 'Abbey', 'Abbi', 'Abbie',
'Abby', 'Abigael', 'Abigail', 'Abigale'] 
```

总共有`7944`个名称，如通过执行以下命令导出的以下输出所示:

```
>>> print(len(names.words()))
7944 
```

探索其他语料库也很有趣。

除了易于使用和丰富的语料库，更重要的是，NLTK 还擅长许多自然语言处理和文本分析任务，包括标记化、词性标注、NER、词干和词条化。

## 标记化

给定一个文本序列，**标记化**是将它分成片段的任务，这些片段可以是单词、字符或句子。某些字符通常会被删除，如标点符号、数字和表情符号。剩下的碎片就是用于进一步加工的所谓**代币**。此外，由一个单词组成的标记在计算语言学中也被称为单个单词**；**二元模型**是由两个连续的单词组成的；**三个连续字的三元组**；以及 n 个连续单词的**n-克**。以下是令牌化的一个示例:**

<figure class="mediaobject">![](../Images/B16326_09_03.png)</figure>

图 9.3:令牌化示例

我们可以使用 NLTK 中的`word_tokenize`函数实现基于单词的标记化。我们将使用输入文本`'''I am reading a book.`，在下一行中，以`It is Python Machine Learning By Example,`，然后是`3rd edition.'''`为例，如下命令所示:

```
>>> from nltk.tokenize import word_tokenize
>>> sent = '''I am reading a book.
...           It is Python Machine Learning By Example,
...           3rd edition.'''
>>> print(word_tokenize(sent))
['I', 'am', 'reading', 'a', 'book', '.', 'It', 'is', 'Python', 'Machine', 'Learning', 'By', 'Example', ',', '3rd', 'edition', '.'] 
```

获得单词标记。

`word_tokenize`功能保留标点符号和数字，只丢弃空格和换行符。

你可能会认为单词标记化就是简单地用空格和标点符号分割一个句子。这里有一个有趣的例子，表明标记化比您想象的要复杂:

```
>>> sent2 = 'I have been to U.K. and U.S.A.'
>>> print(word_tokenize(sent2))
['I', 'have', 'been', 'to', 'U.K.', 'and', 'U.S.A', '.'] 
```

例如，标记器准确地将单词`'U.K.'`和`'U.S.A'`识别为标记，而不是后面跟有`'K'`的`'U'`和`'.'`。

`spaCy`还有一个突出的令牌化特性。它使用一个经过精确训练的模型，该模型会不断更新。要安装它，我们可以运行以下命令:

```
python -m spacy download en_core_web_sm 
```

然后，我们将加载`en_core_web_sm`模型，并使用该模型解析句子:

```
>>> import spacy
>>> nlp = spacy.load('en_core_web_sm')
>>> tokens2 = nlp(sent2)
>>> print([token.text for token in tokens2])
['I', 'have', 'been', 'to', 'U.K.', 'and', 'U.S.A.'] 
```

我们也可以根据句子分割文本。例如，在相同的输入文本上，使用 NLTK 的`sent_tokenize`函数，我们有以下命令:

```
>>> from nltk.tokenize import sent_tokenize
>>> print(sent_tokenize(sent))
['I am reading a book.', 
'It's Python Machine Learning By Example,\n          3nd edition.'] 
```

返回两个基于句子的标记，因为输入文本中有两个句子。

## PoS 标签

我们可以应用一个来自 NLTK 的现成标记器或者组合多个标记器来定制标记过程。直接使用内置的标签功能`pos_tag`很容易，例如在`pos_tag(input_tokens)`中。但在幕后，它实际上是来自预先构建的监督学习模型的预测。该模型基于由正确标记的单词组成的大型语料库进行训练。

重用前面的例子，我们可以如下执行 PoS 标记:

```
>>> import nltk
>>> tokens = word_tokenize(sent)
>>> print(nltk.pos_tag(tokens))
[('I', 'PRP'), ('am', 'VBP'), ('reading', 'VBG'), ('a', 'DT'), ('book', 'NN'), ('.', '.'), ('It', 'PRP'), ('is', 'VBZ'), ('Python', 'NNP'), ('Machine', 'NNP'), ('Learning', 'NNP'), ('By', 'IN'), ('Example', 'NNP'), (',', ','), ('2nd', 'CD'), ('edition', 'NN'), ('.', '.')] 
```

返回每个令牌后面的 PoS 标签。我们可以使用`help`功能检查标签的含义。例如，查找`PRP`和`VBP`，给出如下输出:

```
>>> nltk.help.upenn_tagset('PRP')
PRP: pronoun, personal
   hers herself him himself hisself it itself me myself one oneself ours ourselves ownself self she thee theirs them themselves they thou thy us
>>> nltk.help.upenn_tagset('VBP')
VBP: verb, present tense, not 3rd person singular
   predominate wrap resort sue twist spill cure lengthen brush terminate appear tend stray glisten obtain comprise detest tease attract emphasize mold postpone sever return wag ... 
```

在 s `paCy`中，获取 PoS 标签也很容易。从输入句子中解析出的`token`对象有一个名为`pos_`的属性，这就是我们要找的标签。让我们打印每个令牌的`pos_`，如下所示:

```
>>> print([(token.text, token.pos_) for token in tokens2])
[('I', 'PRON'), ('have', 'VERB'), ('been', 'VERB'), ('to', 'ADP'), ('U.K.', 'PROPN'), ('and', 'CCONJ'), ('U.S.A.', 'PROPN')] 
```

我们刚刚玩转了带有 NLP 包的 PoS 标签。NER 怎么样？让我们在下一节看到。

## NER

给定一个文本序列，NER 的任务是定位和识别具有确定类别的单词或短语，例如人名、公司名、地点名和日期名。让我们来看一个 NER 使用`spaCy`的例子。

首先，像往常一样标记一个输入句子`The book written by Hayden Liu in 2020 was sold at $30 in America`，如下命令所示:

```
>>> tokens3 = nlp('The book written by Hayden Liu in 2020 was sold at $30 in America') 
```

生成的`token`对象包含一个名为`ents`的属性，这是命名实体。我们可以提取每个已识别命名实体的标签，如下所示:

```
>>> print([(token_ent.text, token_ent.label_) for token_ent in tokens3.ents])
[('Hayden Liu', 'PERSON'), ('2018', 'DATE'), ('30', 'MONEY'), ('America', 'GPE')] 
```

从结果可以看出`Hayden Liu`为`PERSON`，`2018`为`DATE`，`30`为`MONEY`，`America`为`GPE`(国家)。有关命名实体标签的完整列表，请参考[https://spacy.io/api/annotation#section-named-entities](https://spacy.io/api/annotation#section-named-entities)。

## 词干和引理化

词干**词干**是一个过程，将一个屈折变化的或派生的单词恢复到它的词根形式。比如*机器*是*机器*的梗，*学习*和*学习*是从*学习*作为它们的梗中产生的。

单词**引理**是词干的谨慎版本。在进行词干分析时，它会考虑单词的词性。此外，它可以追溯到这个词的引理。我们将很快更详细地讨论这两种文本预处理技术，词干和引理化。现在，让我们通过执行以下步骤来快速了解一下它们是如何分别在 NLTK 中实现的:

1.  导入`porter`作为三个内置词干算法之一(`LancasterStemmer`和`SnowballStemmer`是另外两个)并初始化词干器如下:

    ```
    >>> from nltk.stem.porter import PorterStemmer
    >>> porter_stemmer = PorterStemmer() 
    ```

2.  Stem `machines` and `learning`, as shown in the following codes:

    ```
    >>> porter_stemmer.stem('machines')
    'machin'
    >>> porter_stemmer.stem('learning')
    'learn' 
    ```

    如有必要，词干有时会涉及到字母的切分，如您在前面的命令输出中的`machin` 中所见。

3.  Now, import a lemmatization algorithm based on the built-in WordNet corpus and initialize a `lemmatizer`:

    ```
    >>> from nltk.stem import WordNetLemmatizer
    >>> lemmatizer = WordNetLemmatizer() 
    ```

    类似于词干，我们引理`machines`，和`learning`:

    ```
    >>> lemmatizer.lemmatize('machines')
    'machine'
    >>> lemmatizer.lemmatize('learning')
    'learning' 
    ```

`learning`为什么不变？原来这个算法默认只对名词进行引理。

## 语义和主题建模

Gensim 以其强大的语义和主题建模算法而闻名。主题建模是一个典型的文本挖掘任务，发现文档中隐藏的语义结构。简单英语中的语义结构是单词出现的分布。这显然是一个无监督的学习任务。我们需要做的是输入纯文本，让模型找出抽象的主题。我们将在*第 10 章*、*中详细研究主题建模，通过聚类和主题建模*发现新闻组数据集中的底层主题。

除了健壮的语义建模方法，gensim 还提供以下功能:

*   **单词嵌入**:又称为**单词矢量化**，这是一种在保留单词共现特征的同时表示单词的创新方式。我们将在*第 11 章*、*机器学习最佳实践*中详细学习单词嵌入。
*   **相似性查询**:该功能检索与给定查询对象相似的对象。这是一个建立在单词嵌入之上的功能。
*   **分布式计算**:这个功能使得从数百万个文档中高效学习成为可能。

最后但同样重要的是，正如第一章*中提到的，开始使用机器学习和 Python* ，scikit-learn 是我们在整本书中使用的主要包。幸运的是，它提供了我们需要的所有文本处理功能，如标记化，以及全面的机器学习功能。此外，它还带有一个内置的 20 个新闻组数据集加载器。

现在工具可用并且安装正确，那么数据呢？

# 获取新闻组数据

本章中的项目是关于 20 个新闻组数据集。顾名思义，它由来自新闻组文章的文本组成。它最初是由 Ken Lang 收集的，现在已经被广泛用于机器学习技术，特别是自然语言处理技术的文本应用实验。

该数据包含 20 个在线新闻组中的大约 20，000 个文档。新闻组是互联网上人们可以就某个话题提问和回答问题的地方。数据已经被清理到一定程度，并且已经被分成训练集和测试集。截止点在某个日期。

原始数据来自[http://qwone.com/~jason/20Newsgroups/](http://qwone.com/~jason/20Newsgroups/)，列出了 20 个不同的主题，如下所示:

*   `comp.graphics`
*   `comp.os.ms-windows.misc`
*   `comp.sys.ibm.pc.hardware`
*   `comp.sys.mac.hardware`
*   `comp.windows.x`
*   `rec.autos`
*   `rec.motorcycles`
*   `rec.sport.baseball`
*   `rec.sport.hockey`
*   `sci.crypt`
*   `sci.electronics`
*   `sci.med`
*   `sci.space`
*   `misc.forsale`
*   `talk.politics.misc`
*   `talk.politics.guns`
*   `talk.politics.mideast`
*   `talk.religion.misc`
*   `alt.atheism`
*   `soc.religion.christian`

数据集中的所有文档都是英文的。我们可以很容易地从新闻组的名称中推断出主题。

数据集被标记，每个文档由文本数据和组标签组成。这也使得它非常适合有监督的学习，例如文本分类。在这一章的最后，用你在这本书里学到的知识在这个数据集上自由练习分类。

有些新闻组关系密切甚至重叠，例如那五个计算机组(`comp.graphics`、`comp.os.ms-windows.misc`、`comp.sys.ibm.pc.hardware`、`comp.sys.mac.hardware`和`comp.windows.x`)，而有些则彼此关系不密切，如基督教(`soc.religion.christian`)和棒球(`rec.sport.baseball`)。

因此，它是无监督学习(如聚类)的完美用例，通过它我们可以看到相似的主题是否被分组在一起，不相关的主题是否相距甚远。此外，我们甚至可以使用主题建模技术发现原始 20 个标签之外的抽象主题。

现在，让我们专注于探索和分析文本数据。我们将从获取数据开始。

可以从原始网站或许多其他在线存储库中手动下载数据集。然而，数据集也有许多版本——有些是以某种方式清理的，有些是原始形式的。为了避免混淆，最好使用一致的采集方法。scikit-learn 库提供了一个加载数据集的实用函数。一旦数据集被下载，它就会被自动缓存。我们不需要两次下载同一个数据集。

在大多数情况下，缓存数据集，尤其是对于相对较小的数据集，被认为是一种好的做法。其他 Python 库也提供数据下载实用程序，但并非所有库都实现了自动缓存。这是我们热爱 scikit-learn 的另一个原因。

和往常一样，我们首先导入 20 个新闻组数据的加载器函数，如下所示:

```
>>> from sklearn.datasets import fetch_20newsgroups 
```

然后，我们下载带有所有默认参数的数据集，如下所示:

```
>>> groups = fetch_20newsgroups()
Downloading 20news dataset. This may take a few minutes.
Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB) 
```

我们还可以指定一个或多个特定的主题组和特定的部分(培训、测试或两者都有)，并在程序中加载这样的数据子集。下表总结了装载机功能的参数和选项的完整列表:

<colgroup><col> <col> <col> <col></colgroup> 
| 参数 | 缺省值 | 示例值 | 描述 |
| `subset` | “火车” | “训练”、“测试”、“全部” | 要加载的数据集:训练集和/或测试集。 |
| `data_home` | `~/scikit_learn_data` | `~/myfolder` | 存储和缓存文件的目录 |
| `categories` | 没有人 | [' `sci.space`"，`alt.atheism` '] | 要加载的新闻组列表。如果没有，将加载所有新闻组。 |
| `shuffle` | 真实的 | 真，假 | 布尔值，指示是否对数据进行无序播放 |
| `random_state` | forty-two | 7, 43 | 用于洗牌数据的随机种子整数 |
| `remove` | Zero | (“页眉”、“页脚”、“引号”) | 元组，指示每个新闻组帖子的页眉、页脚和引号中要省略的部分。默认情况下不会删除任何内容。 |
| `download_if_missing` | 真实的 | 真，假 | 布尔值，指示如果在本地找不到数据，是否下载数据 |

表 9.2:fetch _ 20 newsggroups()函数的参数列表

记住`random_state`对于再现性是有用的。每次运行脚本都可以获得相同的数据集。否则，处理以不同顺序混合的数据集可能会带来不必要的变化。

在本节中，我们加载了新闻组数据。接下来我们来探索一下。

# 探索新闻组数据

在我们以任何我们喜欢的方式下载了 20 个新闻组数据集之后，`groups`的`data`对象现在被缓存在内存中。`data`对象是键值字典的形式。它的键如下:

```
>>> groups.keys()
dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR']) 
```

`target_names`键给出新闻组名称:

```
>>> groups['target_names']
   ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc'] 
```

`target`键对应于一个新闻组，但被编码为一个整数:

```
>>> groups.target
array([7, 4, 4, ..., 3, 1, 8]) 
```

那么，这些整数的不同值是什么？我们可以使用 NumPy 中的`unique`函数来计算:

```
>>> import numpy as np
>>> np.unique(groups.target)
array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]) 
```

它们的范围从`0`到`19`，代表`groups['target_names']`的第 1、2、3、…、20 个新闻组主题。

在多个主题或类别的上下文中，了解主题的分布是很重要的。平衡的类别分布是最容易处理的，因为没有代表不足或代表过多的类别。然而，我们经常有一个或多个类别占主导地位的偏斜分布。

我们将使用`seaborn`包([https://seaborn.pydata.org/](https://seaborn.pydata.org/))来计算类别直方图，并使用`matplotlib`包([https://matplotlib.org/](https://matplotlib.org/))绘制直方图。我们可以通过`pip`安装两个软件包，如下所示:

```
python -m pip install -U matplotlib
pip install seaborn 
```

在`conda`的情况下，可以执行以下命令行:

```
conda install -c conda-forge matplotlib
conda install seaborn 
```

记得在`seaborn`之前安装`matplotlib`，因为`matplotlib`是`seaborn`包的依赖项之一。

现在，让我们显示类的分布，如下所示:

```
>>> import seaborn as sns
>>> import matplotlib.pyplot as plt
>>> sns.distplot(groups.target)
<matplotlib.axes._subplots.AxesSubplot object at 0x108ada6a0>
>>> plt.show() 
```

有关最终结果，请参考以下屏幕截图:

<figure class="mediaobject">![](../Images/B16326_09_05.png)</figure>

图 9.4:新闻组类的分布

如您所见，分布大致均匀，因此不用担心。

可视化数据有助于大致了解数据的结构、可能出现的问题以及是否存在我们必须处理的违规行为。

其他键非常简单明了:`data`包含所有新闻组文档，`filenames`存储每个文档在文件系统中的路径。

现在，让我们通过执行以下命令来查看第一个文档及其主题号和名称:

```
>>> groups.data[0]
"From: lerxst@wam.umd.edu (where's my thing)\nSubject: WHAT car is this!?\nNntp-Posting-Host: rac3.wam.umd.edu\nOrganization: University of Maryland, College Park\nLines: 15\n\n I was wondering if anyone out there could enlighten me on this car I saw\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\nthe front bumper was separate from the rest of the body. This is \nall I know. If anyone can tellme a model name, engine specs, years\nof production, where this car is made, history, or whatever info you\nhave on this funky looking car, please e-mail.\n\nThanks,\n- IL\n ---- brought to you by your neighborhood Lerxst ----\n\n\n\n\n"
>>> groups.target[0]
7
>>> groups.target_names[groups.target[0]]
'rec.autos' 
```

如果`random_state`不是固定的(默认为`42` ，运行前面的脚本可能会得到不同的结果。

如您所见，第一个文档来自`rec.autos`新闻组，被分配了编号`7`。阅读这篇文章，我们可以很容易地发现它是关于汽车的。`car`这个词实际上在文档中出现了很多次。`bumper`之类的词似乎也很以车为本。然而，像`doors`这样的词不一定与汽车有关，因为它们也可能与家装或其他话题有关。

作为旁注，不区分`doors`和`door`或者大小写不同的同一个词，如`Doors`是有道理的。在一些罕见的情况下，大写字母确实很重要，例如，如果我们试图找出一份文件是关于被称为`The Doors`的乐队还是更常见的概念`the doors`(在伍德)。

# 关于文本数据特征的思考

从前面的分析中，我们可以有把握地得出结论，如果我们想弄清楚一个文档是否来自`rec.autos`新闻组，那么像`car`、`doors`和`bumper`这样的单词的存在与否可能是非常有用的特征。一个单词的有无是一个布尔变量，我们也可以看看某些单词的数量。例如，`car`在文档中出现多次。也许这样的词在文本中出现的次数越多，文档就越有可能与汽车有关。

## 计算每个单词标记的出现次数

我们似乎只对某些单词的出现、它们的计数或相关的度量感兴趣，而不是对单词的顺序感兴趣。因此，我们可以将文本视为单词的集合。这叫**包字** ( **弓**)模型。这是一个非常基本的模型，但它在实践中运行得相当好。我们可以选择定义一个更复杂的模型，考虑单词和 PoS 标签的顺序。然而，这样的模型计算成本更高，编程难度更大。实际上，在大多数情况下，基本的 BoW 模型就足够了。我们可以试一试，看看 BoW 模型是否有意义。

我们首先将文档转换成一个矩阵，其中每行代表一个新闻组文档，每列代表一个单词标记，或者具体来说，首先是一个单幅图。矩阵中每个元素的值是单词(列)在文档(行)中出现的次数。我们正在利用 scikit 的`CountVectorizer`课程——学习做以下工作:

```
>>> from sklearn.feature_extraction.text import CountVectorizer 
```

下表总结了计数转换功能的重要参数和选项:

<colgroup><col> <col> <col> <col></colgroup> 
| 构造函数参数 | 缺省值 | 示例值 | 描述 |
| `ngram_range` | (1,1) | (1,2), (2,2) | 要在输入文本中提取的 n-grams 的下限和上限，例如(1，1)表示 unigram，(1，2)表示 unigram 和 bigram |
| `stop_words` | 诺亚 | 英语，或列出['a '，' of']或无 | 使用哪个停止词列表，可以是“英语”指的是内置列表，或者是自定义的输入列表。如果“无”，则不会删除任何单词。 |
| `lowercase` | 真实的 | 真，假 | 是否将所有字符转换为小写 |
| `max_features` | 没有人 | 没有，200，500 | 要考虑的顶级(最频繁)令牌数，如果没有，则为所有令牌数 |
| `binary` | 错误的 | 真，假 | 如果为真，所有非零计数变为 1 |

表 9.3:countvectorzer()函数的参数列表

首先用`500`顶级特征初始化计数向量器(500 个最常见的标记):

```
>>>  count_vector = CountVectorizer(max_features=500) 
```

使用它来适应原始文本数据，如下所示:

```
>>> data_count = count_vector.fit_transform(groups.data) 
```

现在，计数矢量器捕获前 500 个特征，并根据原始文本输入生成一个标记计数矩阵:

```
>>> data_count
<11314x500 sparse matrix of type '<class 'numpy.int64'>'
      with 798221 stored elements in Compressed Sparse Row format>
>>> data_count[0]
<1x500 sparse matrix of type '<class 'numpy.int64'>'
      with 53 stored elements in Compressed Sparse Row format> 
```

结果计数矩阵是稀疏的矩阵，其中每行仅存储非零元素(因此，仅存储`798,221`元素而不是`11314 * 500 = 5,657,000`)。例如，第一个文档被转换成由 53 个非零元素组成的稀疏向量。如果您有兴趣查看整个矩阵，请运行以下命令:

```
>>> data_count.toarray() 
```

如果您只想要第一行，请运行以下命令:

```
>>> data_count.toarray()[0] 
```

让我们看看从前面的命令中得到的以下输出:

<figure class="mediaobject">![](../Images/B16326_09_07.png)</figure>

图 9.5:计数矢量化的输出

那么，那 500 个顶级功能是什么？它们可以在以下输出中找到:

```
>>> print(count_vector.get_feature_names())
['00', '000', '0d', '0t', '10', '100', '11', '12', '13', '14', '145', '15', '16', '17', '18', '19', '1993', '1d9', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '34u', '35', '40', '45', '50', '55', '80', '92', '93', '__', '___', 'a86', 'able', 'ac', 'access', 'actually', 'address', 'ago', 'agree', 'al', 'american
……
……
……
 'user', 'using', 'usually', 'uucp', 've', 'version', 'video', 'view', 'virginia', 'vs', 'want', 'wanted', 'war', 'washington', 'way', 'went', 'white', 'win', 'window', 'windows', 'won', 'word', 'words', 'work', 'working', 'works', 'world', 'wouldn', 'write', 'writes', 'wrong', 'wrote', 'year', 'years', 'yes', 'york'] 
```

我们的第一次试验看起来并不完美。显然，最流行的代币是数字，或者是带有数字的字母，比如`a86`，它们并不能传达重要的信息。而且还有很多没有实际意义的词，比如`you`、`the`、`them`、`then`。另外，有些词包含相同的信息，例如`tell`和`told`、`use`和`used`、`time`和`times`。让我们解决这些问题。

## 文本预处理

我们从保留只包含字母的单词开始，这样像`00`和`000`这样的数字以及像`b8f`这样的字母和数字的组合将被删除。过滤器功能定义如下:

```
>>> data_cleaned = []
>>> for doc in groups.data:
...     doc_cleaned = ' '.join(word for word in doc.split() 
                                             if word.isalpha())
...     data_cleaned.append(doc_cleaned) 
```

这将生成新闻组数据的清理版本。

## 删除停止词

我们没有把`stop_words`作为`CountVectorizer`中的重要参数来谈。**停止词**是那些在帮助区分文档方面没有什么价值的常用词。一般来说，停止词会给 BoW 模型增加噪音，可以删除。

没有通用的停止词列表。因此，根据您使用的工具或软件包，您将删除不同的停止词集。以 scikit-learn 为例，您可以按如下方式查看列表:

```
>>> from sklearn.feature_extraction import stop_words
>>> print(stop_words.ENGLISH_STOP_WORDS)
frozenset({'most', 'three', 'between', 'anyway', 'made', 'mine', 'none', 'could', 'last', 'whenever', 'cant', 'more', 'where', 'becomes', 'its', 'this', 'front', 'interest', 'least', 're', 'it', 'every', 'four', 'else', 'over', 'any', 'very', 'well', 'never', 'keep', 'no', 'anything', 'itself', 'alone', 'anyhow', 'until', 'therefore', 'only', 'the', 'even', 'so', 'latterly', 'above', 'hereafter', 'hereby', 'may', 'myself', 'all', 'those', 'down',
……
……
'him', 'somehow', 'or', 'per', 'nowhere', 'fifteen', 'via', 'must', 'someone', 'from', 'full', 'that', 'beyond', 'still', 'to', 'get', 'himself', 'however', 'as', 'forty', 'whatever', 'his', 'nothing', 'though', 'almost', 'become', 'call', 'empty', 'herein', 'than', 'while', 'bill', 'thru', 'mostly', 'yourself', 'up', 'former', 'each', 'anyone', 'hundred', 'several', 'others', 'along', 'bottom', 'one', 'five', 'therein', 'was', 'ever', 'beside', 'everyone'}) 
```

要从新闻组数据中删除停止词，我们只需指定参数:

```
>>> count_vector_sw = CountVectorizer(stop_words="english", max_features=500) 
```

除了停止词，你可能会注意到名字包含在顶部特征中，例如`andrew`。我们可以用刚刚处理过的 NLTK 中的`Name`语料库过滤名字。

## 减少单词的屈折和派生形式

如前所述，我们有两种基本策略来处理来自同一个词根的单词——词干化和引理化。词干是一种更快的方法，如果有必要的话，包括切掉字母；比如*字*在词干后就变成了*字*。词干的结果不一定是一个有效的词。比如*试**试*变成*三*。另一方面，引理更慢，但更准确。它执行字典查找并保证返回一个有效的单词。回想一下，我们在前一节中使用 NLTK 实现了词干化和引理化。

将所有这些(预处理、删除停止词、引理化和计数矢量化)放在一起，我们得到以下结果:

```
>>> from nltk.corpus import names
>>> all_names = set(names.words())
>>> count_vector_sw = CountVectorizer(stop_words="english", max_features=500)
>>> from nltk.stem import WordNetLemmatizer
>>> lemmatizer = WordNetLemmatizer()
>>> data_cleaned = []
>>> for doc in groups.data:
...     doc = doc.lower()
...     doc_cleaned = ' '.join(lemmatizer.lemmatize(word)
                               for word in doc.split()
                               if word.isalpha() and
                               word not in all_names)
...     data_cleaned.append(doc_cleaned)
>>> data_cleaned_count = count_vector_sw.fit_transform(data_cleaned) 
```

现在的特性更有意义了:

```
>>> print(count_vector_sw.get_feature_names())
['able', 'accept', 'access', 'according', 'act', 'action', 'actually', 'add', 'address', 'ago', 'agree', 'algorithm', 'allow', 'american', 'anonymous', 'answer', 'anybody', 'apple', 'application', 'apr', 'april', 'arab', 'area', 'argument', 'armenian', 'article', 'ask', 'asked', 'assume', 'atheist', 'attack', 'attempt', 'available', 'away', 'bad', 'based', 'belief', 'believe', 'best', 'better', 'bible', 'big', 'bike', 'bit', 'black', 'board', 'body', 'book', 'box', 'build', 'bus', 'buy', 'ca', 'california', 'called', 'came', 'canada', 'car', 'card', 'care', 'carry', 'case', 'cause', 'center', 'certain', 'certainly', 'chance', 'change', 'check', 'child', 'chip', 'christian', 'church', 'city', 'claim', 'clear', 'clinton', 'clipper', 'code', 'college', 'color', 'come', 'coming', 'command', 'comment', 'common', 'communication', 'company', 'computer', 'consider', 'considered', 'contact', 'control', 'copy',
……
……
'short', 'shot', 'similar', 'simple', 'simply', 'single', 'site', 'situation', 'size', 'small', 'software', 'sort', 'sound', 'source', 'space', 'special', 'specific', 'speed', 'standard', 'start', 'started', 'state', 'statement', 'steve', 'stop', 'strong', 'study', 'stuff', 'subject', 'sun', 'support', 'sure', 'taken', 'taking', 
'talk', 'talking', 'tape', 'tax', 'team', 'technical', 'technology', 'tell', 'term', 'test', 'texas', 'text', 'thanks', 'thing', 'think', 'thinking', 'thought', 'time', 'tin', 'today', 'told', 'took', 'total', 'tried', 'true', 'truth', 'try', 'trying', 'turkish', 'turn', 'type', 'understand', 'united', 'university', 'unix', 'unless', 'usa', 'use', 'used', 'user', 'using', 'usually', 'value', 'various', 'version', 'video', 'view', 'wa', 'want', 'wanted', 'war', 'water', 'way', 'weapon', 'week', 'went', 'western', 'white', 'widget', 'win', 'window', 'woman', 'word', 'work', 'working', 'world', 'worth', 'write', 'written', 'wrong', 'year', 'york', 'young'] 
```

我们刚刚将每个原始新闻组文档中的文本转换成大小为 500 的稀疏向量。对于文档中的向量，每个元素表示单词标记在该文档中出现的次数。此外，这 500 个单词的标记是基于它们在文本预处理、停止词的移除和引理化之后的总出现次数来选择的。现在您可能会问这样的问题，这样的出现向量是否足够有代表性，或者这样的出现向量是否传达了足够的信息，可以用来将文档与其他主题的文档区分开来？你将在下一部分看到答案。

# 用 t-SNE 可视化新闻组数据

我们可以通过可视化那些表示向量来轻松回答这些问题。如果我们可以看到来自同一个主题的文档向量形成一个聚类，我们就很好地将文档映射成向量。但是怎么做呢？它们有 500 个维度，而我们最多可以可视化**的数据**三维。我们可以借助 t-SNE 进行降维。

## 什么是降维？

**降维**是一种重要的机器学习技术，可以减少特征的数量，同时保留尽可能多的信息。它通常通过获得一组新的主要特征来执行。

如前所述，高维数据很难可视化。给定一个三维图，我们有时发现观察任何发现并不简单，更不用说 10、100 或 1000 维了。此外，高维数据中的一些特征可能是相关的，因此会带来冗余。这就是为什么我们需要降维。

降维不是简单地从原始特征空间中取出一对两个特征。它正在将原来的特征空间转换为更少维度的新空间。数据转换可以是线性的，例如著名的、**主成分分析** ( **PCA** )，它将高维空间中的数据映射到低维空间，在低维空间中数据的方差最大化，如*第 3 章*、*使用支持向量机识别人脸*中所述，也可以是非线性的，例如神经网络和即将推出的 t-SNE。**非负矩阵分解** ( **NMF** )是另一个强大的算法，我们将在*第 10 章*、*利用聚类和主题建模发现新闻组数据集中的底层主题*中详细研究。

说到底，大多数降维算法都属于**无监督学习**族，因为的目标或标签信息(如果有的话)不用于数据转换。

## 降维的 t-SNE 方法

**t-SNE** 代表**t-分布式随机邻居嵌入**。这是一种由劳伦斯·范德马滕和杰弗里·辛顿开发的非线性降维技术。t-SNE 已被广泛用于各种领域的数据可视化，包括计算机视觉、自然语言处理、生物信息学和计算基因组学。

顾名思义，t-SNE 将高维数据嵌入到低维(通常是二维或三维)空间中，在该空间中数据样本之间的相似性(邻居信息)得以保留。它首先通过给相似的数据点分配高概率，给不相似的数据点分配极小的概率，来模拟数据点周围邻居的概率分布。请注意，相似性和相邻距离是通过欧几里德距离或其他度量来衡量的。然后，t-SNE 构造了一个低维空间的投影，在那里输入分布和输出分布之间的差异被最小化。原始高维空间建模为高斯分布，而输出低维空间建模为 t 分布。

我们将在此使用 scikit-learn 的`TSNE`类实现 t-SNE:

```
>>> from sklearn.manifold import TSNE 
```

现在，让我们使用 t-SNE 来验证我们的计数向量表示。

我们选择三个不同的主题:`talk.religion.misc`、`comp.graphics`和`sci.space`，并从这三个主题中可视化文档向量。

首先，只需加载这三个标签的文档，如下所示:

```
>>> categories_3 = ['talk.religion.misc', 'comp.graphics', 'sci.space']
>>> groups_3 = fetch_20newsgroups(categories=categories_3) 
```

我们通过同样的过程，生成一个计数矩阵`data_cleaned_count_3`，从输入`groups_3`中得到 500 个特征。您可以参考前面几节中的步骤，因为您只需要重复相同的代码。

接下来，我们应用 t-SNE 将 500 维矩阵简化为二维矩阵:

```
>>> tsne_model = TSNE(n_components=2, perplexity=40,
                     random_state=42, learning_rate=500)
>>> data_tsne = tsne_model.fit_transform(data_cleaned_count_3.toarray()) 
```

我们在`TSNE`对象中指定的参数如下:

*   `n_components`:输出维度
*   `perplexity`:算法中被认为是邻居的最近数据点的数量，典型值在 5 到 50 之间
*   `random_state`:程序再现性的随机种子
*   `learning_rate`:影响寻找典型值在 10 到 1000 之间的最优映射空间过程的因素

请注意`TSNE`对象只接受密集矩阵，因此我们使用`toarray()`将稀疏矩阵`data_cleaned_count_3`转换为密集矩阵。

我们刚刚成功地将输入维度从 500 减少到 2。最后，我们可以很容易地将其可视化为二维散点图，其中 *x* 轴是第一维， *y* 轴是第二维，颜色`c`基于每个原始文档的主题标签:

```
>>> import matplotlib.pyplot as plt
>>> plt.scatter(data_tsne[:, 0], data_tsne[:, 1], c=groups_3.target)
>>> plt.show() 
```

有关最终结果，请参考以下屏幕截图:

<figure class="mediaobject">![](../Images/B16326_09_08.png)</figure>

图 9.6:将 t-SNE 应用于三个不同主题的数据

三个主题的数据点有不同的颜色，如绿色、紫色和黄色。我们可以观察到三个清晰的星团。同一主题的数据点彼此接近，而不同主题的数据点则相距甚远。显然，计数向量是原始文本数据的很好的表示，因为它们保留了三个不同主题之间的区别。

你也可以玩参数，看看你是否能得到一个更好的图，在那里三个集群被更好地分开。

计数矢量化在保持文档差异方面做得很好。保持相似怎么样？我们还可以检查使用来自重叠主题的文档，例如这五个主题:`comp.graphics`、`comp.os.ms-windows.misc`、`comp.sys.ibm.pc.hardware`、`comp.sys.mac.hardware`和`comp.windows.x`:

```
>>> categories_5 = ['comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x']
>>> groups_5 = fetch_20newsgroups(categories=categories_5) 
```

重复类似的过程(包括文本清理、计数矢量化和 t-SNE)，结果图显示如下:

<figure class="mediaobject">![](../Images/B16326_09_09.png)</figure>

图 9.7:将 t-SNE 应用于五个类似主题的数据

这五个与计算机相关的主题的数据点到处都是，这意味着它们在上下文中是相似的。总之，计数向量是原始文本数据的很好的表示，因为它们也善于保持相关主题之间的相似性。

# 摘要

在本章中，您学习了自然语言处理的基本概念，它是机器学习中的一个重要子领域，包括标记化、词干化和词条化以及词性标注。我们还探索了三个强大的 NLP 包，并使用 NLTK 和`spaCy`处理了一些常见的任务。然后，我们继续进行探索新闻组数据的主要项目。我们从使用标记化技术提取特征开始，经历了文本预处理、停止词移除以及词干和词条化。然后，我们使用 t-SNE 进行降维和可视化，并证明计数矢量化是文本数据的良好表示。

使用降维作为一种无监督的方法来挖掘新闻组数据，我们从中获得了一些乐趣。展望未来，在下一章，我们将继续我们的无监督学习之旅，特别关注主题建模和聚类。

# 练习

1.  你认为所有 500 个单词的单词都包含有价值的信息吗？如果没有，你能强加另一个停止词列表吗？
2.  你能用词干代替词条化来处理新闻组数据吗？
3.  能否将`CountVectorizer`中的`max_features`从`500`增加到`5000`，看看 t-SNE 可视化会受到怎样的影响？
4.  尝试可视化来自六个主题(相似或不同)的文档，并调整参数，使形成的聚类看起来合理。