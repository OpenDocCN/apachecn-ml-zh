# five

# 用逻辑回归预测在线广告点击率

在前一章中，我们使用树算法预测了广告点击率。在这一章中，我们将继续我们解决十亿美元问题的旅程。我们将集中学习一个非常(可能是最)可扩展的分类模型——逻辑回归。我们将探讨什么是逻辑回归函数，如何训练逻辑回归模型，向模型添加正则化，以及适用于非常大数据集的逻辑回归变体。除了在分类中的应用，我们还将讨论如何使用逻辑回归和随机森林来挑选重要特征。您不会感到厌烦，因为 scikit-learn 和 TensorFlow 将有许多从头开始的实现。

在本章中，我们将涵盖以下主题:

*   分类特征编码
*   逻辑函数
*   什么是逻辑回归？
*   梯度下降和随机梯度下降
*   逻辑回归的实现
*   逻辑回归点击率预测
*   L1 和 L2 正则化的逻辑回归
*   特征选择的逻辑回归
*   在线学习
*   选择要素的另一种方式——随机森林

# 将分类特征转换为数字-一热编码和序数编码

在*第 4 章*、*用基于树的算法预测在线广告点击率*中，我提到了**一键编码**如何将分类特征转换为数字特征，以便在 scikit-learn 和 TensorFlow 中的树算法中使用它们。如果我们使用一次性编码将分类特征转换为数字特征，我们不会将算法的选择局限于能够处理分类特征的基于树的算法。

在将分类特征转换为 *k* 可能值方面，我们能想到的最简单的解决方案是将其映射为数值从 1 到 *k* 的数字特征。例如，[Tech，Fashion，Fashion，Sports，Tech，Tech，Sports]变成[1，2，2，3，1，1，3]。然而，这将强加一个序数特征，例如体育大于科技，以及一个距离属性，例如体育更接近时尚**而不是科技。**

相反，一键编码将分类特征转换为 *k* 二进制特征。每个二进制特征指示对应的可能值的存在或不存在。因此，前面的例子变成了下面的例子:

<figure class="mediaobject">![](../Images/B16326_05_01.png)</figure>

图 5.1:通过一键编码将用户兴趣转化为数字特征

之前我们使用过 scikit-learn 中的`OneHotEncoder`将一个字符串矩阵转换成二进制矩阵，但是在这里，我们再来看看另一个模块`DictVectorizer`，它也提供了一个高效的转换。它将字典对象(分类特征:值)转换成单热编码向量。

例如，看看下面的代码:

```
>>> from sklearn.feature_extraction import DictVectorizer
>>> X_dict = [{'interest': 'tech', 'occupation': 'professional'},
...           {'interest': 'fashion', 'occupation': 'student'},
...           {'interest': 'fashion','occupation':'professional'},
...           {'interest': 'sports', 'occupation': 'student'},
...           {'interest': 'tech', 'occupation': 'student'},
...           {'interest': 'tech', 'occupation': 'retired'},
...           {'interest': 'sports','occupation': 'professional'}]
>>> dict_one_hot_encoder = DictVectorizer(sparse=False)
>>> X_encoded = dict_one_hot_encoder.fit_transform(X_dict)
>>> print(X_encoded)
[[ 0\.  0\. 1\. 1\.  0\. 0.]
 [ 1\.  0\. 0\. 0\.  0\. 1.]
 [ 1\.  0\. 0\. 1\.  0\. 0.]
 [ 0\.  1\. 0\. 0\.  0\. 1.]
 [ 0\.  0\. 1\. 0\.  0\. 1.]
 [ 0\.  0\. 1\. 0\.  1\. 0.]
 [ 0\.  1\. 0\. 1\.  0\. 0.]] 
```

我们也可以通过执行以下操作来查看映射:

```
>>> print(dict_one_hot_encoder.vocabulary_)
{'interest=fashion': 0, 'interest=sports': 1,
'occupation=professional': 3, 'interest=tech': 2,
'occupation=retired': 4, 'occupation=student': 5} 
```

谈到新数据，我们可以通过以下方式进行转换:

```
>>> new_dict = [{'interest': 'sports', 'occupation': 'retired'}]
>>> new_encoded = dict_one_hot_encoder.transform(new_dict)
>>> print(new_encoded)
[[ 0\. 1\. 0\. 0\. 1\. 0.]] 
```

我们可以将编码后的特征反向转换回原始特征，如下所示:

```
>>> print(dict_one_hot_encoder.inverse_transform(new_encoded))
[{'interest=sports': 1.0, 'occupation=retired': 1.0}] 
```

需要注意的一点是，如果在新数据中遇到新的(在训练数据中没有看到的)类别，则应该忽略它(否则，编码器会抱怨看不见的类别值)。`DictVectorizer`隐式处理(而`OneHotEncoder`需要指定参数`ignore`):

```
>>> new_dict = [{'interest': 'unknown_interest',
               'occupation': 'retired'},
...             {'interest': 'tech', 'occupation':
               'unseen_occupation'}]
>>> new_encoded = dict_one_hot_encoder.transform(new_dict)
>>> print(new_encoded)
[[ 0\.  0\. 0\. 0\.  1\. 0.]
 [ 0\.  0\. 1\. 0\.  0\. 0.]] 
```

有时，我们更喜欢将具有 *k* 可能值的分类特征转换为数值范围从 *1* 到 *k* 的数值特征。我们进行**序数编码**，以便在我们的学习中使用序数或排序知识；例如，大、中和小分别变成 3、2 和 1；好的和坏的变成 1 和 0，而一热编码不能保存这些有用的信息。我们可以通过`pandas`的使用轻松实现序数编码，例如:

```
>>> import pandas as pd
>>> df = pd.DataFrame({'score': ['low',
...                              'high',
...                              'medium',
...                              'medium',
...                              'low']})
>>> print(df)
   score
0     low
1    high
2  medium
3  medium
4     low
>>> mapping = {'low':1, 'medium':2, 'high':3}
>>> df['score'] = df['score'].replace(mapping)
>>> print(df)
  score
0      1
1      3
2      2
3      2
4      1 
```

我们根据我们定义的映射将字符串特征转换为序数值。

我们已经讨论了将分类特征转换成数字特征。接下来，我们将讨论逻辑回归，一个只接受数字特征的分类器。

# 用逻辑回归对数据进行分类

在最后一章中，我们只基于 4000 万个样本中的前 30 万个样本来训练基于树的模型。我们这样做只是因为在一个大数据集上训练一棵树在计算上非常昂贵和耗时。由于一热编码，我们现在不局限于直接采用分类特征的算法，我们应该转向一种对大型数据集具有高度可扩展性的新算法。如上所述，逻辑回归是最，或者可能是最可扩展的分类算法之一。

## 开始使用物流功能

在我们深入研究算法本身之前，让我们首先介绍一下作为算法核心的**逻辑函数**(更常见的说法是为 **sigmoid 函数**)。它基本上将输入映射为 *0* 和 *1* 之间的值的输出，定义如下:

<figure class="mediaobject">![](../Images/B16326_05_001.png)</figure>

我们可以通过执行以下步骤来可视化它的外观:

1.  定义物流函数:

    ```
    >>> import numpy as np
    >>> def sigmoid(input):
    ...     return 1.0 / (1 + np.exp(-input)) 
    ```

2.  输入变量从- `8`到`8`，以及相应的输出，如下:

    ```
    >>> z = np.linspace(-8, 8, 1000)
    >>> y = sigmoid(z)
    >>> import matplotlib.pyplot as plt
    >>> plt.plot(z, y)
    >>> plt.axhline(y=0, ls='dotted', color='k')
    >>> plt.axhline(y=0.5, ls='dotted', color='k')
    >>> plt.axhline(y=1, ls='dotted', color='k')
    >>> plt.yticks([0.0, 0.25, 0.5, 0.75, 1.0])
    >>> plt.xlabel('z')
    >>> plt.ylabel('y(z)')
    >>> plt.show() 
    ```

最终结果参见下面的截图:

<figure class="mediaobject">![](../Images/B16326_05_02.png)</figure>

图 5.2:逻辑功能

在 S 形曲线中，所有输入都被转换到 0 到 1 的范围内。对于正输入，值越大，输出越接近 1；对于负输入，较小的值产生接近 0 的输出；当输入为 0 时，输出为中点 0.5。

## 从逻辑函数到逻辑回归的跳跃

现在您已经了解了逻辑函数，很容易将其映射到源自该函数的算法。在逻辑回归中，函数输入 *z* 变成特征的加权和。给定一个带有 *n* 特征的数据样本 *x* ，*x*T10】1T12】，xT14】2T16，…，xT18】n(*x*代表一个特征向量，并且*x =(x*T24】1T26】，xT28】2T32 以及型号 *w* ( *w* 代表矢量*(w*T47】1T49，wT51】2T53，…，wT55】nT57)】z【T69】

<figure class="mediaobject">![](../Images/B16326_05_002.png)</figure>

此外，偶尔，模型附带一个**截距**(也称为**偏差**)， *w* <sub class="" style="font-style: italic;">0</sub> 。在这种情况下，前面的线性关系变为:

<figure class="mediaobject">![](../Images/B16326_05_003.png)</figure>

对于 0 到 1 范围内的输出 *y(z)* ，在算法中变成目标为 *1* 或正类的概率:

<figure class="mediaobject">![](../Images/B16326_05_004.png)</figure>

因此，逻辑回归是一个概率分类器，类似于朴素贝叶斯分类器。

从训练数据中学习逻辑回归模型，或者更具体地说，其权重向量 *w* ，目标是预测尽可能接近于 *1* 的正样本，并预测尽可能接近于 0 的负样本。在数学语言中，训练权重以使定义为**均方误差** ( **均方误差**)的成本最小化，其中测量真实值和预测值之间差值的平方平均值。给定 m 个训练样本，![](../Images/B16326_05_005.png)、![](../Images/B16326_05_006.png)、…![](../Images/B16326_05_007.png)……、![](../Images/B16326_05_008.png)，其中 *y* <sup class="" style="font-style: italic;">(i)</sup> 为 *1* (正类)或 *0* (负类)成本函数 *J(w)* 关于待优化权重表示如下:

<figure class="mediaobject">![](../Images/B16326_05_009.png)</figure>

然而，前面的代价函数是**非凸的**，这意味着在搜索最优 *w* 时，发现了许多局部(次优)最优，并且函数没有收敛到全局最优。

下面分别绘制了**凸**和**非凸**函数的示例:

<figure class="mediaobject">![](../Images/B16326_05_03.png)</figure>

图 5.3:凸函数和非凸函数的例子

在凸的例子中，只有一个全局最优，而在非凸的例子中有两个最优。关于凸函数和非凸函数的更多信息，请随时查看[https://en.wikipedia.org/wiki/Convex_function](https://en.wikipedia.org/wiki/Convex_function)和[https://web . Stanford . edu/class/ee 364 a/讲座/函数. pdf](https://web.stanford.edu/class/ee364a/lectures/functions.pdf) 。

为了克服这一点，实际中的成本函数定义如下:

<figure class="mediaobject">![](../Images/B16326_05_010.png)</figure>

我们可以仔细看看单个培训样本的成本:

<figure class="mediaobject">![](../Images/B16326_05_04.png)</figure>

当地面真相 *y* <sup class="" style="font-style: italic;">(i)</sup> *= 1* 时，如果模型完全可信地预测正确(概率为 100%的正类)，则样本成本 *j* 为*0*；当预测概率![](../Images/B16326_05_011.png)降低时，成本 *j* 增加。如果模型错误地预测没有正类的机会，那么成本是无限高的。我们可以将其可视化如下:

```
>>> y_hat = np.linspace(0, 1, 1000)
>>> cost = -np.log(y_hat)
>>> plt.plot(y_hat, cost)
>>> plt.xlabel('Prediction')
>>> plt.ylabel('Cost')
>>> plt.xlim(0, 1)
>>> plt.ylim(0, 7)
>>> plt.show() 
```

有关最终结果，请参考下图:

<figure class="mediaobject">![](../Images/B16326_05_05.png)</figure>

图 5.4:y = 1 时逻辑回归的成本函数

相反，当地面真值 *y* <sup class="" style="font-style: italic;">(i)</sup> *= 0* 时，如果模型完全可信地正确预测(概率为 *0* 的正类，或概率为 100%的负类)，则样本成本 *j* 为*0*；当预测概率![](../Images/B16326_05_011.png)增加时，成本 j 增加。当它错误地预测没有负类的机会时，成本变得无限高。我们可以使用以下代码将其可视化:

```
>>> y_hat = np.linspace(0, 1, 1000)
>>> cost = -np.log(1 - y_hat)
>>> plt.plot(y_hat, cost)
>>> plt.xlabel('Prediction')
>>> plt.ylabel('Cost')
>>> plt.xlim(0, 1)
>>> plt.ylim(0, 7)
>>> plt.show() 
```

下图是结果输出:

<figure class="mediaobject">![](../Images/B16326_05_06.png)</figure>

图 5.5:y = 0 时逻辑回归的成本函数

最小化这个替代成本函数实际上相当于最小化基于 MSE 的成本函数。选择它比选择 MSE 的优势包括以下几点:

*   显然，是凸的，这样可以找到最佳的模型权重
*   预测的对数的总和![](../Images/B16326_05_013.png)或![](../Images/B16326_05_014.png)简化了其相对于权重的导数的计算，这将在后面讨论

由于对数函数，成本函数![](../Images/B16326_05_015.png)也称为**对数损失**，或简称**对数损失**。

既然我们已经准备好了成本函数，我们如何训练逻辑回归模型来最小化成本函数？让我们在下一节看到。

# 训练逻辑回归模型

现在的问题是我们如何获得最优的 *w* 使得 *J(w)* 最小化。我们可以使用梯度下降来做到这一点。

## 使用梯度下降训练逻辑回归模型

**梯度下降**(也称为**最速下降**)是通过一阶迭代优化最小化目标函数的过程。在每次迭代中，它在当前点移动一个与目标函数的负导数成比例的步长。这意味着待优化点朝着目标函数的最小值迭代地向下移动。我们刚才提到的比例是称为**学习率**，或者**步长**。它可以用一个数学方程式概括如下:

<figure class="mediaobject">![](../Images/B16326_05_016.png)</figure>

这里左边的 *w* 是学习一步后的权重向量，右边的 *w* 是移动前的权重向量， *η* 是学习率，*∮w*是一阶导数，梯度。

在我们的例子中，让我们从成本函数 *J(w)* 相对于 *w* 的导数开始。这可能需要一些微积分的知识，但是不要担心，我们会一步一步地完成它:

1.  我们首先计算![](../Images/B16326_05_017.png)相对于 *w* 的导数。我们这里以 *j-th* 权重， *w* <sub class="" style="font-style: italic;">j</sub> 为例(注*z = w*<sup class="" style="font-style: italic;">T</sup>*x*，为简单起见省略<sup class="" style="font-style: italic;">(I)</sup>:

    <figure class="mediaobject">![](../Images/B16326_05_07.png)</figure>

2.  然后，我们计算样本成本的导数 *J(w)* 如下:

    <figure class="mediaobject">![](../Images/B16326_05_08.png)</figure>

3.  最后，我们计算整个成本超过 *m* 样品如下:

    <figure class="mediaobject">![](../Images/B16326_05_018.png)</figure>

4.  然后我们将其概括为*∮w*:

    <figure class="mediaobject">![](../Images/B16326_05_019.png)</figure>

5.  Combined with the preceding derivations, the weights can be updated as follows:

    <figure class="mediaobject">![](../Images/B16326_05_020.png)</figure>

    这里， *w* 在每次迭代中更新。

6.  经过大量迭代后，所学习的 *w* 和 *b* 然后被用于通过以下等式对新样本 *x* 进行分类:

<figure class="mediaobject">![](../Images/B16326_05_09.png)</figure>

决策阈值默认为 0.5，但肯定可以是其他值。在无论如何都应该避免假阴性的情况下，例如，当预测警报的火灾发生(阳性类别)时，决策阈值可以低于 0.5，例如 0.3，这取决于我们有多偏执以及我们想要多主动地防止阳性事件发生。另一方面，当假阳性类别是应该回避的类别时，例如，当预测质量保证的产品成功率(阳性类别)时，决策阈值可以大于 0.5，例如 0.7，或者小于 0.5，这取决于您设置的标准有多高。

彻底了解基于梯度下降的训练和预测过程后，我们现在将从头开始实施逻辑回归算法:

1.  我们首先定义用当前权重计算预测的函数【T1:

    ```
    >>> def compute_prediction(X, weights):
    ...     """
    ...     Compute the prediction y_hat based on current weights
    ...     """
    ...     z = np.dot(X, weights)
    ...     predictions = sigmoid(z)
    ...     return predictions 
    ```

2.  这样，我们就能够以梯度下降的方式一步一步地继续更新权重![](../Images/B16326_05_022.png)的功能。看看下面的代码:

    ```
    >>> def update_weights_gd(X_train, y_train, weights,
                                               learning_rate):
    ...     """
    ...     Update weights by one step
    ...     """
    ...     predictions = compute_prediction(X_train, weights)
    ...     weights_delta = np.dot(X_train.T, y_train - predictions)
    ...     m = y_train.shape[0]
    ...     weights += learning_rate / float(m) * weights_delta
    ...     return weights 
    ```

3.  然后，计算成本 *J(w)* 的功能也被实现:

    ```
    >>> def compute_cost(X, y, weights):
    ...     """
    ...     Compute the cost J(w)
    ...     """
    ...     predictions = compute_prediction(X, weights)
    ...     cost = np.mean(-y * np.log(predictions)
                          - (1 - y) * np.log(1 - predictions))
    ...     return cost 
    ```

4.  Now, we connect all these functions to the model training function by executing the following:
    *   在每次迭代中更新`weights`向量
    *   打印出每一次`100`(这可以是另一个值)迭代的当前成本，以确保`cost`正在减少，并且事情在正确的轨道上

    它们在以下功能中实现:

    ```
    >>> def train_logistic_regression(X_train, y_train, max_iter,
                              learning_rate, fit_intercept=False):
    ...     """ Train a logistic regression model
    ...     Args:
    ...         X_train, y_train (numpy.ndarray, training data set)
    ...         max_iter (int, number of iterations)
    ...         learning_rate (float)
    ...         fit_intercept (bool, with an intercept w0 or not)
    ...     Returns:
    ...         numpy.ndarray, learned weights
    ...     """
    ...     if fit_intercept:
    ...         intercept = np.ones((X_train.shape[0], 1))
    ...         X_train = np.hstack((intercept, X_train))
    ...     weights = np.zeros(X_train.shape[1])
    ...     for iteration in range(max_iter):
    ...         weights = update_weights_gd(X_train, y_train,
                                           weights, learning_rate)
    ...         # Check the cost for every 100 (for example)       
                 iterations
    ...         if iteration % 100 == 0:
    ...             print(compute_cost(X_train, y_train, weights))
    ...     return weights 
    ```

5.  最后，我们使用训练好的模型预测新输入的结果如下:

    ```
    >>> def predict(X, weights):
    ...     if X.shape[1] == weights.shape[0] - 1:
    ...         intercept = np.ones((X.shape[0], 1))
    ...         X = np.hstack((intercept, X))
    ...     return compute_prediction(X, weights) 
    ```

正如您刚刚看到的，实现逻辑回归非常简单。现在让我们用一个玩具例子来检验它:

```
>>> X_train = np.array([[6, 7],
...                     [2, 4],
...                     [3, 6],
...                     [4, 7],
...                     [1, 6],
...                     [5, 2],
...                     [2, 0],
...                     [6, 3],
...                     [4, 1],
...                     [7, 2]])
>>> y_train = np.array([0,
...                     0,
...                     0,
...                     0,
...                     0,
...                     1,
...                     1,
...                     1,
...                     1,
...                     1]) 
```

我们为`1000` 迭代训练逻辑回归模型，在基于包含截距的权重训练`0.1`的学习率:

```
>>> weights = train_logistic_regression(X_train, y_train, 
             max_iter=1000, learning_rate=0.1, fit_intercept=True)
0.574404237166
0.0344602233925
0.0182655727085
0.012493458388
0.00951532913855
0.00769338806065
0.00646209433351
0.00557351184683
0.00490163225453
0.00437556774067 
```

成本的降低意味着模型会随着时间的推移而优化。我们可以在新样本上检查模型的性能，如下所示:

```
>>> X_test = np.array([[6, 1],
...                    [1, 3],
...                    [3, 1],
...                    [4, 5]])
>>> predictions = predict(X_test, weights)
>>> predictions
array([ 0.9999478 , 0.00743991, 0.9808652 , 0.02080847]) 
```

要可视化这一点，请执行以下代码:

```
>>> import matplotlib.pyplot as plt
>>> plt.scatter(X_train[:,0], X_train[:,1], c=['b']*5+['k']*5, 
                                                      marker='o') 
```

蓝色点是 0 班的训练样本，黑色点是 1 班的。使用`0.5`作为分类决策阈值:

```
>>> colours = ['k' if prediction >= 0.5 else 'b' 
                                  for prediction in predictions]
>>> plt.scatter(X_test[:,0], X_test[:,1], marker='*', c=colours) 
```

蓝星是测试从 0 级预测的样本，而黑星是从 1 级预测的样本:

```
>>> plt.xlabel('x1')
>>> plt.ylabel('x2')
>>> plt.show() 
```

有关最终结果，请参考以下屏幕截图:

<figure class="mediaobject">![](../Images/B16326_05_10.png)</figure>

图 5.6:玩具示例的训练和测试集

我们训练的模型正确预测了新样本(恒星)的类。

## 梯度下降逻辑回归预测广告点击率

在这个简短的例子之后，我们将现在部署我们刚刚在点进预测项目中开发的算法。

我们在此仅从 10，000 个训练样本开始(您很快就会明白为什么我们不像上一章那样从 270，000 个样本开始):

```
>>> import pandas as pd
>>> n_rows = 300000
>>> df = pd.read_csv("train", nrows=n_rows)
>>> X = df.drop(['click', 'id', 'hour', 'device_id', 'device_ip'], 
                                                     axis=1).values
>>> Y = df['click'].values
>>> n_train = 10000
>>> X_train = X[:n_train]
>>> Y_train = Y[:n_train]
>>> X_test = X[n_train:]
>>> Y_test = Y[n_train:]
>>> from sklearn.preprocessing import OneHotEncoder
>>> enc = OneHotEncoder(handle_unknown='ignore')
>>> X_train_enc = enc.fit_transform(X_train)
>>> X_test_enc = enc.transform(X_test) 
```

以带有偏差的`0.01`的学习率，在`10000`迭代上训练逻辑回归模型:

```
>>> import timeit
>>> start_time = timeit.default_timer()
>>> weights = train_logistic_regression(X_train_enc.toarray(), 
              Y_train, max_iter=10000, learning_rate=0.01, 
              fit_intercept=True)
0.6820019456743648
0.4608619713011896
0.4503715555130051
…
…
…
0.41485094023829017
0.41477416506724385
0.41469802145452467
>>> print(f"--- {(timeit.default_timer() - start_time)}.3fs seconds ---")
--- 232.756s seconds --- 
```

需要`232 seconds`对模型进行优化。训练好的模型在测试集上的表现如下:

```
>>> pred = predict(X_test_enc.toarray(), weights)
>>> from sklearn.metrics import roc_auc_score
>>> print(f'Training samples: {n_train}, AUC on testing set: {roc_auc_score(Y_test, pred):.3f}')
Training samples: 10000, AUC on testing set: 0.703 
```

现在，让我们使用 10 万个训练样本(`n_train = 100000`)重复同样的过程。用时 5240.4 秒，差不多 1.5 小时。容纳 10 倍大小的数据需要 22 倍的时间。正如我在本章开头提到的，逻辑回归分类器在大数据集上的训练非常好。但是我们的检测结果似乎与此相矛盾。我们如何高效地处理更大的训练数据集，不仅仅是 10 万个样本，而是数百万个样本？在下一节中，让我们看看训练逻辑回归模型的更有效的方法。

## 使用随机梯度下降训练逻辑回归模型

在基于梯度下降的逻辑回归模型中，**所有**训练样本用于在每次迭代中更新权重。因此，如果训练样本的数量很大，整个训练过程将变得非常耗时，并且计算成本很高，正如您刚刚在我们的最后一个示例中看到的那样。

幸运的是，一个小小的调整将使逻辑回归适合大规模数据集。每次权重更新，**只消耗一个**训练样本，而不是**完整的**训练集。该模型基于单个训练样本计算的误差移动一步。一旦使用了所有样本，一次迭代就结束了。这个梯度下降的高级版本被称为**随机梯度下降** ( **SGD** )。用公式表示，对于每次迭代，我们执行以下操作:

![](../Images/B16326_05_023.png)

<figure class="mediaobject">![](../Images/B16326_05_024.png)</figure>

SGD 通常比梯度下降收敛得更快，梯度下降通常需要大量的迭代。

为了实现基于 SGD 的逻辑回归，我们只需要稍微修改一下`update_weights_gd`函数:

```
>>> def update_weights_sgd(X_train, y_train, weights, 
                                           learning_rate):
...     """ One weight update iteration: moving weights by one 
            step based on each individual sample
...     Args:
...     X_train, y_train (numpy.ndarray, training data set)
...     weights (numpy.ndarray)
...     learning_rate (float)
...     Returns:
...     numpy.ndarray, updated weights
...     """
...     for X_each, y_each in zip(X_train, y_train):
...         prediction = compute_prediction(X_each, weights)
...         weights_delta = X_each.T * (y_each - prediction)
...         weights += learning_rate * weights_delta
...     return weights 
```

在`train_logistic_regression`功能中，应用 SGD:

```
>>> def train_logistic_regression_sgd(X_train, y_train, max_iter, 
                              learning_rate, fit_intercept=False):
...     """ Train a logistic regression model via SGD
...     Args:
...     X_train, y_train (numpy.ndarray, training data set)
...     max_iter (int, number of iterations)
...     learning_rate (float)
...     fit_intercept (bool, with an intercept w0 or not)
...     Returns:
...     numpy.ndarray, learned weights
...     """
...     if fit_intercept:
...         intercept = np.ones((X_train.shape[0], 1))
...         X_train = np.hstack((intercept, X_train))
...     weights = np.zeros(X_train.shape[1])
...     for iteration in range(max_iter):
...         weights = update_weights_sgd(X_train, y_train, weights, 
                                                     learning_rate)
...         # Check the cost for every 2 (for example) iterations
...         if iteration % 2 == 0:
...             print(compute_cost(X_train, y_train, weights))
...     return weights 
```

现在，让我们来看看SGD 有多强大。我们将处理 10 万个训练样本，选择`10`作为迭代次数，`0.01`作为作为学习率，每隔一次迭代打印出当前成本:

```
>>> start_time = timeit.default_timer()
>>> weights = train_logistic_regression_sgd(X_train_enc.toarray(), 
        Y_train, max_iter=10, learning_rate=0.01, fit_intercept=True)
0.4127864859625796
0.4078504597223988
0.40545733114863264
0.403811787845451
0.4025431351250833
>>> print(f"--- {(timeit.default_timer() - start_time)}.3fs seconds ---")
--- 40.690s seconds ---
>>> pred = predict(X_test_enc.toarray(), weights)
>>> print(f'Training samples: {n_train}, AUC on testing set: {roc_auc_score(Y_test, pred):.3f}')
Training samples: 100000, AUC on testing set: 0.732 
```

训练过程只需 40 秒就结束了！

像往常一样，在从头开始成功实现基于 SGD 的逻辑回归算法之后，我们使用 scikit-learn 的`SGDClassifier`模块来实现它:

```
>>> from sklearn.linear_model import SGDClassifier
>>> sgd_lr = SGDClassifier(loss='log', penalty=None, 
             fit_intercept=True, max_iter=10, 
             learning_rate='constant', eta0=0.01) 
```

这里，`loss`参数的`'log'`表示代价函数是对数损失，`penalty`是减少过拟合的正则化项，我们将在下一节中进一步讨论，`max_iter`是迭代次数，其余两个参数表示学习率为`0.01`，在训练过程中不变。需要注意的是，默认`learning_rate`为`'optimal'`，随着更新越来越多，学习率略有下降。这有利于在大型数据集上找到最优解。

现在，训练模型并测试它:

```
>>> sgd_lr.fit(X_train_enc.toarray(), Y_train)
>>> pred = sgd_lr.predict_proba(X_test_enc.toarray())[:, 1]
>>> print(f'Training samples: {n_train}, AUC on testing set: {roc_auc_score(Y_test, pred):.3f}')
Training samples: 100000, AUC on testing set: 0.734 
```

又快又容易！

## 正则化训练逻辑回归模型

正如我在前面的部分简要提到的，逻辑回归`SGDClassifier`中的`penalty`参数与模型**正则化**相关。正规化有和两种基本形式， **L1** (也叫**套索**)和 **L2** (也叫**脊**)。无论采用哪种方式，正则化都是原始成本函数之上的附加项:

<figure class="mediaobject">![](../Images/B16326_05_025.png)</figure>

这里， *α* 是乘以正则化项的常数， *q* 是代表 L1 或 L2 正则化的 *1* 或 *2* ,适用于以下情况:

<figure class="mediaobject">![](../Images/B16326_05_026.png)</figure>

训练逻辑回归模型是将成本降低为权重的函数的过程。如果到了某些权重，比如*w*<sub class="" style="font-style: italic;">I</sub>*w*<sub class="" style="font-style: italic;">j</sub>*w*<sub class="" style="font-style: italic;">k</sub>相当大的时候，整个成本就由这些大权重决定了。在这种情况下，学习的模型可能只是记住了训练集，而不能推广到看不见的数据。这里引入正则化项是为了惩罚较大的权重，因为权重现在成为最小化成本的一部分。因此，正则化消除了过拟合。最后，参数α提供了对数损失和泛化之间的权衡。如果 *α* 太小，则无法压缩大的权重，模型可能会出现高方差或过拟合；另一方面，如果 *α* 过大，模型可能会过度泛化，在拟合数据集方面表现不佳，这就是拟合不足的综合症。 *α* 是一个重要的参数进行调整，以获得最佳的正则化逻辑回归模型。

至于在 L1 和 L2 形态之间选择，经验法则是基于是否期望**特征选择**。在机器学习分类中，特征选择是挑选重要特征子集的过程，用于更好的模型构建。实际上，并非数据集中的每一个特征都携带有对鉴别样本有用的信息；有些功能要么是冗余的，要么是不相关的，因此可以很少损失地丢弃。在逻辑回归分类器中，特征选择只能通过 L1 正则化来实现。为了理解这一点，让我们考虑两个权重向量， *w* <sub class="" style="font-style: italic;">1</sub> *= (1，0)* 和*w*<sub class="" style="font-style: italic;">2</sub>*=(0.5，0.5)*；假设它们产生相同数量的对数损失，每个权重向量的 L1 和 L2 正则化项如下:

<figure class="mediaobject">![](../Images/B16326_05_028.png)</figure>

<figure class="mediaobject">![](../Images/B16326_05_029.png)</figure>

两个向量的 L1 项等价，而 *w* <sub class="" style="font-style: italic;">2</sub> 的 L2 项小于 *w* <sub class="" style="font-style: italic;">1</sub> 的项。这表明，与 L1 正则化相比，L2 正则化对由大大小小的权重组成的权重的惩罚更大。换句话说，L2 正则化倾向于所有权重的相对小的值，并且避免任何权重的显著大的和小的值，而 L1 正则化允许一些权重具有显著小的值和一些权重具有显著大的值。只有通过 L1 正则化，才能将部分权重压缩到接近或精确到 *0* ，从而实现特征选择。

在 scikit-learn 中，正则化类型可以通过`penalty`参数指定，选项有`none`(无正则化)、`"l1"`、`"l2"`和`"elasticnet"`(L1 和 L2 的混合)，乘数α可以通过 alpha 参数指定。

## 使用 L1 正则化的特征选择

我们在这里检查 L1 正则化用于特征选择。

使用 L1 正则化初始化 SGD 逻辑回归模型，并基于 10，000 个样本训练模型:

```
>>> sgd_lr_l1 = SGDClassifier(loss='log', penalty='l1', alpha=0.0001, fit_intercept=True, max_iter=10, 
learning_rate='constant', eta0=0.01)
>>> sgd_lr_l1.fit(X_train_enc.toarray(), Y_train) 
```

利用训练好的模型，我们获得了其系数的绝对值:

```
>>> coef_abs = np.abs(sgd_lr_l1.coef_)
>>> print(coef_abs)
[[0\. 0.09963329 0\. ... 0\. 0\. 0.07431834]] 
```

底部`10`系数及其值打印如下:

```
>>> print(np.sort(coef_abs)[0][:10])
[0\. 0\. 0\. 0\. 0\. 0\. 0\. 0\. 0\. 0.]
>>> bottom_10 = np.argsort(coef_abs)[0][:10] 
```

我们可以通过以下代码了解这 10 个特性:

```
>>> feature_names = enc.get_feature_names()
>>> print('10 least important features are:\n', 
                                   feature_names[bottom_10])
10 least important features are:
 ['x0_1001' 'x8_851897aa' 'x8_85119990' 'x8_84ebbcd4' 'x8_84eb6b0e'
 'x8_84dda655' 'x8_84c2f017' 'x8_84ace234' 'x8_84a9d4ba' 'x8_84915a27'] 
```

它们是`X_train`中`0`列的`1001`(即`C1`列)`8`列的`"851897aa"`等等。

类似地，前 10 个系数和它们的值可以得到如下:

```
>>> print(np.sort(coef_abs)[0][-10:])
[0.67912376 0.70885933 0.79975917 0.8828797 0.98146351 0.98275124
 1.08313767 1.13261091 1.18445527 1.40983505]
>>> top_10 = np.argsort(coef_abs)[0][-10:]
>>> print('10 most important features are:\n', feature_names[top_10])
10 most important features are:
 ['x7_cef3e649' 'x3_7687a86e' 'x18_61' 'x18_15' 'x5_9c13b419' 
'x5_5e3f096f' 'x2_763a42b5' 'x2_d9750ee7' 'x3_27e3c518' 
'x5_1779deee'] 
```

分别是`X_train`中`7`列的`"cef3e649"`(即`app_category`)、【第三列的`"7687a86e"`(即`site_domain`)等等。

# 通过在线学习在大型数据集上进行培训

到目前为止，我们已经在不超过 30 万个样本上训练了我们的模型。如果我们超过这个数字，内存可能会过载，因为它容纳了太多的数据，程序会崩溃。在本节中，我们将探索如何使用**在线学习**在大规模数据集上进行训练。

SGD 是从梯度下降进化而来的，通过一次一个训练样本地顺序更新模型，而不是一次更新整个训练集。我们可以通过在线学习技术进一步扩大 SGD。在在线学习中，用于培训的新数据可以按顺序或实时获得，而不是在离线学习环境中一次性获得。一次加载和预处理相对较小的数据块用于训练，这释放了用于保存整个大数据集的内存。除了更好的计算可行性之外，在线学习也被使用，因为它适应实时生成新数据的情况，并且是模型现代化所需要的。例如，股票价格预测模型通过及时的市场数据以在线学习的方式更新；点击率预测模型需要包含反映用户最新行为和口味的最新数据；垃圾邮件检测器必须通过考虑动态生成的新功能来应对不断变化的垃圾邮件发送者。

由先前数据集训练的现有模型现在可以仅基于最近可用的数据集进行更新，而不是像离线学习中那样，基于先前和最近的数据集从头开始重建:

<figure class="mediaobject">![](../Images/B16326_05_11.png)</figure>

图 5.7:在线学习与离线学习

在前面的例子中，在线学习允许模型使用新到达的数据继续训练。然而，在离线学习中，我们必须用新到达的数据和旧数据重新训练整个模型。

scikit-learn 中的`SGDClassifier`模块通过`partial_fit`方法实现在线学习(而`fit`方法应用于离线学习，正如您所见)。我们将用 1，000，000 个样本来训练模型，其中我们一次输入 100，000 个样本来模拟在线学习环境。我们将在另外 100，000 个样本上测试训练好的模型，如下所示:

```
>>> n_rows = 100000 * 11
>>> df = pd.read_csv("train", nrows=n_rows)
>>> X = df.drop(['click', 'id', 'hour', 'device_id', 'device_ip'], 
                                                      axis=1).values
>>> Y = df['click'].values
>>> n_train = 100000 * 10
>>> X_train = X[:n_train]
>>> Y_train = Y[:n_train]
>>> X_test = X[n_train:]
>>> Y_test = Y[n_train:] 
```

将编码器安装在整个训练装置上，如下所示:

```
>>> enc = OneHotEncoder(handle_unknown='ignore')
>>> enc.fit(X_train) 
```

初始化一个 SGD 逻辑回归模型，我们将迭代次数设置为`1`，以便部分拟合模型并启用在线学习:

```
>>> sgd_lr_online = SGDClassifier(loss='log', penalty=None, 
                               fit_intercept=True, max_iter=1, 
                               learning_rate='constant', eta0=0.01) 
```

循环每个`100000`样本，并部分拟合模型:

```
>>> start_time = timeit.default_timer()
>>> for i in range(10):
...     x_train = X_train[i*100000:(i+1)*100000]
...     y_train = Y_train[i*100000:(i+1)*100000]
...     x_train_enc = enc.transform(x_train)
...     sgd_lr_online.partial_fit(x_train_enc.toarray(), y_train, 
                                                    classes=[0, 1]) 
```

再次，我们使用`partial_fit`方法进行在线学习。另外，我们指定`classes`参数，这是在线学习所必需的:

```
>>> print(f"--- {(timeit.default_timer() - start_time)}.3fs seconds ---")
--- 167.399s seconds --- 
```

将训练好的模型应用于测试集，即接下来的 100，000 个样本，如下所示:

```
>>> x_test_enc = enc.transform(X_test)
>>> pred = sgd_lr_online.predict_proba(x_test_enc.toarray())[:, 1]
>>> print(f'Training samples: {n_train * 10}, AUC on testing set: {roc_auc_score(Y_test, pred):.3f}')
Training samples: 10000000, AUC on testing set: 0.761 
```

通过在线学习，基于总共 100 万个样本的训练只需要 167 秒，并且产生更好的准确性。

到目前为止，我们一直使用逻辑回归进行二元分类。我们能用它来处理多类案例吗？是的。然而，我们确实需要做一些小的调整。让我们在下一节看到这一点。

# 处理多类分类

最后一件值得注意的事情是逻辑回归算法如何处理多类分类。虽然我们在多类情况下与 scikit-learn 分类器的交互方式与在二进制情况下相同，但是理解逻辑回归在多类分类中的工作原理是很有用的。

超过两类的逻辑回归也被称为**多项式逻辑回归**，或者更广为人知的后来被称为**软最大回归**。正如您在二进制情况下看到的，模型由一个权重向量 *w* 表示，目标为 *1* 或正类的概率写如下:

<figure class="mediaobject">![](../Images/B16326_05_004.png)</figure>

在 *K* 类情况下，模型由 *K* 权向量、*w*T6】1、*w*T10】2 表示，...， *w* <sub xmlns:epub="http://www.idpf.org/2007/ops" class="" style="font-style: italic;">K</sub> ，目标为类 *k* 的概率写如下:

<figure class="mediaobject">![](../Images/B16326_05_031.png)</figure>

注意术语![](../Images/B16326_05_032.png)将概率![](../Images/B16326_05_033.png) ( *k* 从 *1* 到 *K* )归一化，使得它们总计为 *1* 。二进制情况下的成本函数表示如下:

<figure class="mediaobject">![](../Images/B16326_05_025.png)</figure>

同样，多类情况下的成本函数如下:

<figure class="mediaobject">![](../Images/B16326_05_035.png)</figure>

在这里，函数![](../Images/B16326_05_036.png)只有在![](../Images/B16326_05_037.png)为真时才为 *1* ，否则为 *0* 。

在定义了成本函数的情况下，我们获得了 *j* 权重向量的步长![](../Images/B16326_05_038.png)，就像我们在二进制情况下获得步长*⇼w*一样:

<figure class="mediaobject">![](../Images/B16326_05_039.png)</figure>

以类似的方式，在每次迭代中更新所有 *K* 权重向量。经过充分的迭代，学习到的权重向量，*w*<sub class="" style="font-style: italic;">1</sub>*w*<sub class="" style="font-style: italic;">2</sub>，...然后， *w* <sub class="" style="font-style: italic;">K</sub> 用于通过以下公式对新样品 *x* 进行分类:

<figure class="mediaobject">![](../Images/B16326_05_040.png)</figure>

为了更好地理解，让我们用一个经典的数据集进行实验，用于分类的手写数字:

```
>>> from sklearn import datasets
>>> digits = datasets.load_digits()
>>> n_samples = len(digits.images) 
```

由于图像数据存储在 8*8 矩阵中，我们需要将其展平，如下所示:

```
>>> X = digits.images.reshape((n_samples, -1))
>>> Y = digits.target 
```

然后，我们将数据拆分如下:

```
>>> from sklearn.model_selection import train_test_split
>>> X_train, X_test, Y_train, Y_test = train_test_split(X, Y, 
                                    test_size=0.2, random_state=42) 
```

然后，我们将网格搜索和交叉验证结合起来，找到最优的多类逻辑回归模型，如下所示:

```
>>> from sklearn.model_selection import GridSearchCV
>>> parameters = {'penalty': ['l2', None],
...               'alpha': [1e-07, 1e-06, 1e-05, 1e-04],
...               'eta0': [0.01, 0.1, 1, 10]}
>>> sgd_lr = SGDClassifier(loss='log', learning_rate='constant', 
                          eta0=0.01, fit_intercept=True, max_iter=10)
>>> grid_search = GridSearchCV(sgd_lr, parameters, 
                               n_jobs=-1, cv=3)
>>> grid_search.fit(term_docs_train, label_train)
>>> print(grid_search.best_params_)
{'alpha': 1e-07, 'eta0': 0.1, 'penalty': None} 
```

为了使用最优模型进行预测，我们应用以下内容:

```
>>> sgd_lr_best = grid_search.best_estimator_
>>> accuracy = sgd_lr_best.score(term_docs_test, label_test)
>>> print(f'The accuracy on testing set is: {accuracy*100:.1f}%')
The accuracy on testing set is: 94.2% 
```

看起来和前面的例子没什么不同，因为`SGDClassifier`内部处理多类。请随意计算混淆矩阵作为练习。看看模型在单个类上的表现会很有趣。

下一部分将是奖励部分，我们将使用 TensorFlow 实现逻辑回归，并以点击预测为例。

# 使用张量流实现逻辑回归

我们在此将前 30 万个样本的 90%用于训练，其余 10%用于测试，并假设`X_train_enc`、`Y_train`、`X_test_enc`和`Y_test`包含正确的数据:

1.  首先我们导入 TensorFlow，将`X_train_enc`、`X_test_enc`变换成`numpy`阵，施放`X_train_enc`、`Y_train`、`X_test_enc`、`Y_test`到【T7:

    ```
    >>> import tensorflow as tf
    >>> X_train_enc = X_train_enc.toarray().astype('float32')
    >>> X_test_enc = X_test_enc.toarray().astype('float32')
    >>> Y_train = Y_train.astype('float32')
    >>> Y_test = Y_test.astype('float32') 
    ```

2.  We use the `tf.data` API to shuffle and batch data:

    ```
    >>> batch_size = 1000
    >>> train_data = tf.data.Dataset.from_tensor_slices((X_train_enc, Y_train))
    >>> train_data = train_data.repeat().shuffle(5000).batch(batch_size).prefetch(1) 
    ```

    每次权重更新，只消耗**一批**样本，而不是一个样本或整个训练集。该模型基于一批样本计算的误差移动一步。在本例中，批次大小为 1，000。

3.  然后，我们定义逻辑回归模型的权重和偏差:

    ```
    >>> n_features = int(X_train_enc.shape[1])
    >>> W = tf.Variable(tf.zeros([n_features, 1]))
    >>> b = tf.Variable(tf.zeros([1])) 
    ```

4.  然后，我们创建一个梯度下降优化器，通过最小化损失来搜索最佳系数。我们在这里使用 Adam 作为优化器，它是一种高级梯度下降，具有自适应梯度的学习率(从 0.0008 开始):

    ```
    >>> learning_rate = 0.0008
    >>> optimizer = tf.optimizers.Adam(learning_rate) 
    ```

5.  We define the optimization process where we compute the current prediction and cost and update the model coefficients following the computed gradients:

    ```
    >>> def run_optimization(x, y):
    ...     with tf.GradientTape() as g:
    ...         logits = tf.add(tf.matmul(x, W), b)[:, 0]
    ...         cost =  
                tf.reduce_mean(
                tf.nn.sigmoid_cross_entropy_with_logits(
                 labels=y, logits=logits))
    ...     gradients = g.gradient(cost, [W, b])
    ...     optimizer.apply_gradients(zip(gradients, [W, b])) 
    ```

    这里，`tf.GradientTape`允许我们跟踪张量流计算，并计算给定变量的梯度。

6.  We run the training for 6,000 steps (one step is with one batch of random samples):

    ```
    >>> training_steps = 6000
    >>> for step, (batch_x, batch_y) in 
                  enumerate(train_data.take(training_steps), 1):
    ...     run_optimization(batch_x, batch_y)
    ...     if step % 500 == 0:
    ...         logits = tf.add(tf.matmul(batch_x, W), b)[:, 0]
    ...         loss = 
                tf.reduce_mean(
                tf.nn.sigmoid_cross_entropy_with_logits(
                 labels=batch_y, logits=logits))
    ...         print("step: %i, loss: %f" % (step, loss))
    step: 500, loss: 0.448672
    step: 1000, loss: 0.389186
    step: 1500, loss: 0.413012
    step: 2000, loss: 0.445663
    step: 2500, loss: 0.361000
    step: 3000, loss: 0.417154
    step: 3500, loss: 0.359435
    step: 4000, loss: 0.393363
    step: 4500, loss: 0.402097
    step: 5000, loss: 0.376734
    step: 5500, loss: 0.372981
    step: 6000, loss: 0.406973 
    ```

    每走 500 步，我们计算并打印出当前成本，以检查培训绩效。如你所见，训练损失总体上在减少。

7.  After the model is trained, we use it to make predictions on the testing set and report the AUC metric:

    ```
    >>> logits = tf.add(tf.matmul(X_test_enc, W), b)[:, 0]
    >>> pred = tf.nn.sigmoid(logits)
    >>> auc_metric = tf.keras.metrics.AUC()
    >>> auc_metric.update_state(Y_test, pred)
    >>> print(f'AUC on testing set: {auc_metric.result().numpy():.3f}')
    AUC on testing set: 0.771 
    ```

    我们能够通过基于张量流的逻辑回归模型实现`0.771`的`AUC`。您还可以调整学习速率、训练步骤数和其他超参数，以获得更好的性能。这将是本章结尾的一个有趣的练习。

您已经在上一节*使用 L1 正则化的特征选择*中看到了特征选择如何与 L1 正则化的逻辑回归一起工作，其中不重要特征的权重被压缩到接近或精确到 0。除了 L1 正则逻辑回归，随机森林是另一种常用的特征选择技术。让我们在下一节看到更多。

# 使用随机森林的特征选择

简单回顾一下，随机森林正在一组独立的决策树上打包。当在每个节点上搜索最佳分割点时，每个树考虑特征的随机子集。而且，在决策树中，只有那些重要的特征(以及它们的分裂值)被用来构成树节点。将森林视为一个整体:一个特征在树节点中使用的频率越高，它就越重要。换句话说，我们可以根据特征在所有树的节点中出现的次数来排列特征的重要性，并选择最重要的。

scikit-learn 中的一个训练好的`RandomForestClassifier`模块带有一个属性`feature_importances_`，表示特征重要性，它是以树节点中出现的比例来计算的。同样，我们将在具有 100，000 个 ad 点击样本的数据集上使用随机森林检查特征选择:

```
>>> from sklearn.ensemble import RandomForestClassifier
>>> random_forest = RandomForestClassifier(n_estimators=100, 
                 criterion='gini', min_samples_split=30, n_jobs=-1)
>>> random_forest.fit(X_train_enc.toarray(), Y_train) 
```

在拟合随机森林模型之后，我们获得特征重要性分数，如下所示:

```
>>> feature_imp = random_forest.feature_importances_
>>> print(feature_imp)
[1.60540750e-05 1.71248082e-03 9.64485853e-04 ... 5.41025913e-04
 7.78878273e-04 8.24041944e-03] 
```

看看下面的 10 个功能得分和相应的 10 个最不重要的功能:

```
>>> feature_names = enc.get_feature_names()
>>> print(np.sort(feature_imp)[:10])
[0\. 0\. 0\. 0\. 0\. 0\. 0\. 0\. 0\. 0.]
>>> bottom_10 = np.argsort(feature_imp)[:10]
>>> print('10 least important features are:\n', feature_names[bottom_10])
10 least important features are:
 ['x8_ea4912eb' 'x8_c2d34e02' 'x6_2d332391' 'x2_ca9b09d0' 
'x2_0273c5ad' 'x8_92bed2f3' 'x8_eb3f4b48' 'x3_535444a1' 'x8_8741c65a' 
'x8_46cb77e5'] 
```

现在，来看看的前 10 个特征得分和对应的 10 个最重要的特征:

```
>>> print(np.sort(feature_imp)[-10:])
[0.00809279 0.00824042 0.00885188 0.00897925 0.01080301 0.01088246
 0.01270395 0.01392431 0.01532718 0.01810339]
>>> top_10 = np.argsort(feature_imp)[-10:]
>>> print('10 most important features are:\n', feature_names[top_10])
10 most important features are:
 ['x17_-1' 'x18_157' 'x12_300' 'x13_250' 'x3_98572c79' 'x8_8a4875bd' 'x14_1993' 'x15_2' 'x2_d9750ee7' 'x18_33'] 
```

在本节中，我们介绍了随机森林如何用于特征选择。

# 摘要

在本章中，我们继续研究在线广告点击率预测项目。这一次，我们通过一键编码技术克服了分类特征的挑战。然后，我们求助于一种新的分类算法，逻辑回归，因为它对大数据集具有很高的可扩展性。对逻辑回归算法的深入讨论始于逻辑函数的引入，这导致了算法本身的机制。接下来是如何使用梯度下降训练逻辑回归模型。

在手工实现逻辑回归分类器并在我们的点进数据集上测试之后，您学习了如何使用 SGD 以更高级的方式训练逻辑回归模型，并且我们相应地调整了我们的算法。我们还练习了如何使用 scikit-learn 中基于 SGD 的逻辑回归分类器，并将其应用到我们的项目中。

然后，我们继续解决我们在使用逻辑回归时可能面临的问题，包括消除过拟合的 L1 和 L2 正则化、用于大规模数据集培训的在线学习技术以及处理多类场景。您还学习了如何使用 TensorFlow 实现逻辑回归。最后，本章以将随机森林模型应用于特征选择作为 L1 正则逻辑回归的替代方法而结束。

您可能很好奇，我们如何在 4000 万个样本的整个数据集上高效地训练模型。在下一章中，我们将利用 Spark 和`PySpark`模块等工具来扩展我们的解决方案。

# 练习

1.  在基于 logistic 回归的点进预测项目中，是否也可以在`SGDClassifier`模型中调整`penalty`、`eta0`、`alpha`等超参数？你能达到的最高测试 AUC 是多少？
2.  在在线学习解决方案中，您能否尝试使用更多的训练样本，例如 1000 万个样本？
3.  在基于 TensorFlow 的解决方案中，您能否调整学习速率、训练步骤数和其他超参数以获得更好的性能？