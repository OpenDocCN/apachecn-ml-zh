# Twelve

# 用卷积神经网络对服装图像进行分类

前一章总结了我们对通用和传统机器学习的最佳实践的介绍。从这一章开始，我们将深入探讨深度学习和强化学习的更高级主题。

当我们处理图像分类时，我们通常会将图像展平并获得像素向量，然后将其馈送给神经网络(或另一个模型)。尽管这可能会完成任务，但我们会丢失关键的空间信息。在本章中，我们将使用**卷积神经网络** ( **中枢神经系统**)从图像中提取丰富且可区分的表示。你会看到美国有线电视新闻网的表述是如何把一个“9”变成一个“9”，一个“4”变成一个“4”，一只猫变成一只猫，或者一只狗变成一只狗。

我们将从探索美国有线电视新闻网架构中的单个构件开始。然后，我们将在 TensorFlow 中开发一个 CNN 分类器来对服装图像进行分类，并解开卷积机制。最后，我们将引入数据增强来提升 CNN 模型的性能。

我们将在本章中讨论以下主题:

*   美国有线电视新闻网积木
*   分类用氯化萘
*   用 TensorFlow 和 Keras 实现中枢神经系统
*   用氯化萘对服装图像进行分类
*   卷积滤波器的可视化
*   数据扩充和实施

# 开始使用美国有线电视新闻网积木

虽然规则的隐藏层(我们到目前为止已经看到的完全连接的层)在从特定级别的数据中提取特征方面做得很好，但是这些表示在区分不同类别的图像时可能没有用。CNNs 可用于提取更丰富、更易区分的表示，例如，使汽车成为汽车，使飞机成为飞机，或手写字母“y”a“y”“z”a“z”等等。中枢神经系统是一种受人类视觉皮层生物启发的神经网络。为了揭开有线电视新闻网的神秘面纱，我将首先介绍典型有线电视新闻网的组成部分，包括卷积层、非线性层和汇聚层。

## 卷积层

**卷积层**是 CNN 中的第一层，或者如果 CNN 有多个卷积层，则是 CNN 中的前几层。它接收输入图像或矩阵，并通过对输入进行卷积运算来模拟神经元细胞对感受野的反应方式。数学上，它计算卷积层的节点和输入层中各个小区域之间的**点积**。小区域是感受野，卷积层的节点可以看作滤波器上的值。随着滤波器在输入层上移动，计算滤波器和当前感受野(子区域)之间的点积。在滤波器已经在所有子区域上卷积之后，获得称为**特征图**的新层。让我们看一个简单的例子，如下所示:

<figure class="mediaobject">![](../Images/B16326_12_01.png)</figure>

图 12.1:如何生成要素图

在本例中，图层 *l* 有 5 个节点，过滤器由 3 个节点组成【 *w* <sub class="Subscript--PACKT-">1</sub> 、 *w* <sub class="Subscript--PACKT-">2</sub> 、 *w* <sub class="Subscript--PACKT-">3</sub> 】。我们首先计算过滤器与图层 *l* 中前三个节点的点积，得到输出特征图中的第一个节点；然后，我们计算过滤器和中间三个节点之间的点积，并生成输出特征图中的第二个节点；最后第三个节点由层 *l* 中最后三个节点上的卷积生成。

现在，我们在下面的例子中仔细看看卷积是如何工作的:

<figure class="mediaobject">![](../Images/B16326_12_02.png)</figure>

图 12.2:卷积是如何工作的

在这个例子中，一个 3*3 的滤波器围绕一个 5*5 的输入矩阵从左上子区域滑动到右下子区域。对于每个子区域，使用过滤器计算点积。以左上角子区域(在橙色矩形中)为例:我们有 1 * 1 + 1 * 0 + 1 * 1 = 2，因此要素图中左上角节点(在左上角橙色矩形中)的值为 2。对于下一个最左边的子区域(在蓝色矩形中)，我们将卷积计算为 1 * 1 + 1 * 1 + 1 * 1 = 3，因此结果要素图中下一个节点(在中上蓝色矩形中)的值变为 3。最后，生成一个 3*3 的要素图。

那么我们用卷积层做什么呢？它们实际上用于提取边缘和曲线等特征。如果相应的感受野包含被过滤器识别的边缘或曲线，则输出特征图中的像素将具有高值。例如，在前面的例子中，过滤器描绘了一个反斜杠形状的“\”对角线边缘；蓝色矩形中的感受野包含类似的曲线，因此产生最高强度 3。但是，右上角的感受野不包含这样的反斜杠形状，因此它会在输出要素图中产生一个值为 0 的像素。卷积层充当曲线检测器或形状检测器。

此外，卷积层通常具有检测不同曲线和形状的多个滤波器。在前面的简单示例中，我们只应用了一个过滤器并生成了一个要素图，该图表明输入图像中的形状与过滤器中表示的曲线有多相似。为了从输入数据中检测更多的模式，我们可以使用更多的过滤器，例如水平、垂直曲线、30 度和直角形状。

此外，我们可以堆叠几个卷积层，以产生更高级别的表示，如整体形状和轮廓。链接更多的层将导致更大的感受野，能够捕捉更多的全局模式。

实际上，中枢神经系统，特别是它们的卷积层，模仿我们的视觉细胞的工作方式，如下所示:

*   我们的视觉皮层有一组复杂的神经元细胞，它们对视野的特定子区域敏感，这些子区域被称为**感受野**。例如，一些细胞只在有垂直边缘的情况下才作出反应；有些细胞只有暴露在水平边缘时才会着火；有些人在看到某个方向的边缘时反应更强烈。这些细胞被组织在一起，产生整个视觉感知，每个细胞都有一个特定的组成部分。美国有线电视新闻网的卷积层由一组过滤器组成，这些过滤器充当人类视觉皮层中的细胞。
*   一个简单的细胞只有当边缘样模式出现在它的感受子区域时才会有反应。一个更复杂的细胞对更大的子区域敏感，因此可以对整个视野中的边缘样模式做出反应。卷积层的堆叠是一堆复杂的单元，可以在更大的范围内检测模式。

就在每个卷积层之后，我们通常应用一个非线性层。

## 非线性层

非线性层基本上就是我们在*第八章*、*中看到的用人工神经网络*预测股价的激活层。显然，它是用来引入非线性的。回想一下在卷积层，我们只进行线性运算(乘法和加法)。不管一个神经网络有多少个线性隐藏层，它都将表现为一个单层感知器。因此，我们需要在卷积层之后立即进行非线性激活。同样，ReLU 是深度神经网络中非线性层最受欢迎的候选对象。

## 汇集层

通常在一个或多个卷积层之后(伴随着非线性激活)，我们可以直接使用导出的特征进行分类。例如，我们可以在多类分类的情况下应用 softmax 层。但是让我们先做一些数学。

给定 28 * 28 个输入图像，假设我们在第一个卷积层中应用 20 个 5 * 5 滤波器，我们将获得 20 个输出特征图，并且每个特征图层的大小为(28–5+1)*(28–5+1)= 24 * 24 = 576。这意味着作为下一层输入的要素数量从 784 个(28 * 28)增加到 11，520 个(20 * 576)。然后，我们在第二卷积层应用 50 个 5 * 5 滤波器。输出大小增长到 50 * 20 *(24–5+1)*(24–5+1)= 400，000。这比我们最初的 784 要高得多。我们可以看到，在最终的 softmax 层之前，每个卷积层的维数都会急剧增加。这可能会有问题，因为它很容易导致过度训练，更不用说训练如此大量的重量的成本了。

为了解决维度急剧增长的问题，我们通常在卷积和非线性层之后使用一个“T1”汇聚层“T2”。汇聚层也称为**下采样层**。可以想象，它降低了要素地图的维度。这是通过聚集子区域上的特征统计来完成的。典型的池化方法包括:

*   最大池，取所有非重叠子区域的最大值
*   平均池，取所有不重叠的子区域的平均值

在以下示例中，我们在 4 * 4 要素图上应用了 2 * 2 最大池过滤器，并输出了 2 * 2 过滤器:

<figure class="mediaobject">![](../Images/B16326_12_03.png)</figure>

图 12.3:最大池如何工作

除了降维，汇聚层还有另一个优势:平移不变性。这意味着即使输入矩阵经过少量的转换，它的输出也不会改变。例如，如果我们将输入图像向左或向右移动几个像素，只要最高像素在子区域中保持不变，最大池层的输出将仍然相同。换句话说，使用汇集层时，预测变得对位置不那么敏感。以下示例说明了 max pooling 如何实现平移不变性。

这是 4 * 4 原始图像，以及带有 2 * 2 过滤器的最大池输出:

<figure class="mediaobject">![](../Images/B16326_12_04.png)</figure>

图 12.4:原始图像和最大池的输出

如果我们将图像向右移动 1 像素，我们会得到以下移动的图像和相应的输出:

<figure class="mediaobject">![](../Images/B16326_12_05.png)</figure>

图 12.5:移位图像和输出

即使我们水平移动输入图像，我们也有相同的输出。合并图层增加了图像转换的鲁棒性。

你现在已经学会了美国有线电视新闻网的所有组成部分。比你想象的要简单，对吧？接下来让我们看看他们是如何组成美国有线电视新闻网的。

# 为分类设计美国有线电视新闻网

将三种类型的卷积相关层以及全连接层放在一起，我们可以构建 CNN 模型进行分类，如下所示:

<figure class="mediaobject">![](../Images/B16326_12_06.png)</figure>

图 12.6: CNN 架构

在这个例子中，输入图像首先被馈送到由一堆滤波器组成的卷积层(通过 ReLU 激活)。卷积滤波器的系数是可训练的。训练有素的初始卷积层能够导出输入图像的良好低级表示，这对于下游卷积层(如果有)以及下游分类任务至关重要。然后，池图层对每个生成的要素图进行下采样。

接下来，聚集的特征映射被馈送到第二卷积层。类似地，第二个池层减小了输出要素地图的大小。您可以根据需要链接任意多对卷积层和池层。第二(或更多，如果有的话)卷积层试图通过一系列从先前层导出的低级表示来组成高级表示，例如整体形状和轮廓。

到目前为止，特征图都是矩阵。在执行任何下游分类之前，我们需要将它们展平成一个向量。展平的要素仅被视为一个或多个完全连接的隐藏图层的输入。我们可以把美国有线电视新闻网想象成一个在常规神经网络之上的分层特征提取器。中枢神经系统非常适合利用强而独特的特征来区分图像。

如果我们处理一个二元分类问题，网络以一个逻辑函数结束，一个软最大值函数用于多类情况，或者一组逻辑函数用于多标签情况。

到现在你应该对 CNNs 有了很好的了解，应该准备好解决服装图像分类问题了。让我们从探索数据集开始。

# 探索服装图像数据集

服装时尚-MNIST([https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist))是来自 Zalando(欧洲最大的在线时尚零售商)的图像数据集。它由 6 万个训练样本和 1 万个测试样本组成。每个样本都是 28 * 28 灰度图像，与以下 10 个类别的标签相关联，每个类别代表一件衣服:

*   0: T 恤/上衣
*   1:裤子
*   2:套头衫
*   3:着装
*   4:外套
*   5:凉鞋
*   6:衬衫
*   7:运动鞋
*   8:包
*   9:踝靴

扎兰多试图让这个数据集像 MNIST 数据集()一样受欢迎，用于基准算法，因此称之为时尚-MNIST。

您可以使用 GitHub 链接从*获取数据*部分的直接链接下载数据集，或者直接从 Keras 导入数据集，Keras 已经包含数据集及其 API。我们将采用后一种方法，如下所示:

```py
>>> import tensorflow as tf
>>> fashion_mnist = tf.keras.datasets.fashion_mnist
>>> (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data() 
```

我们只需导入 TensorFlow 并从 Keras 模块加载时尚 MNIST。我们现在有了训练图像和它们的标签，以及测试图像和它们的标签。请随意打印这四个阵列中的一些样本，例如，如下所示的训练标签:

```py
>>> print(train_labels)
[9 0 0 ... 3 0 5] 
```

标签数组不包括类名。因此，我们将它们定义如下，并在以后用于绘图:

```py
>>> class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'] 
```

看一下图像数据的格式如下:

```py
>>> print(train_images.shape)
(60000, 28, 28) 
```

有 60，000 个训练样本，每个样本表示为 28 * 28 像素。

同样，对于 10，000 个测试样本，我们检查格式如下:

```py
>>> print(test_images.shape)
(10000, 28, 28) 
```

现在让我们检查一个随机训练样本，如下所示:

```py
>>> import matplotlib.pyplot as plt
>>> plt.figure()
>>> plt.imshow(train_images[42])
>>> plt.colorbar()
>>> plt.grid(False)
>>> plt.title(class_names[train_labels[42]])
>>> plt.show() 
```

最终结果如下图所示:

<figure class="mediaobject">![](../Images/B16326_12_07.png)</figure>

图 12.7:来自时尚 MNIST 的训练样本

您可能会遇到类似以下的错误:

```py
OMP: Error #15: Initializing libiomp5.dylib, but found libiomp5.dylib already initialized.
OMP: Hint This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can degrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see [http://www.intel.com/software/products/support/](http://www.intel.com/software/products/support/).
Abort trap: 6 
```

如果是，请在代码的开头添加以下代码:

```py
>>> import os
>>> os.environ['KMP_DUPLICATE_LIB_OK'] = 'True' 
```

在踝靴样本中，像素值在 0 到 255 的范围内。因此，在将数据输入神经网络之前，我们需要将数据重新缩放到 0 到 1 的范围。我们将训练样本和测试样本的值除以 255，如下所示:

```py
>>> train_images = train_images / 255.0
>>> test_images = test_images / 255.0 
```

现在，我们显示预处理后的前 16 个训练样本，如下所示:

```py
>>> for i in range(16):
...     plt.subplot(4, 4, i + 1)
...     plt.subplots_adjust(hspace=.3)
...     plt.xticks([])
...     plt.yticks([])
...     plt.grid(False)
...     plt.imshow(train_images[i], cmap=plt.cm.binary)
...     plt.title(class_names[train_labels[i]])
... plt.show() 
```

最终结果见下图:

<figure class="mediaobject">![](../Images/B16326_12_08.png)</figure>

图 12.8:最终结果

在下一节中，我们将构建我们的 CNN 模型来对这些服装图像进行分类。

# 用氯化萘对服装图像进行分类

如上所述， CNN 模型有两个主要组成部分:由一组卷积层和池层组成的特征提取器，以及类似于常规神经网络的分类器后端。

## 构建有线电视新闻网模型

由于 Keras 中的卷积层只接收三维的单个样本，我们需要首先将数据重塑为四维，如下所示:

```py
>>> X_train = train_images.reshape((train_images.shape[0], 28, 28, 1))
>>> X_test = test_images.reshape((test_images.shape[0], 28, 28, 1))
>>> print(X_train.shape)
(60000, 28, 28, 1) 
```

第一维是样本数，第四维是表示灰度图像的附加维。

在我们开发 CNN 模型之前，让我们在 TensorFlow 中指定随机种子，以实现再现性:

```py
>>> tf.random.set_seed(42) 
```

我们现在从 Keras 导入必要的模块，并初始化一个基于 Keras 的模型:

```py
>>> from tensorflow.keras import datasets, layers, models, losses
>>> model = models.Sequential() 
```

对于卷积提取器，我们将使用三个卷积层。我们从具有 32 个小尺寸 3 * 3 滤波器的第一卷积层开始。这由以下代码实现:

```py
>>> model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1))) 
```

请注意，我们使用 ReLU 作为激活函数。

卷积层之后是最大池层，带有 2 * 2 滤波器:

```py
>>> model.add(layers.MaxPooling2D((2, 2))) 
```

第二个卷积层来了。它有 64 个 3 * 3 过滤器，并带有 ReLU 激活功能:

```py
>>> model.add(layers.Conv2D(64, (3, 3), activation='relu')) 
```

第二个卷积层之后是另一个具有 2 * 2 滤波器的最大池层:

```py
>>> model.add(layers.MaxPooling2D((2, 2))) 
```

我们继续添加第三个卷积层。目前它有 128 个 3 * 3 过滤器:

```py
>>> model.add(layers.Conv2D(128, (3, 3), activation='relu')) 
```

生成的过滤器映射然后被展平，以向下游分类器后端提供特征:

```py
>>> model.add(layers.Flatten()) 
```

对于分类器后端，我们只使用一个 64 节点的隐藏层:

```py
>>> model.add(layers.Dense(64, activation='relu')) 
```

这里的隐藏层是规则的全连通密层，以 ReLU 作为激活函数。

最后，输出层有 10 个节点，在我们的例子中代表 10 个不同的类，以及一个 softmax 激活:

```py
>>> model.add(layers.Dense(10, activation='softmax')) 
```

现在，我们使用 Adam 作为优化器，交叉熵作为损失函数，分类精度作为度量来编译模型:

```py
>>> model.compile(optimizer='adam',
...               loss=losses.sparse_categorical_crossentropy,
...               metrics=['accuracy']) 
```

让我们看一下模型摘要，如下所示:

```py
>>> model.summary()
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d (Conv2D)              (None, 26, 26, 32)        320
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 3, 3, 128)         73856
_________________________________________________________________
flatten (Flatten)            (None, 1152)              0
_________________________________________________________________
dense (Dense)                (None, 64)                73792
_________________________________________________________________
dense_1 (Dense)              (None, 10)                650
=================================================================
Total params: 167,114
Trainable params: 167,114
Non-trainable params: 0
_________________________________________________________________ 
```

它显示模型中的每一层、其单个输出的形状以及其可训练参数的数量。您可能会注意到，卷积层的输出是三维的，其中前两个是特征图的维度，第三个是卷积层中使用的滤波器的数量。在该示例中，最大池输出的大小(前两个维度)是其输入要素图的一半。要素地图由汇集图层进行下采样。如果去掉所有池层，您可能想知道需要训练多少参数。其实是 4，058，314！因此，应用共享的好处是显而易见的:避免过度适配和降低培训成本。

你可能想知道为什么卷积滤波器的数量在各层不断增加。回想一下，每个卷积层都试图捕获特定层次的模式。第一个卷积层捕获低级模式，如边缘、点和曲线。然后，后续的层组合在前几层中提取的图案，以形成高级图案，例如形状和轮廓。当我们在这些卷积层中前进时，在大多数情况下有越来越多的模式组合需要捕获。因此，我们需要不断增加(或至少不减少)卷积层中的滤波器数量。

## 拟合美国有线电视新闻网模型

现在是时候训练我们刚刚建立的模型了。我们对其进行 10 次迭代训练，并使用测试样本进行评估:

```py
>>> model.fit(X_train, train_labels, validation_data=(X_test, test_labels), epochs=10) 
```

请注意，默认情况下，批处理大小为 32。以下是培训的进展情况:

```py
Train on 60000 samples, validate on 10000 samples
Epoch 1/10
60000/60000 [==============================] - 68s 1ms/sample - loss: 0.4703 - accuracy: 0.8259 - val_loss: 0.3586 - val_accuracy: 0.8706
Epoch 2/10
60000/60000 [==============================] - 68s 1ms/sample - loss: 0.3056 - accuracy: 0.8882 - val_loss: 0.3391 - val_accuracy: 0.8783
Epoch 3/10
60000/60000 [==============================] - 69s 1ms/sample - loss: 0.2615 - accuracy: 0.9026 - val_loss: 0.2655 - val_accuracy: 0.9028
Epoch 4/10
60000/60000 [==============================] - 69s 1ms/sample - loss: 0.2304 - accuracy: 0.9143 - val_loss: 0.2506 - val_accuracy: 0.9096
Epoch 5/10
60000/60000 [==============================] - 69s 1ms/sample - loss: 0.2049 - accuracy: 0.9233 - val_loss: 0.2556 - val_accuracy: 0.9058
Epoch 6/10
60000/60000 [==============================] - 71s 1ms/sample - loss: 0.1828 - accuracy: 0.9312 - val_loss: 0.2497 - val_accuracy: 0.9122
Epoch 7/10
60000/60000 [==============================] - 68s 1ms/sample - loss: 0.1638 - accuracy: 0.9386 - val_loss: 0.3006 - val_accuracy: 0.9002
Epoch 8/10
60000/60000 [==============================] - 70s 1ms/sample - loss: 0.1453 - accuracy: 0.9455 - val_loss: 0.2662 - val_accuracy: 0.9119
Epoch 9/10
60000/60000 [==============================] - 69s 1ms/sample - loss: 0.1301 - accuracy: 0.9506 - val_loss: 0.2885 - val_accuracy: 0.9057
Epoch 10/10
60000/60000 [==============================] - 68s 1ms/sample - loss: 0.1163 - accuracy: 0.9559 - val_loss: 0.3081 - val_accuracy: 0.9100
10000/1 - 5s - loss: 0.2933 - accuracy: 0.9100 
```

我们能够在训练集上达到大约 96%的准确率，在测试集上达到 91%。

如果您想再次检查测试集的性能，可以执行以下操作:

```py
>>> test_loss, test_acc = model.evaluate(X_test, test_labels, verbose=2)
>>> print('Accuracy on test set:', test_acc)
Accuracy on test set: 0.91 
```

现在我们有了一个训练有素的模型，我们可以使用以下代码对测试集进行预测:

```py
>>> predictions = model.predict(X_test) 
```

看一看第一个样本；我们的预测如下:

```py
>>> print(predictions[0])
[1.8473367e-11 1.1924335e-07 1.0303306e-13 1.2061150e-12 3.1937938e-07
 3.5260896e-07 6.2364621e-13 9.1853758e-07 4.0739218e-11 9.9999821e-01] 
```

我们有这个样本的预测概率。为了获得预测标签，我们执行以下操作:

```py
>>> import numpy as np
>>> print('Predicted label for the first test sample: ', np.argmax(predictions[0]))
Predicted label for the first test sample: 9 
```

我们做了如下事实核查:

```py
>>> print('True label for the first test sample: ',test_labels[0])
True label for the first test sample: 9 
```

我们进一步绘制了样本图像和预测结果，包括 10 种可能类别的概率:

```py
>>> def plot_image_prediction(i, images, predictions, labels, class_names):
...     plt.subplot(1,2,1)
...     plt.imshow(images[i], cmap=plt.cm.binary)
...     prediction = np.argmax(predictions[i])
...     color = 'blue' if prediction == labels[i] else 'red'
...     plt.title(f"{class_names[labels[i]]} (predicted 
            {class_names[prediction]})", color=color)
...     plt.subplot(1,2,2)
...     plt.grid(False)
...     plt.xticks(range(10))
...     plot = plt.bar(range(10), predictions[i], color="#777777")
...     plt.ylim([0, 1])
...     plot[prediction].set_color('red')
...     plot[labels[i]].set_color('blue')
...     plt.show() 
```

原始图像(左侧)的标题为 *<真实标签>(预测<预测标签> )* 如果预测与标签匹配，则为蓝色，否则为红色。预测概率(右侧)将是真实标签上的蓝色条，或者如果预测标签与真实标签不同，则是预测标签上的红色条。

让我们用第一个测试样本来试试:

```py
>>> plot_image_prediction(0, test_images, predictions, test_labels, class_names) 
```

有关最终结果，请参考以下屏幕截图:

<figure class="mediaobject">![](../Images/B16326_12_09.png)</figure>

图 12.9:原始图像样本及其预测结果

随意玩弄其他样本，尤其是那些预测不准确的，比如第 17 项。

您已经看到了训练好的模型的表现，您可能会想知道学习过的卷积滤波器是什么样子的。你将在下一部分找到答案。

## 可视化卷积滤波器

我们从训练好的模型中提取卷积滤波器，并按照以下步骤进行可视化:

1.  从模型总结中，我们知道模型中索引 0、2 和 4 的层是卷积层。以第二卷积层为例，我们得到它的滤波器如下:

    ```py
    >>> filters, _ = model.layers[2].get_weights() 
    ```

2.  接下来，我们将过滤器值标准化到 0 到 1 的范围，这样我们可以更容易地可视化它们:

    ```py
    >>> f_min, f_max = filters.min(), filters.max()
    >>> filters = (filters - f_min) / (f_max - f_min) 
    ```

3.  Recall we have 64 filters in this convolutional layer. We visualize the first 16 filters in four rows and four columns:

    ```py
    >>> n_filters = 16
    >>> for i in range(n_filters):
    ...     filter = filters[:, :, :, i]
    ...     plt.subplot(4, 4, i+1)
    ...     plt.xticks([])
    ...     plt.yticks([])
    ...     plt.imshow(filter[:, :, 0], cmap='gray')
    ... plt.show() 
    ```

    最终结果参见下面的截图:

    <figure class="mediaobject">![](../Images/B16326_12_10.png)</figure>

图 12.10:训练好的卷积滤波器

在卷积滤波器中，黑色方块表示小权重，白色方块表示大权重。基于这种直觉，我们可以看到，第二行的第二个滤镜检测到了一个感受野的垂直线，而第一行的第三个滤镜检测到了一个从右下方的亮到左上方的暗的渐变。

在前面的例子中，我们用 60，000 个标记样本训练了服装图像分类器。然而，现实中要收集这么大的标注数据集并不容易。具体来说，图像标记既昂贵又耗时。如何用有限的样本有效训练图像分类器？一个解决方案是数据扩充。

# 用数据扩充增强美国有线电视新闻网分类器

**数据扩充**意味着扩展现有训练数据集的大小，以提高泛化性能。它克服了收集和标记更多数据的成本。在 TensorFlow 中，我们使用来自 Keras API 的 ImageDataGenerator 模块([https://www . TensorFlow . org/API _ docs/python/TF/Keras/预处理/image/ImageDataGenerator](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) )实时实现图像增强。

## 数据增强的水平翻转

有很多方法可以扩充图像数据。最简单的可能是水平或垂直翻转图像。例如，如果我们水平翻转一个现有的图像，我们将有一个新的图像。要生成水平图像，我们应该创建图像数据生成器，如下所示:

```py
>>> import os
>>> from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img 
>>> da tagen = ImageDataGenerator(horizontal_flip=True) 
```

我们将使用这个生成器创建被操纵的图像。现在，我们首先开发一个实用函数来生成给定增强图像生成器的图像，并按如下方式显示它们:

```py
>>> def generate_plot_pics(datagen, original_img, save_prefix):
...     folder = 'aug_images'
...     i = 0
...     for batch in datagen.flow(original_img.reshape(
                                   (1, 28, 28, 1)),
...                               batch_size=1,
...                               save_to_dir=folder,
...                               save_prefix=save_prefix,
...                               save_format='jpeg'):
...         i += 1
...         if i > 2:
...             break
...     plt.subplot(2, 2, 1, xticks=[],yticks=[])
...     plt.imshow(original_img)
...     plt.title("Original")
...     i = 1
...     for file in os.listdir(folder):
...         if file.startswith(save_prefix):
...             plt.subplot(2, 2, i + 1, xticks=[],yticks=[])
...             aug_img = load_img(folder + "/" + file)
...             plt.imshow(aug_img)
...             plt.title(f"Augmented {i}")
...             i += 1
...     plt.show() 
```

生成器首先在给定原始图像和增强条件的情况下随机生成三个(在本例中)图像。然后，该函数绘制原始图像和三幅人工图像。生成的图像也存储在本地磁盘名为`aug_images`的文件夹中。

让我们使用第一个训练图像(请随意使用任何其他图像)来试用我们的`horizontal_flip generator`，如下所示:

```py
>>> generate_plot_pics(datagen, train_images[0], 'horizontal_flip') 
```

有关最终结果，请参考以下屏幕截图:

<figure class="mediaobject">![](../Images/B16326_12_11.png)</figure>

图 12.11:用于数据增强的水平翻转图像

正如你看到的，生成的图像不是水平翻转就是不翻转。为什么我们不试试水平和垂直翻转同时进行的呢？我们可以这样做:

```py
>>> datagen = ImageDataGenerator(horizontal_flip=True,
...                              vertical_flip=True)
>>> generate_plot_pics(datagen, train_images[0], 'hv_flip') 
```

有关最终结果，请参考以下屏幕截图:

<figure class="mediaobject">![](../Images/B16326_12_12.png)</figure>

图 12.12:用于数据增强的水平和垂直翻转图像

除了是否水平翻转外，生成的图像要么垂直翻转，要么不翻转。

一般来说，水平翻转的图像传达了与原始图像相同的信息。垂直翻转的图像并不常见。还值得注意的是，翻转只在对方向不敏感的情况下起作用，例如对猫和狗进行分类或识别汽车的部件。相反，在方向很重要的情况下这样做是危险的，比如在右转和左转标志之间进行分类。

## 数据扩充的轮换

不像在水平或垂直翻转中那样每 90 度旋转一次，也可以在图像数据增强中应用小到中等程度的旋转。让我们看看下面例子中的旋转:

```py
>>> datagen = ImageDataGenerator(rotation_range=30)
>>> generate_plot_pics(datagen, train_images[0], 'rotation') 
```

有关最终结果，请参考以下屏幕截图:

<figure class="mediaobject">![](../Images/B16326_12_13.png)</figure>

图 12.13:用于数据增强的旋转图像

在前面的示例中，图像旋转了-30 度(逆时针)到 30 度(顺时针)的任意角度。

## 为数据扩充而转移

换挡是另一种常用的增力方法。它通过将原始图像水平或垂直移动少量像素来生成新图像。在 TensorFlow 中，您可以指定图像移动的最大像素数，也可以指定权重或高度的最大部分。让我们看一下下面的例子，我们将图像水平移动最多 8 个像素:

```py
>>> datagen = ImageDataGenerator(width_shift_range=8)
>>> generate_plot_pics(datagen, train_images[0], 'width_shift') 
```

有关最终结果，请参考以下屏幕截图:

<figure class="mediaobject">![](../Images/B16326_12_14.png)</figure>

12.14:用于数据增强的水平移位图像

如您所见，生成的图像水平移动不超过 8 个像素。现在让我们尝试同时水平和垂直移动:

```py
>>> datagen = ImageDataGenerator(width_shift_range=8,
...                              height_shift_range=8)
>>> generate_plot_pics(datagen, train_images[0], 'width_height_shift') 
```

有关最终结果，请参考以下屏幕截图:

<figure class="mediaobject">![](../Images/B16326_12_15.png)</figure>

图 12.15:用于数据增强的水平和垂直移动图像

# 用数据扩充改进服装图像分类器

用几种常见的增强方法武装，我们现在应用它们在小数据集上训练我们的图像分类器，步骤如下:

1.  We start by constructing a small training set:

    ```py
    >>> n_small = 500
    >>> X_train = X_train[:n_small]
    >>> train_labels = train_labels[:n_small]
    >>> print(X_train.shape)
    (500, 28, 28, 1) 
    ```

    我们只使用 500 个样本进行训练。

2.  We architect the CNN model using the Keras Sequential API:

    ```py
    >>> model = models.Sequential()
    >>> model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))
    >>> model.add(layers.MaxPooling2D((2, 2)))
    >>> model.add(layers.Conv2D(64, (3, 3), activation='relu'))
    >>> model.add(layers.Flatten())
    >>> model.add(layers.Dense(32, activation='relu'))
    >>> model.add(layers.Dense(10, activation='softmax')) 
    ```

    由于我们有小尺寸的训练数据，所以我们只使用两个卷积层，并相应地调整隐藏层的尺寸:第一个卷积层有 32 个小尺寸的 3 * 3 滤波器，第二个卷积层有 64 个相同尺寸的滤波器，全连通隐藏层有 32 个节点。

3.  我们用 Adam 作为优化器，交叉熵作为损失函数，分类精度作为度量来编译模型:

    ```py
    >>> model.compile(optimizer='adam',
    ...               loss=losses.sparse_categorical_crossentropy,
    ...               metrics=['accuracy']) 
    ```

4.  We first train the model without data augmentation:

    ```py
    >>> model.fit(X_train, train_labels, validation_data=(X_test, test_labels), epochs=20, batch_size=40)
    Train on 500 samples, validate on 10000 samples
    Epoch 1/20
    500/500 [==============================] - 6s 11ms/sample - loss: 1.8791 - accuracy: 0.3200 - val_loss: 1.3738 - val_accuracy: 0.4288
    Epoch 2/20
    500/500 [==============================] - 4s 8ms/sample - loss: 1.1363 - accuracy: 0.6100 - val_loss: 1.0929 - val_accuracy: 0.6198
    Epoch 3/20
    500/500 [==============================] - 4s 9ms/sample - loss: 0.8669 - accuracy: 0.7140 - val_loss: 0.9237 - val_accuracy: 0.6753
    ……
    ……
    Epoch 18/20
    500/500 [==============================] - 5s 10ms/sample - loss: 0.1372 - accuracy: 0.9640 - val_loss: 0.7142 - val_
    accuracy: 0.7947
    Epoch 19/20
    500/500 [==============================] - 5s 10ms/sample - loss: 0.1195 - accuracy: 0.9600 - val_loss: 0.6885 - val_accuracy: 0.7982
    Epoch 20/20
    500/500 [==============================] - 5s 10ms/sample - loss: 0.0944 - accuracy: 0.9780 - val_loss: 0.7342 - val_accuracy: 0.7924 
    ```

    我们训练模型 20 次迭代。

5.  Let's see how it performs on the test set:

    ```py
    >>> test_loss, test_acc = model.evaluate(X_test, test_labels, verbose=2)
    >>> print('Accuracy on test set:', test_acc)
       Accuracy on test set: 0.7924 
    ```

    未增加数据的模型在测试集上的分类准确率为 79.24%。

6.  Now we work on the data augmentation and see if it can boost the performance. We first define the augmented data generator:

    ```py
    >>> datagen = ImageDataGenerator(height_shift_range=3,
    ...                              horizontal_flip=True
    ...                              ) 
    ```

    我们在此应用水平翻转和垂直移动。我们注意到没有一个服装图像是颠倒的，因此垂直翻转不会提供任何正常外观的图像。此外，大多数服装图像完全水平居中，所以我们不会执行任何宽度移动。简单地说，我们尽量避免创建看起来与原始图像不同的增强图像。

7.  We clone the CNN model we used previously:

    ```py
    >>> model_aug = tf.keras.models.clone_model(model) 
    ```

    它只是复制了 CNN 架构，并创建了新的权重，而不是共享现有模型的权重。

    我们像以前一样编译克隆模型，用 Adam 作为优化器，交叉熵作为损失函数，分类精度作为度量:

    ```py
    >>> model_aug.compile(optimizer='adam',
    ...               loss=losses.sparse_categorical_crossentropy,
    ...               metrics=['accuracy']) 
    ```

8.  Finally, we fit this CNN model on data with real-time augmentation:

    ```py
    >>> train_generator = datagen.flow(X_train, train_labels, seed=42, batch_size=40)
    >>> model_aug.fit(train_generator, epochs=50, validation_data=(X_test, test_labels))
    Epoch 1/50
    13/13 [==============================] - 5s 374ms/step - loss: 2.2150 - accuracy: 0.2060 - val_loss: 2.0099 - val_accuracy: 0.3104
    ……
    ……
    Epoch 48/50
    13/13 [==============================] - 4s 300ms/step - loss: 0.1541 - accuracy: 0.9460 - val_loss: 0.7367 - val_accuracy: 0.8003
    Epoch 49/50
    13/13 [==============================] - 4s 304ms/step - loss: 0.1487 - accuracy: 0.9340 - val_loss: 0.7211 - val_accuracy: 0.8035
    Epoch 50/50
    13/13 [==============================] - 4s 306ms/step - loss: 0.1031 - accuracy: 0.9680 - val_loss: 0.7446 - val_accuracy: 0.8109 
    ```

    在训练过程中，动态随机生成增强的图像，为模型提供素材。这次我们用数据扩充训练模型 50 次迭代，因为模型需要更多的迭代来学习模式。

9.  Let's see how it performs on the test set:

    ```py
    >>> test_loss, test_acc = model_aug.evaluate(X_test, test_labels, verbose=2)
    >>> print('Accuracy on test set:', test_acc)
       Accuracy on test set: 0.8109 
    ```

    随着数据的增加，准确率从 79.24%提高到 81.09%。

请随意微调超参数，就像我们在*第 8 章*、*中用人工神经网络*预测股价一样，看看能否进一步提高分类性能。

# 摘要

在这一章中，我们致力于使用中枢神经系统对服装图像进行分类。我们首先详细解释了有线电视新闻网模型的各个组成部分，并了解了有线电视新闻网是如何受到我们视觉细胞工作方式的启发的。然后我们开发了一个美国有线电视新闻网模型来分类来自扎兰多的时尚 MNIST 服装图片。我们还讨论了数据增强和几种流行的图像增强方法。我们用 TensorFlow 中的 Keras 模块再次练习实现深度学习模型。

在下一章中，我们将关注另一种深度学习网络:**递归神经网络** ( **RNNs** )。CNNs 和 RNNs 是两个最强大的深度神经网络，它们让深度学习在当今如此流行。

# 练习

1.  如前所述，能否尝试微调 CNN 图像分类器，看看能否击败我们已经取得的成绩？
2.  你也可以使用辍学和提前停止技术吗？