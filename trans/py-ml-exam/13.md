# Thirteen

# 用递归神经网络进行序列预测

在前一章中，我们重点介绍了**卷积神经网络** ( **CNNs** )并将其用于处理图像相关任务。在这一章中，我们将探索**递归神经网络** ( **RNNs** )，它们适用于顺序数据和时间相关数据，例如每日温度、DNA 序列和顾客随时间的购物交易。您将学习循环架构是如何工作的，并看到模型的变体。然后我们将研究它们的应用，包括情感分析和文本生成。最后，作为一个额外的部分，我们将介绍一个最新的顺序学习模型:Transformer。

我们将在本章中讨论以下主题:

*   RNNs 的顺序学习
*   神经网络的机制和培训
*   不同类型的无线网络
*   长短期记忆神经网络
*   情感分析的神经网络
*   用于文本生成的无线网络
*   自我关注和变压器模型

# 引入顺序学习

到目前为止，我们在本书中解决的机器学习问题都是与时间无关的。例如，在我们以前的方法下，广告点击率不依赖于用户的历史广告点击量；在人脸分类中，模型只接受当前人脸图像，而不接受以前的人脸图像。然而，生活中有很多情况取决于时间。比如在金融欺诈检测中，不能只看现在的交易；我们还应该考虑以前的交易，这样我们就可以根据它们的差异进行建模。另一个例子是**词性** ( **PoS** )标注，我们给一个单词分配一个 PoS(动词、名词、副词等)。不要只关注给定的单词，我们必须看前面的单词，有时也要看后面的单词。

在像刚才提到的依赖于时间的情况下，当前输出不仅依赖于当前输入，还依赖于之前的输入；请注意，先前输入的长度不是固定的。用机器学习解决这样的问题叫做**序列学习**，或者**序列建模**。显然，与时间相关的事件叫做序列。除了在不相交的时间间隔内发生的事件(如金融交易、电话等)，文本、语音和视频也是顺序数据。

你可能想知道为什么我们不能通过输入整个序列来以常规方式对序列数据建模。这可能非常有限，因为我们必须固定输入大小。一个问题是，如果一个重要事件位于固定窗口之外，我们将丢失信息。但是我们能不能用一个非常大的时间窗口？请注意，特征空间随窗口大小而增长。如果我们想在某个时间窗口覆盖足够多的事件，特征空间就会变得过大。因此，过度拟合可能是另一个问题。

我希望你现在明白为什么我们需要以不同的方式对顺序数据建模。在下一节中，我们将讨论用于现代序列学习的模型:RNNs。

# 通过实例学习 RNN 建筑

可以想象，rnn 因其循环机制而脱颖而出。我们将在下一节详细解释这一点。之后我们将讨论不同类型的无线网络，以及一些典型的应用。

## 循环机制

回想一下，在前馈网络(如香草神经网络和 CNNs)中，数据是单向移动的，从输入层到输出层。在 RNNs 中，循环架构允许数据循环回到输入层。这意味着数据不限于前馈方向。具体来说，在 RNN 的隐藏层中，来自先前时间点的输出将成为当前时间点的输入的一部分。下图说明了数据在 RNN 总体上是如何流动的:

<figure class="mediaobject">![](../Images/B16326_13_01.png)</figure>

图 13.1:RNN 的一般形式

这样的递归架构使得 RNNs 可以很好地处理顺序数据，包括时间序列(如每日温度、每日产品销售和临床脑电图记录)和一般的顺序连续数据(如句子中的单词、DNA 序列等)。以一款金融诈骗检测仪为例；前一个事务的输出特征进入当前事务的训练。最后，对一个事务的预测取决于它以前的所有事务。让我用数学和视觉的方式来解释这个循环机制。

假设我们有一些输入， *x* <sub class="" style="font-style: italic;">t</sub> 。这里， *t* 代表时间步长或顺序。在前馈神经网络中，我们简单假设不同 *t* 处的输入相互独立。我们表示一个隐藏层在一个时间步长的输出， *t* ，为*h*<sub class="" style="font-style: italic;">t</sub>=*f*(*x*<sub class="" style="font-style: italic;">t</sub>)，其中 *f* 是隐藏层的抽象。

如下图所示:

<figure class="mediaobject">![](../Images/B16326_13_02.png)</figure>

图 13.2:前馈神经网络的一般形式

相反，RNN 中的反馈循环将先前状态的信息馈送到当前状态。一次一个 RNN 的隐藏层的输出 *t* ，可以表示为*h*T5=*f*(*h*T11】tT13】1，*x*T17】t)。如下图所示:

<figure class="mediaobject">![](../Images/B16326_13_03.png)</figure>

图 13.3:展开的循环层随时间的变化

相同的任务 *f* 在序列的每个元素上执行，输出 *h* <sub class="" style="font-style: italic;">t</sub> 依赖于先前计算生成的输出*h*<sub class="" style="font-style: italic;">t</sub><sub class="Subscript--PACKT-">1</sub>。链状架构捕捉到了迄今为止计算出的“记忆”。这就是 RNNs 在处理顺序数据方面如此成功的原因。

此外，由于递归架构，rnn 在处理输入序列和/或输出序列的不同组合时也有很大的灵活性。在下一节中，我们将讨论基于输入和输出的不同类别的无线网络，包括以下内容:

*   多对一
*   一对多
*   多对多(同步)
*   多对多(非同步)

我们将从查看多对一 RNNs 开始。

## 多对一无线网络

RNN 最直观的类型可能是多对一的 T2。一个多对一 RNN 可以有输入序列，你想要多少个时间步长就有多少个，但是它在完成整个序列后只产生一个输出。下图描述了多对一 RNN 的总体结构:

<figure class="mediaobject">![](../Images/B16326_13_04.png)</figure>

图 13.4:多对一 RNN 的一般形式

这里， *f* 代表一个或多个循环隐藏层，其中一个单独的层从之前的时间步长中获取自己的输出。下面是三个隐藏层叠加的示例:

<figure class="mediaobject">![](../Images/B16326_13_05.png)</figure>

图 13.5:三个循环层叠加的例子

多对一 RNNs 广泛用于对顺序数据进行分类。情绪分析就是一个很好的例子，例如，RNN 阅读整个客户评论，并给情绪评分(正面、中立或负面情绪)。同样，我们也可以在新闻文章的主题分类中使用这种 RNNs。识别歌曲的流派是另一个应用，因为模型可以读取整个音频流。我们还可以使用多对一的神经网络来根据脑电图轨迹确定患者是否癫痫发作。

## 一对多无线网络

**一对多**rnn与多对一 rnn 完全相反。它们只接收一个输入(不是一个序列)并产生一个输出序列。下图显示了典型的一对多 RNN:

<figure class="mediaobject">![](../Images/B16326_13_06.png)</figure>

图 13.6:一对多 RNN 的一般形式

同样， *f* 代表一个或多个重复出现的隐藏层。

请注意，这里的“一”并不意味着只有一个输入特征。这意味着输入来自一个时间步长，或者是与时间无关的。

一对多 rnn通常用作序列发生器。例如，我们可以给定一个起始音符或/和一个流派来生成一首音乐。同样，我们也可以像专业编剧一样，用一对多的 rnn，用我们指定的起始词来写电影剧本。图像字幕是另一个有趣的应用:RNN 接收图像并输出图像的描述(一句话)。

## 多对多(同步)无线网络

第三种类型的 RNN，多对多(同步)，允许输入序列中的每个元素都有一个输出。让我们看看数据在以下多对多(同步)RNN 中是如何流动的:

<figure class="mediaobject">![](../Images/B16326_13_07.png)</figure>

图 13.7:多对多(同步)RNN 的一般形式

如您所见，每个输出都是基于其相应的输入和所有先前的输出计算的。

这种类型的 RNN 的一个常见用途是时间序列预测，其中我们希望基于当前和先前观察到的数据在每个时间步长执行滚动预测。以下是一些时间序列预测的示例，我们可以在其中利用同步的多对多网络:

*   商店每天的产品销售
*   股票的每日收盘价
*   工厂每小时的耗电量

它们也被广泛用于解决自然语言处理问题，包括词性标注、命名实体识别和实时语音识别。

## 多对多(非同步)无线网络

有时候，我们只需要在处理完整个输入序列后生成输出序列*。这是多对多 RNN 的**非同步**版本。*

关于多对多(非同步)RNN 的一般结构，请参考下图:

<figure class="mediaobject">![](../Images/B16326_13_08.png)</figure>

图 13.8:多对多(非同步)RNN 的一般形式

注意，输出序列的长度(上图中的 T *y* )可以不同于输入序列的长度(上图中的 T *x* )。这为我们提供了一些灵活性。

这种类型的 RNN 是机器翻译的首选模型。例如，在法英翻译中，模型首先用法语阅读一个完整的句子，然后产生一个用英语翻译的句子。多步提前预测是另一个流行的例子:有时，当给定过去一个月的数据时，我们被要求预测未来多天的销售额。

现在，您已经基于模型的输入和输出了解了四种类型的 RNN。

等等，一对一 RNNs 怎么样？没有这回事。一对一只是一个常规的前馈模型。

在本章的后面，我们将应用这些类型的 RNN 来解决项目，包括情感分析和单词生成。现在，让我们弄清楚 RNN 模型是如何训练的。

# 训练 RNN 模型

为了解释我们如何优化 RNN 的权重(参数)，我们首先注释网络上的权重和数据，如下所示:

*   *U* 表示连接输入层和隐藏层的权重。
*   *V* 表示隐藏层和输出层之间的权重。请注意，为了简单起见，我们只使用了一个循环层。
*   *W* 表示重现层的权重；也就是反馈层。
*   *x* <sub class="" style="font-style: italic;">t</sub> 表示时间步长 *t* 的输入。
*   *s* <sub class="" style="font-style: italic;">t</sub> 表示时间步长 *t* 时的隐藏状态。
*   *h* <sub class="" style="font-style: italic;">t</sub> 表示时间步长 *t* 的输出。

接下来，我们通过三个时间步骤展开简单的 RNN 模型:*t*1、 *t* 和 *t* + 1，如下所示:

<figure class="mediaobject">![](../Images/B16326_13_09.png)</figure>

图 13.9:展开循环层

我们将各层之间的数学关系描述如下:

*   我们让 *a* 表示隐藏层的激活函数。在 RNNs 中，我们通常选择 tanh 或 ReLU 作为隐藏层的激活函数。
*   给定当前输入， *x* <sub xmlns:epub="http://www.idpf.org/2007/ops" class="" style="font-style: italic;">t</sub> ，以及之前的隐藏状态，*s*<sub xmlns:epub="http://www.idpf.org/2007/ops" class="" style="font-style: italic;">t</sub><sub xmlns:epub="http://www.idpf.org/2007/ops" class="Subscript--PACKT-">1</sub>，我们计算当前隐藏状态， *s* <sub xmlns:epub="http://www.idpf.org/2007/ops" class="" style="font-style: italic;">t</sub> ，由*s*<sub xmlns:epub="http://www.idpf.org/2007/ops" class="" style="font-style: italic;">t</sub>=*a*(*Ux*T22】t+*Ws 请随意再次阅读*第八章*、*用人工神经网络预测股价*来复习你的神经网络知识。*
*   以类似的方式，我们基于![](../Images/B16326_13_001.png)计算*s*<sub class="" style="font-style: italic;">t</sub><sub class="Subscript--PACKT-">1</sub>我们重复这一过程，直到 *s* <sub class="Subscript--PACKT-">1</sub> ，这取决于![](../Images/B16326_13_002.png)我们通常将 *s* <sub class="Subscript--PACKT-">0</sub> 设置为全零。
*   我们让 *g* 表示输出层的激活函数。如果我们想要执行二进制分类，它可以是 sigmoid 函数，用于多类分类的 softmax 函数，以及用于回归的简单线性函数(即无激活)。
*   最后，我们计算时间步长 *t* 、![](../Images/B16326_13_003.png)的输出。

随着时间的推移，依赖关系处于隐藏状态(即 *s* <sub class="" style="font-style: italic;">t</sub> 依赖于*s*<sub class="" style="font-style: italic;">t</sub><sub class="Subscript--PACKT-">1</sub>，*s*<sub class="" style="font-style: italic;">t</sub><sub class="Subscript--PACKT-">1</sub>依赖于*s*<sub class="" style="font-style: italic;">t</sub><sub class="Subscript--PACKT-">—2</sub>等等)，递归层给网络带来了内存，网络从所有

正如我们对传统的神经网络所做的那样，我们应用反向传播算法来优化 RNNs 中的所有权重， *U* 、 *V* 和 *W* 。然而，您可能已经注意到，一个时间步长的输出间接依赖于所有先前的时间步长( *h* <sub class="" style="font-style: italic;">t</sub> 依赖于 *s* <sub class="" style="font-style: italic;">t</sub> ，而 *s* <sub class="" style="font-style: italic;">t</sub> 依赖于所有先前的时间步长)。因此，除了当前时间步长之外，我们还需要计算所有先前 *t* -1 时间步长的损失。因此，权重的梯度是这样计算的。例如，如果我们想要计算时间步长 *t* = 4 处的梯度，我们需要反推前四个时间步长( *t* = 3、 *t* = 2、 *t* = 1、 *t* = 0)，并对这五个时间步长上的梯度求和。这个版本的反向传播算法叫做**穿越时间反向传播** ( **BPTT** )。

递归体系结构使 rnn 能够从输入序列的一开始就捕获信息。这提高了序列学习的预测能力。你可能想知道普通的 RNNs 能否处理长序列。理论上可以，但由于**渐变消失**问题，实际上不能。消失的梯度意味着梯度会随着时间的推移变得非常小，这阻止了权重的更新。我将在下一节详细解释这一点，并介绍一种变体架构，长短期内存，它有助于解决这个问题。

# 用长短期记忆克服长期依赖

让我们从香草 RNNs 中的消失渐变问题开始。它是从哪里来的？回想一下，在反向传播期间，梯度随着 RNN 中的每个时间步长(即![](../Images/B16326_13_004.png))而衰减；长输入序列中的早期元素对当前梯度的计算几乎没有贡献。这意味着普通的无线网络只能在短时间窗口内捕获时间相关性。然而，遥远的时间步长之间的相关性有时是预测的关键信号。RNN 变体，包括**长短期记忆** ( **LSTM** )和**门控循环单位** ( **GRU** )是专门设计用来解决需要学习长期依赖关系的问题。

我们将在本书中重点介绍 LSTM，因为它比 GRU 受欢迎得多。LSTM 比 GRU 早 10 年推出，也比 GRU 更成熟。如果您有兴趣了解更多关于 GRU 及其应用的信息，请随时查看由 Yuxi Hayden Liu (Packt Publishing)编写的《Python 深度学习体系结构〉实践》第 6 章〈T1〉、〈T2〉递归神经网络、〈T3〉》。

在 LSTM，我们使用光栅机制来处理长期依赖。它的魔力来自一个记忆单元和三个建立在重现细胞顶部的信息门。“门”这个词取自一个电路中的逻辑门([https://en.wikipedia.org/wiki/Logic_gate](https://en.wikipedia.org/wiki/Logic_gate))。它基本上是一个 sigmoid 函数，其输出值范围从 0 到 1。0 代表“关”逻辑，而 1 代表“开”逻辑。

下图描述了 LSTM 版本的递归单元，就在普通版本之后，以供比较:

<figure class="mediaobject">![](../Images/B16326_13_10.png)</figure>

图 13.10:普通 RNNs 与 LSTM RNNs 中的复发细胞

让我们从左到右详细看看 LSTM 循环细胞:

*   *c* <sub class="" style="font-style: italic;">t</sub> 是**的记忆单位**。它从输入序列的最开始就记忆信息。
*   “f”代表**忘记门**。它决定了要忘记多少来自先前记忆状态的信息，*c*<sub xmlns:epub="http://www.idpf.org/2007/ops" class="" style="font-style: italic;">t</sub><sub xmlns:epub="http://www.idpf.org/2007/ops" class="Subscript--PACKT-">1</sub>，或者换句话说，要向前传递多少信息。让 *W* <sup xmlns:epub="http://www.idpf.org/2007/ops" class="" style="font-style: italic;">f</sup> 表示遗忘门与前一隐藏状态之间的权重，*s*t<sub xmlns:epub="http://www.idpf.org/2007/ops" class="Subscript--PACKT-">1</sub>， *U* <sup xmlns:epub="http://www.idpf.org/2007/ops" class="" style="font-style: italic;">f</sup> 表示遗忘门与当前输入之间的权重， *x* <sub xmlns:epub="http://www.idpf.org/2007/ops" class="" style="font-style: italic;">t</sub> 。
*   “I”代表**输入门**。它控制有多少信息从当前输入到通过。*W*T5】I 和*U*T9】I 分别是连接输入门到前一隐藏状态、*s*<sub xmlns:epub="http://www.idpf.org/2007/ops" class="" style="font-style: italic;">t</sub>T16】1 和当前输入的权重，*x*T20】t。
*   “tanh”只是隐藏状态的激活函数。它就像香草 RNN 中的“a”。它的输出是根据当前输入计算的， *x* <sub class="" style="font-style: italic;">t</sub> ，连同相关的权重， *U* <sup class="" style="font-style: italic;">c</sup> ，先前的隐藏状态，*s*<sub class="" style="font-style: italic;">t</sub><sub class="Subscript--PACKT-">1</sub>，以及相应的权重， *W* <sup class="" style="font-style: italic;">c</sup> 。
*   “o”作为**输出门**。它定义了从内存中为整个循环细胞的输出提取了多少信息。一如既往， *W* <sup class="" style="font-style: italic;">o</sup> 和 *U* <sup class="" style="font-style: italic;">o</sup> 分别是之前隐藏状态和当前输入的关联权重。

我们将这些组件之间的关系描述如下:

*   在时间步长 *t* 处，忘记门 *f* 的输出被计算为![](../Images/B16326_13_005.png)。
*   在时间步长 *t* 处，输入门 *i* 的输出被计算为![](../Images/B16326_13_006.png)。
*   在时间步长 *t* 处，tanh 激活的输出*c’*被计算为![](../Images/B16326_13_007.png)。
*   在时间步长 *t* 处，输出门 *o* 的输出被计算为![](../Images/B16326_13_008.png)。
*   在时间步 *t* 使用![](../Images/B16326_13_009.png)更新存储单元 *c* <sub class="" style="font-style: italic;">t</sub> (此处为操作员。*表示逐元素乘法)。同样，sigmoid 函数的输出值从 0 到 1。因此，遗忘门 *f* 和输入门 *i* 分别控制前一个记忆*c*<sub class="" style="font-style: italic;">t</sub><sub class="Subscript--PACKT-">1</sub>和当前记忆输入*c’*的数量，以进行结转。
*   最后，我们更新隐藏状态， *s* <sub class="" style="font-style: italic;">t</sub> ，在时间步 *t* 由![](../Images/B16326_13_010.png)开始。这里，输出门 *o* 控制有多少更新的存储单元 *c* <sub class="" style="font-style: italic;">t</sub> 将被用作整个单元的输出。

一如既往，我们应用 **BPTT** 算法来训练 LSTM RNNs 中的所有权重，包括四组权重，分别为 *U* 和 *W* ，与三个门和 tanh 激活功能相关联。通过学习这些权重，LSTM 网络以一种有效的方式明确地建模长期依赖关系。因此，在实践中，LSTM 是直接或默认的 RNN 模式。接下来，您将学习如何使用 LSTM 神经网络解决现实世界的问题。我们将从电影评论情绪分类开始。

# 用 RNNs 分析电影评论情绪

所以，我们的第一个 RNN 项目来了:电影评论情感。我们将以 IMDb([https://www.imdb.com/](https://www.imdb.com/))电影评论数据集([https://ai.stanford.edu/~amaas/data/sentiment/](https://ai.stanford.edu/~amaas/data/sentiment/))为例。它包含 25，000 个用于培训的高极性电影评论，还有 25，000 个用于测试。每个评论都标记为 1(阳性)或 0(阴性)。我们将在以下三个部分构建基于 RNN 的电影情感分类器:*分析和预处理电影评论数据*、*开发简单的 LSTM 网络*、*利用多个 LSTM 层*提升性能。

## 分析和预处理数据

我们先从数据分析和预处理开始，如下:

1.  我们从 TensorFlow 导入所有必要的模块:

    ```py
    >>> import tensorflow as tf
    >>> from tensorflow.keras.datasets import imdb
    >>> from tensorflow.keras import layers, models, losses, optimizers
    >>> from tensorflow.keras.preprocessing.sequence import pad_sequences 
    ```

2.  Keras has a built-in IMDb dataset, so first, we load the dataset:

    ```py
    >>> vocab_size = 5000
    >>> (X_train, y_train), (X_test, y_test) = \
                         imdb.load_data(num_words=vocab_size) 
    ```

    在这里，我们设置了词汇量，只保留这许多最频繁的单词。在本例中，这是数据集中出现频率最高的前 5，000 个单词。如果`num_words`是`None`，所有的字都会保留。

3.  Take a look at the training and testing data we just loaded:

    ```py
    >>> print('Number of training samples:', len(y_train))
    Number of training samples: 25000
    >>> print('Number of positive samples', sum(y_train))
    Number of positive samples 12500
    >>> print('Number of test samples:', len(y_test))
    Number of test samples: 25000 
    ```

    训练集是完美平衡的，正负样本数量相同。

4.  Print a training sample, as follows:

    ```py
    >>> print(X_train[0])
    [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32] 
    ```

    如您所见，原始文本已经被转换成一袋单词，每个单词都由一个整数表示。为了方便起见，整数值表示该词在数据集中出现的频率。例如，“1”代表最频繁的单词(“the”，你可以想象)，而“10”代表第十个最频繁的单词。我们能找出单词是什么吗？让我们看看下一步。

5.  We use the word dictionary to map the integer back to the word it represents:

    ```py
    >>> word_index = imdb.get_word_index()
    >>> index_word = {index: word for word, index in word_index.items()} 
    ```

    以第一次复习为例:

    ```py
    >>> print([index_word.get(i, ' ') for i in X_train[0]])
    ['the', 'as', 'you', 'with', 'out', 'themselves', 'powerful', 'lets', 'loves', 'their', 'becomes', 'reaching', 'had', 'journalist', 'of', 'lot', 'from', 'anyone', 'to', 'have', 'after', 'out', 'atmosphere', 'never', 'more', 'room', 'and', 'it', 'so', 'heart', 'shows', 'to', 'years', 'of', 'every', 'never', 'going', 'and', 'help', 'moments', 'or', 'of', 'every', 'chest', 'visual', 'movie', 'except', 'her', 'was', 'several', 'of', 'enough', 'more', 'with', 'is', 'now', 'current', 'film', 'as', 'you', 'of', 'mine', 'potentially', 'unfortunately', 'of', 'you', 'than', 'him', 'that', 'with', 'out', 'themselves', 'her', 'get', 'for', 'was', 'camp', 'of', 'you', 'movie', 'sometimes', 'movie', 'that', 'with', 'scary', 'but', 'and', 'to', 'story', 'wonderful', 'that', 'in', 'seeing', 'in', 'character', 'to', 'of', '70s', 'and', 'with', 'heart', 'had', 'shadows', 'they', 'of', 'here', 'that', 'with', 'her', 'serious', 'to', 'have', 'does', 'when', 'from', 'why', 'what', 'have', 'critics', 'they', 'is', 'you', 'that', "isn't", 'one', 'will', 'very', 'to', 'as', 'itself', 'with', 'other', 'and', 'in', 'of', 'seen', 'over', 'and', 'for', 'anyone', 'of', 'and', 'br', "show's", 'to', 'whether', 'from', 'than', 'out', 'themselves', 'history', 'he', 'name', 'half', 'some', 'br', 'of', 'and', 'odd', 'was', 'two', 'most', 'of', 'mean', 'for', '1', 'any', 'an', 'boat', 'she', 'he', 'should', 'is', 'thought', 'and', 'but', 'of', 'script', 'you', 'not', 'while', 'history', 'he', 'heart', 'to', 'real', 'at', 'and', 'but', 'when', 'from', 'one', 'bit', 'then', 'have', 'two', 'of', 'script', 'their', 'with', 'her', 'nobody', 'most', 'that', 'with', "wasn't", 'to', 'with', 'armed', 'acting', 'watch', 'an', 'for', 'with', 'and', 'film', 'want', 'an'] 
    ```

6.  Next, we analyze the length of each sample (the number of words in each review, for example). We do so because all the input sequences to an RNN model must be the same length:

    ```py
    >>> review_lengths = [len(x) for x in X_train] 
    ```

    绘制这些文档长度的分布，如下所示:

    ```py
    >>> import matplotlib.pyplot as plt
    >>> plt.hist(review_lengths, bins=10)
    >>> plt.show() 
    ```

    分配结果见下图:

    <figure class="mediaobject">![](../Images/B16326_13_11.png)</figure>

    图 13.11:检查长度分布

7.  As you can see, the majority of the reviews are around 200 words long. Next, we set 200 as the universal sequence length by padding shorter reviews with zeros and truncating longer reviews. We use the `pad_sequences` function from Keras to accomplish this:

    ```py
    >>> maxlen = 200
    >>> X_train = pad_sequences(X_train, maxlen=maxlen)
    >>> X_test = pad_sequences(X_test, maxlen=maxlen) 
    ```

    接下来让我们看看输入序列的形状:

    ```py
    >>> print('X_train shape after padding:', X_train.shape)
    X_train shape after padding: (25000, 200)
    >>> print('X_test shape after padding:', X_test.shape)
    X_test shape after padding: (25000, 200) 
    ```

让我们继续建立一个 LSTM 网络。

## 建立一个简单的 LSTM 网络

现在训练和测试数据集已经准备好了，我们可以构建我们的第一个 RNN 模型:

1.  首先，我们修复随机种子并启动一个 Keras 顺序模型:

    ```py
    >>> tf.random.set_seed(42)
    >>> model = models.Sequential() 
    ```

2.  Since our input sequences are word indices that are equivalent to one-hot encoded vectors, we need to embed them in dense vectors using the `Embedding` layer from Keras:

    ```py
    >>> embedding_size = 32
    >>> model.add(layers.Embedding(vocab_size, embedding_size)) 
    ```

    这里，我们将由`vocab_size=5000`唯一单词标记组成的输入序列嵌入到大小为 32 的密集向量中。

    请随意重读*最佳实践 14–使用带有神经网络的单词嵌入从文本数据中提取特征*摘自*第 11 章*、*机器学习最佳实践*。

3.  Now here comes the recurrent layer, the LSTM layer specifically:

    ```py
    >>> model.add(layers.LSTM(50)) 
    ```

    这里，我们只使用一个具有 50 个节点的递归层。

4.  之后，我们添加输出层以及一个 sigmoid 激活函数，因为我们正在处理一个二进制分类问题:

    ```py
    >>> model.add(layers.Dense(1, activation='sigmoid')) 
    ```

5.  显示模型摘要，再次检查图层:

    ```py
    >>> print(model.summary())
    Model: "sequential"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    embedding (Embedding)        (None, None, 32)          160000    
    _________________________________________________________________
    lstm (LSTM)                  (None, 50)                16600     
    _________________________________________________________________
    dense (Dense)                (None, 1)                 51        
    =================================================================
    Total params: 176,651
    Trainable params: 176,651
    Non-trainable params: 0
    _________________________________________________________________ 
    ```

6.  接下来，我们用 Adam 优化器编译模型，并使用二进制交叉熵作为优化目标:

    ```py
    >>> model.compile(loss='binary_crossentropy',
    ...               optimizer='adam',
    ...               metrics=['accuracy']) 
    ```

7.  最后，我们训练三个时期的批量大小为 64 的模型:

    ```py
    >>> batch_size = 64
    >>> n_epoch = 3
    >>> model.fit(X_train, y_train,
    ...           batch_size=batch_size,
    ...           epochs=n_epoch,
    ...           validation_data=(X_test, y_test))
    Train on 25000 samples, validate on 25000 samples
    Epoch 1/3
    391/391 [==============================] - 70s 178ms/step - loss: 0.4284 - accuracy: 0.7927 - val_loss: 0.3396 - val_accuracy: 0.8559
    Epoch 2/3
    391/391 [==============================] - 69s 176ms/step - loss: 0.2658 - accuracy: 0.8934 - val_loss: 0.3034 - val_accuracy: 0.8730
    Epoch 3/3
    391/391 [==============================] - 69s 177ms/step - loss: 0.2283 - accuracy: 0.9118 - val_loss: 0.3118 - val_accuracy: 0.8705 
    ```

8.  Using the trained model, we evaluate the classification accuracy on the testing set:

    ```py
    >>> acc = model.evaluate(X_test, y_test, verbose = 0)[1]
    >>> print('Test accuracy:', acc)
    Test accuracy: 0.8705199956893921 
    ```

    我们获得了 87.05%的测试准确率。

## 堆叠多个 LSTM 层

现在，让我们尝试堆叠两个循环层。下图显示了如何堆叠两个循环层:

<figure class="mediaobject">![](../Images/B16326_13_12.png)</figure>

图 13.12:展开两个堆叠的循环层

让我们看看是否可以通过以下步骤构建多层 RNN 模型来超越以前的准确性:

1.  Initiate a new model and add an embedding layer, two LSTM layers, and an output layer:

    ```py
    >>> model = models.Sequential()
    >>> model.add(layers.Embedding(vocab_size, embedding_size))
    >>> model.add(layers.LSTM(50, return_sequences=True, dropout=0.2))
    >>> model.add(layers.LSTM(50, dropout=0.2))
    >>> model.add(layers.Dense(1, activation='sigmoid')) 
    ```

    这里，第一个 LSTM 层带有`return_sequences=True`，因为我们需要将其整个输出序列馈送到第二个 LSTM 层。我们还为两个 LSTM 图层添加了 20%的缺失，以减少过度拟合，因为我们将有更多参数需要训练:

    ```py
    >>> print(model.summary())
    Model: "sequential_1"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    embedding_1 (Embedding)      (None, None, 32)          160000    
    _________________________________________________________________
    lstm_1 (LSTM)                (None, None, 50)          16600     
    _________________________________________________________________
    lstm_2 (LSTM)                (None, 50)                20200     
    _________________________________________________________________
    dense_1 (Dense)              (None, 1)                 51        
    =================================================================
    Total params: 196,851
    Trainable params: 196,851
    Non-trainable params: 0
    _________________________________________________________________
    None 
    ```

2.  同样，我们用 Adam 优化器以`0.003`学习速率

    ```py
    >>> optimizer = optimizers.Adam(lr=0.003)
    >>> model.compile(loss='binary_crossentropy',
    ...               optimizer=optimizer,
    ...               metrics=['accuracy']) 
    ```

    编译模型
3.  然后，我们训练 7 个时期的堆叠模型:

    ```py
    >>> n_epoch = 7
    >>> model.fit(X_train, y_train,
    ...           batch_size=batch_size,
    ...           epochs=n_epoch,
    ...           validation_data=(X_test, y_test))
    Train on 25000 samples, validate on 25000 samples
    Epoch 1/7
    391/391 [==============================] - 139s 356ms/step - loss: 0.4755 - accuracy: 0.7692 - val_loss: 0.3438 - val_accuracy: 0.8511
    Epoch 2/7
    391/391 [==============================] - 140s 357ms/step - loss: 0.3272 - accuracy: 0.8631 - val_loss: 0.3407 - val_accuracy: 0.8573
    Epoch 3/7
    391/391 [==============================] - 137s 350ms/step - loss: 0.3042 - accuracy: 0.8782 - val_loss: 0.3436 - val_accuracy: 0.8580
    Epoch 4/7
    391/391 [==============================] - 136s 349ms/step - loss: 0.2468 - accuracy: 0.9028 - val_loss: 0.6771 - val_accuracy: 0.7860
    Epoch 5/7
    391/391 [==============================] - 137s 350ms/step - loss: 0.2201 - accuracy: 0.9117 - val_loss: 0.3273 - val_accuracy: 0.8684
    Epoch 6/7
    391/391 [==============================] - 137s 349ms/step - loss: 0.1867 - accuracy: 0.9278 - val_loss: 0.3352 - val_accuracy: 0.8736
    Epoch 7/7
    391/391 [==============================] - 138s 354ms/step - loss: 0.1586 - accuracy: 0.9398 - val_loss: 0.3335 - val_accuracy: 0.8756 
    ```

4.  Finally, we verify the test accuracy:

    ```py
    >>> acc = model.evaluate(X_test, y_test, verbose=0)[1]
    >>> print('Test accuracy with stacked LSTM:', acc)
    Test accuracy with stacked LSTM: 0.8755999803543091 
    ```

    获得了 87.56%的较好测试精度。

至此，我们刚刚使用 RNNs 完成了评论情感分类项目。rnn 是多对一结构中的。在下一个项目中，我们将开发一个多对多结构下的 RNN 来写一部“小说”

# 用无线网络写你自己的战争与和平

在这个项目中，我们将研究一个有趣的语言建模问题——文本生成。

基于 RNN 的文本生成器可以写任何东西，这取决于我们输入的文本。训练文本可以来自小说，比如《T0》《权力的游戏》《T1》，莎士比亚的一首诗，或者《T2》《黑客帝国》《T3》的电影剧本。如果模型训练有素，生成的人工文本应该和原始文本相似(但不完全相同)。在这一部分，我们要用俄罗斯作家列夫·托尔斯泰的小说《RNNs》来写我们自己的《T4》《战争与和平》。随意在你喜欢的任何一本书上训练你自己的神经网络。

在构建训练集之前，我们将从数据采集和分析开始。之后，我们将建立并训练一个用于文本生成的 RNN 模型。

## 获取并分析训练数据

我推荐从目前不受版权保护的书籍中下载文本数据进行训练。古腾堡工程([www.gutenberg.org](http://www.gutenberg.org))是一个非常适合的地方。它提供超过 60，000 本版权已经过期的免费电子书。

原作品*《战争与和平》*可以从[http://www.gutenberg.org/ebooks/2600](http://www.gutenberg.org/ebooks/2600)下载，但注意会有一些清理，比如去掉目录中多余的开头部分“古腾堡工程电子书”，以及需要的纯文本 UTF-8 文件([http://www.gutenberg.org/files/2600/2600-0.txt](http://www.gutenberg.org/files/2600/2600-0.txt))的多余附录“战争与和平古腾堡工程电子书的结尾”。所以，我们就不这样做了，直接从[https://cs . Stanford . edu/people/karpathy/char-rnn/war peace _ input . txt](https://cs.stanford.edu/people/karpathy/char-rnn/warpeace_input.txt)下载清理后的文本文件。让我们开始吧:

1.  首先，我们读取文件并将文本转换为小写:

    ```py
    >>> training_file = 'warpeace_input.txt'
    >>> raw_text = open(training_file, 'r').read()
    >>> raw_text = raw_text.lower() 
    ```

2.  然后，我们通过打印出前 200 个字符来快速查看训练文本数据:

    ```py
    >>> print(raw_text[:200])
    "well, prince, so genoa and lucca are now just family estates of the 
    buonapartes. but i warn you, if you don't tell me that this means war,
    if you still try to defend the infamies and horrors perpetr 
    ```

3.  Next, we count the number of unique words:

    ```py
    >>> all_words = raw_text.split()
    >>> unique_words = list(set(all_words))
    >>> print(f'Number of unique words: {len(unique_words)}')
    Number of unique words: 39830 
    ```

    然后，我们统计总的字符数:

    ```py
    >>> n_chars = len(raw_text)
    >>> print(f'Total characters: {n_chars}')
    Total characters: 3196213 
    ```

4.  从这 300 万个字符中，我们获得了唯一的字符，如下所示:

    ```py
    >>> chars = sorted(list(set(raw_text)))
    >>> n_vocab = len(chars)
    >>> print(f'Total vocabulary (unique characters): {n_vocab}')
    Total vocabulary (unique characters): 57
    >>> print(chars)
    ['\n', ' ', '!', '"', "'", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'à', 'ä', 'é', 'ê', '\ufeff'] 
    ```

原始训练文本由 57 个独特的字符组成，并由近 40，000 个独特的单词组成。生成单词需要一步计算 40000 个概率，远比生成字符困难，后者只需要一步计算 57 个概率。因此，我们将一个字符视为一个标记，这里的词汇由 57 个字符组成。

那么，我们如何将字符输入 RNN 模型并生成输出字符呢？让我们在下一节看到。

## 构建 RNN 文本生成器的训练集

回想一下，在同步的“多对多”RNN 中，网络接收一个序列并同时产生一个序列；该模型捕捉序列中元素之间的关系，并基于学习到的模式再现新的序列。至于我们的文本生成器，我们可以输入固定长度的字符序列，并让它生成相同长度的序列，其中每个输出序列是从其输入序列移位的一个字符。以下示例将帮助您更好地理解这一点:

假设我们有一个原始文本样本“学习”，我们希望序列长度为 5。在这里，我们可以有一个输入序列“learn”和一个输出序列“earni”我们可以将它们放入网络，如下所示:

<figure class="mediaobject">![](../Images/B16326_13_13.png)</figure>

图 13.13:给 RNN 喂一套训练设备(“学习”、“恩尼”)

我们刚刚构建了一个训练样本(“learn”“earni”)。同样，从整个原始文本中构造训练样本，首先需要将原始文本拆分成固定长度的序列，*X*；然后，我们需要忽略原始文本的第一个字符，并将其拆分成相同长度的序列， *Y* 。来自 *X* 的序列是训练样本的输入，而来自 *Y* 的对应序列是样本的输出。假设我们有一个原始文本样本，“通过示例进行机器学习”，我们将序列长度设置为 5。我们将构建以下训练样本:

<figure class="mediaobject">![](../Images/B16326_13_14.png)</figure>

图 13.14:从“示例机器学习”构建的训练样本

这里，□表示空间。请注意，剩下的子序列“le”不够长，所以我们干脆放弃它。

我们还需要对输入和输出字符进行一次性编码，因为神经网络模型只接受数字数据。我们只需将 57 个唯一字符映射到从 0 到 56 的索引，如下所示:

```py
>>> index_to_char = dict((i, c) for i, c in enumerate(chars))
>>> char_to_index = dict((c, i) for i, c in enumerate(chars))
>>> print(char_to_index)
{'\n': 0, ' ': 1, '!': 2, '"': 3, "'": 4, '(': 5, ')': 6, '*': 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, ';': 23, '=': 24, '?': 25, 'a': 26, 'b': 27, 'c': 28, 'd': 29, 'e': 30, 'f': 31, 'g': 32, 'h': 33, 'i': 34, 'j': 35, 'k': 36, 'l': 37, 'm': 38, 'n': 39, 'o': 40, 'p': 41, 'q': 42, 'r': 43, 's': 44, 't': 45, 'u': 46, 'v': 47, 'w': 48, 'x': 49, 'y': 50, 'z': 51, 'à': 52, 'ä': 53, 'é': 54, 'ê': 55, '\ufeff': 56} 
```

例如，字符“`c`”变成长度为 57 的向量，索引 28 中的“`1`”和所有其他索引中的“`0`”s；字符“`h`”变成长度为 57 的向量，在索引 33 中带有“`1`”，在所有其他索引中带有“`0`”s。

现在字符查找字典已经准备好了，我们可以构建整个训练集，如下所示:

```py
>>> import numpy as np
>>> seq_length = 160
>>> n_seq = int(n_chars / seq_length) 
```

这里，我们将序列长度设置为`160`并获取`n_seq`训练样本。接下来，我们初始化训练输入和输出，它们都是形状(样本数、序列长度、特征维数):

```py
>>> X = np.zeros((n_seq, seq_length, n_vocab))
>>> Y = np.zeros((n_seq, seq_length, n_vocab)) 
```

Keras 中的 RNN 模型要求输入和输出序列的形状符合形状(样本数、序列长度、特征维数)。

现在，对于每个`n_seq`样本，我们将“`1`”分配给存在相应字符的输入和输出向量的索引:

```py
>>> for i in range(n_seq):
...     x_sequence = raw_text[i * seq_length : 
                              (i + 1) * seq_length]
...     x_sequence_ohe = np.zeros((seq_length, n_vocab))
...     for j in range(seq_length):
...             char = x_sequence[j]
...             index = char_to_index[char]
...             x_sequence_ohe[j][index] = 1.
...     X[i] = x_sequence_ohe
...     y_sequence = raw_text[i * seq_length + 1 : (i + 1) * 
                                                 seq_length + 1]
...     y_sequence_ohe = np.zeros((seq_length, n_vocab))
...     for j in range(seq_length):
...             char = y_sequence[j]
...             index = char_to_index[char]
...             y_sequence_ohe[j][index] = 1.
...     Y[i] = y_sequence_ohe 
```

接下来，看看构建的输入和输出示例的形状:

```py
>>> X.shape
(19976, 160, 57)
>>> Y.shape
(19976, 160, 57) 
```

同样，每个样本(输入或输出序列)由 160 个元素组成。每个元素都是一个 57 维的一维热编码向量。

我们终于准备好了训练集，是时候构建和适应 RNN 模式了。让我们在接下来的两个部分中完成这项工作。

## 构建 RNN 文本生成器

在本节中，我们将构建一个 RNN，它有两个堆叠的循环层。对于复杂的问题，例如文本生成，这比具有单一循环层的 RNN 具有更强的预测能力。让我们开始吧:

1.  首先，我们导入所有必要的模块，并固定一个随机种子:

    ```py
    >>> import tensorflow as tf
    >>> from tensorflow.keras import layers, models, losses, optimizers
    >>> tf.random.set_seed(42) 
    ```

2.  每个循环层包含 700 个单元，具有 0.4 的脱落率和 tanh 激活功能:

    ```py
    >>> hidden_units = 700
    >>> dropout = 0.4 
    ```

3.  我们指定了其他超参数，包括批次大小 100 和时期数量 300:

    ```py
    >>> batch_size = 100
    >>> n_epoch= 300 
    ```

4.  Now, we create the RNN model, as follows:

    ```py
    >>> model = models.Sequential()
    >>> model.add(layers.LSTM(hidden_units, input_shape=(None, n_vocab), return_sequences=True, dropout=dropout))
    >>> model.add(layers.LSTM(hidden_units, return_sequences=True, dropout=dropout))
    >>> model.add(layers.TimeDistributed(layers.Dense(n_vocab, activation='softmax'))) 
    ```

    有几件事值得研究:

    *   `return_sequences=True`对于第一个递归层:第一个递归层的输出是一个序列，这样我们就可以把第二个递归层叠加在上面。
    *   `return_sequences=True`对于第二递归层:第二递归层的输出是一个序列，启用多对多结构。
    *   `Dense(n_vocab, activation='softmax')`:输出序列的每个元素都是一个热门的编码向量，因此 softmax 激活用于计算单个字符的概率。
    *   `TimeDistributed`:由于递归层的输出是一个序列，`Dense`层不接受序列输入，所以`TimeDistributed`被用作适配器，使得`Dense`层可以应用于输入序列的每个元素。
5.  Next, we compile the network. As for the optimizer, we choose `RMSprop` with a learning rate of 0.001:

    ```py
    >>> optimizer = optimizers.RMSprop(lr=0.001)
    >>> model.compile(loss="categorical_crossentropy", 
                      optimizer=optimizer) 
    ```

    这里，损失函数是多类交叉熵。

6.  让我们总结一下刚刚构建的模型:

    ```py
    >>> print(model.summary())  
    Model: "sequential"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    lstm (LSTM)                  (None, None, 700)         2122400   
    _________________________________________________________________
    lstm_1 (LSTM)                (None, None, 700)         3922800   
    _________________________________________________________________
    time_distributed (TimeDistri (None, None, 57)          39957     
    =================================================================
    Total params: 6,085,157
    Trainable params: 6,085,157
    Non-trainable params: 0
    _________________________________________________________________ 
    ```

至此，我们已经刚刚完成的构建，并准备训练模型。我们将在下一节中讨论这个问题。

## 训练 RNN 文本生成器

如模型总结所示，我们有 600 多万个参数需要训练。因此，建议在图形处理器上训练模型。如果你内部没有 GPU，可以使用谷歌 Colab 提供的免费 GPU。您可以按照[https://ml-book.now.sh/free-gpu-for-deep-learning/](https://ml-book.now.sh/free-gpu-for-deep-learning/)的教程进行设置。

此外，对于需要长时间训练的深度学习模型，设置一些回调是一个很好的做法，以便在训练期间跟踪模型的内部状态和性能。在我们的项目中，我们使用以下回调:

*   **模型检查点**:这将在每个纪元后保存模型。如果在培训过程中出现意外的问题，您不必重新培训模型。您可以简单地加载保存的模型，然后从那里继续训练。
*   **提前止损**:我们在*第八章*、*人工神经网络预测股价*中介绍过。
*   **定期用最新的模型生成文本**:这样做，我们可以看到生成的文本是多么的合理。

我们使用这三个回调来训练我们的 RNN 模型，如下所示:

1.  首先，我们导入必要的模块:

    ```py
    >>> from tensorflow.keras.callbacks import Callback, ModelCheckpoint, EarlyStopping 
    ```

2.  Then, we define the model checkpoint callback:

    ```py
    >>> file_path =  
            "weights/weights_epoch_{epoch:03d}_loss_{loss:.4f}.hdf5"
    >>> checkpoint = ModelCheckpoint(file_path, monitor='loss', 
                       verbose=1, save_best_only=True, mode='min') 
    ```

    模型检查点将与由纪元号和训练损失组成的文件名一起保存。

3.  之后，我们创建一个提前停止回调，如果连续 50 个时期验证损失没有减少，就停止训练:

    ```py
    >>> early_stop = EarlyStopping(monitor='loss', min_delta=0, 
                                   patience=50, verbose=1, mode='min') 
    ```

4.  Next, we develop a helper function that generates text of any length, given a model:

    ```py
    >>> def generate_text(model, gen_length, n_vocab, index_to_char):
    ...     """
    ...     Generating text using the RNN model
    ...     @param model: current RNN model
    ...     @param gen_length: number of characters we want to generate
    ...     @param n_vocab: number of unique characters
    ...     @param index_to_char: index to character mapping
    ...     @return: string of text generated
    ...     """
    ...     # Start with a randomly picked character
    ...     index = np.random.randint(n_vocab)
    ...     y_char = [index_to_char[index]]
    ...     X = np.zeros((1, gen_length, n_vocab))
    ...     for i in range(gen_length):
    ...         X[0, i, index] = 1.
    ...         indices = np.argmax(model.predict(
                       X[:, max(0, i - seq_length -1):i + 1, :])[0], 1)
    ...         index = indices[-1]
    ...         y_char.append(index_to_char[index])
    ...     return ''.join(y_char) 
    ```

    它从一个随机挑选的角色开始。然后，输入模型基于其先前生成的字符预测每个剩余的`gen_length-1`字符。

5.  Now, we define the callback class that generates text with the `generate_text` `util` function for every `N` epochs:

    ```py
    >>> class ResultChecker(Callback):
    ...     def __init__(self, model, N, gen_length):
    ...         self.model = model
    ...         self.N = N
    ...         self.gen_length = gen_length
    ...
    ...     def on_epoch_end(self, epoch, logs={}):
    ...         if epoch % self.N == 0:
    ...             result = generate_text(self.model, 
                             self.gen_length, n_vocab, index_to_char)
    ...             print('\nMy War and Peace:\n' + result) 
    ```

    接下来，我们启动文本生成检查器回调:

    ```py
    >>> result_checker = ResultChecker(model, 10, 500) 
    ```

    该模型将为每 10 个时代生成 500 个字符的文本。

6.  Now that all the callback components are ready, we can start training the model:

    ```py
    >>> model.fit(X, Y, batch_size=batch_size, 
          verbose=1, epochs=n_epoch,callbacks=[
          result_checker, checkpoint, early_stop]) 
    ```

    我将在这里只演示 1、51、101 和 291 时代的结果:

    时代 1:

    ```py
    Epoch 1/300
    200/200 [==============================] - 117s 584ms/step - loss: 2.8908
    My War and Peace:
    8 the tout to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to t
    Epoch 00001: loss improved from inf to 2.89075, saving model to weights/weights_epoch_001_loss_2.8908.hdf5 
    ```

    纪元 51:

    ```py
    Epoch 51/300
    200/200 [==============================] - ETA: 0s - loss: 1.7430
    My War and Peace:
    re and the same time the same time the same time he had not yet seen the first time that he was always said to him that the countess was sitting in the same time and the same time that he was so saying that he was saying that he was saying that he was saying that he was saying that he was saying that he was saying that he was saying that he was saying that he was saying that he was saying that he was saying that he was saying that he was saying that he was saying that he was saying that he was sa
    Epoch 00051: loss improved from 1.74371 to 1.74298, saving model to weights/weights_epoch_051_loss_1.7430.hdf5
    200/200 [==============================] - 64s 321ms/step - loss: 1.7430 
    ```

    纪元 101:

    ```py
    Epoch 101/300
    200/200 [==============================] - ETA: 0s - loss: 1.6892
    My War and Peace:
    's and the same time and the same sonse of his life and her face was already in her hand.
    "what is it?" asked natasha. "i have not the post and the same to
    her and will not be able to say something to her and went to
    the door.
    "what is it?" asked natasha. "i have not the post and the same to her
    and that i shall not be able to say something to her and
    went on to the door.
    "what a strange in the morning, i am so all to say something to her,"
    said prince andrew, "i have not the post and the
    same
    Epoch 00101: loss did not improve from 1.68711
    200/200 [==============================] - 64s 321ms/step - loss: 1.6892 
    ```

    纪元 291:

    ```py
    Epoch 291/300
    200/200 [==============================] - ETA: 0s - loss: 1.6136
    My War and Peace:
    à to the countess, who was sitting in the same way the sound of a sound of company servants were standing in the middle of the road.
    "what are you doing?" said the officer, turning to the princess with
    a smile.
    "i don't know what to say and want to see you."
    "yes, yes," said prince andrew, "i have not been the first to see you
    and you will be a little better than you are and we will be
    married. what a sin i want to see you."
    "yes, yes," said prince andrew, "i have not been the first to see yo
    Epoch 00291: loss did not improve from 1.61188
    200/200 [==============================] - 65s 323ms/step - loss: 1.6136 
    ```

在特斯拉 K80 GPU 上，每个纪元大约需要 60 秒。经过几个小时的训练，这个位于 RNN 的文本生成器可以写出一个真实有趣的《战争与和平》版本。这样，我们成功地使用了多对多类型的 RNN 来生成文本。

具有多对多结构的 RNN 是一种序列对序列(`seq2seq`)模型，它接收一个序列并输出另一个序列。一个典型的例子是机器翻译，将一种语言的单词序列转换成另一种语言的序列。`state-of-the-art seq2seq` 模型是 Transformer 模型，是谷歌大脑开发的。我们将在下一节简要讨论它。

# 使用 Transformer 模型促进语言理解

`Transformer`模型最早是在*关注是你所需要的*([https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762))中提出的。它可以有效地处理长期依赖，这在 LSTM 仍然是一个挑战。在本节中，我们将介绍 Transformer 的架构和构建模块，以及它最关键的部分:自我关注层。

## 探索变压器的架构

我们先来看看Transformer 模型的高层架构(图片来自*注意力是你需要的全部*):

<figure class="mediaobject">![](../Images/B16326_13_15.png)</figure>

图 13.15:变压器架构

可以看到，变压器由两部分组成:**编码器**(左侧大矩形)和**解码器**(右侧大矩形)。编码器对输入序列进行加密。它有一个多头注意力层(我们接下来会谈到这一点)和一个常规前馈层。另一方面，解码器产生输出序列。它有一个屏蔽的多头注意力层，以及一个多头注意力层和一个常规前馈层。

在步骤 *t* 中，变压器模型输入步骤*x*T5】1、*x*T9】2、…、*x<sub xmlns:epub="http://www.idpf.org/2007/ops" class="" style="font-style: italic;">t</sub>并输出步骤*y*T17】1、*y*T21】2、…、*y*T25】t-1 然后它预测 *y* <sub xmlns:epub="http://www.idpf.org/2007/ops" class="" style="font-style: italic;">t</sub> 。这与多对多的 RNN 模式没有什么不同。*

多头关注层可能是您唯一觉得奇怪的地方，所以我们将在下一节中查看它。

## 理解自我关注

让我们讨论一下在下面的例子中**自我关注**层在变形金刚中是如何发挥关键作用的:

“我通过示例阅读了 Python 机器学习，它确实是一本很棒的书。”显然是指 *Python 机器学习举例*。当 Transformer 模型处理这句话的时候，自我关注会联想到 *Python 机器学习举例*。给定输入序列中的一个单词，自我注意允许模型在不同的注意力水平上观察序列中的其他单词，这有助于`seq2seq`任务中的语言理解和学习。

现在，让我们看看如何计算注意力分数。

如架构图所示，注意力层有三个输入向量:

*   查询向量 *Q* ，表示序列中的查询词(即当前词)
*   关键向量 *K* ，表示序列中的单个单词
*   值向量 *V* ，也表示序列中的单个单词

这三个向量是在训练过程中训练的。

关注层的输出计算如下:

<figure class="mediaobject">![](../Images/B16326_13_011.png)</figure>

这里， *d* <sub class="" style="font-style: italic;">k</sub> 是关键向量的维数。以序列 *python 机器学习举例*为例；我们按照以下步骤计算第一个单词 *python* 的自我关注度:

1.  我们计算序列中每个单词和单词 *python* 之间的点积。分别是 *q* <sub xmlns:epub="http://www.idpf.org/2007/ops" class="Subscript--PACKT-">1</sub> 、*k*T8】1、*q*T12】1、*k*T16】2、*q*T20】1、*k*T24】3、*q*T28】1、 *k 这里， *q* <sub xmlns:epub="http://www.idpf.org/2007/ops" class="Subscript--PACKT-">1</sub> 是第一个词的查询向量， *k* <sub xmlns:epub="http://www.idpf.org/2007/ops" class="Subscript--PACKT-">1</sub> 到 *k* <sub xmlns:epub="http://www.idpf.org/2007/ops" class="Subscript--PACKT-">5</sub> 分别是这五个词的关键向量。*
2.  我们通过除法和 softmax 激活来归一化结果点积:

    <figure class="mediaobject">![](../Images/B16326_13_012.png)</figure>

3.  然后，我们将得到的 softmax 向量乘以值向量![](../Images/B16326_13_013.png)，并对结果求和:

<figure class="mediaobject">![](../Images/B16326_13_014.png)</figure>

*z* <sub class="Subscript--PACKT-">1</sub> 是序列中第一个单词 *python* 的自我关注得分。我们对序列中剩余的每个单词重复这个过程，以获得它的注意力得分。现在，你应该明白为什么这个叫做**多头注意力**:自我注意力不是只为一个词(一个步骤)计算的，而是为所有词(所有步骤)计算的。

然后，所有输出的注意力分数被连接并馈送到下游的常规前馈层。

在本节中，我们已经介绍了 Transformer 模型的主要概念。它已经成为自然语言处理中许多复杂问题的选择模型，如语音转文本、文本摘要和问答。通过增加注意力机制，Transformer 模型可以有效地处理顺序学习中的长期依赖关系。此外，它允许在训练过程中并行化，因为自我注意力可以针对单个步骤独立计算。

如果您有兴趣阅读更多内容，以下是使用 Transformer 取得的一些最新进展:

*   **变形金刚** ( **伯特**)的双向编码器表示，由谷歌([https://arxiv.org/abs/1810.04805v2](https://arxiv.org/abs/1810.04805v2))开发
*   **生成式预训练变压器** ( **GPT** )，由 OpenAI 提出([https://S3-us-west-2 . amazonaws . com/OpenAI-assets/research-covers/language-unsupervised/language _ understanding _ paper . pdf](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_unde))
*   使用变形金刚进行物体检测([https://ai . Facebook . com/blog/端到端使用变形金刚进行物体检测/](https://ai.facebook.com/blog/end-to-end-object-detection-with-transformers/) )

# 摘要

在这一章中，我们研究了两个自然语言处理项目:情感分析和使用神经网络的文本生成。我们首先详细解释了不同形式的输入和输出序列的循环机制和不同的 RNN 结构。您还学习了 LSTM 如何改进香草 RNNs。最后，作为奖励部分，我们介绍了 Transformer，这是一种最新的顺序学习模型。

在下一章中，我们将重点讨论第三类机器学习问题:强化学习。你将学习强化学习模式如何通过与环境互动来达到学习目标。

# 练习

1.  使用双向递归层(自己学习足够容易)，并将其应用到情感分析项目中。你能打败我们取得的成就吗？如果你想看一个例子，请阅读[https://www . tensorflow . org/API _ docs/python/TF/keras/layers/双向](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional)。
2.  随意微调超参数，就像我们在*第 8 章*、*用人工神经网络*预测股价一样，看看能否进一步提高分类性能。