# Fourteen

# 基于强化学习的复杂环境决策

在前一章中，我们重点介绍了用于顺序学习的 RNNs。本书的最后一章将讲述强化学习，这是本书开头提到的第三类机器学习任务。您将看到从经验中学习和通过与环境交互进行学习与之前介绍的有监督和无监督学习有何不同。

我们将在本章中讨论以下主题:

*   为强化学习建立工作空间
*   强化学习基础
*   开放健身环境的模拟
*   价值迭代和策略迭代算法
*   政策评估和控制的蒙特卡罗方法
*   问学习算法

# 设置工作环境

让我们开始设置工作环境，包括作为主框架的 PyTorch，以及 OpenAI Gym，这个工具包为你提供了各种环境来开发你的学习算法。

## 安装 PyTorch

**py Torch**([https://pytorch.org/](https://pytorch.org/))是脸书 AI 实验室在 Torch([http://torch.ch/](http://torch.ch/))之上开发的新潮机器学习库。它提供了强大的计算图形和对图形处理器的高兼容性，以及简单友好的界面。PyTorch 在学术界迅速扩张，被越来越多的公司大量采用。下图(摘自[http://horace.io/pytorch-vs-tensorflow/](http://horace.io/pytorch-vs-tensorflow/))显示了 PyTorch 在顶级机器学习会议上的增长情况:

<figure class="mediaobject">![](../Images/B16326_14_01.png)</figure>

图 14.1:顶级机器学习会议中 PyTorch 论文的数量

在过去的一年里，在这些会议上，提到 PyTorch 的次数比 TensorFlow 还多。希望你有足够的动力和 PyTorch 一起工作。现在让我们看看如何正确安装它。

首先，在[https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)页面的下表中，您可以为您的环境选择正确的配置:

<figure class="mediaobject">![](../Images/B16326_14_02.png)</figure>

图 14.2:使用系统配置安装 PyTorch

这里，我以本地运行的 Mac、Conda 和 Python 3.7(无 CUDA)为例，运行建议的命令行:

```
conda install pytorch torchvision -c pytorch 
```

接下来，您可以在 Python 中运行以下代码行来确认正确安装:

```
>>> import torch
>>> x = torch.empty(3, 4)
>>> print(x)
tensor([[7.8534e+34, 4.7418e+30, 5.9663e-02, 7.0374e+22],
        [3.5788e+01, 4.5825e-41, 4.0272e+01, 4.5825e-41],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]]) 
```

这里，PyTorch 中的张量类似于 NumPy 中的 ndarrays，或者 TensorFlow 中的张量。我们刚刚创建了一个大小为 3 * 4 的张量。它是一个空矩阵，有一堆无意义的占位符浮动。同样，这与 NumPy 的空数组非常相似。

如果你想更熟悉 PyTorch，可以浏览官方教程[https://pytorch.org/tutorials/#getting-started](https://pytorch.org/tutorials/#getting-started)中的*入门*章节。我建议你至少完成这两个:

*   什么是 PyTorch？:[https://py torch . org/教程/初学者/blitz/tensor _ tutorial . html # sphx-glr-初学者-blitz-tensor-tutorial-py](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tut)
*   用例子学习 py torch:[https://py torch . org/教程/初学者/pytorch_with_examples.html](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html)

到目前为止，我们已经成功建立了 PyTorch。让我们在下一节看看如何安装 OpenAI 健身房。

强化学习不限于 PyTorch。TensorFlow 总是一个不错的选择。在本书的最后一章学习趋势框架 PyTorch 是非常有益的。

## 安装 OpenAI 健身房

**OpenAI 健身房**([https://gym.openai.com/](https://gym.openai.com/))是一个强大的开源工具包，用于开发和比较强化学习算法。它提供了多种环境来开发你的强化学习算法。是由专注于打造安全有益的**人工通用智能** ( **AGI** )的非营利研究公司**欧派**([https://openai.com/](https://openai.com/))开发的。

安装健身房有两种方法。第一种是通过`pip`，如下:

```
pip install gym 
```

另一种方法是通过从 Git 存储库中克隆包并从那里安装它来从源构建:

```
git clone https://github.com/openai/gym
cd gym
pip install -e . 
```

安装后，您可以通过运行以下代码来检查可用的健身房环境:

```
>>> from gym import envs
>>> print(envs.registry.all())
dict_values([EnvSpec(Copy-v0), EnvSpec(RepeatCopy-v0), EnvSpec(ReversedAddition-v0), EnvSpec(ReversedAddition3-v0), EnvSpec(DuplicatedInput-v0), EnvSpec(Reverse-v0), EnvSpec(CartPole-v0), EnvSpec(CartPole-v1), EnvSpec(MountainCar-v0), EnvSpec(MountainCarContinuous-v0), EnvSpec(Pendulum-v0), EnvSpec(Acrobot-v1), EnvSpec(LunarLander-v2), EnvSpec(LunarLanderContinuous-v2), EnvSpec(BipedalWalker-v2), EnvSpec(BipedalWalkerHardcore-v2), EnvSpec(CarRacing-v0), EnvSpec(Blackjack-v0)
……
…… 
```

你可以在[https://gym.openai.com/envs/](https://gym.openai.com/envs/)看到完整的环境列表，包括步行、登月、赛车和雅达利游戏。随意玩健身房。

在对标不同的强化学习算法时，我们需要在标准化的环境中应用它们。健身房是一个拥有多种多样环境的完美场所。这类似于使用 MNIST、ImageNet 和汤森路透新闻等数据集作为有监督和无监督学习的基准。

健身房有一个易于使用的强化学习环境界面，我们可以编写**代理**与之交互。那么什么是强化学习呢？什么是特工？让我们在下一节看到。

# 用例子介绍强化学习

在这一章中，我将首先介绍强化学习的要素以及一个有趣的例子，然后将继续讨论我们如何衡量来自环境的反馈，并遵循解决强化学习问题的基本方法。

## 强化学习的要素

你可能在年轻的时候玩过超级马里奥(或索尼克)。在电子游戏中，你控制马里奥收集硬币，同时避开障碍物。如果马里奥撞到障碍物或掉进缺口，游戏就结束了。你会在游戏结束前尽可能多的得到硬币。

强化学习和超级马里奥游戏非常相似。强化学习就是学习做什么。它观察环境中的情况，并确定正确的行动，以便最大限度地获得数字奖励。以下是强化学习任务中的元素列表(我还将每个元素链接到超级马里奥和其他示例，以便更容易理解):

*   **环境**:环境是任务或者模拟。在超级马里奥游戏中，游戏本身就是环境。在自动驾驶中，道路和交通就是环境。在 AlphaGo 下棋中，棋盘就是环境。环境的输入是从发送到**代理**的动作，输出是发送到代理的**状态**和**奖励**。
*   **Agent**:Agent是根据强化学习模型采取**动作**的组件。它与环境交互并观察状态以输入模型。代理的目标是解决环境问题——找到一组最佳的行动来获得最大的回报。超级马里奥游戏中的代理是马里奥，自动驾驶汽车是用于自动驾驶的。
*   **动作**:这是特工可能的动作。在强化学习任务中，当模型开始学习环境时，它通常是随机的。马里奥可能的动作包括左右移动、跳跃和蹲伏。
*   **状态**:状态是对环境的观察。他们用数字的方式描述每一个时间步的情况。对于棋局，状态是棋盘上所有棋子的位置。对于超级马里奥，状态包括马里奥的坐标和时间框架中的其他元素。对于一个学习走路的机器人来说，它两条腿的位置就是状态。
*   **奖励**:每次代理采取行动，都会收到来自环境的数字反馈。这个反馈叫做**奖励**。可以为正、负或零。例如，超级马里奥游戏中的奖励可以是+1(如果马里奥收集硬币)，+2(如果他避开障碍物)，-10(如果他碰到障碍物)，或者 0(对于其他情况)。

下图总结了强化学习的过程:

<figure class="mediaobject">![](../Images/B16326_14_03.png)</figure>

图 14.3:强化学习过程

强化学习过程是一个迭代循环。一开始，代理从环境中观察初始状态 *s* <sub class="Subscript--PACKT-">0</sub> 。然后代理采取行动， *a* <sub class="Subscript--PACKT-">0</sub> ，根据型号。代理移动后，环境现在处于新状态， *s* <sub class="Subscript--PACKT-">1</sub> ，并给出反馈奖励， *R* <sub class="Subscript--PACKT-">1</sub> 。然后，代理采取一个动作，即 *a* <sub class="Subscript--PACKT-">1</sub> ，该动作由模型用输入 *s* <sub class="Subscript--PACKT-">1</sub> 和 *R* <sub class="Subscript--PACKT-">1</sub> 计算得出。这个过程一直持续到终止、完成或永远。

强化学习模型的目标是使总回报最大化。那么如何计算总奖励呢？仅仅是通过总结所有时间步骤的奖励吗？让我们在下一节看到。

## 累积奖励

在时间步 *t* ，**累计奖励**(也称返回**)*G*<sub xmlns:epub="http://www.idpf.org/2007/ops" class="Subscript--PACKT-">1</sub>可以写成:**

<figure class="mediaobject">![](../Images/B16326_14_001.png)</figure>

这里， *T* 是终止时间步长或无穷大。 *G* <sub class="" style="font-style: italic;">t</sub> 表示在时间 *t* 采取行动*a*t<sub class="" style="font-style: italic;">t</sub>后的未来总奖励。在每个时间步骤 *t* 中，强化学习模型试图学习最佳可能的动作，以便最大化 *G* <sub class="" style="font-style: italic;">t</sub> 。

然而，在许多现实世界的情况下，事情并不是这样，我们只是简单地总结所有未来的回报。看看下面的例子:

股票 A 在第一天结束时上涨 6 美元，在第二天结束时下跌 5 美元。股票 B 在第一天下跌 5 美元，在第二天上涨 6 美元。两天后，两只股票都上涨了 1 美元。那么第一天开始你会买哪一个呢？很明显，股票 A 是因为你不会亏钱，如果你在第二天开始时卖掉它，你甚至可以获利 6 美元。

这两种股票的总回报是一样的，但是我们更喜欢股票 A，因为我们更关心即期回报，而不是远期回报。同样，在强化学习中，我们在遥远的未来对奖励进行折扣，折扣系数与时间范围相关联。更长的时间跨度对累积回报的影响应该更小。这是因为较长的时间跨度包含了更多不相关的信息，因此具有更高的方差。

我们定义了一个折扣系数![](../Images/B16326_14_002.png)，其值介于 0 和 1 之间。我们重写了包含折扣因素的累积奖励:

<figure class="mediaobject">![](../Images/B16326_14_003.png)</figure>

可以看到![](../Images/B16326_14_004.png)越大，折扣越小，反之亦然。如果![](../Images/B16326_14_005.png)，实际上没有折扣，模型基于所有未来奖励的总和来评估一个动作。如果![](../Images/B16326_14_006.png)，模型只关注即时奖励 *R* <sub class="" style="font-style: italic;"> t </sub> <sub class="Subscript--PACKT-">+1</sub> 。

现在我们知道如何计算累计奖励，接下来要讲的是如何最大化。

## 强化学习方法

解决强化学习问题主要有两种方法，即寻找最优动作，使累积奖励最大化。一种是基于政策的方法，另一种是基于价值的方法。

一个**策略**是一个功能![](../Images/B16326_14_007.png)，将每个输入状态映射到一个动作:

<figure class="mediaobject">![](../Images/B16326_14_008.png)</figure>

它可以是确定性的，也可以是随机的:

*   **确定性**:从输入状态到输出动作存在一对一映射
*   **随机**:它给出了所有可能动作的概率分布![](../Images/B16326_14_009.png)

在基于**策略的**方法中，模型学习将每个输入状态映射到最佳动作的最优策略。

一个州的**值** *V* 定义为期望未来从该州收取的累计奖励:

<figure class="mediaobject">![](../Images/B16326_14_010.png)</figure>

在基于**值的**方法中，模型学习使输入状态的值最大化的最优值函数。换句话说，代理采取行动以达到实现最大值的状态。

在基于策略的算法中，模型从随机策略开始。然后计算该策略的价值函数。这个步骤叫做**政策评估步骤**。在此之后，基于价值函数找到了一个新的更好的策略。这是政策改进的一步。重复这两个步骤，直到找到最佳策略。而在基于值的算法中，模型从随机值函数开始。然后，它以迭代的方式找到一个新的和改进的值函数，直到它达到最优值函数。

我们了解到有两种主要的方法来解决强化学习问题。在下一节中，让我们看看如何分别以基于策略和基于值的方式，使用具体的算法，即动态规划方法，来解决一个具体的强化学习示例(FrozenLake)。

# 用动态规划求解 FrozenLake 环境

我们将在本节中重点介绍基于策略的和基于值的动态规划算法。但是让我们从模拟 FrozenLake 环境开始。

## 模拟冰湖环境

FrozenLake 是一个典型的 OpenAI 健身房环境，其中有离散的 T2 州。它是关于在网格中将代理从起始区块移动到目标区块，同时避免陷阱。电网要么是 4 * 4([https://gym.openai.com/envs/FrozenLake-v0/](https://gym.openai.com/envs/FrozenLake-v0/)，要么是 8 * 8([https://gym.openai.com/envs/FrozenLake8x8-v0/](https://gym.openai.com/envs/FrozenLake8x8-v0/))。网格中有四种类型的图块:

*   **S** :起始牌。这是状态 0，并附带 0 奖励。
*   **G** :球门牌。它是 4 * 4 网格中的状态 15。它给予+1 奖励并终止一集。
*   **F** :冰冻的瓷砖。在 4 * 4 网格中，状态 1、2、3、4、6、8、9、10、13 和 14 是可行走的图块。它给出 0 奖励。
*   **H** :洞瓦。在 4 * 4 网格中，状态 5、7、11 和 12 是孔图块。它给出 0 奖励并终止一集。

在这里，**集**意味着强化学习环境的模拟。它包含从初始状态到最终状态的状态列表，动作和奖励列表。在 4 * 4 FrozenLake 环境中，有 16 种可能的状态，因为代理可以移动到 16 个图块中的任何一个。有四种可能的动作:向左(0)、向下(1)、向右(2)和向上(3)。

这种环境的棘手之处在于，由于冰表面很滑，代理不会总是朝着它想要的方向移动，而是可以朝着任何其他可以行走的方向移动，或者在某些概率下保持不动。例如，它可能会向右移动，即使它打算向上移动。

现在，让我们按照以下步骤模拟 4 * 4 FrozenLake 环境:

1.  要模拟任何 OpenAI 健身房环境，我们首先需要在表[https://github.com/openai/gym/wiki/Table-of-environments](https://github.com/openai/gym/wiki/Table-of-environments)中查找它的名字。在我们的例子中，我们得到“`FrozenLake-v0`”。
2.  We import the Gym library and create a FrozenLake instance:

    ```
    >>> import gym
    >>> env = gym.make("FrozenLake-v0")
    >>> n_state = env.observation_space.n
    >>> print(n_state)
    16
    >>> n_action = env.action_space.n
    >>> print(n_action)
    4 
    ```

    我们还获得了环境的维度。

3.  Every time we run a new episode, we need to reset the environment:

    ```
    >>> env.reset() 
    0 
    ```

    这意味着代理从状态 0 开始。同样，有 16 种可能的状态，0，1，…，15。

4.  We render the environment to display it:

    ```
    >>> env.render() 
    ```

    您将看到一个 4 * 4 矩阵，代表 FrozenLake 网格和代理所在的图块(状态 0):

    <figure class="mediaobject">![](../Images/B16326_14_04.png)</figure>

    图 14.4:霜湖的初始状态

5.  Let's take a right action since it is walkable:

    ```
    >>> new_state, reward, is_done, info = env.step(2)
    >>> print(new_state)
    1
    >>> print(reward)
    0.0
    >>> print(is_done)
    False
    >>> print(info)
    {'prob': 0.3333333333333333} 
    ```

    代理向右移动到状态 1，概率为 33.33%，由于剧集尚未完成，因此获得 0 奖励。另请参见渲染结果:

    ```
    >>> env.render() 
    ```

    <figure class="mediaobject">![](../Images/B16326_14_05.png)</figure>

    图 14.5:代理向右移动的结果

    您可能会得到完全不同的结果，因为代理可以以 33.33%的概率向下移动到状态 4，或者以 33.33%的概率停留在状态 0。

6.  Next, we define a function that simulates a FrozenLake episode under a given policy and returns the total reward (as an easy start, let's just assume discount factor ![](../Images/B16326_14_011.png)):

    ```
    >>> def run_episode(env, policy):
    ...     state = env.reset()
    ...     total_reward = 0
    ...     is_done = False
    ...     while not is_done:
    ...         action = policy[state].item()
    ...         state, reward, is_done, info = env.step(action)
    ...         total_reward += reward
    ...         if is_done:
    ...             break
    ...     return total_reward 
    ```

    这里，`policy`是 PyTorch 张量，`.item()`提取张量上某个元素的值。

7.  Now let's play around with the environment using a random policy. We will implement a random policy (where random actions are taken) and calculate the average total reward over 1,000 episodes:

    ```
    >>> n_episode = 1000
    >>> total_rewards = []
    >>> for episode in range(n_episode):
    ...     random_policy = torch.randint(high=n_action, size=(n_state,))
    ...     total_reward = run_episode(env, random_policy)
    ...     total_rewards.append(total_reward)
    ...
    >>> print(f'Average total reward under random policy: {sum(total_rewards)/n_episode}')
    Average total reward under random policy: 0.014 
    ```

    平均而言，如果我们采取随机行动，代理有 1.4%的机会达到目标。这告诉我们，解决 FrozenLake 环境并不像你想象的那么容易。

8.  As a bonus step, you can look into the transition matrix. The **transition matrix** ![](../Images/B16326_14_012.png) contains probabilities of taking action *a* from state *s* then reaching ![](../Images/B16326_14_013.png). Take state 6 as an example:

    ```
    >>> print(env.env.P[6])
    {0: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 10, 0.0, False)], 1: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 7, 0.0, True)], 2: [(0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 2, 0.0, False)], 3: [(0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 5, 0.0, True)]} 
    ```

    返回字典 0、1、2、3 的键代表四种可能的动作。键值是与操作相关联的元组列表。元组的格式为(转移概率、新状态、奖励、是否为终端状态)。例如，如果代理打算从状态 6 采取动作 1(向下)，它将以 33.33%的概率移动到状态 5 (H)，并获得 0 奖励，剧集将因此结束；它将以 33.33%的概率移动到状态 10，并获得 0 奖励；它将以 33.33%的概率移动到状态 7 (H)，并获得 0 奖励并终止该集。

我们在这一部分试验了随机策略，只有 1.4%的时间成功。但这让您为下一部分做好了准备，在下一部分中，我们将使用基于值的动态编程算法找到最佳策略，该算法称为**值迭代算法**。

## 用数值迭代算法求解 FrozenLake

数值迭代是一种迭代算法。它以随机策略值 *V* 开始，然后基于**贝尔曼最优性方程**([https://en.wikipedia.org/wiki/Bellman_equation](https://en.wikipedia.org/wiki/Bellman_equation))迭代更新值，直到值收敛。

这些值通常很难完全收敛。因此，有两个收敛标准。一种是传递固定次数的迭代，比如 1000 次或 10000 次。另一种方法是指定一个阈值(如 0.0001 或 0.00001)，如果所有值的变化都小于阈值，我们将终止该过程。

重要的是，在每次迭代中，它选择最大化策略值的操作，而不是取所有操作的期望值(平均值)。迭代过程可以表达如下:

<figure class="mediaobject">![](../Images/B16326_14_014.png)</figure>

这里，![](../Images/B16326_14_015.png)是最优值函数；![](../Images/B16326_14_016.png)表示通过采取动作 *a* 从状态 *s* 移动到状态![](../Images/B16326_14_017.png)的转移概率；而![](../Images/B16326_14_018.png)是状态![](../Images/B16326_14_019.png)下通过采取行动![](../Images/B16326_14_020.png)提供的奖励。

一旦我们获得了最优值，我们就可以很容易地相应地计算出最优策略:

<figure class="mediaobject">![](../Images/B16326_14_021.png)</figure>

让我们使用值迭代算法求解 FrozenLake 环境，如下所示:

1.  首先我们设置 0.99 为折扣因子，0.0001 为收敛阈值:

    ```
    >>> gamma = 0.99
    >>> threshold = 0.0001 
    ```

2.  We develop the value iteration algorithm, which computes the optimal values:

    ```
    >>> def value_iteration(env, gamma, threshold):
    ...     """
    ...     Solve a given environment with value iteration algorithm
    ...     @param env: OpenAI Gym environment
    ...     @param gamma: discount factor
    ...     @param threshold: the evaluation will stop once values for all states are less than the threshold
    ...     @return: values of the optimal policy for the given environment
    ...     """
    ...     n_state = env.observation_space.n
    ...     n_action = env.action_space.n
    ...     V = torch.zeros(n_state)
    ...     while True:
    ...         V_temp = torch.empty(n_state)
    ...         for state in range(n_state):
    ...             v_actions = torch.zeros(n_action)
    ...             for action in range(n_action):
    ...                 for trans_prob, new_state, reward, _ in \
                                           env.env.P[state][action]:
    ...                     v_actions[action] += trans_prob * (
                                         reward + gamma * V[new_state])
    ...             V_temp[state] = torch.max(v_actions)
    ...         max_delta = torch.max(torch.abs(V - V_temp))
    ...         V = V_temp.clone()
    ...         if max_delta <= threshold:
    ...             break
    ...     return V 
    ```

    `value_iteration` 功能执行以下任务:

    *   从策略值全为 0 开始
    *   基于贝尔曼最优性方程更新值
    *   计算所有状态值的最大变化
    *   如果最大变化大于收敛阈值，则继续更新这些值
    *   否则，终止迭代过程并返回最后的值作为最优值
3.  我们应用该算法来求解 FrozenLake 环境以及指定的参数:

    ```
    >>> V_optimal = value_iteration(env, gamma, threshold)
    Take a look at the resulting optimal values:
    >>> print('Optimal values:\n', V_optimal)
    Optimal values:
    tensor([0.5404, 0.4966, 0.4681, 0.4541, 0.5569, 0.0000, 0.3572, 0.0000, 0.5905, 0.6421, 0.6144, 0.0000, 0.0000, 0.7410, 0.8625, 0.0000]) 
    ```

4.  因为我们有最优值，所以我们可以从这些值中提取最优策略。为此，我们开发了以下函数:

    ```
    >>> def extract_optimal_policy(env, V_optimal, gamma):
    ...     """
    ...     Obtain the optimal policy based on the optimal values
    ...     @param env: OpenAI Gym environment
    ...     @param V_optimal: optimal values
    ...     @param gamma: discount factor
    ...     @return: optimal policy
    ...     """
    ...     n_state = env.observation_space.n
    ...     n_action = env.action_space.n
    ...     optimal_policy = torch.zeros(n_state)
    ...     for state in range(n_state):
    ...         v_actions = torch.zeros(n_action)
    ...         for action in range(n_action):
    ...             for trans_prob, new_state, reward, _ in 
                                       env.env.P[state][action]:
    ...                 v_actions[action] += trans_prob * (
                               reward + gamma * V_optimal[new_state])
    ...         optimal_policy[state] = torch.argmax(v_actions)
    ...     return optimal_policy 
    ```

5.  Then we obtain the optimal policy based on the optimal values:

    ```
    >>> optimal_policy = extract_optimal_policy(env, V_optimal, gamma) 
    ```

    看看由此产生的最佳策略:

    ```
    >>> print('Optimal policy:\n', optimal_policy)
    Optimal policy:
    tensor([0., 3., 3., 3., 0., 3., 2., 3., 3., 1., 0., 3., 3., 2., 1., 3.]) 
    ```

    这意味着状态 0 下的最佳动作是 0(左)，状态 1 下的 3(上)等。如果你看网格，这看起来不是很直观。但是请记住，网格很滑，代理可以向不同于期望方向的另一个方向移动。

6.  If you doubt that it is the optimal policy, you can run 1,000 episodes with the policy and gauge how good it is by checking the average reward, as follows:

    ```
    >>> n_episode = 1000
    >>> total_rewards = []
    >>> for episode in range(n_episode):
    ...     total_reward = run_episode(env, optimal_policy)
    ...     total_rewards.append(total_reward) 
    ```

    在这里，我们重用上一节中定义的`run_episode`函数。然后我们打印出平均奖励:

    ```
    >>> print('Average total reward under the optimal policy:', sum(total_rewards) / n_episode)
    Average total reward under the optimal policy: 0.75 
    ```

在由值迭代算法计算的最优策略下，代理有 75%的时间达到目标瓦片。我们可以用基于政策的方法做类似的事情吗？让我们在下一节看到。

## 用策略迭代算法求解 FrozenLake

**策略迭代**算法有两个组成部分，策略评估和策略改进。与值迭代类似，它从任意策略开始，然后是一系列迭代。

在每次迭代的策略评估步骤中，我们首先根据 Bellman 期望方程计算最新策略的值:

<figure class="mediaobject">![](../Images/B16326_14_022.png)</figure>

在策略改进步骤中，我们基于最新的策略值导出改进的策略，同样基于贝尔曼最优性方程:

<figure class="mediaobject">![](../Images/B16326_14_023.png)</figure>

这两个步骤重复进行，直到政策收敛。在收敛时，最新的策略及其值是最优策略和最优值。

让我们开发策略迭代算法，并使用它来解决 FrozenLake 环境，如下所示:

1.  We start with the `policy_evaluation` function that computes the values of a given policy:

    ```
    >>> def policy_evaluation(env, policy, gamma, threshold):
    ...  """
    ...     Perform policy evaluation
    ...     @param env: OpenAI Gym environment
    ...     @param policy: policy matrix containing actions and
            their probability in each state
    ...     @param gamma: discount factor
    ...     @param threshold: the evaluation will stop once values 
            for all states are less than the threshold
    ...     @return: values of the given policy
    ...  """
    ...     n_state = policy.shape[0]
    ...     V = torch.zeros(n_state)
    ...     while True:
    ...         V_temp = torch.zeros(n_state)
    ...         for state in range(n_state):
    ...             action = policy[state].item()
    ...             for trans_prob, new_state, reward, _ in \
                                         env.env.P[state][action]:
    ...                 V_temp[state] += trans_prob * (
                                         reward + gamma * V[new_state])
    ...         max_delta = torch.max(torch.abs–V - V_temp))
    ...         V = V_temp.clone()
    ...         if max_delta <= threshold:
    ...             break
    ...     return V 
    ```

    功能执行以下任务:

    *   用全 0 初始化策略值
    *   基于贝尔曼期望方程更新值
    *   计算所有状态值的最大变化
    *   如果最大变化大于阈值，则继续更新这些值
    *   否则，终止评估过程并返回最新值
2.  Next, we develop the second component, the policy improvement, in the following function:

    ```
    >>> def policy_improvement(env, V, gamma):
    ...  """"""
    ...     Obtain an improved policy based on the values
    ...     @param env: OpenAI Gym environment
    ...     @param V: policy values
    ...     @param gamma: discount factor
    ...     @return: the policy
    ...  """"""
    ...     n_state = env.observation_space.n
    ...     n_action = env.action_space.n
    ...     policy = torch.zeros(n_state)
    ...     for state in range(n_state):
    ...         v_actions = torch.zeros(n_action)
    ...         for action in range(n_action):
    ...             for trans_prob, new_state, reward, _ in 
                                          env.env.P[state][action]:
    ...                 v_actions[action] += trans_prob * (
                                      reward + gamma * V[new_state])
    ...         policy[state] = torch.argmax(v_actions)
    ...     return policy 
    ```

    它基于贝尔曼最优性方程，从输入的策略值中推导出新的更好的策略。

3.  With both components ready, we now develop the whole policy iteration algorithm:

    ```
    >>> def policy_iteration(env, gamma, threshold):
    ...  """
    ...     Solve a given environment with policy iteration algorithm
    ...     @param env: OpenAI Gym environment
    ...     @param gamma: discount factor
    ...     @param threshold: the evaluation will stop once values for all states are less than the threshold
    ...     @return: optimal values and the optimal policy for the given environment
    ...  """
    ...     n_state = env.observation_space.n
    ...     n_action = env.action_space.n
    ...     policy = torch.randint(high=n_action, 
                                   size=(n_state,)).float()
    ...     while True:
    ...         V = policy_evaluation(env, policy, gamma, threshold)
    ...         policy_improved = policy_improvement(env, V, gamma)
    ...         if torch.equal(policy_improved, policy):
    ...             return V, policy_improved
    ...         policy = policy_improved 
    ```

    该功能执行以下任务:

    *   初始化随机策略
    *   执行策略评估以更新策略值
    *   执行策略改进以生成新策略
    *   如果新策略与旧策略不同，则更新策略并运行另一次策略评估和改进迭代
    *   否则，终止迭代过程并返回最新的策略及其值
4.  接下来，我们使用策略迭代来解决 FrozenLake 环境:

    ```
    >>> V_optimal, optimal_policy = policy_iteration(env, gamma, threshold) 
    ```

5.  Finally, we display the optimal policy and its values:

    ```
    >>> pri't('Optimal values'\n', V_optimal)
    Optimal values:
    tensor([0.5404, 0.4966, 0.4681, 0.4541, 0.5569, 0.0000, 0.3572, 0.0000, 0.5905, 0.6421, 0.6144, 0.0000, 0.0000, 0.7410, 0.8625, 0.0000])
    >>> pri't('Optimal policy'\n', optimal_policy)
    Optimal policy:
    tensor([0., 3., 3., 3., 0., 3., 2., 3., 3., 1., 0., 3., 3., 2., 1., 3.]) 
    ```

    我们得到了与值迭代算法相同的结果。

我们刚刚用策略迭代算法解决了 FrozenLake 环境。您可能想知道如何在值迭代和策略迭代算法之间进行选择。请看下表:

<figure class="mediaobject">![](../Images/B16326_14_06.png)</figure>

表 14.1:在策略迭代和值迭代算法之间进行选择

我们用动态规划方法解决了一个强化学习问题。它们需要一个完全已知的环境转换矩阵和奖励矩阵。而且，对于具有多种状态的环境，它们的可扩展性有限。在下一节中，我们将继续使用蒙特卡罗方法的学习之旅，该方法不需要环境的先验知识，并且可扩展性更强。

# 执行蒙特卡罗学习

**蒙特卡洛**(**MC**)-基于的强化学习是一种**无模型**方法，这意味着不需要已知的转移矩阵和奖励矩阵。在本节中，您将了解 21 点环境下的 MC 策略评估，并使用 MC 控制算法解决该环境。21 点是一个典型的过渡矩阵未知的环境。我们先来模拟一下二十一点的环境。

## 模拟 21 点环境

二十一点是一种流行的纸牌游戏。游戏有以下规则:

*   玩家与庄家竞争，如果他们牌的总价值较高且不超过 21，则获胜。
*   从 2 到 10 的牌的值从 2 到 10。
*   卡 J、K 和 Q 的值为 10。
*   ace 的值可以是 1 或 11(称为“可用”ace)。
*   开始时，双方都随机得到两张牌，但只有一张庄家的牌透露给玩家。玩家可以请求额外的卡(称为**击中**)或停止拥有更多的卡(称为**棒**)。在玩家叫牌之前，如果玩家的牌总数超过 21 张(称为**半身像**)，玩家就会输。玩家坚持后，庄家继续抽牌，直到牌数总和达到 17。如果庄家的牌总数超过 21 张，玩家就赢了。如果双方都不失败，得分较高的一方将获胜，或者可能是平局。

健身房的21 点环境([https://github . com/open ai/Gym/blob/master/Gym/envs/toy _ text/21 点. py](https://github.com/openai/gym/blob/master/gym/envs/toy_text/blackjack.py) )制定如下:

*   一集的环境开始于每一方两张牌，并且只观察到庄家牌中的一张。
*   如果有一场胜利或平局，一集就结束了。
*   一集的最终奖励如果玩家赢了是+1，如果玩家输了是-1，如果有平局是 0。
*   在每一回合中，玩家可以采取两个动作中的任何一个，击打(1)和击打(0)

现在，让我们模拟 21 点环境，探索其状态和动作:

1.  首先创建一个`Blackjack`实例:

    ```
    >>> env = gym.make('Blackjack'v0') 
    ```

2.  Reset the environment:

    ```
    >>> env.reset()
    (7, 10, False) 
    ```

    它返回初始状态(三维向量):

    *   玩家当前点数(本例中为`7`)
    *   庄家的牌面点数(本例中为`10`)
    *   有没有可用的 ace(本例中为`False`)

    可用的王牌变量是`True`只有当玩家有一张可以算作 11 的王牌而不会造成半身像。如果玩家没有王牌，或者有王牌但是它爆了，这个状态变量就会变成`False`。

    对于另一个状态示例(18，6，True)，这意味着玩家有一张王牌被计为 11 和 7，并且庄家的透露牌值为 6。

3.  Let's now take some actions to see how the environment works. First, we take a hit action since we only have 7 points:

    ```
    >>> env.step(1)
    ((13, 10, False), 0.0, False, {}) 
    ```

    它返回一个状态(`13, 10, False`)，一个 0 奖励，并且该集没有完成(如`False`)。

4.  让我们再来一次因为我们只有 13 分:

    ```
    >>> env.step(1)
    ((19, 10, False), 0.0, False, {}) 
    ```

5.  We have 19 points and think it is good enough. Then we stop drawing cards by taking action stick (0):

    ```
    >>> env.step(0)
    ((19, 10, False), 1.0, True, {}) 
    ```

    庄家拿到一些牌，然后被抓。因此玩家获胜并获得+1 奖励。这集结束了。

随意玩转 21 点环境。一旦您对环境感到满意，您就可以进入下一部分，关于简单策略的 MC 策略评估。

## 执行蒙特卡罗策略评估

在上一节中，我们应用了动态规划来执行策略评估，这是策略的价值函数。然而，在大多数事先不知道转移矩阵的实际情况下，它是不起作用的。在这种情况下，我们可以使用 MC 方法评估价值函数。

为了估计价值函数，MC 方法使用经验平均回报代替预期回报(如在动态规划中)。计算经验平均收益有两种方法。一个是第一次访问，在所有剧集中，平均只返回一个州*的**第一次出现**的**。另一个是每次访问，它平均返回所有剧集中状态 *s* 的每次出现**。显然，第一次访问方法的计算量要少得多，因此更常用。在本章中，我将只介绍第一次访问方法。*****

 **在这一节中，我们用一个简单的策略进行实验，我们不断添加新卡，直到总值达到 18(或者 19，或者 20，如果你喜欢的话)。我们对简单的政策进行首次访问 MC 评估，如下所示:

1.  We first need to define a function that simulates a Blackjack episode under the simple policy:

    ```
    >>> def run_episode(env, hold_score):
    ...     state = env.reset()
    ...     rewards = []
    ...     states = [state]
    ...     while True:
    ...         action = 1 if state[0] < hold_score else 0
    ...         state, reward, is_done, info = env.step(action)
    ...         states.append(state)
    ...         rewards.append(reward)
    ...         if is_done:
    ...             break
    ...     return states, rewards 
    ```

    在每集的每一轮中，如果当前分数小于`hold_score`或一根棍子，则代理将获得一次命中。

2.  In the MC settings, we need to keep track of states and rewards over all steps. And in first-visit value evaluation, we average returns only for the first occurrence of a state among all episodes. We define a function that evaluates the simple Blackjack policy with first-visit MC:

    ```
    >>> from collections import defaultdict
    >>> def mc_prediction_first_visit(env, hold_score, gamma, n_episode):
    ...     V = defaultdict(float)
    ...     N = defaultdict(int)
    ...     for episode in range(n_episode):
    ...         states_t, rewards_t = run_episode(env, hold_score)
    ...         return_t = 0
    ...         G = {}
    ...         for state_t, reward_t in zip(
                               states_t[1::-1], rewards_t[::-1]):
    ...             return_t = gamma * return_t + reward_t
    ...             G[state_t] = return_t
    ...         for state, return_t in G.items():
    ...             if state[0] <= 21:
    ...                 V[state] += return_t
    ...                 N[state] += 1
    ...     for state in V:
    ...         V[state] = V[state] / N[state]
    ...     return V 
    ```

    该功能执行以下任务:

    *   在带有功能`run_episode`的简单 21 点策略下运行`n_episode`集
    *   对于每个集，计算每个州第一次访问的返回`G`
    *   对于每个状态，通过平均所有剧集的第一次返回来获得值
    *   返回结果值

    请注意，这里我们忽略玩家崩溃的状态，因为我们知道它们的值是-1。

3.  我们指定`hold_score`为`18`，折扣率为`1`作为一集 21 点足够短，将模拟 50 万集:

    ```
    >>> hold_score = 18
    >>> gamma = 1
    >>> n_episode = 500000 
    ```

4.  Now we plug in all variables to perform MC first-visit evaluation:

    ```
    >>> value = mc_prediction_first_visit(env, hold_score, gamma, n_episode) 
    ```

    然后，我们打印结果值:

    ```
    >>> print(value)
    defaultdict(<cla's 'fl'at'>, {(20, 6, False): 0.6923485653560042, (17, 5, False): -0.24390243902439024, (16, 5, False): -0.19118165784832453, (20, 10, False): 0.4326379146490474, (20, 7, False): 0.7686220540168588, (16, 6, False): -0.19249478804725503,
    ……
    ……
    (5, 9, False): -0.20612244897959184, (12, 7, True): 0.058823529411764705, (6, 4, False): -0.26582278481012656, (4, 8, False): -0.14937759336099585, (4, 3, False): -0.1680327868852459, (4, 9, False): -0.20276497695852536, (4, 4, False): -0.3201754385964912, (12, 8, True): 0.11057692307692307}) 
    ```

    我们刚刚计算了所有可能的 280 种状态的值:

    ```
    >>> print('Number of stat's:', len(value))
    Number of states: 280 
    ```

我们刚刚体验了在 21 点环境中使用 MC 方法在简单策略下计算 280 个状态的值。21 点环境的过渡矩阵事先并不知道。此外，如果我们采用动态规划方法，获得转移矩阵(大小为 280 * 280)将非常昂贵。在基于 MC 的解决方案中，我们只需要模拟一堆剧集并计算经验平均值。以类似的方式，我们将在下一节中搜索最佳策略。

## 执行策略蒙特卡罗控制

**MC 控制**用于为转移矩阵未知的环境寻找最优策略。MC 控制有两种类型，策略内和策略外。在**政策方法**中，我们执行政策，并对其进行迭代评估和改进；而在脱离策略的方法中，我们使用另一个策略生成的数据来训练最优策略。

在本节中，我们将重点关注政策方法。它的工作方式与策略迭代方法非常相似。它在以下两个阶段(评估和改进)之间迭代，直到收敛:

*   在评估阶段，我们不是评估状态值，而是评估**动作值**，通常称为**Q 值**。Q 值 *Q* ( *s* ， *a* )是给定策略下，在状态 *s* 下采取动作 *a* 时，状态-动作对( *s* ， *a* )的值。评估可以以首次访问或每次访问的方式进行。
*   在改进阶段，我们通过在每个状态分配最佳操作来更新策略:

<figure class="mediaobject">![](../Images/B16326_14_024.png)</figure>

现在，让我们按照以下步骤搜索具有策略上 MC 控制的最佳 21 点策略:

1.  We start with developing a function that executes an episode by taking the best actions under the given Q-values:

    ```
    >>> def run_episode(env, Q, n_action):
    ...     """
    ...     Run a episode given Q-values
    ...     @param env: OpenAI Gym environment
    ...     @param Q: Q-values
    ...     @param n_action: action space
    ...     @return: resulting states, actions and rewards for the entire episode
    ...     """
    ...     state = env.reset()
    ...     rewards = []
    ...     actions = []
    ...     states = []
    ...     action = torch.randint(0, n_action, [1]).item()
    ...     while True:
    ...         actions.append(action)
    ...         states.append(state)
    ...         state, reward, is_done, info = env.step(action)
    ...         rewards.append(reward)
    ...         if is_done:
    ...             break
    ...         action = torch.argmax(Q[state]).item()
    ...     return states, actions, rewards 
    ```

    这是改进阶段。具体来说，它执行以下任务:

    *   初始化一集
    *   采取随机行动作为探索的开始
    *   第一次动作后，根据给定的 Q 值表进行动作，即![](../Images/B16326_14_025.png)
    *   存储剧集中所有步骤的状态、动作和奖励，用于评估
2.  Next, we develop the on-policy MC control algorithm:

    ```
    >>> def mc_control_on_policy(env, gamma, n_episode):
    ...     """
    ...     Obtain the optimal policy with on-policy MC control method
    ...     @param env: OpenAI Gym environment
    ...     @param gamma: discount factor
    ...     @param n_episode: number of episodes
    ...     @return: the optimal Q-function, and the optimal policy
    ...     """
    ...     G_sum = defaultdict(float)
    ...     N = defaultdict(int)
    ...     Q = defaultdict(lambda: torch.empty(env.action_space.n))
    ...     for episode in range(n_episode):
    ...         states_t, actions_t, rewards_t = 
                           run_episode(env,  Q,  env.action_space.n)
    ...         return_t = 0
    ...         G = {}
    ...         for state_t, action_t, reward_t in zip(
                     states_t[::-1], actions_t[::-1],                                     rewards_t[::-1]):
    ...             return_t = gamma * return_t + reward_t
    ...             G[(state_t, action_t)] = return_t
    ...         for state_action, return_t in G.items():
    ...             state, action = state_action
    ...             if state[0] <= 21:
    ...                 G_sum[state_action] += return_t
    ...                 N[state_action] += 1
    ...                 Q[state][action] = 
                              G_sum[state_action] / N[state_action]
    ...     policy = {}
    ...     for state, actions in Q.items():
    ...         policy[state] = torch.argmax(actions).item()
    ...     return Q, policy 
    ```

    该功能执行以下任务:

    *   随机初始化 Q 值
    *   运行`n_episode`集
    *   对于每一集，执行策略改进并获得训练数据；对结果状态、动作和奖励执行首次访问策略评估，并更新 Q 值
    *   最后，确定最优 Q 值和最优策略
3.  Now that the MC control function is ready, we compute the optimal policy:

    ```
    >>> gamma = 1
    >>> n_episode = 500000 
    >>> optimal_Q, optimal_policy = mc_control_on_policy(env, gamma, n_episode) 
    ```

    看看最佳策略:

    ```
    >>> print(optimal_policy)
    {(16, 8, True): 1, (11, 2, False): 1, (15, 5, True): 1, (14, 9, False): 1, (11, 6, False): 1, (20, 3, False): 0, (9, 6, False): 
    0, (12, 9, False): 0, (21, 2, True): 0, (16, 10, False): 1, (17, 5, False): 0, (13, 10, False): 1, (12, 10, False): 1, (14, 10, False): 0, (10, 2, False): 1, (20, 4, False): 0, (11, 4, False): 1, (16, 9, False): 0, (10, 8, 
    ……
    ……
    1, (18, 6, True): 0, (12, 2, True): 1, (8, 3, False): 1, (13, 3, True): 0, (4, 7, False): 1, (18, 8, True): 0, (6, 5, False): 1, (17, 6, True): 0, (19, 9, True): 0, (4, 4, False): 0, (14, 5, True): 1, (12, 6, True): 0, (4, 9, False): 1, (13, 4, True): 1, (4, 8, False): 1, (14, 3, True): 1, (12, 4, True): 1, (4, 6, False): 0, (12, 5, True): 0, (4, 2, False): 1, (4, 3, False): 1, (5, 4, False): 1, (4, 1, False): 0} 
    ```

你可能想知道这个最优策略是否真的是最优的，比之前的简单策略(保持在 18 点)更好。让我们分别在最优策略和简单策略下模拟 10 万次 21 点游戏:

1.  我们从简单策略下模拟一集的函数开始:

    ```
    >>> def simulate_hold_episode(env, hold_score):
    ...     state = env.reset()
    ...     while True:
    ...         action = 1 if state[0] < hold_score else 0
    ...         state, reward, is_done, _ = env.step(action)
    ...         if is_done:
    ...             return reward 
    ```

2.  接下来，我们在最优策略下进行模拟功能:

    ```
    >>> def simulate_episode(env, policy):
    ...     state = env.reset()
    ...     while True:
    ...         action = policy[state]
    ...         state, reward, is_done, _ = env.step(action)
    ...         if is_done:
    ...             return reward 
    ```

3.  We then run 100,000 episodes for both policies and keep track of their winning times:

    ```
    >>> n_episode = 100000
    >>> hold_score = 18
    >>> n_win_opt = 0
    >>> n_win_hold = 0
    >>> for _ in range(n_episode):
    ...     reward = simulate_episode(env, optimal_policy)
    ...     if reward == 1:
    ...         n_win_opt += 1
    ...     reward = simulate_hold_episode(env, hold_score)
    ...     if reward == 1:
    ...         n_win_hold += 1 
    ```

    我们将结果打印如下:

    ```
    >>> print(f'Winning probability:\nUnder the simple policy: {n_win_hold/n_episode}\nUnder the optimal policy: {n_win_opt/n_episode}')
    Winning probability:
    Under the simple policy: 0.39955
    Under the optimal policy: 0.42779 
    ```

    在最优策略下玩有 43%的胜算，而在简单策略下玩只有 40%的胜算。

在这一节中，我们用无模型算法 MC 学习来解决 21 点环境。在 MC 学习中，Q 值会一直更新到一集结束。这对于长流程来说可能是个问题。在下一节中，我们将讨论 Q 学习，它会更新每集每一步的 Q 值。你会看到它如何提高学习效率。

# 用 Q 学习算法求解出租车问题

q 学习也是无模型学习算法。它为一集的每一步更新 Q 功能。我们将演示如何使用 Q 学习来解决出租车环境。这是一个典型的情节相对较长的环境。因此，让我们首先模拟出租车环境。

## 模拟出租车环境

在出租车环境([https://gym.openai.com/envs/Taxi-v3/](https://gym.openai.com/envs/Taxi-v3/))中，代理充当出租车司机，从一个地点接乘客，在目的地让乘客下车。

所有科目都在一个 5 * 5 的网格上。看看下面的例子:

<figure class="mediaobject">![](../Images/B16326_14_07.png)</figure>

图 14.6:出租车环境示例

某些颜色的瓷砖具有以下含义:

*   **黄色**:空车位置(无乘客)
*   **蓝色**:乘客所在位置
*   **紫色**:乘客目的地
*   **绿色**:有乘客的出租车位置

每集随机分配空车和乘客的起始位置以及乘客的目的地。

四个字母 R、Y、B 和 G 是唯一允许乘客上下车的四个位置。紫色的是目的地，蓝色的是乘客的位置。

出租车可以采取以下六种行动中的任何一种:

*   0:向南移动
*   1:向北移动
*   2:向东移动
*   3:向西移动
*   4:接乘客
*   5:让乘客下车

两块瓷砖之间有一根柱子“|”，防止出租车在两块瓷砖之间移动。

在每一步中，奖励遵循以下规则:

*   +20 用于将乘客送往目的地。这种情况下一集就结束了。而另一种一集就要结束的情况是有 200 步的时候。
*   -10 分，因为试图非法接送(不在 R、Y、B 或 G 中的任何一个上)。
*   -1 否则。

最后但同样重要的是，实际上有 500 种可能的状态:显然，出租车可以在 25 个瓦片中的任何一个上，乘客可以在 R、Y、B、G 中的任何一个上，或者在出租车内，目的地可以是 R、Y、B、G 中的任何一个；因此，我们有 25 * 5 * 4 = 500 种可能的状态。

现在让我们用环境来玩如下:

1.  First we create an instance of the Taxi environment:

    ```
    >>> env = gym.make('Taxi-v3')
    >>> n_state = env.observation_space.n
    >>> print(n_state)
    500
    >>> n_action = env.action_space.n
    >>> print(n_action)
    6 
    ```

    我们也知道状态是用 0 到 499 的整数表示的，有 6 种可能的动作。

2.  We reset the environment and render it:

    ```
    >>> env.reset() 
    262
    >>> env.render() 
    ```

    您将看到一个类似于下面的 5 * 5 网格:

    <figure class="mediaobject">![](../Images/B16326_14_08.png)</figure>

    图 14.7:出租车环境的示例开始步骤

    乘客在蓝色 R 瓷砖上，目的地在紫色 y 上。

3.  Now let's go pick up the passenger by heading west for three tiles and north for two tiles (you will need to take different actions according to your initial state) then take the "pick-up" action:

    ```
    >>> print(env.step(3))
    (242, -1, False, {'prob': 1.0})
    >>> print(env.step(3))
    (222, -1, False, {'prob': 1.0})
    >>> print(env.step(3))
    (202, -1, False, {'prob': 1.0})
    >>> print(env.step(1))
    (102, -1, False, {'prob': 1.0})
    >>> print(env.step(1))
    (2, -1, False, {'prob': 1.0})
    >>> print(env.step(4))
    (18, -1, False, {'prob': 1.0}) 
    ```

    渲染环境:

    ```
    >>> env.render() 
    ```

    <figure class="mediaobject">![](../Images/B16326_14_09.png)</figure>

    图 14.8:乘客在出租车内的状态示例

    出租车变绿，意味着乘客在出租车内。

4.  Now let's head to the destination by taking the "down" action four times (again, you will need to take your own set of actions) then executing a "drop-off":

    ```
    >>> print(env.step(0))
    (118, -1, False, {'prob': 1.0})
    >>> print(env.step(0))
    (218, -1, False, {'prob': 1.0})
    >>> print(env.step(0))
    (318, -1, False, {'prob': 1.0})
    >>> print(env.step(0))
    (418, -1, False, {'prob': 1.0})
    >>> print(env.step(5))
    (410, 20, True, {'prob': 1.0}) 
    ```

    最后，成功下车者将获得+20 奖励。

5.  我们最终渲染环境:

    ```
    >>> env.render() 
    ```

<figure class="mediaobject">![](../Images/B16326_14_10.png)</figure>

图 14.9:乘客到达目的地的状态示例

可以采取一些随机动作，看看一个模型解决环境有多难。我们将在下一节讨论 Q 学习算法。

## 开发 Q 学习算法

**Q-learning** 是一种非策略学习算法，基于行为策略生成的数据优化 Q 值。行为策略是一种贪婪的策略，它采取行动为给定的州实现最高的回报。行为策略生成学习数据，目标策略(我们尝试优化的策略)基于以下等式更新 Q 值:

<figure class="mediaobject">![](../Images/B16326_14_026.png)</figure>

这里，![](../Images/B16326_14_027.png)是采取行动后的结果状态 *a* 从状态 *s* 和 *r* 是关联奖励。![](../Images/B16326_14_028.png)表示行为策略产生最高 Q 值给定状态![](../Images/B16326_14_029.png)。最后，超参数![](../Images/B16326_14_030.png)和![](../Images/B16326_14_031.png)分别是学习率和折扣因子。

从另一个策略产生的经验中学习，使 Q-learning 能够在一集的每一步中优化其 Q 值。我们从贪婪策略中获取信息，并使用这些信息立即更新目标值。

还有一点需要注意的是，目标策略是ε-贪婪的，这意味着它采取的随机动作的概率为![](../Images/B16326_14_032.png)(值从 0 到 1)采取的贪婪动作的概率为![](../Images/B16326_14_033.png)。ε-贪婪政策将**开发**和**探索**相结合:在探索不同行动的同时，利用最佳行动。

现在是时候开发 Q 学习算法来解决 Taxi 环境了:

1.  We start with defining the epsilon-greedy policy:

    ```
    >>> def gen_epsilon_greedy_policy(n_action, epsilon):
    ...     def policy_function(state, Q):
    ...         probs = torch.ones(n_action) * epsilon / n_action
    ...         best_action = torch.argmax(Q[state]).item()
    ...         probs[best_action] += 1.0 - epsilon
    ...         action = torch.multinomial(probs, 1).item()
    ...         return action
    ...     return policy_function 
    ```

    给定|A|个可能的动作，以概率![](../Images/B16326_14_034.png)采取每个动作，以附加概率![](../Images/B16326_14_035.png)选择具有最高状态-动作值的动作。

2.  Now we create an instance of the epsilon-greedy-policy:

    ```
    >>> epsilon = 0.1
    >>> epsilon_greedy_policy = gen_epsilon_greedy_policy(env.action_space.n, epsilon) 
    ```

    这里，![](../Images/B16326_14_036.png)，就是勘探比。

3.  Next, we develop the Q-learning algorithm:

    ```
    >>> def q_learning(env, gamma, n_episode, alpha):
    ...     """
    ...     Obtain the optimal policy with off-policy Q-learning method
    ...     @param env: OpenAI Gym environment
    ...     @param gamma: discount factor
    ...     @param n_episode: number of episodes
    ...     @return: the optimal Q-function, and the optimal policy
    ...     """
    ...     n_action = env.action_space.n
    ...     Q = defaultdict(lambda: torch.zeros(n_action))
    ...     for episode in range(n_episode):
    ...         state = env.reset()
    ...         is_done = False
    ...         while not is_done:
    ...             action = epsilon_greedy_policy(state, Q)
    ...             next_state, reward, is_done, info = 
                                              env.step(action)
    ...             delta = reward + gamma * torch.max(Q[next_state]) - 
                                          Q[state][action]
    ...             Q[state][action] += alpha * delta
    ...             length_episode[episode] += 1 
    ...             total_reward_episode[episode] += reward
    ...             if is_done:
    ...                 break
    ...             state = next_state
    ...     policy = {}
    ...     for state, actions in Q.items():
    ...         policy[state] = torch.argmax(actions).item()
    ...     return Q, policy 
    ```

    我们首先初始化 Q 表。然后在每一集中，我们让代理遵循ε-贪婪策略采取行动，并基于非策略学习方程更新每一步的 Q 函数。我们运行`n_episode`集，最终获得最优策略和 Q 值。

4.  然后我们启动两个变量来存储 1000 集的每一集的表现、集长(一集的步数)和总奖励:

    ```
    >>> n_episode = 1000
    >>> length_episode = [0] * n_episode
    >>> total_reward_episode = [0] * n_episode 
    ```

5.  Finally, we perform Q-learning to obtain the optimal policy for the Taxi problem:

    ```
    >>> gamma = 1
    >>> alpha = 0.4
    >>> optimal_Q, optimal_policy = q_learning(env, gamma, n_episode, alpha) 
    ```

    这里是折扣率![](../Images/B16326_14_037.png)，学习率![](../Images/B16326_14_038.png)。

6.  After 1,000 episodes of learning, we plot the total rewards over episodes as follows:

    ```
    >>> import matplotlib.pyplot as plt
    >>> plt.plot(total_reward_episode)
    >>> plt.title('Episode reward over time')
    >>> plt.xlabel('Episode')
    >>> plt.ylabel('Total reward')
    >>> plt.ylim([-200, 20])
    >>> plt.show() 
    ```

    有关最终结果，请参考以下屏幕截图:

    <figure class="mediaobject">![](../Images/B16326_14_11.png)</figure>

    图 14.10:剧集总奖励

    总奖励在学习中不断提高。而且 600 集后都在+5 左右。

7.  We also plot the lengths over episodes as follows:

    ```
    >>> plt.plot(length_episode)
    >>> plt.title('Episode length over time')
    >>> plt.xlabel('Episode')
    >>> plt.ylabel('Length')
    >>> plt.show() 
    ```

    有关最终结果，请参考以下屏幕截图:

    <figure class="mediaobject">![](../Images/B16326_14_12.png)</figure>

图 14.11:剧集长度

可以看到，剧集长度从最大 200 集减少到 10 集左右，模型收敛在 600 集左右。这意味着经过训练，模型能够在 10 步左右解决问题。

在这一节中，我们用非政策问答学习解决了出租车问题。该算法通过学习贪婪策略产生的经验来优化每一步的 Q 值。

# 摘要

我们从设置工作环境开始这一章。之后，我们学习了强化学习的基础知识和一些例子。在探索了 FrozenLake 环境之后，我们使用了两种动态规划算法:值迭代和策略迭代来解决这个问题。我们谈到了蒙特卡罗学习，并将其用于 21 点环境中的值逼近和控制。最后，开发了 Q 学习算法，解决了出租车问题。

# 练习

1.  能否尝试用价值迭代或策略迭代算法求解 8 * 8 FrozenLake 环境？
2.  你能实现每次访问 MC 策略评估算法吗？
3.  你能在 Q 学习算法中使用不同的探索比![](../Images/B16326_14_039.png)看看事情是如何变化的吗？**