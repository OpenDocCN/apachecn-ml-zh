# 第二章。工具和技术

Python 附带了一个大型的机器学习任务包库。

我们将在本章中看到的软件包如下:

*   IPython 控制台
*   NumPy，它是一个扩展，增加了对多维数组、矩阵和高级数学函数的支持
*   SciPy，这是一个科学公式、常数和数学函数的库
*   Matplotlib，用于创建地块
*   Scikit-learn，这是一个机器学习任务库，如分类、回归和聚类

空间只够给你一个*味的*这些庞大的库，一个重要的技巧就是能够找到并理解各种包的参考资料。不可能在教程风格的文档中呈现所有不同的功能，重要的是能够在有时密集的 API 引用中找到自己的方法。需要记住的一点是，这些包的大部分是由开源社区组合在一起的。它们并不像您从商业产品中期望的那样是单一的结构，因此，理解各种包分类可能会令人困惑。然而，开源软件方法的多样性，以及想法不断被贡献的事实，给了它一个重要的优势。

然而，开源软件不断发展的质量也有其负面影响，尤其是对于 ML 应用程序。例如，代表 Python 机器学习用户群体的人相当不愿意从 Python 2 转到 3。因为 Python 3 打破了向后兼容；重要的是，就其数值处理而言，更新相关包并不是一个微不足道的过程。在写作的时候，所有的重要(对我来说很重要！)包，以及本书中使用的所有包，都与 Python 2.7 或 3x 一起工作。Python 的主要发行版有 Python 3 版本，它们的包集略有不同。

# 机器学习用 Python

Python 是一种通用的编程语言。这是一种解释语言，可以从控制台交互运行。它不需要像 C++或者 Java 那样的编译器，所以开发时间往往会更短。它可以免费下载，并且可以安装在许多不同的操作系统上，包括 UNIX、Windows 和 Macintosh。它特别受科学和数学应用的欢迎。与 C++和 Java 等语言相比，Python 相对容易学习，类似的任务使用更少的代码行。

Python 不是机器学习的唯一平台，但它肯定是使用最多的平台之一。它的主要替代品之一是 **R** 。像 Python 一样，它是开源的，虽然它在应用机器学习中很受欢迎，但它缺乏 Python 的大型开发社区。r 是机器学习和统计分析的专用工具。Python 是一种通用的、广泛使用的编程语言，也有优秀的机器学习应用程序库。

另一种选择是 **Matlab** 。不像 R 和 Python，它是一个商业产品。正如预期的那样，它包含了完善的用户界面和详尽的文档。然而，和 R 一样，它缺乏 Python 的通用性。Python 是一种非常有用的语言，与其他平台相比，您学习它的努力将带来更大的回报。它还拥有优秀的网络、web 开发和微控制器编程库。这些应用程序可以补充或增强您在机器学习方面的工作，而没有笨拙集成的痛苦，也没有学习或记住不同语言的细节。

# IPython 控制台

随着版本 4 的发布，`Ipython`包有了一些显著的变化。一个以前的单片封装结构，它已经被分成子封装。几个 IPython 项目已经分成了各自独立的项目。大部分仓库已经被转移到**朱皮特**项目([jupyter.org](http://jupyter.org))。

IPython 的核心是 IPython 控制台:一个强大的交互式解释器，允许您以非常快速和直观的方式测试您的想法。不需要每次测试代码片段时都创建、保存和运行文件，只需将它键入控制台即可。IPython 的一个强大功能是，它解除了大多数计算平台所基于的传统读取-评估-打印循环。IPython 将评估阶段放入自己的过程中:一个内核(不要与机器学习算法中使用的内核函数混淆)。重要的是，不止一个客户端可以访问内核。这意味着您可以在许多文件中运行代码并访问它们，例如，从控制台运行一个方法。此外，内核和客户端不需要在同一台机器上。这对分布式和网络计算有着强大的影响。

IPython 控制台增加了命令行功能，例如标签完成和`%magic`命令，它们复制终端命令。如果您没有使用已经安装了 IPython 的 Python 发行版，您可以通过在 Python 命令行中键入`ipython`来启动 IPython。在 IPython 控制台中输入`%quickref`会给你一个命令列表和它们的功能。

还应该提到 IPython 笔记本。笔记本已经合并到另一个名为 Jupyter([jupyter.org](http://jupyter.org))的项目中。这个网络应用程序是一个用 40 多种语言进行数值计算的强大平台。笔记本允许您共享和协作实时代码，并发布丰富的图形和文本。

# 安装 SciPy 堆栈

**SciPy** 栈由 Python以及最常用的科学、数学和 ML 库组成。(参观:[scipy.org](http://scipy.org))。这些包括 **NumPy** 、 **Matplotlib** 、 SciPy 库本身和 IPython。这些包可以单独安装在现有 Python 安装的基础上，或者作为一个完整的发行版(**发行版**)。如果你的电脑上没有安装 Python，最简单的入门方法就是使用发行版。主要的 Python 发行版适用于大多数平台，它们在一个包中包含了您需要的一切。单独安装所有包及其依赖项确实需要一些时间，但是如果您的机器上已经配置了 Python 安装，这可能是一个选项。

大多数发行版都为您提供了您需要的所有工具，并且许多都带有强大的开发环境。最好的两个是**蟒蛇**([www.continuum.io/downloads](http://www.continuum.io/downloads))和**天篷**([http://www.enthought.com/products/canopy/](http://www.enthought.com/products/canopy/))。两者都有免费版和商业版。作为参考，我将使用 Python 的 Anaconda 发行版。

安装主要的发行版通常是一项相当轻松的任务。

### 型式

请注意，并非所有发行版都包含相同的 Python 模块集，您可能需要安装模块，或者重新安装模块的正确版本。

# NumPY

我们应该知道，在 Python 中有一个表示数据的类型层次结构。根是不可变的对象，如整数、浮点数和布尔值。基于此，我们有序列类型。这些是由非负整数索引的有序对象集。它们是迭代对象，包括字符串、列表和元组。序列类型有一组常见的操作，例如返回一个元素( *s[i]* )或一个切片( *s[i:j]* )，并找到长度( *len(s)* )或总和( *sum(s)* )。最后，我们有映射类型。这些是由另一个关键对象集合索引的对象集合。映射对象是无序的，由数字、字符串或其他对象索引。内置的 Python 映射类型是字典。

NumPy 通过提供另外两个对象来构建这些数据对象:一个 N 维数组对象(`ndarray`)和一个通用函数对象(`ufunc`)。`ufunc`对象提供对`ndarray`对象的逐元素操作，允许类型转换和数组广播。类型转换是将一种数据类型转换成另一种数据类型的过程，广播描述了在算术运算过程中如何处理不同大小的数组。有线性代数(`linalg`)、随机数生成(`random`)、离散傅立叶变换(`fft`)和单元测试(`testing`)的子包。

NumPy 使用一个`dtype`对象来描述数据的各个方面。这包括浮点、整数等数据类型，数据类型中的字节数(如果数据是结构化的)，以及字段名称和任何子数组的形状。NumPy 有几种新的数据类型，包括:

*   8、16、32 和 64 位`int`值
*   16、32 和 64 位浮点值
*   64 位和 128 位复杂类型
*   `Ndarray`结构化数组类型

我们可以使用`np.cast`对象在类型之间转换。这只是一个字典，根据目标转换类型键入，其值是执行转换的适当函数。这里我们将一个整数转换成一个浮点数 32:

*f= np.cast['f'] (2)*

可以通过多种方式创建 NumPy 数组，例如使用内置数组创建对象(如`arange()`、`ones()`、`zeros()`)从其他 Python 数据结构中转换它们，或者从文件(如`.csv`或`.html`)中创建它们。

`Indexing`和`slicingNumPy`基于序列中使用的切片和索引技术。您应该已经熟悉使用`[i:j:k]`语法在 Python 中对序列(如列表和元组)进行切片，其中`i`是开始索引，`j`是结束，`k`是步骤。NumPy 将选择元组的概念扩展到了 N 维。

启动 Python 控制台并键入以下命令:

```py
import numpy as np
a=np.arange(60).reshape(3,4,5)
print(a)

```

您将观察到以下情况:

![NumPY](graphics/B05198_2_10.jpg)

这将打印前面的3 乘 4 乘 5 数组。您应该知道，我们可以使用像`a[2,3,4]`这样的符号来访问数组中的每个项目。这返回`59`。请记住，索引从 0 开始。

我们可以使用切片技术返回数组的切片。

下图显示了`A[1:2:]`数组:

![NumPY](graphics/B05198_2_14.jpg)

使用椭圆(…)，我们可以选择任何剩余的未指定尺寸。例如`a[...,1]`相当于`a[:,:,1]`:

![NumPY](graphics/B05198_2_11.jpg)

您也可以使用负数从轴的末端开始计数:

![NumPY](graphics/B05198_2_15.jpg)

通过切片，我们正在创建视图；原始数组保持不变，视图保留对原始数组的引用。这意味着，当我们创建切片时，即使我们将其分配给新的变量，如果我们更改原始数组，这些更改也会反映在新数组中。下图演示了这一点:

![NumPY](graphics/B05198_2_16.jpg)

这里 **a** 和 **b** 指的是同一个数组。当我们在`a`赋值时，这也体现在`b`上。要复制一个数组而不是简单地引用它，我们使用标准库中`copy`包中的深度`copy()`函数:

```py
import copy
c=copy.deepcopy(a)

```

在这里，我们创建了一个新的独立数组，`c`。数组`a`中所做的任何更改都不会反映在数组`c`中。

## 数组的构造和转换

这个切片功能也可以和几个 NumPy 类一起使用，作为构造数组的有效方法。例如，`numpy.mgrid`对象创建了一个 `meshgrid`对象，在某些情况下，它提供了一个比`arange()`更方便的选择。它的主要目的是为指定的 N 维体积建立一个坐标阵列。以下图为例:

![Constructing and transforming arrays](graphics/B05198_2_17.jpg)

有时，我们需要以其他方式操纵我们的数据结构。其中包括:

*   **concatenating**: By using the `np.r_` and `np.c_` functions, we can concatenate along one or two axes using the slicing constructs. Here is an example:

    ![Constructing and transforming arrays](graphics/B05198_2_18.jpg)

    这里我们使用了复数 **5j** 作为步长，Python 将它解释为点的数量，包括点在内，以在指定范围之间拟合，这里是 **-1** 到 **1** 。

*   **newaxis**: This object expands the dimensions of an array:

    ![Constructing and transforming arrays](graphics/B05198_2_19.jpg)

    这将在第一维度上创建一个额外的轴。下面在第二维中创建新轴:

    ![Constructing and transforming arrays](graphics/B05198_2_20.jpg)

    您也可以使用布尔运算符进行筛选:

    ```py
    a[a<5]
    Out[]: array([0, 1, 2, 3, 4])

    ```

*   Find the sum of a given axis:

    ![Constructing and transforming arrays](graphics/B05198_2_21.jpg)

    这里我们用轴 2 求和。

## 数学运算

正如您所料，您可以在 NumPy 数组上执行数学运算，如加法、减法、乘法以及三角函数。不同形状阵列上的算术运算可以通过称为**广播**的过程来执行。当在两个数组上操作时，NumPy 从尾部维度逐元素比较它们的形状。如果两个尺寸相同，或者其中一个尺寸为 1，则两个尺寸是兼容的。如果不满足这些条件，则抛出`ValueError`异常。

这都是在后台使用`ufunc`对象完成的。该对象在逐元素的基础上对`ndarrays`进行操作。它们本质上是包装器，为标量函数提供一致的接口，以允许它们使用 NumPy 数组。有超过 60 个`ufunc`对象，涵盖各种各样的操作和类型。当您使用`+`操作符执行诸如添加两个数组的操作时，会自动调用`ufunc`对象。

让我们看看一些额外的数学特征:

*   **Vectors**: We can also create our own vectorized versions of scalar functions using the `np.vectorize()` function. It takes a Python `scalar` function or method as a parameter and returns a vectorized version of this function:

    ```py
    def myfunc(a,b):
    def myfunc(a,b):
    if a > b:
     return a-b
     else:
     return a + b
    vfunc=np.vectorize(myfunc)

    ```

    我们将观察以下输出:

    ![Mathematical operations](graphics/B05198_2_22.jpg)

*   **Polynomial functions**: The `poly1d` class allows us to deal with polynomial functions in a natural way. It accepts as a parameter an array of coefficients in decreasing powers. For example, the polynomial, *2x<sup>2</sup>* + *3x + 4*, can be entered by the following:

    ![Mathematical operations](graphics/B05198_2_23.jpg)

    我们可以看到它以人类可读的方式打印出多项式。我们可以对多项式执行各种操作，例如在某个点进行求值:

    ![Mathematical operations](graphics/B05198_2_24.jpg)

*   Find the roots:

    ![Mathematical operations](graphics/B05198_2_25.jpg)

我们可以使用`asarray(p)`给多项式的系数一个数组，这样它就可以用在所有接受数组的函数中。

正如我们将看到的，构建在 NumPy 上的包为机器学习提供了一个强大而灵活的框架。

# Matplotlib

Matplotlib，或者更重要的是它的子包`PyPlot`，是 Python 中可视化二维数据必不可少的工具。我在这里只简单地提到它，因为当我们通过例子工作时，它的使用应该变得显而易见。它被构建为像 Matlab 一样工作，具有命令风格的功能。每个`PyPlot`函数都会对一个`PyPlot`实例进行一些更改。`PyPlot`的核心是`plot`法。最简单的实现是传递一个列表或一个 1D 数组。如果只有一个参数被传递到绘图，它假设它是一系列 *y* 值，它将自动生成 *x* 值。更常见的是，我们传递坐标为 *x* 和 *y* 的两个 1D 阵列或列表。`plot`方法还可以接受一个参数来指示线条属性，如线条宽度、颜色和样式。这里有一个例子:

```py
import numpy as np
import matplotlib.pyplot as plt

x = np.arange(0., 5., 0.2)
plt.plot(x, x**4, 'r', x, x*90, 'bs', x, x**3, 'g^')
plt.show()

```

该代码打印三行不同样式的:一条红线、蓝色方块和绿色三角形。请注意，我们可以通过一对以上的坐标阵列来绘制多条线。如需线条样式的完整列表，请键入`help(plt.plot)`功能。

Pyplot 和 Matlab 一样，对当前轴应用绘图命令。使用`subplot`命令可以创建多个轴。这里有一个例子:

```py
x1 = np.arange(0., 5., 0.2)
x2 = np.arange(0., 5., 0.1)

plt.figure(1)
plt.subplot(211)
plt.plot(x1, x1**4, 'r', x1, x1*90, 'bs', x1, x1**3, 'g^',linewidth=2.0)

plt.subplot(212)
plt.plot(x2,np.cos(2*np.pi*x2), 'k')
plt.show()

```

前面代码的输出如下:

![Matplotlib](graphics/B05198_2_26.jpg)

另一个有用的图是直方图。`hist()`对象接受输入值的数组或数组序列。第二个参数是箱的数量。在这个例子中，我们将一个分布分成 10 个箱。当设置为`1`或`true`时，赋范参数将计数归一化为以形成概率密度。还要注意，在这个代码中，我们已经标记了 *x* 和 *y* 轴，并在坐标给定的位置显示了一个标题和一些文本:

```py
mu, sigma = 100, 15
x = mu + sigma * np.random.randn(1000)
n, bins, patches = plt.hist(x, 10, normed=1, facecolor='g')
plt.xlabel('Frequency')
plt.ylabel('Probability')
plt.title('Histogram Example')
plt.text(40,.028, 'mean=100 std.dev.=15')
plt.axis([40, 160, 0, 0.03])
plt.grid(True)
plt.show()

```

这段代码的输出如下所示:

![Matplotlib](graphics/B05198_2_27.jpg)

我们要看的最后一个 2D 图是散点图。`scatter`对象采用两个长度相同的序列对象，如数组和可选参数来表示颜色和样式属性。让我们看看这段代码:

```py
N = 100
x = np.random.rand(N)
y = np.random.rand(N)
#colors = np.random.rand(N)
colors=('r','b','g')
area = np.pi * (10 * np.random.rand(N))**2  # 0 to 10 point radiuses
plt.scatter(x, y, s=area, c=colors, alpha=0.5)
plt.show()

```

我们将观察以下输出:

![Matplotlib](graphics/B05198_2_28.jpg)

Matplotlib 还有一个强大的工具箱，用于渲染 3D 图。以下代码演示是三维线、散点图和曲面图的简单示例。三维地块的创建方式与 2D 地块非常相似。这里我们用`gca`功能得到当前轴，将投影参数设置为 3D。所有绘图方法的工作原理都与 2D 的方法非常相似，除了它们现在采用第三组输入值用于 *z* 轴:

```py
import matplotlib as mpl
from mpl_toolkits.mplot3d import Axes3D
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import cm

mpl.rcParams['legend.fontsize'] = 10

fig = plt.figure()
ax = fig.gca(projection='3d')
theta = np.linspace(-3 * np.pi, 6 * np.pi, 100)
z = np.linspace(-2, 2, 100)
r = z**2 + 1
x = r * np.sin(theta)
y = r * np.cos(theta)
ax.plot(x, y, z)

theta2 = np.linspace(-3 * np.pi, 6 * np.pi, 20)
z2 = np.linspace(-2, 2, 20)
r2=z2**2 +1
x2 = r2 * np.sin(theta2)
y2 = r2 * np.cos(theta2)

ax.scatter(x2,y2,z2, c= 'r')
x3 = np.arange(-5, 5, 0.25)
y3 = np.arange(-5, 5, 0.25)
x3, y3 = np.meshgrid(x3, y3)
R = np.sqrt(x3**2 + y3**2)
z3 = np.sin(R)
surf = ax.plot_surface(x3,y3,z3, rstride=1, cstride=1, cmap=cm.Greys_r,
 linewidth=0, antialiased=False)
ax.set_zlim(-2, 2)
plt.show()

```

我们将观察这个输出:

![Matplotlib](graphics/B05198_2_29.jpg)

# 熊猫

熊猫库通过引入几个有用的数据结构和功能来读取和处理数据，从而在 NumPy 上构建了 T2。熊猫是一般数据收集的好工具。它可以轻松处理常见任务，如处理丢失的数据、操作形状和大小、在数据格式和结构之间转换以及从不同来源导入数据。

熊猫引入的主要数据结构有:

*   系列
*   数据框
*   面板

数据框可能是使用最广泛的。它是一个二维结构，实际上是一个由 NumPy 数组、列表、字典或序列创建的表。您也可以通过读取文件来创建数据帧。

或许对熊猫有所了解的最好方法是通过一个典型的用例。假设我们的任务是发现日最高温度是如何随时间变化的。对于这个例子，我们将使用来自塔斯马尼亚霍巴特气象站的历史天气观测。下载以下 ZIP 文件，并将其内容提取到 Python 工作目录中名为**数据**的文件夹中:

[http://davejulian.net/mlbook/data](http://davejulian.net/mlbook/data)

我们要做的第一件事是从中创建一个数据帧:

```py
import pandas as pd
df=pd.read_csv('data/sampleData.csv')

```

检查该数据的前几行:

```py
df.head()
```

我们可以看到每一行的产品代码和站号都是相同的，这些信息是多余的。此外，我们的目的不需要累积最高温度的天数，因此我们也将删除它们:

```py
del df['Bureau of Meteorology station number']
del df['Product code']
del df['Days of accumulation of maximum temperature']

```

让我们通过缩短列标签使数据更容易阅读:

```py
df=df.rename(columns={'Maximum temperature (Degree C)':'maxtemp'})

```

我们只对高质量的数据感兴趣，所以我们只在质量栏中包含有 *Y* 的记录:

```py
df=df[(df.Quality=='Y')]

```

我们可以得到数据的统计摘要:

```py
df.describe()

```

![Pandas](graphics/B05198_2_12.jpg)

如果我们导入`matplotlib.pyplot`包，我们可以绘制数据:

```py
import matplotlib.pyplot as plt
plt.plot(df.Year, df.maxtemp)

```

![Pandas](graphics/B05198_2_30.jpg)

请注意，PyPlot 正确地格式化了日期轴，并通过连接任一侧的两个已知点来处理丢失的数据。我们可以使用以下方法将数据帧转换为 NumPy 数组:

```py
ndarray = df.values

```

如果数据框包含混合的数据类型，那么这个函数将把它们转换成最小公分母类型，这意味着将选择容纳所有值的类型。例如，如果 DataFrame 由 float16 和 float32 混合类型组成，则值将转换为 float 32。

熊猫数据框是查看和处理简单文本和数字数据的绝佳对象。然而，熊猫可能不是更复杂的数字处理的合适工具，比如计算点积，或者寻找线性系统的解。对于数值应用，我们通常使用 NumPy 类。

# 黑桃

**SciPy** (发音为唏嘘 pi)在 NumPy 更纯粹的数学构造之上，为 NumPy 添加了一层，包裹了常见的科学和统计应用。SciPy 提供了操作和可视化数据的高级功能，在交互式使用 Python 时尤其有用。SciPy 被组织成涵盖不同科学计算应用的子包。与 ML 及其功能最相关的软件包列表如下所示:

<colgroup><col style="text-align: left"> <col style="text-align: left"></colgroup> 
| 

包裹

 | 

描述

 |
| --- | --- |
| `cluster` | 这包含两个子包:`cluster.vq`用于 K 均值聚类和矢量量化。`cluster.hierachy`用于分层和聚集聚类，这对于距离矩阵、计算聚类统计以及用树图可视化聚类非常有用。 |
| `constants` | 这些是物理常数和数学常数，如*π*和 *e* 。 |
| `integrate` | 这些是微分方程解算器 |
| `interpolate` | 这些是用于在已知点的范围内创建新数据点的插值函数。 |
| `io` | 这指的是用于创建字符串、二进制或原始数据流以及读写文件的输入和输出函数。 |
| `optimize` | 这是指优化和寻根。 |
| `linalg` | 这指的是线性代数例程，例如基本矩阵计算、求解线性系统、寻找行列式和范数以及分解。 |
| `ndimage` | 这是 N 维图像处理。 |
| `odr` | 这就是正交距离回归。 |
| `stats` | 这指的是统计分布和函数。 |

许多 NumPy 模块与 SciPy 包中的模块具有相同的名称和相似的功能。在很大程度上，SciPy 导入了它的 NumPy 等价物并扩展了它的功能。但是，请注意，与 NumPy 中的功能相比，SciPy 模块中一些名称相同的功能可能会有稍有不同。还应该提到的是，许多 SciPy 类在 scikit-learn 包中都有便利包装器，有时使用它们会更容易。

这些包中的每一个都需要显式导入；这里有一个例子:

```py
import scipy.cluster

```

您可以从 SciPy 网站([scipy.org](http://scipy.org))或控制台获取文档，例如`help(sicpy.cluster)`。

正如我们所看到的，在许多不同的 ML 设置中，一个共同的任务是优化。我们在最后一章研究了单纯形算法的数学。下面是使用 SciPy 的实现。我们记得单纯形优化了一组线性方程。我们研究的问题如下:

最大化*x<sub>1</sub>*+*x<sub>2</sub>*在:*2x<sub>1</sub>T11】+*x<sub>2</sub>≤4*和*x<sub>1</sub>T19】+*2x<sub>2</sub>≤3***

`linprog`对象可能是解决这个问题最简单的对象。这是一个最小化算法，所以我们反转目标的符号。

从`scipy.optimize`开始，导入`linprog`:

```py
objective=[-1,-1]
con1=[[2,1],[1,2]]
con2=[4,3]
res=linprog(objective,con1,con2)
print(res)

```

您将看到以下输出:

![SciPy](graphics/B05198_2_13.jpg)

还有一个`optimisation.minimize`对象，适合稍微复杂一点的问题。此对象将求解器作为参数。目前大约有十几个解算器可用，如果你需要一个更具体的解算器，你可以自己写。最常用、最适合大多数问题的是**内尔德-米德**求解器。该特定解算器使用的**下坡单纯形**算法，该算法基本上是一种启发式搜索，用位于剩余点质心的一个点替换具有高误差的每个测试点。它迭代这个过程，直到收敛到最小值。

在这个例子中，我们使用 **罗森布鲁克**函数作为我们的测试问题。这是一个非凸函数，常用于测试优化问题。这个函数的全局最小值在一个长的抛物线谷上，这使得算法在一个大的、相对平坦的谷中寻找最小值具有挑战性。我们将看到更多此功能:

```py
import numpy as np
from scipy.optimize import minimize
def rosen(x):
 return sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)

def nMin(funct,x0):

 return(minimize(rosen, x0, method='nelder-mead', options={'xtol':
 1e-8, 'disp': True}))

x0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2])

nMin(rosen,x0)

```

前面代码的输出如下:

![SciPy](graphics/B05198_2_32.jpg)

最小化函数采用两个强制参数。这些是目标函数和 x0 的初始值。最小化函数还为求解器方法取了一个可选参数，在本例中我们使用`nelder-mead`方法。选项是一组特定于求解器的键值对，表示为字典。这里，`xtol`是收敛可以接受的相对误差，`disp`设置为打印消息。另一个对机器学习应用非常有用的包是`scipy.linalg`。这个包增加了执行任务的能力，如矩阵求逆、计算特征值和矩阵分解。

# 科学学习

这包括最常见的机器学习任务的算法，如分类、回归、聚类、降维、模型选择和预处理。

Scikit-learn 附带了几个真实世界的数据集供我们练习。让我们来看看其中的一个 Iris 数据集:

```py
from sklearn import datasets
iris = datasets.load_iris()
iris_X = iris.data
iris_y = iris.target
iris_X.shape
(150, 4)

```

数据集包含三种类型的虹膜(濑户鸢尾、云芝和弗吉尼亚鸢尾)的 150 个样本，每个样本具有四个特征。我们可以得到数据集的描述:

```py
iris.DESCR

```

我们可以看到，这四个属性，或者说特征，分别是萼片宽度、萼片长度、花瓣长度和花瓣宽度，单位为厘米。每个样本与三个类中的一个相关联。濑户、云芝和弗吉尼亚。它们分别由 0、1 和 2 表示。

让我们使用这些数据来看一个简单的分类问题。我们想根据它的特征来预测鸢尾的类型:萼片和花瓣的长度和宽度。典型地，scikit-learn 使用估计器来实现`fit(X, y)`方法和训练分类器，以及`predict(X)`方法，如果给定未标记的观测值，`X`，则返回预测的标记，`y`。`fit()`和`predict()`方法通常采用类似 2D 阵列的物体。

这里，我们将使用 **K 最近邻(K-NN)** 技术来解决这个分类问题。K-NN 背后的原理相对简单。我们根据最近邻的分类对未标记样本进行分类。每个数据点根据其最近邻居的少数 *k* 的多数类被分配类成员资格。K-NN 是基于实例的学习的一个例子，其中分类不是根据一个内置的模型，而是参考一个标记的测试集来完成的。K-NN 算法被称为非泛化，因为它只需记住所有的训练数据，并将其与每个新样本进行比较。尽管表面上看起来很简单，或者可能是因为它的简单，但是 K-NN 是一种非常好的解决各种分类和回归问题的技术。

在 **Sklearn** 中有两个不同的 K-NN 分类器。**kneighgborksclassifier**要求用户指定 *k* ，最近邻居的数量。另一方面，**radiusneigoresclassifier**基于每个训练点的固定半径 *r* 内的邻居数量来实现学习。KNeighborsClassifier 是更常用的一个。 *k* 的最佳值在很大程度上取决于数据。一般来说，较大的 *k* 值用于有噪声的数据。作为分类边界的权衡变得不那么明显。如果数据不是均匀采样的，那么 RadiusNeighborsClassifier 可能是更好的选择。由于邻居的数量是基于半径的，因此每个点的 *k* 会有所不同。在稀疏区域， *k* 将低于高样本密度区域:

```py
from sklearn.neighbors import KNeighborsClassifier as knn
from sklearn import datasets
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap

def knnDemo(X,y, n):

 #cresates the the classifier and fits it to the data
 res=0.05
 k1 = knn(n_neighbors=n,p=2,metric='minkowski')
 k1.fit(X,y)

 #sets up the grid
 x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
 x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
 xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, res),np.arange(x2_min, x2_max, res))

 #makes the prediction
 Z = k1.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
 Z = Z.reshape(xx1.shape)

 #creates the color map
 cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])
 cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])

 #Plots the decision surface
 plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap_light)
 plt.xlim(xx1.min(), xx1.max())
 plt.ylim(xx2.min(), xx2.max())

 #plots the samples
 for idx, cl in enumerate(np.unique(y)):
 plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)

 plt.show()

iris = datasets.load_iris()
X1 = iris.data[:, 0:3:2]
X2 = iris.data[:, 0:2]
X3 = iris.data[:,1:3]
y = iris.target
knnDemo(X2,y,15)

```

以下是上述命令的输出:

![Scikit-learn](graphics/B05198_02_02.jpg)

现在让我们来看看 Sklearn 的回归问题。最简单的解决方案是最小化平方误差的总和。这是由`LinearRegression`对象执行的。该对象有一个`fit()`方法，该方法采用两个向量: *X* 作为特征向量， *y* 作为目标向量:

```py
from sklearn import linear_model
clf = linear_model.LinearRegression()
clf.fit ([[0, 0], [1, 1], [2, 2]], [0, 1, 2])
clf.coef_
array([ 0.5,  0.5])

```

`LinearRegression`对象有四个可选参数:

*   `fit_intercept`:布尔值，如果设置为`false`，将假设数据居中，模型在其计算中不会使用截距。默认值为`true`。
*   `normalize`:如果`true`， *X* 回归前归一化为零均值和单位方差。这有时很有用，因为它可以使系数的解释更加明确。默认为`false`。
*   `copy_X`:默认为`true`。如果设置为`false`，将允许 *X* 被覆盖。
*   `n_jobs`:用于计算的作业数。这默认为`1`。这可以用来加快多个处理器上大型问题的计算速度。

其输出具有以下属性:

*   `coef_`:线性回归问题的估计系数数组。如果 y 是多维的，也就是有多个目标变量，那么`coef_`将是一个 2D 数组的形式(`n_targets`，`n_features`)。如果只传递了一个目标变量，那么`coef_`将是一个长度为(`n_features`)的 1D 数组。
*   `intercept_`:这是线性模型中截距或独立项的数组。

对于**普通最小二乘**到的工作，我们假设特征是独立的。当这些项相关时，矩阵 *X* 可以接近奇点。这意味着估计对输入数据的微小变化变得高度敏感。这被称为多重共线性，并导致大的方差和最终的不稳定性。我们稍后会更详细地讨论这个问题，但是现在，让我们来看一个在某种程度上解决这些问题的算法。

岭回归不仅解决了多重共线性的问题，还解决了输入变量的数量大大超过样本数量的情况。`linear_model.Ridge()`对象使用所谓的 L2 正则化。直观地说，我们可以把这理解为在权重向量的极值上加一个惩罚。这有时被称为**收缩**，因为使平均重量变小。这往往会使模型更加稳定，因为它降低了对极值的敏感性。

Sklearn 对象`linear_model.ridge`添加了正则化参数`alpha`。一般来说，`alpha`的小正值会提高模型的稳定性。它可以是浮点数，也可以是数组。如果是一个数组，则假设该数组对应于特定的目标，因此，它必须与目标大小相同。我们可以用下面的简单函数来尝试一下:

```py
from sklearn.linear_model import Ridge
import numpy as np

def ridgeReg(alpha):

 n_samples, n_features = 10, 5
 y = np.random.randn(n_samples)
 X = np.random.randn(n_samples, n_features)
 clf = Ridge(.001)
 res=clf.fit(X, y)
 return(res)
res= ridgeReg(0.001)
print (res.coef_)
print (res.intercept_)

```

现在让我们看看一些用于降维的 scikit-learn 算法。这对机器学习很重要，因为它减少了模型必须考虑的输入变量或特征的数量。这使得模型更加高效，并且可以使结果更容易解释。它还可以通过减少过拟合来提高模型的泛化能力。

当然，重要的是不要丢弃会降低模型准确性的信息。确定什么是冗余的或不相关的是降维算法的主要功能。基本上有两种方法:特征提取和特征选择。特征选择试图找到原始特征变量的子集。另一方面，特征提取通过组合相关变量来创建新的特征变量。

我们先来看看大概最常见的特征提取算法，也就是**主成分分析**或者 **PCA** 。这使用正交变换将一组相关变量转换为一组不相关变量。重要的信息、向量的长度以及它们之间的角度不会改变。这些信息在内积中定义，并保存在正交变换中。主成分分析构建特征向量的方式是，第一个成分尽可能多地解释数据的可变性。随后的组成部分解释了变化量的减少。这意味着，对于许多模型，我们可以只选择前几个主要组件，直到我们确信它们在我们的数据中所占的可变性与实验规范所要求的一样多。

可能最通用的核函数，也是在大多数情况下给出好结果的核函数，是**径向基函数** ( **径向基函数**)。rbf 核取一个参数`gamma`，这个参数可以松散地解释为每个样本的影响范围的倒数。γ值较低意味着每个样本对模型选择的样本具有较大的影响半径。`KernalPCA fit_transform`方法获取训练向量，将其拟合到模型，然后将其转换为其主成分。让我们看看命令:

```py
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import KernelPCA
from sklearn.datasets import make_circles
np.random.seed(0)
X, y = make_circles(n_samples=400, factor=.3, noise=.05)
kpca = KernelPCA(kernel='rbf', gamma=10)
X_kpca = kpca.fit_transform(X)
plt.figure()
plt.subplot(2, 2, 1, aspect='equal')
plt.title("Original space")
reds = y == 0
blues = y == 1
plt.plot(X[reds, 0], X[reds, 1], "ro")
plt.plot(X[blues, 0], X[blues, 1], "bo")
plt.xlabel("$x_1$")
plt.ylabel("$x_2$")
plt.subplot(2, 2, 3, aspect='equal')
plt.plot(X_kpca[reds, 0], X_kpca[reds, 1], "ro")
plt.plot(X_kpca[blues, 0], X_kpca[blues, 1], "bo")
plt.title("Projection by KPCA")
plt.xlabel("1st principal component in space induced by $\phi$")
plt.ylabel("2nd component")
plt.subplots_adjust(0.02, 0.10, 0.98, 0.94, 0.04, 0.35)
plt.show()
#print('gamma= %0.2f' %gamma)

```

正如我们所看到的，监督学习算法成功的一个主要障碍是从训练数据到测试数据的转换。标记的训练集可能具有新的未标记数据中不存在的独特特征。我们已经看到，我们可以训练我们的模型在训练数据上相当精确，然而这种精度可能不会转化为我们的未标记测试数据。过拟合是监督学习中的一个重要问题，有许多技术可以用来最小化它。评估模型在训练集上的估计器性能的一种方法是使用交叉验证。让我们使用支持向量机在虹膜数据上进行测试。我们需要做的第一件事是将数据分成训练集和测试集。`train_test_split`方法采用两种数据结构:数据本身和目标。它们可以是 NumPy 数组、熊猫数据帧列表或 SciPy 矩阵。如您所料，目标需要与数据一样长。`test_size`参数可以是一个介于`0`和`1`之间的浮点数，表示分割中包含的数据比例，也可以是一个表示测试样本数量的整数。这里，我们使用了一个`test_size`对象作为`.3`，表示我们保留了 40%的数据用于测试。

在本例中，我们使用`svm.SVC`类和`.score`方法返回预测标签时测试数据的平均准确度:

```py
from sklearn.cross_validation import train_test_split
from sklearn import datasets
from sklearn import svm
from sklearn import cross_validation
iris = datasets.load_iris()
X_train, X_test, y_train, y_test = train_test_split (iris.data, iris.target, test_size=0.4, random_state=0)
clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)
scores=cross_validation.cross_val_score(clf, X_train, y_train, cv=5)
print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))

```

您将看到以下输出:

![Scikit-learn](graphics/B05198_2_33.jpg)

支持向量机有一个`penalty`参数需要手动设置，很有可能我们会多次运行 SVC 并调整这个参数，直到得到最佳拟合。然而，这样做将信息从训练集泄露到测试集，因此我们可能仍然存在过度拟合的问题。对于任何具有必须手动设置的参数的估计器来说，这都是一个问题，我们将在[第 4 章](4.html "Chapter 4. Models – Learning from Information")、*模型–从信息中学习*中对此进行进一步探讨。

# 总结

我们已经看到了一套基本的机器学习工具，以及它们在简单数据集上的一些用法。你可能开始想知道这些工具如何应用于现实世界的问题。我们讨论过的每个库之间都有相当多的重叠。许多人执行相同的任务，但以不同的方式添加或执行相同的功能。为每个问题选择使用哪个库不一定是一个明确的决定。没有最好的图书馆；只有首选库，这因人而异，当然也因应用程序的具体情况而异。

在下一章中，我们将研究机器学习最重要的，也是经常被忽视的一个方面，那就是数据。