# 第三章。预测建模

在本章中，我们将介绍以下食谱:

*   利用支持向量机构建线性分类器
*   利用支持向量机构建非线性分类器
*   解决阶级不平衡
*   提取置信度度量
*   寻找最优超参数
*   构建事件预测器
*   估计流量

# 简介

**预测建模**可能是数据分析中最令人兴奋的领域之一。近年来，由于大量数据在许多不同的垂直行业中可用，它受到了广泛关注。它非常常用于数据挖掘领域，用于预测未来趋势。

预测建模是一种用于预测系统未来行为的分析技术。它是算法的集合，可以识别独立输入变量和目标响应之间的关系。我们根据观察建立一个数学模型，然后用这个模型来估计未来会发生什么。

在预测建模中，我们需要收集具有已知响应的数据来训练我们的模型。一旦我们创建了这个模型，我们就使用一些指标来验证它，然后用它来预测未来的价值。我们可以使用许多不同类型的算法来创建预测模型。在本章中，我们将使用支持向量机来建立线性和非线性模型。

使用可能影响系统行为的许多特征来建立预测模型。例如，为了估计天气状况，我们可以使用各种类型的数据，如温度、气压、降水和其他大气过程。同样，当我们处理其他类型的系统时，在训练我们的模型之前，我们需要决定哪些因素可能会影响其行为，并将它们作为特征向量的一部分。

# 利用支持向量机构建线性分类器

支持向量机是监督学习模型，用于构建分类器和回归器。一个 SVM 通过求解一个数学方程组找到了两组点之间的最佳分离边界。如果你不熟悉支持向量机，这里有几个很好的教程可以让你开始:

*   [http://web.mit.edu/zoya/www/SVM.pdf](http://web.mit.edu/zoya/www/SVM.pdf)
*   [http://www.support-vector.net/icml-tutorial.pdf](http://www.support-vector.net/icml-tutorial.pdf)
*   [http://www.svms.org/tutorials/Berwick2003.pdf](http://www.svms.org/tutorials/Berwick2003.pdf)

让我们看看如何使用 SVM 构建线性分类器。

## 做好准备

让我们可视化我们的数据来理解手头的问题。我们将使用已经提供给您的`svm.py`作为参考。在我们建造 SVM 之前，让我们先了解一下我们的数据。我们将使用已经提供给您的`data_multivar.txt`文件。让我们看看如何可视化数据。创建一个新的 Python 文件，并向其中添加以下行:

```
import numpy as np
import matplotlib.pyplot as plt

import utilities 

# Load input data
input_file = 'data_multivar.txt'
X, y = utilities.load_data(input_file)
```

我们只是导入了几个包并命名了输入文件。我们来看看`load_data()`法:

```
# Load multivar data in the input file
def load_data(input_file):
    X = []
    y = []
    with open(input_file, 'r') as f:
        for line in f.readlines():
            data = [float(x) for x in line.split(',')]
            X.append(data[:-1])
            y.append(data[-1]) 

    X = np.array(X)
    y = np.array(y)

    return X, y
```

我们需要将数据分成类，如下所示:

```
class_0 = np.array([X[i] for i in range(len(X)) if y[i]==0])
class_1 = np.array([X[i] for i in range(len(X)) if y[i]==1])
```

现在我们已经分离了数据，让我们绘制它:

```
plt.figure()
plt.scatter(class_0[:,0], class_0[:,1], facecolors='black', edgecolors='black', marker='s')
plt.scatter(class_1[:,0], class_1[:,1], facecolors='None', edgecolors='black', marker='s')
plt.title('Input data')
plt.show()
```

如果运行此代码，您将看到下图:

![Getting ready](images/B05485_03_01.jpg)

上图中的由两种类型的点组成–**实心方块** 和**空心方块**。在机器学习行话中，我们说我们的数据由两类组成。我们的目标是建立一个模型，可以分离实心方块和空心方块。

## 怎么做…

1.  我们需要将数据集分为训练数据集和测试数据集。在同一个 Python 文件中添加以下几行:

    ```
    # Train test split and SVM training
    from sklearn import cross_validation
    from sklearn.svm import SVC

    X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.25, random_state=5)
    ```

2.  让我们使用线性内核初始化 SVM 对象。如果你不知道内核是什么，你可以查看[http://www . Eric-Kim . net/Eric-Kim-net/post/1/kernel _ trick . html](http://www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html)。在文件中添加以下几行:

    ```
    params = {'kernel': 'linear'}
    classifier = SVC(**params)
    ```

3.  我们现在准备训练线性 SVM 分类器:

    ```
    classifier.fit(X_train, y_train)
    ```

4.  我们现在可以看到分类器的表现:

    ```
    utilities.plot_classifier(classifier, X_train, y_train, 'Training dataset')
    plt.show()
    ```

5.  If you run this code, you will get the following figure:

    ![How to do it…](images/B05485_03_02.jpg)

    `plot_classifier`功能与我们在上一章中讨论的相同。它有一些小的增加。您可以查看已经提供给您的`utilities.py`文件了解更多详情。

6.  让我们看看在测试数据集上的表现。在同一文件中添加以下行:

    ```
    y_test_pred = classifier.predict(X_test)
    utilities.plot_classifier(classifier, X_test, y_test, 'Test dataset')
    plt.show()
    ```

7.  If you run this code, you will see the following figure:

    ![How to do it…](images/B05485_03_03.jpg)

8.  让我们计算训练集的精确度。在同一文件中添加以下行:

    ```
    from sklearn.metrics import classification_report

    target_names = ['Class-' + str(int(i)) for i in set(y)]
    print "\n" + "#"*30
    print "\nClassifier performance on training dataset\n"
    print classification_report(y_train, classifier.predict(X_train), target_names=target_names)
    print "#"*30 + "\n"
    ```

9.  If you run this code, you will see the following on your Terminal:

    ![How to do it…](images/B05485_03_04.jpg)

10.  最后让我们来看看测试数据集的分类报告:

    ```
    print "#"*30
    print "\nClassification report on test dataset\n"
    print classification_report(y_test, y_test_pred, target_names=target_names)
    print "#"*30 + "\n"
    ```

11.  If you run this code, you will see the following on the Terminal:

    ![How to do it…](images/B05485_03_05.jpg)

从我们可视化数据的图中，我们可以看到实心方块被空的方块完全包围。这意味着数据不是线性可分的。我们不能画一条漂亮的直线来分隔两组点！因此，我们需要一个非线性分类器来分离这些数据点。

# 利用支持向量机构建非线性分类器

一个 SVM 提供了多种选项来构建一个非线性分类器。我们需要使用各种核构建一个非线性分类器。为了简单起见，我们在这里考虑两种情况。当我们想要表示两组点之间的曲线边界时，我们可以使用多项式函数或径向基函数来实现。

## 怎么做…

1.  For the first case, let's use a polynomial kernel to build a nonlinear classifier. In the same Python file, search for the following line:

    ```
    params = {'kernel': 'linear'}
    ```

    将这一行改为:

    ```
    params = {'kernel': 'poly', 'degree': 3}
    ```

    这意味着我们使用一个 3 次多项式函数。如果你增加度数，这意味着我们允许多项式更有曲线。然而，曲线性是有代价的，因为它需要更多的时间来训练，因为它的计算成本更高。

2.  If you run this code now, you will get the following figure:

    ![How to do it…](images/B05485_03_06.jpg)

3.  You will also see the following classification report printed on your Terminal:

    ![How to do it…](images/B05485_03_07.jpg)

4.  We can also use a radial basis function kernel to build a nonlinear classifier. In the same Python file, search for the following line:

    ```
    params = {'kernel': 'poly', 'degree': 3}
    ```

    将此行替换为以下一行:

    ```
    params = {'kernel': 'rbf'} 
    ```

5.  If you run this code now, you will get the following figure:

    ![How to do it…](images/B05485_03_08.jpg)

6.  You will also see the following classification report printed on your Terminal:

    ![How to do it…](images/B05485_03_09.jpg)

# 解决阶级不平衡

直到现在，我们处理的问题是我们所有的类中有相似数量的数据点。在现实世界中，我们可能无法以如此有序的方式获取数据。有时，一个类中数据点的数量比其他类中数据点的数量多得多。如果发生这种情况，那么分类器往往会产生偏差。边界不会反映数据的真实性质，因为两个类之间的数据点数量有很大差异。因此，解释这种差异并消除它以使我们的分类器保持不偏不倚变得很重要。

## 怎么做…

1.  让我们加载数据:

    ```
    input_file = 'data_multivar_imbalance.txt'
    X, y = utilities.load_data(input_file)
    ```

2.  Let's visualize the data. The code for visualization is exactly the same as it was in the previous recipe. You can also find it in the file named `svm_imbalance.py` already provided to you. If you run it, you will see the following figure:

    ![How to do it…](images/B05485_03_10.jpg)

3.  Let's build an SVM with a linear kernel. The code is the same as it was in the previous recipe. If you run it, you will see the following figure:

    ![How to do it…](images/B05485_03_11.jpg)

4.  You might wonder why there's no boundary here! Well, this is because the classifier is unable to separate the two classes at all, resulting in 0% accuracy for `Class-0`. You will also see a classification report printed on your Terminal, as shown in the following screenshot:

    ![How to do it…](images/B05485_03_12.jpg)

    如我们所料，`Class-0`的精度为 0%。

5.  Let's go ahead and fix this! In the Python file, search for the following line:

    ```
    params = {'kernel': 'linear'} 
    ```

    将前一行改为:

    ```
    params = {'kernel': 'linear', 'class_weight': 'auto'} 
    ```

    `class_weight`参数将计算每个类中的数据点数量，以调整权重，从而使不平衡不会对性能产生不利影响。

6.  You will get the following figure once you run this code:

    ![How to do it…](images/B05485_03_13.jpg)

7.  Let's look at the classification report, as follows:

    ![How to do it…](images/B05485_03_14.jpg)

如我们所见，`Class-0`现在以非零百分比精度被检测到。

# 提取置信度度量

知道我们对未知数据分类的置信度就好了。当一个新的数据点被归入一个已知的类别时，我们可以训练 SVM 来计算这个输出的置信水平。

## 怎么做…

1.  完整的代码在已经提供给你的`svm_confidence.py`文件中给出。我们在这里只讨论食谱的核心。让我们定义一些输入数据:

    ```
    # Measure distance from the boundary
    input_datapoints = np.array([[2, 1.5], [8, 9], [4.8, 5.2], [4, 4], [2.5, 7], [7.6, 2], [5.4, 5.9]])
    ```

2.  让我们测量到边界的距离:

    ```
    print "\nDistance from the boundary:"
    for i in input_datapoints:
        print i, '-->', classifier.decision_function(i)[0]
    ```

3.  You will see the following printed on your Terminal:

    ![How to do it…](images/B05485_03_15.jpg)

4.  Distance from the boundary gives us some information about the datapoint, but it doesn't exactly tell us how confident the classifier is about the output tag. To do this, we need **Platt scaling**. This is a method that converts the distance measure into probability measure between classes. You can check out the following tutorial to learn more about Platt scaling: [http://fastml.com/classifier-calibration-with-platts-scaling-and-isotonic-regression](http://fastml.com/classifier-calibration-with-platts-scaling-and-isotonic-regression). Let's go ahead and train an SVM using Platt scaling:

    ```
    # Confidence measure
    params = {'kernel': 'rbf', 'probability': True}
    classifier = SVC(**params)
    ```

    `probability`参数告诉 SVM，它也应该训练计算概率。

5.  让我们训练分类器:

    ```
    classifier.fit(X_train, y_train)
    ```

6.  Let's compute the confidence measurements for these input datapoints:

    ```
    print "\nConfidence measure:"
    for i in input_datapoints:
        print i, '-->', classifier.predict_proba(i)[0]
    ```

    `predict_proba`功能测量置信度值。

7.  You will see the following on your Terminal:

    ![How to do it…](images/B05485_03_16.jpg)

8.  让我们看看点在哪里，期待边界:

    ```
    utilities.plot_classifier(classifier, input_datapoints, [0]*len(input_datapoints), 'Input datapoints', 'True')
    ```

9.  If you run this, you will get the following figure:

    ![How to do it…](images/B05485_03_17.jpg)

# 寻找最优超参数

如前一章所述，超参数在决定分类器的性能时非常重要。让我们看看如何为支持向量机提取最优超参数。

## 怎么做…

1.  完整的代码在已经提供给你的`perform_grid_search.py`文件中给出。我们在这里只讨论食谱的核心部分。我们将在这里使用交叉验证，这在前面的食谱中已经介绍过了。加载数据并将其拆分为训练和测试数据集后，将以下内容添加到文件中:

    ```
    # Set the parameters by cross-validation
    parameter_grid = [  {'kernel': ['linear'], 'C': [1, 10, 50, 600]},
                        {'kernel': ['poly'], 'degree': [2, 3]},
                        {'kernel': ['rbf'], 'gamma': [0.01, 0.001], 'C': [1, 10, 50, 600]},
                     ]
    ```

2.  让我们定义我们想要使用的指标:

    ```
    metrics = ['precision', 'recall_weighted']
    ```

3.  让我们开始搜索每个指标的最优超参数:

    ```
    for metric in metrics:
        print "\n#### Searching optimal hyperparameters for", metric

        classifier = grid_search.GridSearchCV(svm.SVC(C=1), 
                parameter_grid, cv=5, scoring=metric)
        classifier.fit(X_train, y_train)
    ```

4.  我们来看看分数:

    ```
        print "\nScores across the parameter grid:"
        for params, avg_score, _ in classifier.grid_scores_:
            print params, '-->', round(avg_score, 3)
    ```

5.  让我们打印最佳参数集:

    ```
        print "\nHighest scoring parameter set:", classifier.best_params_
    ```

6.  If you run this code, you will see the following on your Terminal:

    ![How to do it…](images/B05485_03_18.jpg)

7.  正如我们在上图中可以看到的，它搜索所有的最优超参数。在这种情况下，超参数是核的类型、C 值和伽玛。它将尝试这些参数的各种组合，以找到最佳参数。让我们在测试数据集上测试一下:

    ```
        y_true, y_pred = y_test, classifier.predict(X_test)
        print "\nFull performance report:\n"
        print classification_report(y_true, y_pred)
    ```

8.  If you run this code, you will see the following on your Terminal:

    ![How to do it…](images/B05485_03_19.jpg)

# 构建事件预测器

让我们把所有这些知识应用到现实世界的问题中。我们将建造一个 SVM 来预测进出建筑物的人数。数据集可在[获取。我们将使用这个数据集的稍加修改的版本，以便更容易分析。修改后的数据可以在已经提供给你的`building_event_binary.txt`和`building_event_multiclass.txt`文件中找到。](https://archive.ics.uci.edu/ml/datasets/CalIt2+Building+People+Counts)

## 做好准备

在开始构建模型之前，让我们先了解一下数据格式。`building_event_binary.txt`中的每一行由六个逗号分隔的字符串组成。这六个字符串的顺序如下:

*   一天
*   日期
*   时间
*   走出大楼的人数
*   进入大楼的人数
*   指示它是否是事件的输出

第一个五个字符串组成输入数据，我们的任务是预测建筑内是否有事件发生。

`building_event_multiclass.txt`中的每一行由六个逗号分隔的字符串组成。从输出是建筑中正在发生的事件的确切类型的意义上来说，这比前一个文件更精细。这六个字符串的顺序如下:

*   一天
*   日期
*   时间
*   走出大楼的人数
*   进入大楼的人数
*   指示事件类型的输出

前五个字符串构成输入数据，我们的任务是预测建筑物中正在发生什么类型的事件。

## 怎么做…

1.  We will use `event.py` that's already provided to you for reference. Create a new Python file, and add the following lines:

    ```
    import numpy as np
    from sklearn import preprocessing
    from sklearn.svm import SVC

    input_file = 'building_event_binary.txt'

    # Reading the data
    X = []
    count = 0
    with open(input_file, 'r') as f:
        for line in f.readlines():
            data = line[:-1].split(',')
            X.append([data[0]] + data[2:])

    X = np.array(X)
    ```

    我们刚刚将所有数据加载到`X`中。

2.  让我们将数据转换成数字形式:

    ```
    # Convert string data to numerical data
    label_encoder = [] 
    X_encoded = np.empty(X.shape)
    for i,item in enumerate(X[0]):
        if item.isdigit():
            X_encoded[:, i] = X[:, i]
        else:
            label_encoder.append(preprocessing.LabelEncoder())
            X_encoded[:, i] = label_encoder[-1].fit_transform(X[:, i])

    X = X_encoded[:, :-1].astype(int)
    y = X_encoded[:, -1].astype(int)
    ```

3.  让我们使用径向基函数、普拉特缩放和类平衡来训练 SVM:

    ```
    # Build SVM
    params = {'kernel': 'rbf', 'probability': True, 'class_weight': 'auto'} 
    classifier = SVC(**params)
    classifier.fit(X, y)
    ```

4.  我们现在准备执行交叉验证:

    ```
    # Cross validation
    from sklearn import cross_validation

    accuracy = cross_validation.cross_val_score(classifier, 
            X, y, scoring='accuracy', cv=3)
    print "Accuracy of the classifier: " + str(round(100*accuracy.mean(), 2)) + "%"
    ```

5.  让我们在一个新的数据点上测试我们的 SVM
6.  如果您运行该代码，您将在您的终端上看到以下输出:

    ```
    Accuracy of the classifier: 89.88%
    Output class: event

    ```

7.  如果您使用`building_event_multiclass.txt`文件代替`building_event_binary.txt`作为输入数据文件，您将在您的终端上看到以下输出:

    ```
    Accuracy of the classifier: 65.9%
    Output class: eventA

    ```

# 估计流量

支持向量机的一个有趣的应用是根据相关数据预测流量。在之前的食谱中，我们使用了 SVM 作为分类器。在这个食谱中，我们将使用它作为回归来估计流量。

## 做好准备

我们将使用[https://archive.ics.uci.edu/ml/datasets/Dodgers+Loop+Sensor](https://archive.ics.uci.edu/ml/datasets/Dodgers+Loop+Sensor)提供的数据集。这是一个数据集，统计了洛杉矶道奇主场体育场棒球比赛期间经过的汽车数量。我们将使用稍微修改的数据集形式，以便更容易分析。您可以使用已经提供给您的`traffic_data.txt`文件。该文件中的每一行都包含逗号分隔的字符串，格式如下:

*   一天
*   时间
*   对手队
*   无论棒球比赛是否正在进行
*   路过的汽车数量

## 怎么做…

1.  Let's see how to build an SVM regressor. We will use `traffic.py` that's already provided to you as a reference. Create a new Python file, and add the following lines:

    ```
    # SVM regressor to estimate traffic

    import numpy as np
    from sklearn import preprocessing
    from sklearn.svm import SVR

    input_file = 'traffic_data.txt'

    # Reading the data
    X = []
    count = 0
    with open(input_file, 'r') as f:
        for line in f.readlines():
            data = line[:-1].split(',')
            X.append(data)

    X = np.array(X)
    ```

    我们将所有输入数据加载到`X`中。

2.  让我们对这个数据进行编码:

    ```
    # Convert string data to numerical data
    label_encoder = [] 
    X_encoded = np.empty(X.shape)
    for i,item in enumerate(X[0]):
        if item.isdigit():
            X_encoded[:, i] = X[:, i]
        else:
            label_encoder.append(preprocessing.LabelEncoder())
            X_encoded[:, i] = label_encoder[-1].fit_transform(X[:, i])

    X = X_encoded[:, :-1].astype(int)
    y = X_encoded[:, -1].astype(int)
    ```

3.  Let's build and train the SVM regressor using the radial basis function:

    ```
    # Build SVR
    params = {'kernel': 'rbf', 'C': 10.0, 'epsilon': 0.2} 
    regressor = SVR(**params)
    regressor.fit(X, y)
    ```

    在前几行中，`C`参数指定了错误分类的惩罚，`epsilon`指定了不应用惩罚的限制。

4.  让我们执行交叉验证来检查回归器的性能:

    ```
    # Cross validation
    import sklearn.metrics as sm

    y_pred = regressor.predict(X)
    print "Mean absolute error =", round(sm.mean_absolute_error(y, y_pred), 2)
    ```

5.  让我们在数据点

    ```
    # Testing encoding on single data instance
    input_data = ['Tuesday', '13:35', 'San Francisco', 'yes']
    input_data_encoded = [-1] * len(input_data)
    count = 0
    for i,item in enumerate(input_data):
        if item.isdigit():
            input_data_encoded[i] = int(input_data[i])
        else:
            input_data_encoded[i] = int(label_encoder[count].transform(input_data[i]))
            count = count + 1 

    input_data_encoded = np.array(input_data_encoded)

    # Predict and print output for a particular datapoint
    print "Predicted traffic:", int(regressor.predict(input_data_encoded)[0])
    ```

    上测试一下
6.  如果您运行此代码，您将会在您的终端上看到以下内容:

    ```
    Mean absolute error = 4.08
    Predicted traffic: 29

    ```