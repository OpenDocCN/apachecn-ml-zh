# 第六章。分析文本数据

在本章中，我们将介绍以下食谱:

*   使用标记化预处理数据
*   词干文本数据
*   使用引理化将文本转换为基本形式
*   使用分块分割文本
*   构建单词包模型
*   构建文本分类器
*   确定性别
*   分析句子的情感
*   使用主题建模识别文本中的模式

# 简介

文本分析和**自然语言处理** ( **NLP** )是现代人工智能系统不可或缺的一部分。计算机擅长理解种类有限的严格结构化的数据。然而，当我们处理非结构化的自由格式文本时，事情开始变得困难。开发自然语言处理应用程序具有挑战性，因为计算机很难理解底层概念。我们交流事物的方式也有许多微妙的变化。这些可以是方言、上下文、俚语等形式。

为了解决这个问题，基于机器学习开发了自然语言处理应用程序。这些算法检测文本数据中的模式，以便我们可以从中提取见解。人工智能公司大量使用自然语言处理和文本分析来提供相关结果。自然语言处理最常见的应用包括搜索引擎、情感分析、主题建模、词性标注、实体识别等。自然语言处理的目标是开发一套算法，这样我们就可以用简单的英语与计算机交互。如果我们能做到这一点，那么我们就不需要编程语言来指导计算机应该做什么。在这一章中，我们将看看一些专注于文本分析的方法，以及我们如何从文本数据中提取有意义的信息。本章我们将大量使用名为**自然语言工具包** ( **NLTK** )的 Python 包。确保安装后再继续。您可以在[http://www.nltk.org/install.html](http://www.nltk.org/install.html)找到安装步骤。您还需要安装 NLTK 数据，它包含许多语料库和训练好的模型。这是文本分析不可或缺的一部分！您可以在[http://www.nltk.org/data.html](http://www.nltk.org/data.html)找到安装步骤。

# 使用标记化预处理数据

**标记化** 是将文本分割成一组有意义的片段的过程。这些棋子被称为**代币**。举个的例子，我们可以把一大块文字分成单词，也可以把它分成句子。根据手头的任务，我们可以定义自己的条件，将输入文本分成有意义的标记。让我们看看如何做到这一点。

## 怎么做…

1.  创建一个新的 Python 文件，并添加以下行。让我们定义一些示例文本进行分析:

    ```py
    text = "Are you curious about tokenization? Let's see how it works! We need to analyze a couple of sentences with punctuations to see it in action."
    ```

2.  让我们从句子标记化开始。NLTK 提供了一个句子标记器，所以让我们导入这个:

    ```py
    # Sentence tokenization
    from nltk.tokenize import sent_tokenize
    ```

3.  对输入文本运行句子标记器并提取标记:

    ```py
    sent_tokenize_list = sent_tokenize(text)
    ```

4.  打印句子列表，看看是否正确:

    ```py
    print "\nSentence tokenizer:"
    print sent_tokenize_list
    ```

5.  单词标记化在自然语言处理中非常常用。NLTK 附带了几个不同的单词标记器。让我们从基本单词标记器开始:

    ```py
    # Create a new word tokenizer
    from nltk.tokenize import word_tokenize

    print "\nWord tokenizer:"
    print word_tokenize(text)
    ```

6.  还有另一个单词标记器，叫做`PunktWord`标记器。这会在标点符号上分割文本，但会将其保留在单词内:

    ```py
    # Create a new punkt word tokenizer
    from nltk.tokenize import PunktWordTokenizer

    punkt_word_tokenizer = PunktWordTokenizer()
    print "\nPunkt word tokenizer:"
    print punkt_word_tokenizer.tokenize(text)
    ```

7.  如果你想把这些标点符号分成独立的符号，那么我们需要使用`WordPunct`符号化器:

    ```py
    # Create a new WordPunct tokenizer
    from nltk.tokenize import WordPunctTokenizer

    word_punct_tokenizer = WordPunctTokenizer()
    print "\nWord punct tokenizer:"
    print word_punct_tokenizer.tokenize(text)
    ```

8.  The full code is in the `tokenizer.py` file. If you run this code, you will see the following output on your Terminal:

    ![How to do it…](images/B05485_06_01.jpg)

# 提取文本数据

当我们处理一个文本文档时，我们会遇到不同形式的单词。想想“玩”这个词。这个词可以以各种形式出现，比如“玩”“玩”“玩家”“玩”等等。这些基本上都是意义相似的词族。在文本分析过程中，提取这些单词的基本形式是很有用的。这将有助于我们提取一些统计数据来分析整个文本。词干的目标是将这些不同的形式简化为一个共同的基本形式。这使用启发式过程来切断单词的末端以提取基本形式。让我们看看如何在 Python 中做到这一点。

## 怎么做…

1.  新建一个 Python 文件，导入以下包:

    ```py
    from nltk.stem.porter import PorterStemmer
    from nltk.stem.lancaster import LancasterStemmer
    from nltk.stem.snowball import SnowballStemmer
    ```

2.  我们来定义几个可以玩的词，如下:

    ```py
    words = ['table', 'probably', 'wolves', 'playing', 'is', 
            'dog', 'the', 'beaches', 'grounded', 'dreamt', 'envision']
    ```

3.  我们将定义一个我们想要使用的词干分析器列表:

    ```py
    # Compare different stemmers
    stemmers = ['PORTER', 'LANCASTER', 'SNOWBALL']
    ```

4.  初始化所有三个词干分析器所需的对象:

    ```py
    stemmer_porter = PorterStemmer()
    stemmer_lancaster = LancasterStemmer()
    stemmer_snowball = SnowballStemmer('english')
    ```

5.  为了以整洁的表格形式打印输出数据，我们需要以正确的方式格式化它:

    ```py
    formatted_row = '{:>16}' * (len(stemmers) + 1)
    print '\n', formatted_row.format('WORD', *stemmers), '\n'
    ```

6.  让我们遍历单词列表，并使用三个词干分析器对它们进行词干分析:

    ```py
    for word in words:
        stemmed_words = [stemmer_porter.stem(word), 
                stemmer_lancaster.stem(word),
                stemmer_snowball.stem(word)]
        print formatted_row.format(word, *stemmed_words)
    ```

7.  The full code is in the `stemmer.py` file. If you run this code, you will see the following output on your Terminal. Observe how the Lancaster stemmer behaves differently for a couple of words:

    ![How to do it…](images/B05485_06_02.jpg)

## 它是如何工作的…

所有三种词干算法基本上都是为了达到同样的目的。三种词干算法之间的区别基本上是它们操作的严格程度。如果您观察输出，您会看到兰开斯特词干分析器比其他两个词干分析器更严格。就严格程度而言，搬运工是最少的，兰开斯特是最严格的。我们从兰开斯特词干师那里得到的词干往往会变得混乱和模糊。算法确实很快，但是会减少很多单词。因此，一个很好的经验法则是使用雪球茎杆。

# 使用引理化将文本转换为基本形式

引理化的目标也是将单词简化为它们的基本形式，但这是一种更结构化的方法。在前面的食谱中，我们看到我们使用词干分析器获得的基本单词并没有真正的意义。例如，“狼”这个词被简化为“wolv”，这不是一个真实的词。引理化通过使用词汇和单词的形态学分析来解决这个问题。它删除屈折词尾，如“ing”或“ed”，并返回单词的基本形式。这种基本形式被称为引理。如果你把“狼”这个词具体化，你会得到“狼”作为输出。输出取决于标记是动词还是名词。让我们看看如何在这个食谱中做到这一点。

## 怎么做…

1.  创建一个新的 Python 文件，导入如下包:

    ```py
    from nltk.stem import WordNetLemmatizer
    ```

2.  让我们定义我们在词干制作过程中使用的同一组词:

    ```py
    words = ['table', 'probably', 'wolves', 'playing', 'is', 
            'dog', 'the', 'beaches', 'grounded', 'dreamt', 'envision']
    ```

3.  我们将比较两个引理器，`NOUN`和`VERB`引理器。让我们把它们列举如下:

    ```py
    # Compare different lemmatizers
    lemmatizers = ['NOUN LEMMATIZER', 'VERB LEMMATIZER']
    ```

4.  基于`WordN` `et`引理创建对象:

    ```py
    lemmatizer_wordnet = WordNetLemmatizer()
    ```

5.  为了以表格形式打印输出，我们需要以正确的方式格式化输出:

    ```py
    formatted_row = '{:>24}' * (len(lemmatizers) + 1)
    print '\n', formatted_row.format('WORD', *lemmatizers), '\n'
    ```

6.  遍历单词并将其引理:

    ```py
    for word in words:
        lemmatized_words = [lemmatizer_wordnet.lemmatize(word, pos='n'),
               lemmatizer_wordnet.lemmatize(word, pos='v')]
        print formatted_row.format(word, *lemmatized_words)
    ```

7.  The full code is in the `lemmatizer.py` file. If you run this code, you will see the following output. Observe how `NOUN` and `VERB` lemmatizers differ when they lemmatize the word "is" in the following image:

    ![How to do it…](images/B05485_06_03.jpg)

# 使用分块分割文本

分块 是指将输入的文本分割成基于任意随机条件的片段。这不同于标记化，因为不存在任何约束，并且块根本不需要有意义。这在文本分析过程中经常使用。当您处理非常大的文本文档时，您需要将其分成几个块进行进一步分析。在这个食谱中，我们将输入的文本分成若干部分，其中每一部分都有固定数量的单词。

## 怎么做…

1.  新建一个 Python 文件，导入以下包:

    ```py
    import numpy as np
    from nltk.corpus import brown
    ```

2.  让我们定义一个函数来将文本分割成块。第一步是根据空格分割文本:

    ```py
    # Split a text into chunks 
    def splitter(data, num_words):
        words = data.split(' ')
        output = []
    ```

3.  初始化几个必需的变量:

    ```py
        cur_count = 0
        cur_words = []
    ```

4.  让我们遍历单词:

    ```py
        for word in words:
            cur_words.append(word)
            cur_count += 1
    ```

5.  一旦你点击了所需的字数，重置变量:

    ```py
            if cur_count == num_words:
                output.append(' '.join(cur_words))
                cur_words = []
                cur_count = 0
    ```

6.  将组块追加到输出变量，并返回:

    ```py
        output.append(' '.join(cur_words) )

        return output
    ```

7.  我们现在可以定义`main`函数。从布朗语料库中加载数据。我们将使用前一万个单词:

    ```py
    if __name__=='__main__':
        # Read the data from the Brown corpus
        data = ' '.join(brown.words()[:10000])
    ```

8.  定义每个组块的字数:

    ```py
        # Number of words in each chunk 
        num_words = 1700
    ```

9.  初始化几个相关变量:

    ```py
        chunks = []
        counter = 0
    ```

10.  对该文本数据调用`splitter`功能，打印输出:

    ```py
        text_chunks = splitter(data, num_words)

        print "Number of text chunks =", len(text_chunks)
    ```

11.  完整代码在`chunking.py`文件中。如果您运行此代码，您将看到终端上打印出的块的数量。应该是 6！

# 构建单词包模型

当我们处理包含数百万个单词的文本文档时，我们需要将它们转换成某种数字表示。这样做的原因是为了使它们可用于机器学习算法。这些算法需要数字数据，以便分析它们并输出有意义的信息。这就是**词汇袋**的方法出现的地方。这基本上是一个模型从所有文档中的所有单词中学习一个词汇。之后，它通过构建文档中所有单词的直方图来为每个文档建模。

## 怎么做…

1.  新建一个 Python 文件，导入以下包:

    ```py
    import numpy as np
    from nltk.corpus import brown
    from chunking import splitter
    ```

2.  我们来定义`main`函数。从布朗语料库中加载输入数据:

    ```py
    if __name__=='__main__':
        # Read the data from the Brown corpus
        data = ' '.join(brown.words()[:10000])
    ```

3.  将文本数据分成五大块:

    ```py
        # Number of words in each chunk 
        num_words = 2000

        chunks = []
        counter = 0

        text_chunks = splitter(data, num_words)
    ```

4.  创建一个基于这些文本块的字典:

    ```py
        for text in text_chunks:
            chunk = {'index': counter, 'text': text}
            chunks.append(chunk)
            counter += 1
    ```

5.  下一步是提取文档术语矩阵。这基本上是一个计算文档中每个单词出现次数的矩阵。我们将使用 scikit-learn 来做到这一点，因为与 NLTK 相比，它在这个特定的任务中有更好的规定。导入以下包:

    ```py
        # Extract document term matrix
        from sklearn.feature_extraction.text import CountVectorizer
    ```

6.  定义对象，提取文档术语矩阵:

    ```py
        vectorizer = CountVectorizer(min_df=5, max_df=.95)
        doc_term_matrix = vectorizer.fit_transform([chunk['text'] for chunk in chunks])
    ```

7.  从`vectorizer`对象中提取词汇并打印出来:

    ```py
        vocab = np.array(vectorizer.get_feature_names())
        print "\nVocabulary:"
        print vocab
    ```

8.  打印文档术语矩阵:

    ```py
        print "\nDocument term matrix:"
        chunk_names = ['Chunk-0', 'Chunk-1', 'Chunk-2', 'Chunk-3', 'Chunk-4']
    ```

9.  要以表格形式打印，我们需要将其格式化，如下所示:

    ```py
        formatted_row = '{:>12}' * (len(chunk_names) + 1)
        print '\n', formatted_row.format('Word', *chunk_names), '\n'
    ```

10.  遍历单词，打印每个单词在不同组块中出现的次数:

    ```py
        for word, item in zip(vocab, doc_term_matrix.T):
            # 'item' is a 'csr_matrix' data structure
            output = [str(x) for x in item.data]
            print formatted_row.format(word, *output)
    ```

11.  The full code is in the `bag_of_words.py` file. If you run this code, you will see two main things printed on the Terminal. The first output is the vocabulary as shown in the following image:

    ![How to do it…](images/B05485_06_04.jpg)

12.  The second thing is the document term matrix, which is a pretty long. The first few lines will look like the following:

    ![How to do it…](images/B05485_06_05.jpg)

## 它是如何工作的…

考虑以下句子:

*   **第一句**:棕色的狗在跑。
*   **第二句**:黑狗在黑屋子里。
*   **第三句**:禁止在室内跑步。

如果你考虑这三句话，我们有以下九个独特的词:

*   这
*   棕色
*   狗
*   是
*   运转
*   黑色
*   在
*   房间
*   被禁止的

现在，让我们使用每个句子中的字数将每个句子转换成直方图。每个特征向量都是 9 维的，因为我们有 9 个唯一的词:

*   **第一句**:【1，1，1，1，0，0，0，0】
*   **第二句**:【2，0，1，1，0，2，1，1，0】
*   **第三句**:【0，0，0，1，1，0，1，1，1】

一旦我们提取出这些特征向量，就可以使用机器学习算法进行分析。

# 构建文本分类器

文本分类的目标是将文本文档分类为不同的类别。这是自然语言处理中极其重要的分析技术。我们将使用一种技术，该技术基于一个名为 **tf-idf** 的统计数据，该统计数据代表 **术语频率—反向文档频率**。这是一个分析工具，帮助我们理解一个单词对于一组文档中的一个文档有多重要。这是用来对文档进行分类的特征向量。你可以在[http://www.tfidf.com](http://www.tfidf.com)了解更多。

## 怎么做…

1.  新建一个 Python 文件，导入如下包:

    ```py
    from sklearn.datasets import fetch_20newsgroups
    ```

2.  让我们选择一个类别列表，并使用字典映射来命名它们。这些类别是我们刚刚导入的新闻组数据集的一部分:

    ```py
    category_map = {'misc.forsale': 'Sales', 'rec.motorcycles': 'Motorcycles', 
            'rec.sport.baseball': 'Baseball', 'sci.crypt': 'Cryptography', 
            'sci.space': 'Space'}
    ```

3.  根据我们刚刚定义的类别加载训练数据:

    ```py
    training_data = fetch_20newsgroups(subset='train', 
            categories=category_map.keys(), shuffle=True, random_state=7)
    ```

4.  导入特征提取器:

    ```py
    # Feature extraction
    from sklearn.feature_extraction.text import CountVectorizer
    ```

5.  使用训练数据提取特征:

    ```py
    vectorizer = CountVectorizer()
    X_train_termcounts = vectorizer.fit_transform(training_data.data)
    print "\nDimensions of training data:", X_train_termcounts.shape
    ```

6.  我们现在已经准备好训练分类器了。我们将使用多项式朴素贝叶斯分类器:

    ```py
    # Training a classifier
    from sklearn.naive_bayes import MultinomialNB
    from sklearn.feature_extraction.text import TfidfTransformer
    ```

7.  定义几个随机输入的句子:

    ```py
    input_data = [
        "The curveballs of right handed pitchers tend to curve to the left", 
        "Caesar cipher is an ancient form of encryption",
        "This two-wheeler is really good on slippery roads"
    ]
    ```

8.  定义 tf-idf 转换器对象并对其进行训练:

    ```py
    # tf-idf transformer
    tfidf_transformer = TfidfTransformer()
    X_train_tfidf = tfidf_transformer.fit_transform(X_train_termcounts)
    ```

9.  一旦我们有了特征向量，使用这个数据训练多项式朴素贝叶斯分类器:

    ```py
    # Multinomial Naive Bayes classifier
    classifier = MultinomialNB().fit(X_train_tfidf, training_data.target)
    ```

10.  使用字数转换输入数据:

    ```py
    X_input_termcounts = vectorizer.transform(input_data)
    ```

11.  使用 tf-idf 转换器转换输入数据:

    ```py
    X_input_tfidf = tfidf_transformer.transform(X_input_termcounts)
    ```

12.  使用训练好的分类器预测这些输入句子的输出类别:

    ```py
    # Predict the output categories
    predicted_categories = classifier.predict(X_input_tfidf)
    ```

13.  打印输出，如下所示:

    ```py
    # Print the outputs
    for sentence, category in zip(input_data, predicted_categories):
        print '\nInput:', sentence, '\nPredicted category:', \
                category_map[training_data.target_names[category]]
    ```

14.  The full code is in the `tfidf.py` file. If you run this code, you will see the following output printed on your Terminal:

    ![How to do it…](images/B05485_06_06.jpg)

## 它是如何工作的…

tf-idf 技术在信息检索中经常使用。目标是理解文档中每个单词的重要性。我们希望识别在文档中出现多次的单词。同时，像“是”和“是”这样的常用词并不能真正反映内容的本质。所以我们需要提取出真正的指标。每个单词的重要性随着字数的增加而增加。同时，随着它的大量出现，这个词的出现频率也增加了。这两件事往往会相互抵消。我们从每个句子中提取术语计数。一旦我们将其转换为特征向量，我们就训练分类器来对这些句子进行分类。

**词频** ( **TF** ) 衡量一个单词在给定文档中出现的频率。由于多个文档的长度不同，直方图中的数字往往变化很大。因此，我们需要使之正常化，使之成为一个公平的竞争环境。为了实现规范化，我们将词频除以给定文档中的总字数。**逆文档频率** ( **IDF** ) 衡量给定单词的重要性。当我们计算 TF 时，所有单词都被认为是同等重要的。为了平衡常用词的出现频率，我们需要把它们的权重降低，把稀有词的权重提高。我们需要计算带有给定单词的文档数量的比率，并将其除以文档总数。IDF 的计算方法是取这个比值的负算法。

例如，简单的单词，如“is”或“the”，往往会在各种文档中出现很多。然而，这并不意味着我们可以根据这些词来描述文档的特征。同时，如果一个单词出现一次，这也没有用。所以，我们寻找出现多次的单词，但不要太多，以免它们变得嘈杂。这是在 tf-idf 技术中制定的，用于对文档进行分类。搜索引擎经常使用这个工具来按相关性对搜索结果进行排序。

# 识别性别

在自然语言处理中，识别名字的性别是一项有趣的任务。我们将使用名称中最后几个字符是其定义特征的启发式方法。例如，如果名字以“la”结尾，则很可能是女性的名字，如“Angela”或“蕾拉”。另一方面，如果名字以“im”结尾，很可能是男性的名字，比如“Tim”或“Jim”。由于我们确定要使用的字符的确切数量，我们将对此进行实验。让我们看看怎么做。

## 怎么做…

1.  新建一个 Python 文件，导入以下包:

    ```py
    import random
    from nltk.corpus import names
    from nltk import NaiveBayesClassifier
    from nltk.classify import accuracy as nltk_accuracy
    ```

2.  我们需要定义一个从输入单词中提取特征的函数:

    ```py
    # Extract features from the input word
    def gender_features(word, num_letters=2):
        return {'feature': word[-num_letters:].lower()}
    ```

3.  我们来定义`main`函数。我们需要一些标注的训练数据:

    ```py
    if __name__=='__main__':
        # Extract labeled names
        labeled_names = ([(name, 'male') for name in names.words('male.txt')] +
                [(name, 'female') for name in names.words('female.txt')])
    ```

4.  播种随机数发生器，洗牌训练数据:

    ```py
        random.seed(7)
        random.shuffle(labeled_names)
    ```

5.  定义一些可以玩的输入名称:

    ```py
        input_names = ['Leonardo', 'Amy', 'Sam']
    ```

6.  由于不知道需要考虑多少结尾字符，我们将参数空间从`1`扫至`5`。每次，我们都会提取特征，如下所示:

    ```py
        # Sweeping the parameter space
        for i in range(1, 5):
            print '\nNumber of letters:', i
            featuresets = [(gender_features(n, i), gender) for (n, gender) in labeled_names]
    ```

7.  将其分为训练和测试数据集:

    ```py
            train_set, test_set = featuresets[500:], featuresets[:500]
    ```

8.  我们将使用朴素贝叶斯分类器来做到这一点:

    ```py
            classifier = NaiveBayesClassifier.train(train_set)
    ```

9.  评估参数空间中每个值的分类器:

    ```py
            # Print classifier accuracy
            print 'Accuracy ==>', str(100 * nltk_accuracy(classifier, test_set)) + str('%')

    # Predict outputs for new inputs
            for name in input_names:
                print name, '==>', classifier.classify(gender_features(name, i))
    ```

10.  The full code is in the `gender_identification.py` file. If you run this code, you will see the following output printed on your Terminal:

    ![How to do it…](images/B05485_06_07.jpg)

# 分析句子的情绪

**情感分析** 是 NLP 最热门的应用之一。情感分析是指确定给定文本是积极的还是消极的过程。在一些变体中，我们认为“中性”是第三种选择。这种技术通常用于发现人们对某个特定话题的感受。这个用来分析用户各种形式的情绪，比如营销活动、社交媒体、电商客户等等。

## 怎么做…

1.  新建一个 Python 文件，导入以下包:

    ```py
    import nltk.classify.util
    from nltk.classify import NaiveBayesClassifier
    from nltk.corpus import movie_reviews
    ```

2.  定义提取特征的函数:

    ```py
    def extract_features(word_list):
        return dict([(word, True) for word in word_list])
    ```

3.  为此我们需要训练数据，所以我们将使用 NLTK 中的电影评论:

    ```py
    if __name__=='__main__':
        # Load positive and negative reviews  
        positive_fileids = movie_reviews.fileids('pos')
        negative_fileids = movie_reviews.fileids('neg')
    ```

4.  让我们把这些分为正面和负面评论:

    ```py
        features_positive = [(extract_features(movie_reviews.words(fileids=[f])), 
                'Positive') for f in positive_fileids]
        features_negative = [(extract_features(movie_reviews.words(fileids=[f])), 
                'Negative') for f in negative_fileids]
    ```

5.  将数据分为训练和测试数据集:

    ```py
        # Split the data into train and test (80/20)
        threshold_factor = 0.8
        threshold_positive = int(threshold_factor * len(features_positive))
        threshold_negative = int(threshold_factor * len(features_negative))
    ```

6.  提取特征:

    ```py
        features_train = features_positive[:threshold_positive] + features_negative[:threshold_negative]
        features_test = features_positive[threshold_positive:] + features_negative[threshold_negative:]  
        print "\nNumber of training datapoints:", len(features_train)
        print "Number of test datapoints:", len(features_test)
    ```

7.  我们将使用朴素贝叶斯分类器。定义对象并训练:

    ```py
        # Train a Naive Bayes classifier
        classifier = NaiveBayesClassifier.train(features_train)
        print "\nAccuracy of the classifier:", nltk.classify.util.accuracy(classifier, features_test)
    ```

8.  分类器对象包含在分析过程中获得的信息量最大的单词。这些词基本上在被归类为积极或消极的评论中有很强的发言权。让我们打印出来:

    ```py
        print "\nTop 10 most informative words:"
        for item in classifier.most_informative_features()[:10]:
            print item[0]
    ```

9.  创建几个随机输入的句子:

    ```py
        # Sample input reviews
        input_reviews = [
            "It is an amazing movie", 
            "This is a dull movie. I would never recommend it to anyone.",
            "The cinematography is pretty great in this movie", 
            "The direction was terrible and the story was all over the place" 
        ]
    ```

10.  对这些输入句子运行分类器，获得预测:

    ```py
        print "\nPredictions:"
        for review in input_reviews:
            print "\nReview:", review
            probdist = classifier.prob_classify(extract_features(review.split()))
            pred_sentiment = probdist.max()
    ```

11.  打印输出:

    ```py
            print "Predicted sentiment:", pred_sentiment 
            print "Probability:", round(probdist.prob(pred_sentiment), 2)
    ```

12.  The full code is in the `sentiment_analysis.py` file. If you run this code, you will see three main things printed on the Terminal. The first is the accuracy, as shown in the following image:

    ![How to do it…](images/B05485_06_08.jpg)

13.  The next is a list of most informative words:

    ![How to do it…](images/B05485_06_09.jpg)

14.  The last is the list of predictions, which are based on the input sentences:

    ![How to do it…](images/B05485_06_10.jpg)

## 它是如何工作的…

我们在这里使用 NLTK 的朴素贝叶斯分类器来完成我们的任务。在特征提取器功能中，我们基本上提取所有唯一的单词。然而，NLTK 分类器需要数据以字典的形式排列。因此，我们以这样一种方式安排它，使得 NLTK 分类器对象可以摄取它。

一旦我们将数据划分为训练和测试数据集，我们就训练分类器将句子分为肯定和否定。如果你看一下顶部的信息词，你可以看到我们有“优秀”这样的词来表示正面评价，有“侮辱”这样的词来表示负面评价。这是一个有趣的信息，因为它告诉我们什么词被用来表示强烈的反应。

# 使用主题建模识别文本中的模式

**主题建模** 是指识别文本数据中隐藏的模式的过程。目标是在一组文档中发现一些隐藏的主题结构。这将帮助我们以更好的方式组织我们的文档，以便我们可以使用它们进行分析。这是自然语言处理中一个活跃的研究领域。你可以在[http://www.cs.columbia.edu/~blei/topicmodeling.html](http://www.cs.columbia.edu/~blei/topicmodeling.html)了解更多。在这个食谱中，我们将使用一个名为`gensim`的库。在继续之前，请确保安装了此软件。安装步骤见[https://radimrehurek.com/gensim/install.html](https://radimrehurek.com/gensim/install.html)。

## 怎么做…

1.  创建一个新的 Python 文件并导入以下包:

    ```py
    from nltk.tokenize import RegexpTokenizer  
    from nltk.stem.snowball import SnowballStemmer
    from gensim import models, corpora
    from nltk.corpus import stopwords
    ```

2.  定义一个函数来加载输入数据。我们将使用已经提供给您的`data_topic_modeling.txt`文本文件:

    ```py
    # Load input data
    def load_data(input_file):
        data = []
        with open(input_file, 'r') as f:
            for line in f.readlines():
                data.append(line[:-1])

        return data
    ```

3.  让我们定义一个类来预处理文本。这个预处理器负责创建所需的对象，并从输入文本中提取相关特征:

    ```py
    # Class to preprocess text
    class Preprocessor(object):
        # Initialize various operators
        def __init__(self):
            # Create a regular expression tokenizer
            self.tokenizer = RegexpTokenizer(r'\w+')
    ```

4.  我们需要一个停止单词的列表，这样我们就可以将它们排除在分析之外。这些都是常用词，如“在”、“该”、“是”等:

    ```py
            # get the list of stop words 
            self.stop_words_english = stopwords.words('english')
    ```

5.  定义雪球茎干:

    ```py
            # Create a Snowball stemmer 
            self.stemmer = SnowballStemmer('english')
    ```

6.  定义一个处理标记化、停止词移除和词干的处理器函数:

    ```py
        # Tokenizing, stop word removal, and stemming
        def process(self, input_text):
            # Tokenize the string
            tokens = self.tokenizer.tokenize(input_text.lower())
    ```

7.  从文本中删除停止词:

    ```py
            # Remove the stop words 
            tokens_stopwords = [x for x in tokens if not x in self.stop_words_english]
    ```

8.  对标记执行词干:

    ```py
            # Perform stemming on the tokens 
            tokens_stemmed = [self.stemmer.stem(x) for x in tokens_stopwords]
    ```

9.  返回已处理的代币:

    ```py
            return tokens_stemmed
    ```

10.  我们现在准备定义`main`函数。从文本文件加载输入数据:

    ```py
    if __name__=='__main__':
        # File containing linewise input data 
        input_file = 'data_topic_modeling.txt'

        # Load data
        data = load_data(input_file)
    ```

11.  定义一个基于我们定义的类的对象:

    ```py
        # Create a preprocessor object
        preprocessor = Preprocessor()
    ```

12.  我们需要处理文件中的文本，提取处理后的令牌:

    ```py
        # Create a list for processed documents
        processed_tokens = [preprocessor.process(x) for x in data]
    ```

13.  创建一个基于标记化文档的字典，以便可以用于主题建模:

    ```py
        # Create a dictionary based on the tokenized documents
        dict_tokens = corpora.Dictionary(processed_tokens)
    ```

14.  我们需要使用处理过的标记创建一个文档术语矩阵，如下所示:

    ```py
        # Create a document-term matrix
        corpus = [dict_tokens.doc2bow(text) for text in processed_tokens]
    ```

15.  假设我们知道文本可以分为两个主题。我们将使用一种称为 **潜在狄利克雷分配** ( **LDA** )的技术进行主题建模。定义所需参数，初始化 LDA 模型对象:

    ```py
        # Generate the LDA model based on the corpus we just created
        num_topics = 2
        num_words = 4

        ldamodel = models.ldamodel.LdaModel(corpus, 
                num_topics=num_topics, id2word=dict_tokens, passes=25)
    ```

16.  一旦确定了这两个主题，我们就可以通过查看贡献最大的单词来了解它是如何将这两个主题分开的:

    ```py
        print "Most contributing words to the topics:"
        for item in ldamodel.print_topics(num_topics=num_topics, num_words=num_words):
            print "\nTopic", item[0], "==>", item[1]
    ```

17.  The full code is in the `topic_modeling.py` file. If you run this code, you will see the following printed on your Terminal:

    ![How to do it…](images/B05485_06_11.jpg)

## 它是如何工作的…

主题建模通过识别文档中主题的重要单词来工作。这些词往往决定了的话题是什么。我们使用正则表达式标记器，因为我们只需要没有任何标点符号或其他类型标记的单词。因此，我们使用它来提取标记。停止单词删除是另一个重要的步骤，因为这有助于我们消除由于单词引起的噪音，例如“is”或“the”。在这之后，我们需要把单词词干，以得到它们的基本形式。这整个东西被打包成文本分析工具中的预处理块。这也是我们在这里做的！

我们使用一种称为潜在狄利克雷分配(LDA)的技术来建模主题。LDA 基本上将文档表示为倾向于吐出单词的不同主题的混合。这些话是以一定的概率说出来的。目标是找到这些话题！这是一个生成模型，试图找到负责生成给定文档集的主题集。你可以在[http://blog . echen . me/2011/08/22/潜在狄利克雷分配介绍](http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation)了解更多。

从输出中可以看到，我们有“天赋”和“训练”这样的词来表征体育话题，而我们有“加密”来表征密码学话题。我们正在处理一个非常小的文本文件，这就是为什么有些单词看起来不太相关的原因。显然，如果使用更大的数据集，精度将会提高。