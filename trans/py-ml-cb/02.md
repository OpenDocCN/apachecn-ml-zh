# 第二章。构建分类器

在本章中，我们将介绍以下食谱:

*   构建简单的分类器
*   构建逻辑回归分类器
*   构建朴素贝叶斯分类器
*   分割数据集用于训练和测试
*   使用交叉验证评估准确性
*   可视化混淆矩阵
*   提取性能报告
*   根据汽车的特性对其进行评估
*   提取验证曲线
*   提取学习曲线
*   估计收入等级

# 简介

在机器学习的领域中，分类是指利用数据的特征将其分成一定数量的类的过程。这不同于我们在上一章中讨论的回归，在那里输出是一个实数。监督学习分类器使用标记的训练数据建立模型，然后使用该模型对未知数据进行分类。

分类器可以是实现分类的任何算法。在简单的情况下，这个分类器可以是一个简单的数学函数。在更真实的情况下，这个分类器可以采取非常复杂的形式。在学习过程中，我们会看到分类可以是二进制的，我们将数据分成两类，也可以是多类的，我们将数据分成两类以上。设计用来处理分类问题的数学技术倾向于处理两个类，所以我们用不同的方式扩展它们来处理多类问题。

评估分类器的精度是世界机器学习的重要一步。我们需要学习如何使用可用的数据来了解这个模型在现实世界中的表现。在这一章中，我们将看看处理所有这些事情的食谱。

# 构建简单的分类器

让我们看看如何使用一些训练数据构建一个简单的分类器。

## 怎么做…

1.  我们将使用已经提供给您的`simple_classifier.py`文件作为参考。假设您像上一章一样导入了`numpy`和`matplotlib.pyplot`包，让我们创建一些示例数据:

    ```
    X = np.array([[3,1], [2,5], [1,8], [6,4], [5,2], [3,5], [4,7], [4,-1]])
    ```

2.  让我们给这些点分配一些标签:

    ```
    y = [0, 1, 1, 0, 0, 1, 1, 0]
    ```

3.  由于我们只有两个类，`y`列表包含 0 和 1。一般来说，如果你有 *N* 类，那么`y`中的值将从 0 到*N*1。让我们根据标签将数据分类:

    ```
    class_0 = np.array([X[i] for i in range(len(X)) if y[i]==0])
    class_1 = np.array([X[i] for i in range(len(X)) if y[i]==1])
    ```

4.  To get an idea about our data, let's plot it, as follows:

    ```
    plt.figure()
    plt.scatter(class_0[:,0], class_0[:,1], color='black', marker='s')
    plt.scatter(class_1[:,0], class_1[:,1], color='black', marker='x')
    ```

    这是一个散点图，我们用正方形和十字来画点。在这种情况下，`marker`参数指定您想要使用的形状。我们用正方形表示`class_0`中的点，用十字表示`class_1`中的点。如果运行此代码，您将看到下图:

    ![How to do it…](images/B05485_02_01.jpg)

5.  在前面两行的中，我们只是使用`X`和`y`之间的映射来创建两个列表。如果让你目视检查数据点并画一条分隔线，你会怎么做？你只需在它们之间划一条线。让我们继续这样做:

    ```
    line_x = range(10)
    line_y = line_x
    ```

6.  我们刚刚用数学方程 *y = x* 创建了一条线。我们来绘制一下，如下:

    ```
    plt.figure()
    plt.scatter(class_0[:,0], class_0[:,1], color='black', marker='s')
    plt.scatter(class_1[:,0], class_1[:,1], color='black', marker='x')
    plt.plot(line_x, line_y, color='black', linewidth=3)
    plt.show()
    ```

7.  If you run this code, you should see the following figure:

    ![How to do it…](images/B05485_02_02.jpg)

## 还有更多…

我们使用以下规则构建了一个简单分类器:如果`a`大于等于`b`，则输入点(`a`，`b`)属于`class_0`；否则属于`class_1`。如果你逐一检查这些点，你会发现事实上这是真的。就是这里！您刚刚构建了一个线性分类器，可以对未知数据进行分类。这是一个线性分类器，因为分隔线是直线。如果它是一条曲线，那么它就变成了一个非线性分类器。

这种队形效果很好，因为只有有限的几个点，我们可以目视检查它们。如果有几千个点呢？我们如何概括这个过程？让我们在下一个食谱中讨论这个问题。

# 构建逻辑回归分类器

尽管名称中出现了*回归*一词，但逻辑回归实际上是用于分类目的。给定一组数据点，我们的目标是建立一个可以在类之间绘制线性边界的模型。它通过求解从训练数据导出的一组方程来提取这些边界。

## 怎么做…

1.  Let's see how to do this in Python. We will use the `logistic_regression.py` file that is provided to you as a reference. Assuming that you imported the necessary packages, let's create some sample data along with training labels:

    ```
    import numpy as np
    from sklearn import linear_model 
    import matplotlib.pyplot as plt

    X = np.array([[4, 7], [3.5, 8], [3.1, 6.2], [0.5, 1], [1, 2], [1.2, 1.9], [6, 2], [5.7, 1.5], [5.4, 2.2]])
    y = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2])
    ```

    这里，我们假设我们有三个类。

2.  Let's initialize the logistic regression classifier:

    ```
    classifier = linear_model.LogisticRegression(solver='liblinear', C=100)
    ```

    可以为前面的功能指定许多输入参数，但是有几个重要的参数是`solver`和`C`。`solver`参数指定算法将用于求解方程组的求解器类型。`C`参数控制正则化强度。较低的值表示较高的正则化强度。

3.  让我们训练分类器:

    ```
    classifier.fit(X, y)
    ```

4.  Let's draw datapoints and boundaries:

    ```
    plot_classifier(classifier, X, y)
    ```

    我们需要定义这个函数，如下所示:

    ```
    def plot_classifier(classifier, X, y):
        # define ranges to plot the figure 
        x_min, x_max = min(X[:, 0]) - 1.0, max(X[:, 0]) + 1.0
        y_min, y_max = min(X[:, 1]) - 1.0, max(X[:, 1]) + 1.0
    ```

    前面的值表示我们要在图中使用的值的范围。这些值的范围通常从数据中的最小值到最大值。为了清楚起见，我们添加了一些缓冲区，例如前面几行中的 1.0。

5.  In order to plot the boundaries, we need to evaluate the function across a grid of points and plot it. Let's go ahead and define the grid:

    ```
        # denotes the step size that will be used in the mesh grid
        step_size = 0.01

        # define the mesh grid
        x_values, y_values = np.meshgrid(np.arange(x_min, x_max, step_size), np.arange(y_min, y_max, step_size))
    ```

    `x_values`和`y_values`变量包含将评估函数的点网格。

6.  让我们计算所有这些点的分类器输出:

    ```
        # compute the classifier output
        mesh_output = classifier.predict(np.c_[x_values.ravel(), y_values.ravel()])

        # reshape the array
        mesh_output = mesh_output.reshape(x_values.shape)
    ```

7.  Let's plot the boundaries using colored regions:

    ```
        # Plot the output using a colored plot 
        plt.figure()

        # choose a color scheme 
        plt.pcolormesh(x_values, y_values, mesh_output, cmap=plt.cm.gray)
    ```

    这基本上是一个三维绘图仪，它采用 2D 点和相关联的值来使用颜色方案绘制不同的区域。你可以在[上找到所有](http://matplotlib.org/examples/color/colormaps_reference.html)配色方案选项。

8.  Let's overlay the training points on the plot:

    ```
        plt.scatter(X[:, 0], X[:, 1], c=y, s=80, edgecolors='black', linewidth=1, cmap=plt.cm.Paired)

        # specify the boundaries of the figure
        plt.xlim(x_values.min(), x_values.max())
        plt.ylim(y_values.min(), y_values.max())

        # specify the ticks on the X and Y axes
        plt.xticks((np.arange(int(min(X[:, 0])-1), int(max(X[:, 0])+1), 1.0)))
        plt.yticks((np.arange(int(min(X[:, 1])-1), int(max(X[:, 1])+1), 1.0)))

        plt.show()
    ```

    这里，`plt.scatter`绘制了 2D 图上的点。`X[:, 0]`指定我们应该取沿 0 轴(在我们的例子中是 X 轴)的所有值，`X[:, 1]`指定 1 轴(Y 轴)。`c=y`参数表示颜色顺序。我们使用目标标签通过`cmap`映射到颜色。基本上，我们想要基于目标标签的不同颜色。因此，我们使用`y`作为映射。使用`plt.xlim`和`plt.ylim`设置显示图形的限制。为了用数值标记轴，我们需要使用`plt.xticks`和`plt.yticks`。这些函数用值标记轴，以便我们更容易看到点的位置。在前面的代码中，我们希望刻度位于最小值和最大值之间，缓冲区为一个单位。另外，我们希望这些刻度是整数。因此，我们使用`int()`函数来舍入这些值。

9.  If you run this code, you should see the following output:

    ![How to do it…](images/B05485_02_03.jpg)

10.  Let's see how the `C` parameter affects our model. The `C` parameter indicates the penalty for misclassification. If we set it to `1.0`, we will get the following figure:

    ![How to do it…](images/B05485_02_04.jpg)

11.  If we set `C` to `10000`, we get the following figure:

    ![How to do it…](images/B05485_02_05.jpg)

    随着我们增加`C`，误分类的惩罚会更高。因此，边界变得更加优化。

# 构建朴素贝叶斯分类器

朴素贝叶斯分类器是使用贝叶斯定理来建立模型的监督学习分类器。让我们继续构建一个简单的贝叶斯分类器。

## 怎么做…

1.  我们将使用提供给您的T3 作为参考。让我们导入一些东西:

    ```
    from sklearn.naive_bayes import GaussianNB 
    from logistic_regression import plot_classifier
    ```

2.  You were provided with a `data_multivar.txt` file. This contains data that we will use here. This contains comma-separated numerical data in each line. Let's load the data from this file:

    ```
    input_file = 'data_multivar.txt'

    X = []
    y = []
    with open(input_file, 'r') as f:
        for line in f.readlines():
            data = [float(x) for x in line.split(',')]
            X.append(data[:-1])
            y.append(data[-1]) 

    X = np.array(X)
    y = np.array(y)
    ```

    我们现在已经将输入数据加载到`X`中，并将标签加载到`y`中。

3.  Let's build the Naive Bayes classifier:

    ```
    classifier_gaussiannb = GaussianNB()
    classifier_gaussiannb.fit(X, y)
    y_pred = classifier_gaussiannb.predict(X)
    ```

    `GaussianNB`函数指定高斯朴素贝叶斯模型。

4.  让我们计算分类器的精度:

    ```
    accuracy = 100.0 * (y == y_pred).sum() / X.shape[0]
    print "Accuracy of the classifier =", round(accuracy, 2), "%"
    ```

5.  Let's plot the data and the boundaries:

    ```
    plot_classifier(classifier_gaussiannb, X, y)
    ```

    您应该会看到下图:

    ![How to do it…](images/B05485_02_06.jpg)

    这里不限制边界为线性。在前面的例子中，我们使用了所有的数据进行训练。机器学习的一个很好的实践是为训练和测试准备不重叠的数据。理想情况下，我们需要一些未使用的数据进行测试，这样我们就可以准确估计模型在未知数据上的表现。scikit-learn 中有一个条款可以很好地处理这个问题，如下一个食谱所示。

# 分割数据集进行训练和测试

让我们看看如何将我们的数据正确地分割成训练和测试数据集。

## 怎么做…

1.  Add the following code snippet into the same Python file as the previous recipe:

    ```
    from sklearn import cross_validation

    X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.25, random_state=5)
    classifier_gaussiannb_new = GaussianNB()
    classifier_gaussiannb_new.fit(X_train, y_train)
    ```

    这里，我们分配了 25%的数据用于测试，由`test_size`参数指定。剩下的 75%的数据将用于培训。

2.  让我们在测试数据上评估分类器:

    ```
    y_test_pred = classifier_gaussiannb_new.predict(X_test)
    ```

3.  让我们计算分类器的精度:

    ```
    accuracy = 100.0 * (y_test == y_test_pred).sum() / X_test.shape[0]
    print "Accuracy of the classifier =", round(accuracy, 2), "%"
    ```

4.  让我们在测试数据上绘制数据点和边界:

    ```
    plot_classifier(classifier_gaussiannb_new, X_test, y_test)
    ```

5.  You should see the following figure:

    ![How to do it…](images/B05485_02_07.jpg)

# 使用交叉验证评估准确性

**交叉验证** 是机器学习中的一个重要概念。在前面的方法中，我们将数据分成训练和测试数据集。但是为了让它更健壮，我们需要用不同的子集重复这个过程。如果我们只是针对一个特定的子集对其进行微调，我们最终可能会过度拟合模型。过度拟合指的是我们对数据集进行了过多的模型微调，而它在未知数据上表现不佳的情况。我们希望我们的机器学习模型在未知数据上表现良好。

## 准备…

在我们讨论如何执行交叉验证之前，我们先来谈谈性能指标。当我们处理机器学习模型时，我们通常关心三件事:精度、召回率和 F1 分数。我们可以使用参数评分获得所需的性能指标。精度是指正确分类的项目数占列表中项目总数的百分比。召回指的是检索到的项目数占培训列表中项目总数的百分比。

让我们考虑一个包含 100 个项目的测试数据集，其中 82 个是我们感兴趣的。现在，我们希望我们的分类器为我们识别这 82 个项目。我们的分类器挑选出 73 个项目作为感兴趣的项目。在这 73 个项目中，只有 65 个实际上是感兴趣的项目，其余 8 个分类错误。我们可以通过以下方式计算精度:

*   正确标识的数量= 65
*   标识总数= 73
*   精度= 65 / 73 = 89.04%

为了计算召回，我们使用以下方法:

*   数据集中感兴趣的项目总数= 82
*   正确检索的项目数= 65
*   召回率= 65 / 82 = 79.26%

一个好的机器学习模型需要同时具有好的精度和好的召回率。很容易让其中一个达到 100%，但另一个指标会受到影响！我们需要同时保持这两个指标。为了量化这一点，我们使用了一个 F1 分数，它是精度和召回率的结合。这实际上是精确度和召回率的调和平均值:

*F1 得分= 2 *精度*召回/(精度+召回)*

在前一种情况下，F1 的分数如下:

*F1 得分= 2 * 0.89 * 0.79/(0.89+0.79)= 0.8370*

## 怎么做…

1.  让我们看看如何执行交叉验证和提取性能指标。我们从精度开始:

    ```
    num_validations = 5
    accuracy = cross_validation.cross_val_score(classifier_gaussiannb, 
            X, y, scoring='accuracy', cv=num_validations)
    print "Accuracy: " + str(round(100*accuracy.mean(), 2)) + "%"
    ```

2.  我们将使用前面的函数来计算精确度、召回率和 F1 分数:

    ```
    f1 = cross_validation.cross_val_score(classifier_gaussiannb, 
            X, y, scoring='f1_weighted', cv=num_validations)
    print "F1: " + str(round(100*f1.mean(), 2)) + "%"

    precision = cross_validation.cross_val_score(classifier_gaussiannb, 
            X, y, scoring='precision_weighted', cv=num_validations)
    print "Precision: " + str(round(100*precision.mean(), 2)) + "%"

    recall = cross_validation.cross_val_score(classifier_gaussiannb, 
            X, y, scoring='recall_weighted', cv=num_validations)
    print "Recall: " + str(round(100*recall.mean(), 2)) + "%"
    ```

# 可视化混淆矩阵

A 混淆矩阵是我们用来理解分类模型的性能的表格。这有助于我们理解如何将测试数据分为不同的类别。当我们想要微调我们的算法时，我们需要在做出这些改变之前了解数据是如何被错误分类的。有些课比其他课差，混淆矩阵会帮助我们理解这一点。让我们看看下图:

![Visualizing the confusion matrix](images/B05485_02_08.jpg)

在之前的图表中，我们可以看到我们如何将数据分类到不同的类中。理想情况下，我们希望所有非对角元素都为 0。这将表明完美的分类！让我们考虑一下**0 级**。总体而言，52 件物品实际上属于**0 级**。如果我们把第一行的数字加起来，我们得到 52。现在，这些项目中有 45 个预测正确，但我们的分类器显示其中 4 个属于**类 1** ，3 个属于**类 2** 。我们也可以对剩下的两行应用同样的分析。有趣的是**1 类**的 11 个项目被误分类为**0 类**。这约占该类数据点的 16%。这是一个我们可以用来优化模型的洞察力。

## 怎么做…

1.  We will use the `confusion_matrix.py` file that we already provided to you as a reference. Let's see how to extract the confusion matrix from our data:

    ```
    from sklearn.metrics import confusion_matrix
    y_true = [1, 0, 0, 2, 1, 0, 3, 3, 3]
    y_pred = [1, 1, 0, 2, 1, 0, 1, 3, 3]
    confusion_mat = confusion_matrix(y_true, y_pred)
    plot_confusion_matrix(confusion_mat)
    ```

    我们在这里使用一些样本数据。我们有四个类，它们的值从 0 到 3 不等。我们也预测了标签。我们使用`confusion_matrix`方法提取混淆矩阵并绘制出来。

2.  Let's go ahead and define this function:

    ```
    # Show confusion matrix
    def plot_confusion_matrix(confusion_mat):
        plt.imshow(confusion_mat, interpolation='nearest', cmap=plt.cm.Paired)
        plt.title('Confusion matrix')
        plt.colorbar()
        tick_marks = np.arange(4)
        plt.xticks(tick_marks, tick_marks)
        plt.yticks(tick_marks, tick_marks)
        plt.ylabel('True label')
        plt.xlabel('Predicted label')
        plt.show()
    ```

    我们使用`imshow`函数绘制混淆矩阵。函数中的其他一切都很简单！我们只是使用相关函数设置标题、颜色条、刻度和标签。`tick_marks`参数的范围从 0 到 3，因为我们的数据集中有四个不同的标签。`np.arange`功能给了我们这个`numpy`数组。

3.  If you run the preceding code, you will see the following figure:

    ![How to do it…](images/B05485_02_09.jpg)

    对角线颜色很强，我们希望它们很强。黑色表示零。在非对角线空间中有几个灰色，这表明分类错误。例如，当真实标签为 0 时，预测标签为 1，如我们在第一行中所见。事实上，所有的错误分类都属于**类-1** ，因为第二列包含三个非零行。从图中很容易看出这一点。

# 提取绩效报告

我们在 scikit-learn 中还有一个功能，可以直接为我们打印精度、召回率、F1 成绩。让我们看看如何做到这一点。

## 怎么做…

1.  向新的 Python 文件添加以下行:

    ```
    from sklearn.metrics import classification_report
    y_true = [1, 0, 0, 2, 1, 0, 3, 3, 3]
    y_pred = [1, 1, 0, 2, 1, 0, 1, 3, 3]
    target_names = ['Class-0', 'Class-1', 'Class-2', 'Class-3']
    print(classification_report(y_true, y_pred, target_names=target_names))
    ```

2.  If you run this code, you will see the following on your Terminal:

    ![How to do it…](images/B05485_02_10.jpg)

    您可以直接使用这个函数从模型中提取统计数据，而不是单独计算这些指标。

# 根据汽车的特性对其进行评估

让我们看看如何将分类技术应用于现实世界的问题。我们将使用包含一些汽车细节的数据集，例如车门数量、行李箱空间、维护成本等。我们的目标是确定汽车的质量。出于分类的目的，质量可以采用四个值:不可接受、可接受、良好和非常好。

## 做好准备

可以在[https://archive.ics.uci.edu/ml/datasets/Car+Evaluation](https://archive.ics.uci.edu/ml/datasets/Car+Evaluation)下载数据集。

您需要将数据集中的每个值视为一个字符串。我们考虑数据集中的六个属性。这里是属性以及它们可以取的可能值:

*   `buying`:这些将是`vhigh`、`high`、`med`和`low`
*   `maint`:这些将是`vhigh`、`high`、`med`和`low`
*   `doors`:这些会是`2`、`3`、`4`、`5`等等
*   `persons`:这些会是`2`、`4`，更多
*   `lug_boot`:这些会是`small`、`med`和`big`
*   `safety`:这些会是`low`、`med`和`high`

假设每一行都包含字符串，我们需要假设所有的特征都是字符串，并设计一个分类器。在前一章中，我们使用随机森林来构建回归器。在这个食谱中，我们将使用随机森林作为分类器。

## 怎么做…

1.  我们将使用已经提供给您的`car.py`文件作为参考。让我们继续导入几个包:

    ```
    from sklearn import preprocessing
    from sklearn.ensemble import RandomForestClassifier
    ```

2.  Let's load the dataset:

    ```
    input_file = 'path/to/dataset/car.data.txt'

    # Reading the data
    X = []
    count = 0
    with open(input_file, 'r') as f:
        for line in f.readlines():
            data = line[:-1].split(',')
            X.append(data)

    X = np.array(X)
    ```

    每行包含一个逗号分隔的单词列表。因此，我们解析输入文件，拆分每一行，然后将列表附加到主数据上。我们忽略每行的最后一个字符，因为它是一个换行符。Python 包只处理数字数据，所以我们需要将这些属性转换成那些包能够理解的东西。

3.  In the previous chapter, we discussed label encoding. That is what we will use here to convert strings to numbers:

    ```
    # Convert string data to numerical data
    label_encoder = [] 
    X_encoded = np.empty(X.shape)
    for i,item in enumerate(X[0]):
        label_encoder.append(preprocessing.LabelEncoder())
        X_encoded[:, i] = label_encoder[-1].fit_transform(X[:, i])

    X = X_encoded[:, :-1].astype(int)
    y = X_encoded[:, -1].astype(int)
    ```

    由于每个属性可以接受有限数量的值，我们可以使用标签编码器将它们转换为数字。我们需要为每个属性使用不同的标签编码器。例如`lug_boot`属性可以取三个不同的值，我们需要一个知道如何编码这个属性的标签编码器。每行的最后一个值是类，所以我们把它赋给`y`变量。

4.  Let's train the classifier:

    ```
    # Build a Random Forest classifier
    params = {'n_estimators': 200, 'max_depth': 8, 'random_state': 7}
    classifier = RandomForestClassifier(**params)
    classifier.fit(X, y)
    ```

    你可以摆弄一下`n_estimators`和`max_depth`参数，看看它们是如何影响分类精度的。实际上，我们很快就会以标准化的方式做到这一点。

5.  Let's perform cross-validation:

    ```
    # Cross validation
    from sklearn import cross_validation

    accuracy = cross_validation.cross_val_score(classifier, 
            X, y, scoring='accuracy', cv=3)
    print "Accuracy of the classifier: " + str(round(100*accuracy.mean(), 2)) + "%"
    ```

    一旦我们训练了分类器，我们就需要看看它是如何运行的。我们使用三重交叉验证来计算这里的准确性。

6.  One of the main goals of building a classifier is to use it on isolated and unknown data instances. Let's use a single datapoint and see how we can use this classifier to categorize it:

    ```
    # Testing encoding on single data instance
    input_data = ['vhigh', 'vhigh', '2', '2', 'small', 'low'] 
    input_data_encoded = [-1] * len(input_data)
    for i,item in enumerate(input_data):
        input_data_encoded[i] = int(label_encoder[i].transform(input_data[i]))

    input_data_encoded = np.array(input_data_encoded)
    ```

    第一步是将数据转换成数字数据。我们需要使用训练中使用的标签编码器，因为我们希望它是一致的。如果输入数据点中有未知值，标签编码器会抱怨，因为它不知道如何处理这些数据。例如，如果您将列表中的第一个值从`vhigh`更改为`abcd`，那么标签编码器将不起作用，因为它不知道如何解释该字符串。这就像一个检查输入数据点是否有效的错误检查。

7.  We are now ready to predict the output class for this datapoint:

    ```
    # Predict and print output for a particular datapoint
    output_class = classifier.predict(input_data_encoded)
    print "Output class:", label_encoder[-1].inverse_transform(output_class)[0]
    ```

    我们使用`predict`方法来估计输出类别。如果我们输出编码的输出标签，它对我们来说没有任何意义。因此，我们使用`inverse_transform`方法将该标签转换回其原始形式，并打印出输出类。

# 提取验证曲线

在之前的配方中，我们使用了随机森林来构建分类器，但是我们不知道如何定义参数。在我们的例子中，我们处理了两个参数:`n_estimators`和`max_depth`。它们被称为**超参数**，分类器的性能取决于它们。当我们改变超参数时，看到性能如何受到影响会很好。这就是验证曲线出现的地方。这些曲线帮助我们理解每个超参数如何影响训练分数。基本上，所有其他参数保持不变，我们根据我们的范围改变感兴趣的超参数。然后，我们将能够想象这是如何影响分数的。

## 怎么做…

1.  Add the following code to the same Python file, as in the previous recipe:

    ```
    # Validation curves

    from sklearn.learning_curve import validation_curve

    classifier = RandomForestClassifier(max_depth=4, random_state=7)
    parameter_grid = np.linspace(25, 200, 8).astype(int)
    train_scores, validation_scores = validation_curve(classifier, X, y, 
            "n_estimators", parameter_grid, cv=5)
    print "\n##### VALIDATION CURVES #####"
    print "\nParam: n_estimators\nTraining scores:\n", train_scores
    print "\nParam: n_estimators\nValidation scores:\n", validation_scores
    ```

    在这种情况下，我们通过固定`max_depth`参数来定义分类器。我们想要估计要使用的估计器的最佳数量，因此使用`parameter_grid`定义了我们的搜索空间。它将通过分八步从 25 迭代到 200 来提取训练和验证分数。

2.  If you run it, you will see the following on your Terminal:

    ![How to do it…](images/B05485_02_11.jpg)

3.  让我们画出来:

    ```
    # Plot the curve
    plt.figure()
    plt.plot(parameter_grid, 100*np.average(train_scores, axis=1), color='black')
    plt.title('Training curve')
    plt.xlabel('Number of estimators')
    plt.ylabel('Accuracy')
    plt.show()
    ```

4.  Here is the figure that you'll get:

    ![How to do it…](images/B05485_02_12.jpg)

5.  Let's do the same for the `max_depth` parameter:

    ```
    classifier = RandomForestClassifier(n_estimators=20, random_state=7)
    parameter_grid = np.linspace(2, 10, 5).astype(int)
    train_scores, valid_scores = validation_curve(classifier, X, y, 
            "max_depth", parameter_grid, cv=5)
    print "\nParam: max_depth\nTraining scores:\n", train_scores
    print "\nParam: max_depth\nValidation scores:\n", validation_scores
    ```

    我们将`n_estimators`参数固定在 20，看看性能如何随`max_depth`而变化。这是终端上的输出:

    ![How to do it…](images/B05485_02_13.jpg)

6.  让我们画出来:

    ```
    # Plot the curve
    plt.figure()
    plt.plot(parameter_grid, 100*np.average(train_scores, axis=1), color='black')
    plt.title('Validation curve')
    plt.xlabel('Maximum depth of the tree')
    plt.ylabel('Accuracy')
    plt.show()
    ```

7.  If you run this code, you will get the following figure:

    ![How to do it…](images/B05485_02_14.jpg)

# 提取学习曲线

学习曲线有助于我们理解训练数据集的大小如何影响机器学习模型。当您必须处理计算约束时，这非常有用。让我们通过改变训练数据集的大小来绘制学习曲线。

## 怎么做…

1.  Add the following code to the same Python file, as in the previous recipe:

    ```
    # Learning curves

    from sklearn.learning_curve import learning_curve

    classifier = RandomForestClassifier(random_state=7)

    parameter_grid = np.array([200, 500, 800, 1100])
    train_sizes, train_scores, validation_scores = learning_curve(classifier, 
            X, y, train_sizes=parameter_grid, cv=5)
    print "\n##### LEARNING CURVES #####"
    print "\nTraining scores:\n", train_scores
    print "\nValidation scores:\n", validation_scores
    ```

    我们希望使用大小为 200、500、800 和 1100 的训练数据集来评估性能指标。我们使用五重交叉验证，由`learning_curve`方法中的`cv`参数指定。

2.  If you run this code, you will get the following output on the Terminal:

    ![How to do it…](images/B05485_02_15.jpg)

3.  我们来绘制一下:

    ```
    # Plot the curve
    plt.figure()
    plt.plot(parameter_grid, 100*np.average(train_scores, axis=1), color='black')
    plt.title('Learning curve')
    plt.xlabel('Number of training samples')
    plt.ylabel('Accuracy')
    plt.show()
    ```

4.  Here is the output figure:

    ![How to do it…](images/B05485_02_16.jpg)

    虽然较小的训练集似乎给出了更好的准确性，但它们容易过度拟合。如果我们选择更大的训练数据集，它会消耗更多的资源。因此，我们需要在这里进行权衡，以选择合适大小的训练数据集。

# 估算收入等级

我们将构建一个分类器，根据 14 个属性来估计一个人的收入档次。可能的输出等级为高于 50K 的*或低于或等于 50K 的*。从每个数据点都是数字和字符串的混合来看，这个数据集中有一点扭曲。数字数据是有价值的，我们不能在这些情况下使用标签编码器。我们需要设计一个可以同时处理数值和非数值数据的系统。我们将使用[https://archive.ics.uci.edu/ml/datasets/Census+Income](https://archive.ics.uci.edu/ml/datasets/Census+Income)提供的人口普查收入数据集。**

 **## 怎么做…

1.  我们将使用已经提供给您的`income.py`文件作为参考。我们将使用朴素贝叶斯分类器来实现这一点。让我们导入几个包:

    ```
    from sklearn import preprocessing
    from sklearn.naive_bayes import GaussianNB 
    ```

2.  让我们加载数据集:

    ```
    input_file = 'path/to/adult.data.txt'

    # Reading the data
    X = []
    y = []
    count_lessthan50k = 0
    count_morethan50k = 0
    num_images_threshold = 10000
    ```

3.  We will use 20,000 datapoints from the datasets—10,000 for each class to avoid class imbalance. During training, if you use many datapoints that belong to a single class, the classifier tends to get biased toward this class. Therefore, it's better to use the same number of datapoints for each class:

    ```
    with open(input_file, 'r') as f:
        for line in f.readlines():
            if '?' in line:
                continue

            data = line[:-1].split(', ')

            if data[-1] == '<=50K' and count_lessthan50k < num_images_threshold:
                X.append(data)
                count_lessthan50k = count_lessthan50k + 1

            elif data[-1] == '>50K' and count_morethan50k < num_images_threshold:
                X.append(data)
                count_morethan50k = count_morethan50k + 1

            if count_lessthan50k >= num_images_threshold and count_morethan50k >= num_images_threshold:
                break

    X = np.array(X)
    ```

    又是一个逗号分隔的文件。我们只是像以前一样将数据加载到`X`变量中。

4.  We need to convert string attributes to numerical data while leaving out the original numerical data:

    ```
    # Convert string data to numerical data
    label_encoder = [] 
    X_encoded = np.empty(X.shape)
    for i,item in enumerate(X[0]):
        if item.isdigit():
            X_encoded[:, i] = X[:, i]
        else: 
            label_encoder.append(preprocessing.LabelEncoder())
            X_encoded[:, i] = label_encoder[-1].fit_transform(X[:, i])

    X = X_encoded[:, :-1].astype(int)
    y = X_encoded[:, -1].astype(int)
    ```

    `isdigit()`功能帮助我们识别数值数据。我们将字符串数据转换为数字数据，并将所有标签编码器存储在一个列表中，以便在我们想要对未知数据进行分类时使用它。

5.  让我们训练分类器:

    ```
    # Build a classifier
    classifier_gaussiannb = GaussianNB()
    classifier_gaussiannb.fit(X, y)
    ```

6.  让我们将数据分为训练和测试，以提取性能指标:

    ```
    # Cross validation
    from sklearn import cross_validation

    X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.25, random_state=5)
    classifier_gaussiannb = GaussianNB()
    classifier_gaussiannb.fit(X_train, y_train)
    y_test_pred = classifier_gaussiannb.predict(X_test)
    ```

7.  让我们提取性能指标:

    ```
    # compute F1 score of the classifier
    f1 = cross_validation.cross_val_score(classifier_gaussiannb, 
            X, y, scoring='f1_weighted', cv=5)
    print "F1 score: " + str(round(100*f1.mean(), 2)) + "%"
    ```

8.  让我们看看如何对单个数据点进行分类。我们需要将数据点转换成我们的分类器能够理解的东西:

    ```
    # Testing encoding on single data instance
    input_data = ['39', 'State-gov', '77516', 'Bachelors', '13', 'Never-married', 'Adm-clerical', 'Not-in-family', 'White', 'Male', '2174', '0', '40', 'United-States'] 
    count = 0
    input_data_encoded = [-1] * len(input_data)
    for i,item in enumerate(input_data):
        if item.isdigit():
            input_data_encoded[i] = int(input_data[i])
        else:
            input_data_encoded[i] = int(label_encoder[count].transform(input_data[i]))
            count = count + 1 

    input_data_encoded = np.array(input_data_encoded)
    ```

9.  我们现在准备对其进行分类:

    ```
    # Predict and print output for a particular datapoint
    output_class = classifier_gaussiannb.predict(input_data_encoded)
    print label_encoder[-1].inverse_transform(output_class)[0]
    ```

就像之前一样，我们用`predict`方法得到输出类，用`inverse_transform`方法把这个标签转换回原来的形式，在终端上打印出来。**