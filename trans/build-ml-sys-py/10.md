# 主题建模

在[第 6 章](06.html)、*聚类-查找相关帖子*中，我们使用聚类对文本文档进行了分组。这是一个非常有用的工具，但并不总是最好的。聚类结果是每个文本恰好属于一个聚类。这本书是关于机器学习和 Python 的。它应该与其他 Python 相关的作品还是与机器相关的作品分组？在实体书店，我们需要选择一个单一的地方来存放这本书。然而，在一家互联网商店，答案是这本书是关于机器学习*和* Python 的，这本书应该列在这两个部分。当然，这并不意味着这本书会在所有章节中列出。我们不会把这本书和其他烘焙书籍放在一起。

在本章中，我们将学习不将文档聚集成完全独立的组，而是允许每个文档引用几个**主题**的方法。这些主题将从文本文档集合中自动识别。这些文档可以是整本书或较短的文本，如博客文章、新闻故事或电子邮件。

我们还希望能够推断出这样一个事实，即这些文件可能有对它们至关重要的主题，而只是顺便提及其他主题。这本书经常提到绘图，但它不像机器学习那样是一个中心话题。这意味着文档中的主题是它们的核心，而其他主题则是次要的。处理这些问题的机器学习子领域称为**主题建模**，是本章的主题。特别是，我们将了解以下内容:

*   有哪些主题模型，特别是关于**潜在狄利克雷分配** ( **LDA** )
*   如何使用`gensim`包构建主题模型
*   主题模型如何作为不同应用程序的中间表示有用
*   我们如何建立整个英语维基百科的主题模型

# 潜在狄利克雷分配

不幸的是，机器学习中有两种方法，首字母是 LDA:潜在狄利克雷分配，这是一种主题建模方法，以及线性判别分析，这是一种分类方法。它们完全不相关，除了首字母 LDA 可以指任何一个。在某些情况下，这可能会令人困惑。scikit-learn 工具有一个子模块`sklearn.lda`，实现线性判别分析。目前，scikit-learn 没有实现潜在的 Dirichlet 分配。

第一个话题`model`我们要看的是潜狄利克雷分配。LDA 背后的数学思想相当复杂，这里就不赘述了。

对于那些感兴趣并且足够冒险的人，维基百科提供了这些算法背后的所有等式:[http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation](http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)。

但是，我们可以从高层次直观地理解 LDA 背后的思想。LDA 属于一类被称为生成模型的模型，因为它们有一种寓言解释了数据是如何生成的。这个生成性的故事是对现实的简化，当然是为了让机器学习更容易。在 LDA 寓言中，我们首先通过给单词分配概率权重来创建主题。每个主题将为不同的单词分配不同的权重。例如，一个 Python 主题会给单词变量分配高概率，给喝醉的单词分配低概率。当我们希望生成一个新文档时，我们首先选择它将使用的主题，然后从这些主题中混合单词。

例如，假设我们只有三个书籍讨论的主题:

*   机器学习
*   计算机编程语言
*   烘烤

对于每个主题，我们都有一个与之相关的单词列表。这本书将前两个主题混合在一起，也许各占 50%。混合物不需要相等；也可以是 70/30 分割。当我们生成实际的文本时，我们一个字一个字地生成；首先我们决定这个词来自哪个话题。这是基于主题权重的随机决定。一旦选择了一个主题，我们就从该主题的单词列表中生成一个单词。准确地说，我们用题目给出的概率选择一个英语单词。同一个单词可以由多个主题生成。例如，重量是机器学习和烘焙中的常用词(尽管含义不同)。

在这个模型中，单词的顺序并不重要。这是一个**包字**模型，我们在上一章已经看到了。这是对语言的粗略简化，但它通常足够好，因为仅仅知道文档中使用了哪些单词及其频率就足以做出机器学习决策。

在现实世界中，我们不知道主题是什么。我们的任务是收集一些文本，并对这个寓言进行逆向工程，以便发现有哪些主题，同时找出每个文档使用哪些主题。

# 构建主题模型

不幸的是，scikit-learn 没有实现潜在的 Dirichlet 分配。因此，我们将使用 Python 中的`gensim`包。Gensim 是由英国的机器学习研究员和顾问 radim řehůřek 开发的。

作为输入数据，我们将使用来自**美联社** ( **美联社**)的新闻报道集合。这是一个用于文本建模研究的标准数据集，在一些关于主题模型的初始工作中使用过。下载数据后，我们可以通过运行以下代码来加载它:

```
import gensim 
from gensim import corpora, models 
corpus = corpora.BleiCorpus('./data/ap/ap.dat', './data/ap/vocab.txt') 
```

`corpus`变量保存所有文本文档，并以易于处理的格式加载它们。我们现在可以建立一个主题`model`，使用这个对象作为输入:

```
model = models.ldamodel.LdaModel( 
              corpus, 
              num_topics=100, 
              id2word=corpus.id2word) 
```

该单个构造函数调用将统计推断`corpus`中存在哪些主题。我们可以通过多种方式探索最终的模型。我们可以看到使用`model[doc]`语法的`topics`文档列表，如下例所示:

```
doc = corpus.docbyoffset(0) 
topics = model[doc] 
print(topics) 
[(3, 0.023607255776894751), 
 (13, 0.11679936618551275), 
 (19, 0.075935855202707139), 
.... 
 (92, 0.10781541687001292)] 
```

结果几乎肯定会在我们的电脑上看起来不同！学习算法使用了一些随机数，每次在同一个输入数据上学习一个新的题目`model`，结果都不一样。重要的是，如果您的数据表现良好，模型的一些定性属性将在不同的运行中保持稳定。例如，如果您正在使用主题来比较文档，就像我们在这里所做的那样，那么相似性应该是健壮的，并且只有轻微的变化。另一方面，不同主题的顺序也会完全不同。

结果的格式是一个配对列表:`(topic_index, topic_weight)`。我们可以看到每个文档只使用了几个主题(在前面的例子中，主题`0`、`1`、`2`没有权重；这些话题的权重是`0`。主题`model`是一个稀疏模型，因为虽然有很多可能的主题，但是对于每个文档，只使用了其中的几个。严格来说，这不是真的，因为在 LDA 模型中，所有主题都有非零概率，但其中一些主题的概率非常小，我们可以将其四舍五入为零，作为一个很好的近似值。

我们可以通过绘制每个文档涉及的主题数量的直方图来进一步探讨这一点:

```
num_topics_used = [len(model[doc]) for doc in corpus] 
fig,ax = plt.subplots() 
ax.hist(num_topics_used) 
```

你会得到如下图:

![](assets/989c327b-9ba0-4111-bc24-f67f7394f817.png)

Sparsity means that while you may have large matrices and vectors, in principle, most of the values are zero (or so small that we can round them to zero as a good approximation). Therefore, only a few things are relevant at any given time.
Often problems that seem too big to solve are actually feasible because the data is sparse. For example, even though any web page can link to any other web page, the graph of links is actually very sparse as each web page will link to a very tiny fraction of all other web pages.

在上图中，我们可以或多或少地看到大多数文档涉及大约 10 个主题。

这在很大程度上是由于所用参数的值，即`alpha`参数。alpha 的确切含义有点抽象，但是 alpha 值越大，每个文档的主题就越多。

Alpha 需要是大于零的值，但通常设置为较小的值，通常小于 1。`alpha`的值越小，每个文档期望讨论的主题就越少。默认情况下，gensim 会将`alpha`设置为`1/num_topics`，但是您可以通过在`LdaModel`构造函数中将它作为参数进行显式设置，如下所示:

```
model = models.ldamodel.LdaModel( 
              corpus, 
              num_topics=100, 
              id2word=corpus.id2word, 
              alpha=1) 
```

在这种情况下，这是一个比默认值更大的 alpha，这应该会导致每个文档有更多的主题。正如我们在下面给出的组合直方图中所看到的，gensim 的行为符合我们的预期，并为每个文档分配了更多的主题:

现在，我们可以在前面的柱状图中看到，许多文档涉及到 **20** 到 **25** 不同的主题。如果您将该值设置得更低，您将观察到相反的情况(从在线存储库中下载代码将允许您使用这些值)。

这些话题是什么？从技术上讲，正如我们前面讨论的，它们是单词的多项式分布，这意味着它们为词汇表中的每个单词分配了一个概率。概率较高的单词比概率较低的单词更容易与该主题相关联:

![](assets/991e61c4-c53c-4f4e-a8ef-ef4bd8dff75c.png)

我们的大脑不太擅长用概率分布进行推理，但我们可以很容易地理解一系列单词。因此，通常用权重最高的单词列表来总结主题。

在下表中，我们显示了前十个主题:

| **题号** | **话题** |
| one | 着装军事苏联总统新州卡卢奇上尉州领导人立场政府 |
| Two | 科赫赞比亚卢萨卡一党橙色科赫一党政府市长新政治 |
| three | 人权土耳其侵犯皇家汤普森威胁新州写道花园总统 |
| four | 比尔雇员实验莱文税收联邦措施立法参议院主席告密者赞助 |
| five | 俄亥俄州 7 月干旱耶稣灾难百分之哈特福德密西西比州北部河谷作物弗吉尼亚州 |
| six | 美国百分之十亿年总统世界各国人民`i`布什新闻 |
| seven | `b`休斯宣誓书美国盎司平方英尺护理延迟指控不切实际的布什 |
| eight | 尤特·杜卡基斯布什公约农业补贴乌拉圭百分比秘书长`i`告诉 |
| nine | 克什米尔政府人民斯利那加印度甩市两查谟克什米尔集团莫斯利巴基斯坦 |
| Ten | 工人越南爱尔兰工资移民百分比议价最后一个岛警察赫顿`i` |

虽然乍一看令人望而生畏，但当通读单词列表时，我们可以清楚地看到，主题不仅仅是随机的单词，相反，这些是逻辑组。我们也可以看到，这些话题指的是比较老的新闻条目，从苏联还存在，戈尔巴乔夫还是总书记的时候开始。我们还可以将主题表示为单词云，使更有可能的单词变大。例如，这是一个关于警察的主题的可视化:

![](assets/566c3a94-f99c-44d2-b569-aa029352eaba.png)

我们还可以看到，有些词可能应该删除，因为它们信息不多；这些都是空话。在构建主题建模时，过滤掉停止词可能会很有用，否则你可能会得到一个完全由停止词组成的主题。我们可能还希望将文本预处理为词干，以便规范化复数和动词形式。这个过程在[第 6 章](06.html)、*聚类-查找相关帖子*中有介绍，具体可以参考。如果你感兴趣，可以从本书的配套网站下载代码，尝试所有这些变体来绘制不同的图片。

# 按主题比较文档

主题本身对于构建上一个截图中显示的那种带有单词的小短文非常有用。这些可视化可以用于浏览大量文档。例如，网站可以将不同的主题显示为不同的单词云，允许用户点击查看文档。事实上，它们只是以这种方式被用来分析大量的文档集合。

然而，话题往往只是达到另一个目的的中间工具。现在，我们已经估计了每个文档中有多少来自每个主题，我们可以在主题空间中比较这些文档。这仅仅意味着，如果两个文档谈论相同的主题，我们就说它们是相似的，而不是逐字比较。

这可能非常强大，因为两个共享很少单词的文本文档实际上可能指的是同一个主题！他们可能只是使用不同的结构来引用它(例如，一个文档可能会提到英国，而另一个文档会使用缩写 UK)。

Topic models are good on their own to build visualizations and explore data. They are also very useful as an intermediate step in many other tasks.

在这一点上，我们可以通过使用主题来定义相似性，来重复寻找与输入查询最相似的帖子的任务。之前我们通过直接比较两个文档的单词向量来比较它们，现在我们可以通过比较它们的主题向量来比较两个文档。

为此，我们将把文档投影到主题空间。也就是说，我们希望有一个总结文档的主题向量。这是 [第五章](05.html)*降维*中讨论的类型的**降维**的另一个例子。在这里，我们向您展示主题模型是如何准确地用于这一目的的；一旦为每个文档计算了主题，我们就可以对主题向量执行操作，而忘记原始单词。如果主题有意义，它们可能比生字更有信息。此外，这可能带来计算上的优势，因为比较主题权重向量比输入词汇表(包含数千个术语)大的向量要快得多。

使用`gensim`，我们研究了如何计算语料库中所有文档对应的主题。我们现在将为所有文档计算这些，并将其存储在 NumPy 数组中，并计算所有成对距离:

```
from gensim import matutils 
topics = matutils.corpus2dense(model[corpus], num_terms=model.num_topics) 
```

现在，`topics`是一个主题矩阵。我们可以使用 SciPy 中的`pdist`函数来计算所有成对距离。有几个可用的距离。默认情况下，它使用欧几里德距离。也就是说，通过一次函数调用，我们计算出`sum((topics[ti] - topics[tj])**2)`的所有值:

```
from scipy.spatial import distance 
distances = distance.squareform(distance.pdist(topics)) 
```

现在，我们将使用最后一个小技巧；我们将把`distance`矩阵的对角线元素设置为无穷大，以确保它看起来比任何其他元素都大:

```
 for ti in range(len(topics)): 
     distances[ti,ti] = np.inf 
```

我们完了！对于每个文档，我们可以很容易地查找最近的元素(这是一种最近邻分类器):

```
def closest_to(doc_id): 
    return distances[doc_id].argmin() 
```

This will not work if we have not set the diagonal elements to a large value: the function will always return the same element as it is the one most similar to itself (except in the weird case where two elements have exactly the same topic distribution, which is very rare unless they are exactly the same).

例如，这里有一个可能的查询文档(它是我们集合中的第二个文档):

```
From: geb@cs.pitt.edu (Gordon Banks) 
Subject: Re: request for information on "essential tremor" and 
 Indrol? 

In article <1q1tbnINNnfn@life.ai.mit.edu> sundar@ai.mit.edu 
 writes: 

Essential tremor is a progressive hereditary tremor that gets 
 worse 
when the patient tries to use the effected member.  All limbs, 
 vocal 
cords, and head can be involved.  Inderal is a beta-blocker and 
is usually effective in diminishing the tremor.  Alcohol and 
 mysoline 
are also effective, but alcohol is too toxic to use as a 
 treatment. 
-- 
------------------------------------------------------------------
 ---------- 
Gordon Banks  N3JXP      | "Skepticism is the chastity of the 
 intellect, and 
geb@cadre.dsl.pitt.edu   |  it is shameful to surrender it too 
 soon." 
  ----------------------------------------------------------------
 ------------ 
```

如果我们要求与`closest_to(1)`最相似的文档，结果我们会收到以下文档:

```
From: geb@cs.pitt.edu (Gordon Banks) 
Subject: Re: High Prolactin 

In article <93088.112203JER4@psuvm.psu.edu> JER4@psuvm.psu.edu 
 (John E. Rodway) writes: 
>Any comments on the use of the drug Parlodel for high prolactin 
 in the blood? 
> 

It can suppress secretion of prolactin. Is useful in cases of galactorrhea. 
Some adenomas of the pituitary secret too much. 

-- 
------------------------------------------------------------------
 ---------- 
Gordon Banks  N3JXP      | "Skepticism is the chastity of the 
 intellect, and 
geb@cadre.dsl.pitt.edu   |  it is shameful to surrender it too 
 soon." 
```

系统返回同一作者讨论药物的帖子。

# 建模整个维基百科

虽然最初的 LDA 实现可能很慢，这限制了它们在小文档集合中的使用，但是现代算法可以很好地处理非常大的数据集合。根据`gensim`的文档，我们将为整个英语维基百科建立一个主题模型。这需要几个小时，但只需一台笔记本电脑就可以完成！有了一组机器，我们可以让它运行得更快，但我们将在后面的章节中研究这种处理环境。

首先，我们从[http://dumps.wikimedia.org](http://dumps.wikimedia.org/)下载整个维基百科转储。这是一个大文件(目前超过 14 GB)，因此可能需要一段时间，除非您的互联网连接非常快。然后，我们将使用`gensim`工具对其进行索引:

```
python -m gensim.scripts.make_wiki enwiki-latest-pages-articles.xml.bz2 wiki_en_output
```

在命令行上运行前一行，不要在 Python shell 上运行！几个小时后，索引将保存在同一个目录中。此时，我们可以构建最终的主题模型。这个过程看起来和它对小 AP 数据集做的完全一样。我们首先导入几个包:

```
import logging, gensim 
```

现在，我们使用标准的 Python 日志模块(使用`gensim`打印状态消息)设置日志。严格来说，这一步并不是必须的，但是多做一点输出来了解发生了什么是很好的:

```
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
```

现在我们加载预处理数据:

```
id2word = gensim.corpora.Dictionary.load_from_text( 
              'wiki_en_output_wordids.txt') 
mm = gensim.corpora.MmCorpus('wiki_en_output_tfidf.mm') 
```

最后，我们像前面一样构建 LDA 模型:

```
model = gensim.models.ldamodel.LdaModel( 
          corpus=mm, 
          id2word=id2word, 
          num_topics=100) 
```

这又需要几个小时。您将在控制台上看到进度，这可以指示您还需要等待多长时间。

一旦完成，我们可以将主题`model`保存到一个文件中，这样我们就不必重做它了:

```
model.save('wiki_lda.pkl') 
```

如果您退出会话并稍后返回，您可以使用以下命令再次加载模型(当然是在适当的导入之后):

```
model = gensim.models.ldamodel.LdaModel.load('wiki_lda.pkl') 
```

`model`对象可以用来探索文档集合并构建`topics`矩阵，就像我们之前所做的那样。

我们可以看到，这仍然是一个稀疏的模型，即使我们比以前有更多的文档(在我们写这篇文章时超过 400 万):

```
lens = (topics > 0).sum(axis=1) 
print('Mean number of topics mentioned: {0:.4}'.format(np.mean(lens))) 
print('Percentage of articles mentioning <10 topics: {0:.1%}'.format( 
               np.mean(lens <= 10))) 
Mean number of topics mentioned: 6.244 
Percentage of articles mentioning <10 topics: 95.1%  
```

因此，平均文件提到`6.244`话题和`95.1`的百分比提到`10`或更少的话题。

我们可以问维基百科上谈论最多的话题是什么。我们将首先计算每个主题的总权重(通过汇总所有文档的权重)，然后检索与权重最高的主题相对应的单词。这是使用以下代码执行的:

```
weights = topics.sum(axis=0) 
words = model.show_topic(weights.argmax(), 64) 
```

使用与我们之前构建可视化相同的工具，我们可以看到谈论最多的主题与音乐相关，并且是一个非常连贯的主题。整整 18%的维基百科页面与这个主题部分相关(维基百科中所有单词的 5.5%被分配到这个主题)。请看下面的截图:

![](assets/5b93cbed-04bb-4091-813d-aaec50f6f349.png)

These plots and numbers were obtained when the book was being written. As Wikipedia keeps changing, your results will be different. We expect that the trends will be similar, but the details may vary.

或者，我们可以看看最少被谈论的话题:

```
words = model.show_topic(weights.argmin(), 64) 
```

参考以下截图:

![](assets/da4b12d9-09ed-4d4c-b851-1d0e05e8cf13.png)

最少被提及的话题很难解释，但它的许多关键词指的是非洲的一些地方。只有 2.1%的文档涉及到它，它只代表了 0.1%的单词。

# 选择主题数量

到目前为止，在本章中，我们使用了固定数量的主题进行分析，即 100 个。这是一个纯粹任意的数字；我们可以使用 20 或 200 个主题。幸运的是，对于许多用途来说，这个数字并不重要。如果你打算只使用主题作为中间步骤，就像我们之前在寻找类似帖子时所做的那样，系统的最终行为很少对模型中使用的主题的确切数量非常敏感。这意味着，只要你使用足够多的主题，无论你使用 100 个主题还是 200 个主题，这个过程产生的推荐都不会有很大的不同。然而，100 通常是一个足够好的数字(而 20 对于一般的文本文档集合来说太少了)，但是如果我们有更多的文档，我们本可以使用更多的。

设置`alpha`值也是如此。虽然玩弄它可以改变话题，但就这种变化而言，最终结果还是很稳健的。自然，这取决于你的数据的确切性质，应该根据经验进行测试，以确保结果确实是稳定的。

Topic modeling is often an end toward a goal. In that case, it is not always very important which parameter values are used exactly. A different number of topics or values for parameters such as `alpha` will result in systems whose end results are almost identical in their final results.

另一方面，如果你打算直接探索这些主题，或者构建一个可视化工具来展示它们，你可能应该尝试一些值，看看哪个给你最有用或最有吸引力的结果。还有一些统计概念，如困惑，可以用来确定一系列模型中哪一个最适合数据，从而做出更明智的决定。

或者，有几种方法可以根据数据集自动确定主题的数量。一个流行的模型叫做**分层狄利克雷过程** ( **HDP** )。同样，其背后的完整数学模型很复杂，超出了本书的范围。然而，我们可以告诉你的是，不是像在 LDA 生成方法中那样首先固定主题，而是主题本身与数据一起生成，一次一个。每当作者创建一个新文档时，他们可以选择使用已经存在的主题或者创建一个全新的主题。当创建了更多的主题时，创建新主题而不是重用现有主题的可能性就会降低，但可能性总是存在的。

这意味着我们拥有的文档越多，我们最终得到的主题也就越多。这是那些一开始不直观，但经过思考后完全有意义的陈述之一。我们正在对文档进行分组，我们拥有的示例越多，就越能分解它们。如果我们只有几篇新闻文章的例子，那么体育将是一个话题。然而，随着我们拥有的越来越多，我们开始把它分解成单独的形式:曲棍球、足球等等。因为我们有了更多的数据，我们可以开始区分细微差别，关于单个团队甚至单个球员的文章。人也是如此。在一个由许多不同背景的人组成的小组里，有几个电脑人，你可能会把他们放在一起；在稍微大一点的小组中，你将为程序员和系统管理员举行单独的聚会；而在现实世界中，我们甚至为 Python 和 Ruby 程序员举办了不同的聚会。

HDP 在`gensim`有售。使用它是微不足道的。为了修改我们为 LDA 编写的代码，我们只需要将对`gensim.models.ldamodel.LdaModel`的调用替换为对`HdpModel`构造函数的调用，如下所示:

```
hdp = gensim.models.hdpmodel.HdpModel(mm, id2word) 
```

就是这样(只是计算时间长一点——没有免费午餐)。现在，我们可以像使用 LDA 模型一样使用这个模型，只是我们不需要指定主题的数量。

# 摘要

在本章中，我们讨论了主题建模。主题建模比聚类更灵活，因为这些方法允许每个文档在多个组中部分呈现。为了探索这些方法，我们使用了一个新的包，`gensim`。

主题建模最初是为文本开发的，在文本的情况下更容易理解，但是在[第 12 章](12.html)、*计算机视觉*中，我们将看到这些技术如何应用于图像。主题模型在现代计算机视觉研究中非常重要。事实上，与前几章不同，这一章非常接近机器学习算法研究的前沿。最初的 LDA 算法发表在 2003 年的一份科学杂志上，但是`gensim`用来处理维基百科的方法直到 2010 年才被开发出来，而 HDP 算法是从 2011 年开始的。研究还在继续，你可以找到很多名字很棒的变体和模式，比如**印度自助餐流程**(不要和中餐厅流程混淆，中餐厅流程是不同的模式)或者**弹球分配**(弹球是日本游戏的一种，是吃角子老虎和弹球的交叉)。

我们现在已经经历了一些主要的机器学习模式:分类、聚类和主题建模。

在[第 11 章](11.html)、*分类三-音乐流派分类*中，我们回到分类，但这次我们将探索先进的算法和方法。