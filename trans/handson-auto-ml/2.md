# 使用 Python 的机器学习简介

最后一章介绍了**机器学习** ( **ML** )的世界。在本章中，我们将开发构建和使用**自动化 ML** ( **AutoML** )平台所需的 ML 基础。不总是清楚如何最好地应用 ML，或者实现它需要什么。然而，ML 工具的使用变得越来越简单，并且 AutoML 平台使它更容易被更广泛的受众访问。未来，人与机器之间无疑会有更高层次的协作。

ML 的未来可能需要人们为其消费准备数据，并确定用于实现的用例。更重要的是，人们需要解释结果并审核 ML 系统——他们是否遵循正确和最佳的方法来解决问题。未来看起来相当惊人，但我们需要建设那个未来；这就是我们在这本书里要做的。在本章中，我们将引导您完成以下主题:

*   机器学习过程及其不同类型
*   监督学习—回归和分类
*   无监督学习—聚类
*   套装——装袋、助推和堆叠
*   基于数据推断任务
*   特定任务评估指标

我们明白单个章节不足以学习和练习 ML。ML 上已经有很多优秀的书籍和资料，你可以在其中找到关于每个提到的主题的详细讨论。你可以在我们的《后事》的*其他你可能喜欢的书*部分看到一些推荐。本章的目的是为您提供不同 ML 技术的概述，并讨论它的一些基本方面，这些方面对于后续章节的工作是必要的。

所以，机器对学习很兴奋。你准备好帮助他们了吗？抓紧了。我们先来看看什么是机器学习！

# 技术要求

所有代码示例都可以在 GitHub 的`Chapter 02`文件夹中找到。

# 机器学习

机器学习可以追溯到几个世纪前。它诞生于这样一种理论，即计算机可以在没有被编程来执行特定任务的情况下学习。ML 的迭代方面是必不可少的，因为机器总是需要适应新的数据。他们需要从历史数据中学习，为更好的计算进行优化，并且还需要概括自己以提供适当的结果。

我们都知道基于规则的系统，其中我们有一组预定义的条件供机器执行并提供结果。当机器自己学习这些模式，交付结果，并解释它发现的规则时，它会有多棒；这是 ML。它是一个更广泛的术语，用于描述机器从数据中学习的各种方法和算法。作为**人工智能** ( **人工智能**)的一个分支，最大似然算法经常被用来发现隐藏的模式，建立关系，以及预测一些事情。

机器学习依赖于一些格式化的输入，并根据任务提供结果。输入格式特定于所使用的 ML 技术的类型，也特定于所考虑的算法。输入数据的这种特定表示被称为**特征**或**预测器**。

# 机器学习过程

我们如何学习？当我们在学校或大学学习时，我们是由老师教的。我们从他们的教导(训练)中学到了东西。学期结束时，我们需要进行一次测试，基本上是为了验证我们的知识。我们获得的分数决定了我们的命运(评价)。通常，评估是通过考虑一个阈值(基线)来进行的。分数决定了我们是需要重考科目，还是准备进入下一个级别(部署)。

这也正是机器学习的方式。括号中的单词是 ML 专业人员使用的术语。然而，这只是我们和机器学习的方式之一。这是一种典型的监督学习方法。人们有时也从经验中学习，这是无监督的学习。让我们研究一下这些学习方法的更多细节。

Broadly we have two categories of ML algorithms as described earlier—supervised and unsupervised learning. There are a few other types, such as reinforcement learning, transfer learning, and semi-supervised learning, which are less often used and so are not in the scope of this book.

# 监督学习

顾名思义，学习过程是基于特定的目标/结果进行监督的。

The objective of supervised ML models is to learn and discover the patterns that can correctly predict the outcome. In case of supervised learning, there is always a labeled historical dataset with a target attribute. All attributes other than the target are termed as **predictors**/**features**.

目标可以是连续的数字属性、指示是/否决策的二进制属性或具有两个以上结果的多类属性。基于目标，模型识别模式，建立预测器之间的关系，然后使用导出的配方来预测新的独立数据集中的未知目标。

许多 ML 算法都属于这类学习方法，例如线性和物流回归、决策树、随机森林和**支持向量机** ( **支持向量机**)等等。

To identify and select the most suited algorithm for a job is the most critical task in an ML project. This is also an essential section that requires significant attention while creating an AutoML system. There are various factors that govern this selection process and they will be covered at length in this book.

# 无监督学习

同样，在无监督学习的情况下，没有目标属性。无监督学习的目标是通过推断输入数据集中特征的结构和关系来识别模式。它可以用来发现共同定义一个组的规则，如主题生成、划分——如客户细分或确定数据的内部结构，如基因聚类。无监督学习算法的例子包括关联规则挖掘和聚类算法。

在创建自动学习系统之前，了解不同的学习算法是非常必要的。在使用算法之前，了解它的三重**W**——**它是什么**、**它在哪里**以及通过**什么**方法可以实现它是至关重要的。

在接下来的部分中，我们将对不同的算法的三重 W 提出质疑，这将有助于创建一个健壮的 AutoML 系统。

# 线性回归

让我们先从线性回归开始我们的三重 W 会话。

# 什么是线性回归？

这是传统和最常用的回归分析。它被严格研究并广泛用于实际目的。线性回归是一种确定因变量( *y* )和一个或多个自变量( *x* )之间关系的方法。这个导出的关系可以用来根据观察到的 *x* 预测一个无法解释的 *y* 从数学上来说，如果 *x* 是一个自变量(通常称为预测值) *y* 是一个因变量(也称为目标值)，这个关系表示如下:

![](img/f603e0e2-8241-4940-a20f-b9dc86694823.png)

其中 *m* 为直线斜率， *b* 为最佳拟合回归线截距，ε为实际值与预测值偏差的误差项。

这是简单线性回归的方程，因为它只涉及一个预测因子( *x* )和一个目标( *y* )。当预测一个目标涉及多个预测因子时，称为**多元线性回归**。术语*线性*表明有一个基本假设，即基础数据呈现线性关系。

让我们在两个变量之间创建一个散点图:产品的**销售数量**和**收入**。从图中我们可以推断，这两个变量之间存在某种正相关关系，即当产品销量激增时，收入就上升了。然而，我们无法在它们之间建立关系来预测销售数量的收入:

![](img/2d3e0ebb-5b4a-4b53-be29-f27fb9cfc84d.png)

如果我们扩展之前的散点图，并在其中添加趋势线，我们会看到最佳拟合线。位于这条线上的任何数据点都是完美的预测值。当我们离开这条线时，预测的可靠性会降低:

![](img/7cf4c11d-4049-4722-8404-500a6ca4329f.png)

那么，我们如何找到最合适的线路呢？最常见和最广泛使用的技术是**普通最小二乘** ( **OLS** )估计。

# OLS 回归工作

OLS `LinearRegression`方法是将函数拟合到数据的最直接的方法。它通过最小化数据的平方误差 ( **SSE** )的总和**来找到最佳拟合线。上证综指是实际值与平均值的偏差之和。然而，一如既往，简单是有代价的。一个优秀的 OLS 方法的代价是坚持它的几个基本假设。**

# OLS 的假设

为了获得 OLS 回归技术的好处，所有这些关于数据的假设都应该成立:

*   **线性**:真正的 *X* 和 *Y* 之间的潜在关系是线性的。
*   **同态**:残差的方差必须是常数。残差是目标的观察值和预测值之间的差值。
*   **正态性**:残差/误差应为正态分布。
*   **没有或很少多重共线性**:残差/误差必须是独立的。

OLS 也受到数据中异常值的影响。在使用 OLS 线性回归进行线性回归建模之前，异常值处理是必要的。

# 线性回归在哪里使用？

线性回归有许多实际的使用案例，其中大多数属于以下两大类之一:

*   如果目标是预测或预测，则可以使用它来构建一个预测模型，该模型是由相关值和独立值组成的公认数据集
*   如果目标是确定目标变量和预测变量之间关系的强度，则可以将其应用于量化给定值 *X* 的 *Y* 的变化

# 用什么方法可以实现线性回归？

我们可以使用 scikit-learn 的`LinearRegression`方法在 Python 中创建一个线性回归模型。由于这是我们讨论使用 Python 实现模型的第一个例子，我们将绕过算法的讨论，学习一些在 Python 中创建模型所需的基本包:

*   `numpy`:是用于数学函数的数值 Python 模块。它为多维数组和矩阵的有效计算提供了健壮的数据结构。

*   `pandas`:为数据操作提供 DataFrame 对象。数据框可以保存不同类型的值和数组。它用于在 Python 中读取、写入和操作数据。
*   `scikit-learn`:是 Python 中的 ML 库。它包括各种 ML 算法，是一个广泛使用的库，用于在 Python 中创建 ML 模型。除了 ML 算法之外，它还提供了开发模型所需的各种其他功能，如`train_test_split`、模型评估指标和优化指标。

在创建模型之前，我们需要首先将这些必需的库导入 Python 环境。如果是在 Jupyter 笔记本上运行代码，需要声明`%matplotlib inline`在界面内内联查看图形。我们需要导入`numpy`和`pandas`包，以便于数据操作和数值计算。本练习的计划是创建一个线性回归模型，因此我们还需要从 scikit-learn 包中导入`LinearRegression`方法。我们将使用 scikit-learn 的示例`Boston`数据集来完成任务:

```py
%matplotlib inline
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
from sklearn.datasets import load_boston
```

接下来，我们需要使用以下命令加载`Boston`数据集。它是一本字典，我们可以检查它的键来查看它的内容:

```py
boston_data = load_boston()
boston_data.keys()
```

前面代码的输出如下:

![](img/02f28ae9-33d2-4c85-88fe-bab866d243d5.png)

`boston_data`有四个键，它们所指向的值的种类是不言自明的。我们可以从键`data`和`target`中检索数据和目标值。`feature_names`键保存属性的名称，`DESCR`有每个属性的描述。

在处理数据之前，最好先看看数据大小。这有助于决定是使用完整的数据还是使用它的样本，也有助于推断执行可能需要多长时间。

Python 中的`data.shape`函数是查看数据维度(行和列)的绝佳方式:

```py
print(" Number of rows and columns in the data set ", boston_data.data.shape)
print(boston_data.feature_names)
```

前面代码的输出如下:

![](img/ca4cbd94-c50d-4d36-a16c-f8f964255caf.png)

接下来，我们需要将字典转换为数据帧。这可以通过调用`pandas`库的`DataFrame`函数来实现。我们使用`head()`显示记录的子集来验证数据:

```py
boston_df =pd.DataFrame(boston_data.data)
boston_df.head()
```

A DataFrame is a collection of vectors and can be treated as a two-dimensional table. We can consider DataFrame as having each row correspond to some observation and each column to some attribute of the observation. This makes them extremely useful for fitting to a ML modeling task.

前面代码的输出如下:

![](img/bafae005-de96-4b51-a10c-10176ed776fa.png)

列名只是数字索引，并没有给出数据框的含义。因此，让我们将`feature_names`作为列名分配给`boston_df`数据帧，以获得有意义的名称:

```py
boston_df.columns = boston_data.feature_names
```

我们再次查看`boston`房屋租金数据的样本，现在它对栏目的描述比以前更好了:

```py
boston_df.head()
```

前面代码的输出如下:

![](img/b2dea1f8-67a2-48ac-aeb2-e7f585aa730d.png)

在线性回归中，必须有一个数据帧作为目标变量，另一个数据帧具有其他特征作为预测因子。本练习的目的是预测房价，因此我们将`PRICE`指定为目标属性(`Y`)，其余的都指定为预测因子(`X`)。使用`drop`功能将`PRICE`从预测值列表中删除。

接下来，我们打印每个变量的截距和系数。这些系数决定了每个预测者对预测房价的权重和贡献(目标`Y`)。截距提供了一个恒定值，当所有的预测因子都不存在时，我们可以认为它是房价:

```py
boston_df['PRICE'] = boston_data.target
X = boston_df.drop('PRICE', axis=1)
lm = LinearRegression()
lm.fit(X, boston_df.PRICE)
print("Intercept: ", lm.intercept_)
print("Coefficient: ", lm.coef_)
```

前面代码的输出如下:

![](img/cb84f99b-9fd9-4482-8105-3192689048c5.png)

从前面的截图中不清楚哪个系数属于什么预测因子。因此，我们使用以下代码将特征和系数联系在一起:

```py
pd.DataFrame(list(zip(X.columns, lm.coef_)),columns= ['features','estimatedCoefficients'])
```

前面代码的输出如下:

![](img/0b0a0d4a-65d2-4134-8c89-db9b7093c7d4.png)

接下来，我们计算并查看均方误差度量。现在，让我们把它看作是模型在预测房价时的平均误差。评估指标对于理解模型的动态性及其在生产环境中的表现非常重要:

```py
lm.predict(X)[0:5
mseFull = np.mean((boston_df.PRICE - lm.predict(X)) ** 2)
print(mseFull)
```

前面代码的输出如下:

![](img/474add35-56a7-4959-bf27-65197a785b27.png)

我们在整个数据集上创建了模型，但是当在实际生产环境中使用时，确保我们开发的模型在不同的数据集上正常工作是非常重要的。因此，用于建模的数据分为两组，通常比例为 70:30。最重要的分割用于训练模型，另一个用于测试开发的模型。这个独立的测试数据集被认为是一个*虚拟生产环境*，因为它在模型的训练阶段是隐藏的。测试数据集用于生成预测和评估模型的准确性。Scikit-learn 提供了一种`train_test_split`方法，可用于将数据集分成两部分。该功能中的`test_size`参数表示待测试数据的百分比。在下面的代码中，我们将数据集分成`train`和`test`集，并重新训练模型:

```py
#Train and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, boston_df.PRICE, test_size=0.3, random_state=42)
print(X_train)
```

由于我们使用了`test_size=0.3`，70%的数据集将用于创建`train`集，30%将保留给`test`数据集。我们遵循与前面相同的步骤来创建线性回归模型，但是现在我们将只使用训练数据集(`X_train`和`Y_train`)来创建模型:

```py
lm_tts = LinearRegression()
lm_tts.fit(X_train, Y_train)
print("Intercept: ", lm_tts.intercept_)
print("Coefficient: ", lm_tts.coef_)
```

前面代码的输出如下:

![](img/91e0dfab-877d-4afd-bcc1-68d2edbfda46.png)

我们预测`train`和`test`数据集的目标值，并计算它们的**均方误差** ( **均方误差**):

```py
pred_train = lm.predict(X_train)
pred_test = lm.predict(X_test)
print("MSE for Y_train:", np.mean((Y_train - lm.predict(X_train)) ** 2))
print("MSE with Y_test:", np.mean((Y_test - lm.predict(X_test)) ** 2))
```

前面代码的输出如下:

![](img/34a9d87f-8ded-4d49-9727-cc1c03df63f1.png)

我们看到`train`和`test`数据集的均方误差分别是`22.86`和`19.65`。这意味着该模型的性能在训练和测试阶段几乎是相似的，并且可以用于在新的独立的相同数据集上预测房价。

接下来，让我们绘制一个残差图，看看残差是否遵循线性模式:

```py
plt.scatter(pred_train,pred_train - Y_train, c = 'b',s=40,alpha=0.5)
plt.scatter(pred_test,pred_test - Y_test, c = 'r',s=40,alpha=0.7)
plt.hlines(y = 0, xmin=0, xmax = 50)
plt.title('Residual Plot - training data (blue) and test data(green)')
plt.ylabel('Residuals')
```

前面代码的输出如下:

![](img/640802a7-51de-4cb5-a436-eff32f312573.png)

由于残差围绕水平虚线对称分布，因此它们呈现出完美的线性模式。

开发一个模型很容易，但是设计一个有用的模型很难。评估 ML 模型的性能是 ML 管道中至关重要的一步。一旦一个模型准备好了，我们就必须对它进行评估，以确定它的正确性。在下一节中，我们将向您介绍用于评估回归模型的一些广泛使用的评估指标。

# 重要评估指标–回归算法

评估 ML 模型的价值是一个两阶段的过程。首先，必须评估模型的统计准确性，即统计假设是否正确，模型性能是否突出，以及其他独立数据集的性能是否成立。这是使用几个模型评估指标来完成的。然后，评估一个模型，看看结果是否如业务需求所期望的那样，涉众是否真正从中获得了一些见解或有用的预测。

回归模型基于以下指标进行评估:

*   **平均绝对误差** ( **MAE** ):是预测误差绝对值之和。预测误差定义为预测值和实际值之间的差值。这个标准给出了误差大小的概念。然而，我们无法判断该模型是预测过度还是预测不足。一个人应该始终瞄准一个低的 MAE 分数:

![](img/6f48e22e-f215-4a68-b085-0784c6bce8b9.png)

其中， *y <sub class="calibre54">i</sub>* =实际值

![](img/438e30ba-09a6-4428-8fda-cd6e62e315c0.png) =预测值

*n* =病例数(记录)

*   **均方误差**:均方误差之和的平均值。这个度量描述了误差的大小和方向。但是，测量单位会随着值的平方而改变。另一个度量标准弥补了这一不足:均方根误差。分数越低，模型越好:

![](img/053f723d-1dcc-4415-ae06-e784b92cec3d.png)

*   **均方根误差** ( **RMSE** ):该指标由均方误差的平方根计算得出。取平方根会将测量单位转换回原始单位。RMSE 分数低的模型是好模型:

![](img/62876291-3cfc-47eb-9c7d-3bc5832d9ac6.png)

*   **R <sup class="calibre55">2</sup> 评分**:又称**决定系数**。它描述了模型解释的方差百分比。例如 *R <sup class="calibre55">2</sup>* 为 0.9，则模型中使用的属性或特征可以代表其 90%的变异。 *R <sup class="calibre55">2</sup>* 从 0 到 1 不等，这个值越高，模型越好。然而，需要有一个好的测试策略来验证模型不会过度:

![](img/14dbbcdf-f856-4f67-9be5-6711719ed229.png)

其中， *y <sub class="calibre54">i</sub>* =实际值

![](img/438e30ba-09a6-4428-8fda-cd6e62e315c0.png) =预测值

*n* =病例数(记录)

![](img/00d4cbd1-9385-40a9-9d9f-12facddc4b5a.png)= y 的平均值

Overfitting occurs when a machine learning model learns the training data very well. These models have low bias and high variance in their results. In such cases, the model might lead to poor predictions on new data.

在这一节中，我们学习了回归分析作为监督最大似然方法之一。它可以用于目标数据是连续数字数据的情况，例如预测员工的期望工资、预测房价或预测支出值。

如果目标有离散数据怎么办？我们如何预测客户是否会流失？我们如何预测是否应该为潜在客户批准贷款/信用卡？线性回归不适用于这些情况，因为这些问题违反了其基本假设。我们还有其他方法吗？对于这些情况，我们可以使用分类模型。

Classification modeling is another form of supervised machine learning methods that is used to predict targets with discrete input target values. The classification algorithms are known as **classifiers**, as they identify the set of categories that input data support and use this information to assign a class to an unidentified or unknown target label.

在接下来的部分中，我们将介绍一些广泛使用的分类器，如物流回归、决策树、支持向量机和 k 近邻。物流回归可以被认为是回归和分类方法之间的桥梁。这是一个伪装成回归特征的分类器。然而，它是最有效和最容易解释的分类模型之一。

# 逻辑回归

让我们从物流回归的*三重 W* 开始。重申一下 tripe W 方法，我们先问算法是什么，然后问在哪里可以用，最后问用什么方法可以实现模型。

# 什么是逻辑回归？

逻辑回归可以被认为是线性回归算法的扩展。它基本上像线性回归一样工作，但它意味着离散或分类的结果。

# 逻辑回归在哪里使用？

逻辑回归适用于离散目标变量的情况，如二进制响应。在这种情况下，线性回归的一些假设，如目标属性和特征，不遵循线性关系，残差可能不是正态分布，或者误差项是异方差的。在逻辑回归中，目标被重建为其优势比的对数，以拟合回归方程，如下所示:

![](img/a2805696-e749-4c2f-b766-cef23a8d8500.png)

赔率反映了特定事件发生的概率或可能性与同一事件不发生的概率之比。如果 *P* 是一个事件/类别出现的概率，*P–1*是第二个事件/类别出现的概率。

# 用什么方法可以实现逻辑回归？

可以通过导入 scikit-learn 的`LogisticRegression`方法来创建逻辑回归模型。我们像前面创建线性回归模型一样加载包:

```py
import pandas as pd
import numpy as np
from sklearn import preprocessing
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
```

我们将使用一个`HR`部门的数据集，该数据集包含过去流失的员工以及继续工作的员工列表:

```py
hr_data = pd.read_csv('data/hr.csv', header=0)
hr_data.head()
hr_data = hr_data.dropna()
print(hr_data.shape)
print(list(hr_data.columns))
```

前面代码的输出如下:

![](img/0d47dcfc-a9e7-4458-821e-2ff093d4d26a.png)

数据集有`14999`行和`10`列。`data.columns`功能显示属性的名称。`salary`属性有三个值— `high`、`low`、`medium`、`sales`有七个值— `IT`、`RandD`、`marketing`、`product_mng`、`sales`、`support`和`technical`。为了在模型中使用这些离散的输入数据，我们需要将其转换为数字格式。有多种方法可以做到这一点。其中一种方法是对值进行伪编码，也称为**一热编码**。使用这种方法，为分类属性的每个类生成虚拟列。

对于每个伪属性，类的存在由 1 表示，它的不存在由 0 表示。

Discrete data can either be nominal or ordinal. When there is a natural ordering of values in the discrete data, it is termed as **ordinal**. For example, categorical values such as high, medium, and low are ordered values. For these cases, label encoding is mostly used. When we cannot derive any relationship or order from the categorical or discrete values, it is termed as **nominal**. For example, colors such as red, yellow, and green have no order. For these cases, dummy encoding is a popular method.

`pandas`的`get_dummies`方法提供了一个在 Python 中创建虚拟变量的简单界面。函数的输入是数据集和要虚拟编码的属性的名称。在这种情况下，我们将对`HR`数据集的`salary`和`sales`属性进行虚拟编码:

```py
data_trnsf = pd.get_dummies(hr_data, columns =['salary', 'sales'])
data_trnsf.columns
```

前面代码的输出如下:

![](img/a2a9d489-4d0f-4065-94af-505d0a77f647.png)

现在，数据集已经可以建模了。`sales`和`salary`属性成功一键编码。接下来，当我们要预测流失时，我们将使用`left`属性作为目标，因为它包含员工是否流失的信息。我们可以从代码中称为`X`的输入预测数据集中删除`left`数据。左侧属性用`Y`(目标)表示:

```py
X = data_trnsf.drop('left', axis=1)
X.columns
```

前面代码的输出如下:

![](img/09d28bcf-59db-4b22-b2a8-1117e51dbdab.png)

我们将数据集拆分为`train`和`test`集，比例为 70:30。70%的数据将用于训练逻辑回归模型，其余 30%用于评估模型的准确性:

```py
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, data_trnsf.left, test_size=0.3, random_state=42)
print(X_train)
```

当我们执行代码片段时，创建了四个数据集。`X_train`和`X_test`是`train`和`test`输入预测值数据。`Y_train`和`Y_test`是`train`和`test`输入的目标数据。现在，我们将在列车数据上拟合模型，并在测试数据上评估模型的准确性。首先，我们创建一个`LogisticsRegression()`分类器类的实例。接下来，我们在训练数据上拟合分类器:

```py
attrition_classifier = LogisticRegression()
attrition_classifier.fit(X_train, Y_train)
```

一旦模型成功创建，我们使用测试输入预测数据集上的`predict`方法来预测相应的目标值(`Y_pred`):

```py
Y_pred = attrition_classifier.predict(X_test)
```

我们需要创建一个`confusion_matrix`来评估一个分类器。大多数模型评估指标都基于混淆矩阵本身。在这一节之后，将详细讨论混淆矩阵和其他评估指标。现在，让我们将混淆矩阵视为由四个值组成的矩阵，它为我们提供了正确和错误预测的值的计数。基于混淆矩阵中的值，计算分类器的精度。我们的分类器的准确率为 0.79 或 79%，这意味着 79%的案例被正确预测:

```py
from sklearn.metrics import confusion_matrix
confusion_matrix = confusion_matrix(Y_test, Y_pred)
print(confusion_matrix)

print('Accuracy of logistic regression model on test dataset: {:.2f}'.format(attrition_classifier.score(X_test, Y_test)))
```

前面代码的输出如下:

![](img/85a9a59e-d7c1-4a83-8f02-907d099b3398.png)

有时，准确性可能不是判断模型性能的好方法。例如，在不平衡数据集的情况下，预测可能偏向多数类。因此，我们需要查看其他指标，如 f1-score、**曲线下面积** ( **AUC** )、精度和召回率，这些指标给出了关于模型的公正判断。我们可以通过从 scikit-learn 的`metric`方法中导入`classification_report`来检索所有这些指标的分数:

```py
from sklearn.metrics import classification_report
print(classification_report(Y_test, Y_pred))
```

前面代码的输出如下:

![](img/543e3c15-afab-4fef-b0b2-784f0f8b4b45.png)

**接收器操作特性** ( **ROC** )最常用于可视化二进制分类器的性能。AUC 度量是 ROC 曲线下的面积，它提供了一个单一的数字来总结基于 ROC 曲线的分类器的性能。以下代码片段可用于使用 Python 绘制 ROC 曲线:

```py
from sklearn.metrics import roc_curve
from sklearn.metrics import auc

# Compute false positive rate(fpr), true positive rate(tpr), thresholds and roc auc(Area under Curve)
fpr, tpr, thresholds = roc_curve(Y_test, Y_pred)
auc = auc(fpr,tpr)

# Plot ROC curve
plt.plot(fpr, tpr, label='AUC = %0.2f' % auc)
#random prediction curve
plt.plot([0, 1], [0, 1], 'k--') 
#Set the x limits
plt.xlim([0.0, 1.0])
#Set the Y limits
plt.ylim([0.0, 1.0])
#Set the X label
plt.xlabel('False Positive Rate(FPR) ')
#Set the Y label
plt.ylabel('True Positive Rate(TPR)')
#Set the plot title
plt.title('Receiver Operating Characteristic(ROC) Cure')
# Location of the AUC legend
plt.legend(loc="right")
```

前面代码的输出如下:

![](img/6abca7b3-b9f8-41a8-a002-09ea386f32b7.png)

我们车型的 AUC 为 **0.63** 。我们已经看到了一些用于评估分类模型的指标，其中一些看起来很奇怪。因此，在继续讨论分类算法之前，让我们先了解一下这些指标。

# 重要评估指标–分类算法

用于评估分类模型的大多数指标都是基于我们在混淆矩阵的四个象限中获得的值。让我们从了解它是什么开始这一部分:

*   **混淆矩阵**:是评价一个分类模型(即分类器)的基石。顾名思义，矩阵有时令人困惑。让我们尝试将混淆矩阵可视化为图形中的两个轴。 *x* 轴标签为预测，有两个值— **正值**和**负值**。同样的， *y* 轴标签实际上是相同的两个值——**正**和**负**，如下图所示。该矩阵是一个包含分类器实际值和预测值计数信息的表格:

![](img/cb56e88c-6d73-4820-b584-533d9bec1213.png)

*   如果我们试图推断矩阵中每个象限的信息:
    *   象限一是被准确识别的正类预测的数量。所以称之为**真阳性** ( **TP** )。
    *   象限二，也称为**假阳性** ( **FP** )是对实际阳性病例的不准确预测数。
    *   象限三，即**假阴性** ( **FN** )是阴性病例的不准确预测数。
    *   象限四为**真负** ( **TN** )，是准确分类的负类预测数。
*   **准确度**:准确度衡量分类器做出准确预测的频率。它是正确预测数与预测总数的比率:

![](img/27d9e90b-b56b-4c14-a6d6-7bc1d5594617.png) 

*   **精度**:精度估计被准确识别的真实阳性的比例。它是真实阳性与所有预测阳性的比率:

![](img/c3b28460-ce4a-40a7-acac-5f9abdafda3c.png)

*   **召回**:召回也称为灵敏度或**真阳性率** ( **TPR** )。它估计目标所有观察到的正值中的真阳性比例:

![](img/9c02085d-d117-4cc6-a5fb-e45c9184298b.png)

*   **误分类率**:估计分类器预测不准确的频率。它是错误预测与所有预测的比率:

![](img/ae418f25-956d-4700-b0ff-a4e0bc7ee559.png)

*   **特异性**:特异性又称**真阴性率** ( **TNR** )。它估计目标所有观察到的负值中真实负值的比例:

![](img/50b85e81-6d59-4390-9730-cc3ce5a6594d.png)

*   **ROC 曲线**:ROC 曲线总结了分类器在所有可能阈值上的性能。对于所有可能的阈值，ROC 曲线的图形在 *y* 轴上用**真阳性率** ( **TPR** )绘制，在 *x* 轴上用**假阳性率** ( **FPR** )绘制。
*   **AUC** : AUC 是 ROC 曲线下的面积。如果分类器突出，真阳性率会增加，曲线下面积接近 1。如果分类器类似于随机猜测，则真阳性率将随着假阳性率(1–灵敏度)线性增加。在这种情况下，AUC 将在 0.5 左右。AUC 度量越好，模型越好。
*   **提升**:提升有助于估计模型预测能力相对于平均或基线模型的提高。例如，人力资源流失数据集的基线模型精度为 40%，但同一数据集上新模型的精度为 80%。然后，该模型的升力为 2 (80/40)。
*   **平衡精度**:有时候，精度本身并不是评价一个模型的好尺度。对于数据集不平衡的情况，它可能不是一个有用的评估指标。在这种情况下，平衡精度可以用作评估指标之一。平衡精度是根据在以下任一类别中获得的平均精度计算的度量:

![](img/976e3652-bfc4-46a2-95be-80519cd7881b.png)

Unbalanced dataset—Where one class dominates the other class. In such cases, there is an inherent bias in prediction towards the major class. However, this is a problem with base learners such as decision trees and logistic regression. For ensemble models such as random forest it can handle unbalanced classes well.

*   **F1 分数**:F1 分数也是估计不平衡分类器的一个合理的度量。F1 分数是精确度和召回率的调和平均值。它的值介于 0 和 1 之间:

![](img/9cec7c39-c17c-4a06-9c76-2b8cf2475060.png)

*   **海明损失**:这标识了标签被错误预测的分数。
*   **马修斯相关系数** ( **MCC** ): MCC 是目标和预测之间的相关系数。它在-1 和+1 之间变化。-1 当实际值和预测值完全不一致时，1 当实际值和预测值完全一致时，0 当关于实际值的预测值可能是随机的时。因为它涉及混淆矩阵的所有四个象限的值，所以它被认为是一个平衡的度量。

有时候，创建一个预测模型不仅仅是一个要求。我们需要了解模型是如何构建的，以及描述模型的关键特性。在这种情况下，决策树可以用来建模。

# 决策树

决策树是 ML 世界中广泛使用的分类器，因为它们在表示驱动分类/预测的规则时是透明的。让我们向这个算法提出三重问题，以了解更多信息。

# 什么是决策树？

决策树以层次树状结构排列，易于解释和解释。他们对异常值不敏感。创建决策树的过程是一种递归划分方法，它将训练数据分成不同的组，目标是找到同质的纯子组，即只有一个类的数据。

Outliers are values that lie far away from other data points and distort the data distribution.

# 决策树用在哪里？

决策树非常适合需要解释特定决策原因的情况。例如，金融机构可能需要在发放贷款或信用卡之前对影响客户信用评分的规则进行完整的描述。

# 决策树可以通过哪种方法实现？

决策树模型可以通过导入 scikit-learn 的`DecisionTreeClassifier`来创建:

```py
import numpy as np
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn import tree
```

接下来，我们读取`HR`损耗数据集，并执行前面物流回归示例中完成的所有数据预处理:

```py
hr_data = pd.read_csv('data/hr.csv', header=0)
hr_data.head()
hr_data = hr_data.dropna()
print(" Data Set Shape ", hr_data.shape)
print(list(hr_data.columns))
print(" Sample Data ", hr_data.head())
```

前面代码的输出如下:

![](img/2a7b05e2-bf30-4964-9d27-8e88dfa1a1dd.png)

以下代码为分类数据创建虚拟变量，并将数据拆分为`train`和`test`集:

```py
data_trnsf = pd.get_dummies(hr_data, columns =['salary', 'sales'])
data_trnsf.columns
X = data_trnsf.drop('left', axis=1)
X.columns
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, data_trnsf.left, test_size=0.3, random_state=42)
print(X_train)
```

接下来，为了创建一个决策树分类器，我们需要实例化一个`DecisionTreeClassifier`至少有所需的参数。以下是用于生成决策树模型的一些参数:

*   `criterion`:形成决策树的杂质度量；可以是`entropy`也可以是`gini`
*   `max_depth`:树的最大深度
*   `min_samples_leaf`:构建叶节点所需的最小样本数
*   `max_depth`和`min_sample_leafs`是树木预修剪的两个标准

让我们使用以下参数创建一个决策树模型:

```py
attrition_tree = DecisionTreeClassifier(criterion = "gini", random_state = 100,
max_depth=3, min_samples_leaf=5)
attrition_tree.fit(X_train, Y_train)
```

前面代码的输出如下:

![](img/9794af55-a625-4f77-b803-d714da83d82f.png)

接下来，我们生成一个混淆矩阵来评估模型:

```py
Y_pred = attrition_tree.predict(X_test)
from sklearn.metrics import confusion_matrix
confusionmatrix = confusion_matrix(Y_test, Y_pred)
print(confusionmatrix)
```

前面代码的输出如下:

![](img/5f8d1c13-e28f-4239-85ee-5268617ad3aa.png)

如果我们查看混淆矩阵，我们可以假设分类器在对真阳性和真阴性进行分类方面做了可靠的工作。但是，让我们根据总结的评估指标来验证我们的假设:

```py
print('Accuracy of Decision Tree classifier on test set: {:.2f}'.format(attrition_tree.score(X_test, Y_test)))
from sklearn.metrics import classification_report
 print(classification_report(Y_test, Y_pred))
```

前面代码的输出如下:

![](img/d32281c9-7461-4ded-ae39-87b1b7f53949.png)

准确度和其他指标都是`0.95`，这是一个相当不错的分数。

基于树的模型比逻辑回归模型具有更好的结果。现在，让我们了解另一种流行的基于支持向量的分类建模技术。

# 支持向量机

SVM 是一个监督的最大似然算法，主要用于分类任务，然而，它也可以用于回归问题。

# 什么是 SVM？

SVM 是一个基于分离超平面原理的分类器。给定一个训练数据集，算法会找到一个最大化类分离的超平面，并将这些分区用于新数据集的预测。超平面是比其环境平面小一维的子空间。这意味着线是二维数据集的超平面。

# SVM 用在哪里？

SVM 拥有与其他分类器相似的用例，但是 SVM 非常适合特征/属性的数量比数据点/记录的数量多的情况。

# 用什么方法可以实现 SVM？

创建 SVM 模型的过程类似于我们之前研究的其他分类方法。唯一不同的是从 scikit-learn 的库中导入`svm`方法。我们使用`pandas`库导入`HR`损耗数据集，并将数据集拆分为`train`和`test`集:

```py
import numpy as np
import pandas as pd
from sklearn import svm
from sklearn.metrics import accuracy_score

hr_data = pd.read_csv('data/hr.csv', header=0)
hr_data.head()
hr_data = hr_data.dropna()
print(" Data Set Shape ", hr_data.shape)
print(list(hr_data.columns))
print(" Sample Data ", hr_data.head())
data_trnsf = pd.get_dummies(hr_data, columns =['salary', 'sales'])
data_trnsf.columns
X = data_trnsf.drop('left', axis=1)
X.columns
from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(X, data_trnsf.left, test_size=0.3, random_state=42)
print(X_train)
```

接下来，我们创建一个 SVM 模型实例。我们将内核设置为线性的，因为我们希望用一条线来分隔两个类。为线性可分数据寻找最优超平面很容易。然而，当数据不可线性分离时，数据被映射到一个新的空间以使其可线性分离。这种方法被称为**核心技巧**:

```py
attrition_svm = svm.SVC(kernel='linear') 
attrition_svm.fit(X_train, Y_train)
```

前面代码的输出如下:

![](img/3ff63615-e096-4bdb-80c1-5b93c2d00d85.png)

将 SVM 模型实例拟合到列车数据后，我们预测`test`集合的`Y`值，并创建一个混淆矩阵来评估模型性能:

```py
Y_pred = attrition_svm.predict(X_test)
from sklearn.metrics import confusion_matrix
confusionmatrix = confusion_matrix(Y_test, Y_pred)
print(confusionmatrix)
```

前面代码的输出如下:

![](img/c019f6ee-49a6-45c3-a644-4d974b569635.png)

然后，计算模型精度和其他指标的值:

```py
print('Accuracy of SVM classifier on test set: {:.2f}'.format(attrition_svm.score(X_test, Y_test)))
from sklearn.metrics import classification_report
print(classification_report(Y_test, Y_pred))
```

前面代码的输出如下:

![](img/d5ae3337-8749-4745-8f84-652d6efae463.png)

我们看到，带有默认参数的 SVM 模型的表现并不比决策树模型好。因此，到目前为止，决策树在`HR`流失预测排行榜中占据了榜首的位置。让我们尝试另一种分类算法，**k-近邻** ( **KNN** )，它更容易理解和使用，但资源密集得多。

# k-最近邻

在我们为`HR`损耗数据集构建 KNN 模型之前，让我们了解一下 KNN 的三重 w

# 什么是 k 近邻？

KNN 算法是最直接的算法之一，它存储所有可用的数据点，并基于距离相似性度量(如欧氏距离)预测新数据。它是一种可以直接使用训练数据集进行预测的算法。然而，它需要更多的资源，因为它没有任何训练阶段，并且需要内存中的所有数据来预测新的实例。

Euclidean distance is calculated as the square root of the sum of the squared differences between two points. ![](img/a6326b77-f719-497b-8cde-8bcc46436552.png)

# KNN 用在哪里？

KNN 可以用于建立分类和回归模型。它适用于二元和多元分类任务。KNN 甚至可以用于创建推荐系统或输入缺失值。它易于使用，易于训练，易于解释结果。

# 用什么方法可以实现 KNN？

同样，我们对 KNN 采用了类似的流程，就像我们创建之前的模型一样。我们从 scikit-learn 的库中导入`KNeighborsClassifier`方法，使用 KNN 算法进行建模。接下来，我们使用`pandas`库导入`HR`损耗数据集，并将数据集拆分为`train`和`test`集:

```py
import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.neighbors import KNeighborsClassifier
hr_data = pd.read_csv('data/hr.csv', header=0)
hr_data.head()
hr_data = hr_data.dropna()
print(" Data Set Shape ", hr_data.shape)
print(list(hr_data.columns))
print(" Sample Data ", hr_data.head())
data_trnsf = pd.get_dummies(hr_data, columns =['salary', 'sales'])
data_trnsf.columns
X = data_trnsf.drop('left', axis=1)
X.columns
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, data_trnsf.left, test_size=0.3, random_state=42)
print(X_train)
```

为了创建一个 KNN 模型，我们需要指定距离计算要考虑的最近邻居的数量。

In real life, when we create models, we create different models for a range of `n_neighbors` values with various distance measures and choose the model that returns the highest accuracy. This process is also known as **tuning the hyperparameters**.

对于下面的`HR`损耗模型，我们将`n_neighbors`定义为`6`，距离度量为欧几里得:

```py
n_neighbors = 6
attrition_knn = KNeighborsClassifier(n_neighbors=n_neighbors, metric='euclidean')
attrition_knn.fit(X_train, Y_train)
```

前面代码的输出如下:

![](img/611dfc34-768a-4ce0-9b11-e0b61d31cd9f.png)

然后对`test`数据集进行预测，我们回顾混淆矩阵和其他评估指标:

```py
Y_pred = attrition_knn.predict(X_test)
from sklearn.metrics import confusion_matrix
confusionmatrix = confusion_matrix(Y_test, Y_pred)
print(confusionmatrix)
```

前面代码的输出如下:

![](img/8a9892a3-e2c5-4ec0-8e5f-9fcd7b5b943c.png)

以下代码报告了其他指标的准确性分数和值:

```py
print('Accuracy of KNN classifier on test set: {:.2f}'.format(attrition_knn.score(X_test, Y_test)))
from sklearn.metrics import classification_report
print(classification_report(Y_test, Y_pred))
```

前面代码的输出如下:

![](img/8a6ee944-e7e1-4634-94b7-90f1507fafca.png)

KNN 模型的结果比 SVM 模型好，但是，它仍然低于决策树的得分。KNN 是一个资源密集型算法。明智的做法是使用某种不同算法的模型，如果使用 KNN 只是略有改进的话。然而，用户可以根据他们的环境和他们试图解决的问题来决定什么是最好的。

# 集成方法

集合模型是一种增强预测模型效率的稳健方法。这是一个经过深思熟虑的策略，非常类似于一个充满力量的词——团队！！团队完成的任何任务都会带来重大成就。

# 什么是合奏模型？

同样，在 ML 世界中，一个合奏模型是一个由模型组成的*团队*，他们一起工作来提高他们工作的结果。从技术上讲，集成模型由几个单独训练的监督学习模型组成，结果以各种方式合并，以实现最终的预测。该结果比其独立构成的任何学习算法的结果具有更高的预测能力。

通常，使用的集成学习方法有三种:

*   制袋材料
*   助推
*   堆叠/混合

# 制袋材料

打包也称为**引导聚合**。这是减小模型结果方差误差的一种方法。有时弱学习算法非常敏感——稍微不同的输入会导致非常不寻常的输出。随机森林通过运行多个实例减少了这种可变性，从而降低了方差。在这种方法中，使用带有替换模型的随机样本从训练数据集准备随机样本(自举过程)。

使用监督学习方法在每个样本上开发模型。最后，通过平均预测或利用多数投票技术选择最佳预测来合并结果。多数投票是一个过程，其中集成的预测是所有分类器中预测数量最高的类。还有各种其他方法，如加权和秩平均，用于产生最终结果。

scikit-learn 中提供了各种打包算法，如打包决策树、随机森林和额外树。我们将演示最流行的随机森林模型，您可以尝试其他模型。我们可以通过从 scikit-learn 的`ensemble`包导入`RandomForestClassifier`来实现随机森林。由于我们仍在处理`HR`流失数据，本演示的部分代码段也保持不变:

```py
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn import tree
hr_data = pd.read_csv('data/hr.csv', header=0)
hr_data.head()
hr_data = hr_data.dropna()
print(" Data Set Shape ", hr_data.shape)
print(list(hr_data.columns))
print(" Sample Data ", hr_data.head())
data_trnsf = pd.get_dummies(hr_data, columns =['salary', 'sales'])
data_trnsf.columns
X = data_trnsf.drop('left', axis=1)
X.columns
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, data_trnsf.left, test_size=0.3, random_state=42)
print(X_train)
```

没有强制参数来实例化随机林模型。但是，对于创建良好的随机森林模型，有几个重要的参数需要了解，如下所述:

*   `n_estimators`:我们可以指定模型中要创建的树的数量。默认值为 10。
*   `max_features`:指定每次分割时随机选择作为候选的变量/特征的数量。默认为![](img/3c990136-58a3-4579-9b0b-a3551788b71c.png)。

我们创建了一个随机森林模型，使用`n_estimators`作为`100`，使用`max_features`作为`3`，如下面的代码片段所示:

```py
num_trees = 100
max_features = 3
attrition_forest = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)
attrition_forest.fit(X_train, Y_train)
```

前面代码的输出如下:

![](img/3ae7d26c-9faa-4a64-93c1-4461fe8d53eb.png)

一旦模型拟合成功，我们根据`test`预测`Y_pred`或保持数据集:

```py
Y_pred = attrition_forest.predict(X_test)
from sklearn.metrics import confusion_matrix
confusionmatrix = confusion_matrix(Y_test, Y_pred)
print(confusionmatrix)
```

混淆矩阵中的结果看起来非常好，错误分类更少，预测更准确。让我们看看评估指标是如何得出的:

![](img/a9920d00-8a43-4d1d-aba7-55849d8296a4.png)

接下来，我们检查分类报告中`Random Forest classifier`和`print`的准确性:

```py
print('Accuracy of Random Forest classifier on test set: {:.2f}'.format(attrition_forest.score(X_test, Y_test)))
from sklearn.metrics import classification_report
print(classification_report(Y_test, Y_pred))
```

前面代码的输出如下:

![](img/d4535575-8eda-4bfb-be2b-9afdfac717e1.png)

这是一个优秀的模型，所有的评估指标都接近完美的预测。这太令人难以置信了，可能是过度拟合的一个例子。然而，让我们认为随机森林是目前我们的`HR`损耗数据集上的最佳算法，然后继续另一种广泛使用的集成建模技术——boosting。

# 助推

Boosting 是一个迭代的过程，在这个过程中，基于前人的缺陷一个接一个地建立连续的模型。这有助于减少模型中的偏差，也导致方差的减少。Boosting 试图生成新的分类器，以更好地预测先前模型性能较低的值。与打包不同，训练数据的重采样取决于早期分类器的性能。Boosting 使用所有数据来训练单个分类器，但是被前一个分类器错误分类的实例被赋予了更多的重要性，以便后续的分类器增强结果。

**梯度增压机** ( **GBMs** )也称为**随机梯度增压** ( **SGB** )就是增压方法的一个例子。我们再次导入所需的包并加载`HR`损耗数据集。此外，我们执行相同的过程，将分类数据集转换为单一热编码值，并将数据集拆分为比例为 70:30 的`train`和`test`集:

```py
import numpy as np
import pandas as pd
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score
from sklearn import tree
hr_data = pd.read_csv('data/hr.csv', header=0)
hr_data.head()
hr_data = hr_data.dropna()
print(" Data Set Shape ", hr_data.shape)
print(list(hr_data.columns))
print(" Sample Data ", hr_data.head())

data_trnsf = pd.get_dummies(hr_data, columns =['salary', 'sales'])
data_trnsf.columns
X = data_trnsf.drop('left', axis=1)
X.columns

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, data_trnsf.left, test_size=0.3, random_state=42)
print(X_train)
```

有几个最佳参数对`GradientBoostedClassifier`很重要。然而，并非所有都是强制性的:

*   `n_estimators`:这类似于随机森林算法的`n_estimators`，但是树是按顺序创建的，在 boosting 方法中被认为是不同的阶段。使用这些参数，我们指定模型中的树或提升阶段的数量。默认为`100`。
*   `max_depth`:这是寻找最佳分割时要考虑的特征数量。当`max_features`小于特征数时，会导致方差减小，但会增加模型中的偏差。

*   `max_depth`:每棵树要生长的最大深度。默认值为`3`:

```py
num_trees = 100
attrition_gradientboost= GradientBoostingClassifier(n_estimators=num_trees, random_state=42)
attrition_gradientboost.fit(X_train, Y_train)
```

前面代码的输出如下:

![](img/2d0c2233-0ff5-4dbe-a3af-533380072ea4.png)

一旦模型成功拟合到数据集，我们就使用训练好的模型来预测`test`数据的`Y`值:

```py
Y_pred = attrition_gradientboost.predict(X_test)

from sklearn.metrics import confusion_matrix
confusionmatrix = confusion_matrix(Y_test, Y_pred)
print(confusionmatrix)
```

下面的混淆矩阵看起来不错，错误分类错误很少:

![](img/91d85677-f4e9-4367-ac08-f4f71130a6d6.png)

我们打印准确性和其他指标来评估分类器:

```py
print('Accuracy of Gradient Boosting Classifier classifier on test set: {:.2f}'.format(attrition_gradientboost.score(X_test, Y_test)))
from sklearn.metrics import classification_report
print(classification_report(Y_test, Y_pred))
```

前面代码的输出如下:

![](img/df936cf5-8d54-4f8d-b8cd-46c4eb05f044.png)

准确率 97%，很优秀，但不如随机森林模型。还有一种集合模型，我们将在下一节讨论。

# 堆积/混合

在该方法中，多层分类器一层堆叠在另一层上。应用第一层分类器的预测概率来训练第二层分类器，以此类推。最终的结果是通过使用诸如逻辑回归之类的基本分类器获得的。我们还可以使用不同的算法，如决策树、随机森林或 GBM，作为最终的层分类器。

scikit-learn 中没有现成的堆叠系综实现。然而，我们将在[第 4 章](4.html)、*自动算法选择*中演示使用 scikit-learn 的基础算法创建堆叠集成的自动功能。

# 比较分类器的结果

我们已经在`HR`磨损数据集上创建了大约六个分类模型。下表总结了每个模型的评估分数:

![](img/1a3d475d-c7f9-43b5-84aa-cbbe54c1215e.png)

随机森林模型似乎是所有六个模型中的赢家，准确率达到创纪录的 99%。现在，我们不需要进一步改进随机森林模型，而是检查它是否能够很好地推广到新的数据集，并且结果没有过度拟合`train`数据集。方法之一是进行交叉验证。

# 交叉验证

交叉验证是一种在未用于训练的数据集(即训练模型未知的数据样本)上评估模型准确性的方法。这确保了在生产环境中部署时模型在独立数据集上的泛化。其中一种方法是将数据集分成两组——训练集和测试集。我们在前面的例子中演示了这种方法。

另一种流行且更健壮的方法是 k 倍交叉验证方法，其中数据集被划分为大小相等的 *k* 个子样本。其中 *k* 为非零正整数。在训练阶段， *k-1* 样本用于训练模型，剩余的一个样本用于测试模型。这个过程重复 k 次，其中一个 k 个样本只使用一次来测试模型。然后以某种方式对评估结果进行平均或合并，例如多数投票以提供单一估计。

我们将在之前创建的随机森林模型上生成`5`和`10`折叠交叉验证，以评估其性能。只需在随机森林代码的末尾添加以下代码片段:

```py
crossval_5_scores = cross_val_score(attrition_forest, X_train, Y_train, cv=5)
print(crossval_5_scores)
print(np.mean(crossval_5_scores))
crossval_10_scores = cross_val_score(attrition_forest, X_train, Y_train, cv=10)
print(crossval_10_scores)
print(np.mean(crossval_10_scores))
```

`5`和`10`折叠交叉验证的准确率分别为`0.9871`和`0.9875`。这是一个很好的分数，非常接近我们实际的随机森林模型分数 0.99，如下图截图所示。这确保了模型可以很好地推广到其他独立的数据集:

![](img/474baf7d-6547-4ab3-8456-636ab5b6dde2.png)

现在我们已经对有监督机器学习的含义有了一些了解，是时候切换到无监督机器学习了。

我们在本章前面介绍了无监督学习。重申目标:

*无监督学习的目标是通过推断输入数据集中属性的结构和关系来识别模式。*

那么，我们可以用什么算法和方法来识别模式呢？有很多，比如聚类和自动编码器。我们将在下一节中讨论集群和第 7 章、*深入学习*中的自动编码器。

# 使聚集

我们将以一个问题开始这一部分。我们如何开始学习一种新的算法或机器学习方法？我们从三重 w 开始。那么，让我们从聚类方法开始。

# 什么是集群？

聚类是一种将相似的数据分组在一起的技术，一个组有一些不同于其他组的独特特征。可以使用各种方法将数据聚集在一起。其中一种是基于规则的，即根据某些预定义的条件形成组，例如根据客户的年龄或行业对客户进行分组。另一种方法是使用 ML 算法将数据聚集在一起。

# 聚类在哪里使用？

作为一个无监督的学习过程，它最常用于从数据中推导出逻辑关系和模式。集群在各个部门和业务功能中得到应用。它用于信息检索、客户细分、图像分割、对网页、新闻文章等非结构化文本进行聚类等。

# 通过哪种方法可以实现集群？

有各种机器学习方法来创建集群。聚类算法分为以下几类:

*   **层次聚类**:也称为**凝聚聚类**，试图通过距离度量将每个数据点链接到最近的邻居。这是一个递归过程，从一条记录开始，迭代地将它们配对在一起，直到所有记录都合并成一个集群。如果我们想象一下，它的结构类似于一棵倒树，可以通过一个树形图来可视化。使用这种方法的问题之一是确定聚类的过程。这是资源密集型的，但人们可以可视化树图，并选择集群的数量。
*   **基于分区的聚类**:在这种方法中，数据被分割成分区。划分基于数据点之间的距离。k-means 算法是一种常用的划分聚类方法。在这种方法中，选择合适的距离函数会影响簇的形状。欧几里德距离、曼哈顿距离和余弦距离是三种广泛用于创建 k 均值聚类的距离函数。欧氏距离对输入向量的尺度最敏感。在这种情况下，必须对输入向量的比例进行归一化或标准化，或者选择对比例不敏感的距离度量，如余弦距离。
*   **基于密度的技术**:这里的聚类是利用数据点的特定概率分布形成的。这个想法是，只要邻域内的密度超过一个定义的阈值，就继续扩展集群。高密度区域被标记为与低密度区域分离的簇，这可能是噪声。噪声是数据中的随机变化或误差，在统计上是不确定的，无法解释。
*   **基于网格的方法**:该方法首先通过划分数据集的属性来创建超矩形网格单元。然后，它会丢弃低于定义的阈值参数的低密度细胞。然后将相邻的高密度细胞融合在一起，直到达到目标函数或保持不变。得到的细胞被解释为簇。

我们将介绍层次聚类和 k-means 聚类，这是两种在行业中广泛使用的方法。

# 分级聚类

我们可以使用 scikit-learn 在 Python 中执行分层聚类。我们需要从`sklearn.cluster`导入`AgglomerativeClustering`方法来创建集群。分层聚类对距离度量有效，因此我们需要在构建模型之前将分类数据转换为合适的数字格式。我们已经使用了 one-hot 编码将分类属性转换为数字格式，并且存在各种其他方法来完成这个任务。该主题将在下一章中详细介绍:

```py
import pandas as pd
import numpy as np
from sklearn import preprocessing
from sklearn.cluster import AgglomerativeClustering
hr_data = pd.read_csv('data/hr.csv', header=0)
hr_data.head()
hr_data = hr_data.dropna()
print(hr_data.shape)
print(list(hr_data.columns))
data_trnsf = pd.get_dummies(hr_data, columns =['salary', 'sales'])
data_trnsf.columns
```

接下来，我们需要用以下参数实例化`AgglomerativeClustering`，并将数据拟合到模型中:

*   `n_clusters`:要查找的簇数。默认值为 2。
*   `affinity`:是用来计算联动的距离度量。默认为`euclidean`；`manhattan`、`cosine`、`l1`、`l2`和`precomputed`是可以使用的其他距离度量。
*   `linkage`:该参数决定了用于合并集群对的度量。不同的链接指标有:
    *   **沃德**:最小化正在合并的聚类的方差。这是默认参数值。
    *   **平均值**:采用两组每次观测距离的平均值。
    *   **完成**:使用两组所有观测值之间的最大距离。

现在，让我们使用一些描述的参数建立一个`AgglomerativeClustering`模型:

```py
n_clusters = 3
clustering = AgglomerativeClustering(n_clusters=n_clusters,
affinity='euclidean', linkage='complete')
clustering.fit(data_trnsf)
cluster_labels = clustering.fit_predict(data_trnsf)
```

一旦模型准备好了，我们就需要评估模型。评估聚类结果的最佳方法是对形成的聚类进行人工检查，并确定每个聚类代表什么以及每个聚类中的数据有什么共同的值。

结合人体检查，还可以使用轮廓分数来确定最佳模型。轮廓值在-1 和+1 的范围内:

*   +1 表示集群中的数据靠近分配的集群，远离其相邻集群
*   -1 表示数据点与其相邻的簇比与其分配的簇更接近

当一个模型的平均剪影分数为-1 时，它是一个可怕的模型，剪影分数为+1 的模型是一个理想的模型。所以，这就是为什么平均轮廓分数越高，聚类模型越好的原因:

```py
silhouette_avg = silhouette_score(data_trnsf, cluster_labels)
print("For n_clusters =", n_clusters,"The average silhouette_score is :", silhouette_avg)
```

前面代码的输出如下:

![](img/7161fc2c-775f-491b-8acc-299d1aa5fa27.png)

由于我们模型的平均轮廓得分为`0.49`，我们可以假设集群形成良好。

我们可以将这个分数与 k-means 聚类结果进行比较，并在`HR`磨损数据集上选择创建三个聚类的最佳模型。

# 分区聚类

我们需要从 scikit-learn 包中导入一个`KMeans`方法，剩下的代码仍然类似于层次聚类的代码:

```py
import pandas as pd
import numpy as np
from sklearn import preprocessing
import matplotlib.pyplot as plt 
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_samples, silhouette_score
hr_data = pd.read_csv('data/hr.csv', header=0)
hr_data.head()
hr_data = hr_data.dropna()
print(hr_data.shape)
print(list(hr_data.columns))
data_trnsf = pd.get_dummies(hr_data, columns =['salary', 'sales'])
data_trnsf.columns
```

我们需要在 k-means 函数中指定聚类数(`n_clusters`)来创建模型。它是创建 k-均值聚类的一个基本参数。它的默认值是八。接下来，将数据拟合到`KMeans`实例，并建立模型。我们需要`fit_predict`值来获得集群标签，就像对`AgglomerativeClustering`所做的那样:

```py
n_clusters = 3
kmeans = KMeans(n_clusters=n_clusters)
kmeans.fit(data_trnsf)
cluster_labels = kmeans.fit_predict(data_trnsf)
```

如果我们想查看聚类质心和标签，我们可以使用`cluster_centers_`和`means_labels_`来实现:

```py
centroid = kmeans.cluster_centers_
labels = kmeans.labels_
print (centroid)
print(labels)
silhouette_avg = silhouette_score(data_trnsf, cluster_labels)
print("For n_clusters =", n_clusters,"The average silhouette_score is :", silhouette_avg)
```

前面代码的输出如下:

![](img/d47afc9b-c6c1-4c29-82cd-723454562a9b.png)

k-means 聚类的平均值`silhouette_score`为`0.58`，大于层次聚类得到的平均剪影得分。

这意味着这三个聚类在 k-means 模型中比在`HR`磨损数据集上构建的分层模型更好地形成。

# 摘要

ML 及其自动化之旅是漫长的。本章的目的是让我们熟悉机器学习的概念；最重要的是 scikit-learn 和其他 Python 包，这样我们就可以在接下来的章节中顺利地加速我们的学习，创建一个线性回归模型和六个分类模型，并学习聚类技术并相互比较模型。

我们使用单个`HR`磨损数据集来创建所有分类器。我们观察到这些代码有许多相似之处。导入的库除了用于实例化机器学习类的库之外都是相似的。数据预处理模块在所有代码中都是冗余的。机器学习技术基于目标属性的任务和数据而改变。此外，评估方法相当于类似类型的最大似然方法。

你认为这些领域有些是多余的，需要自动化吗？是的，他们可以，但没那么容易。当我们开始考虑自动化时，模型中和模型周围的一切都需要连接在一起。每个代码段都是自己的模块。

下一章是关于数据预处理模块的。这是机器学习项目中最关键、最耗时的课题。我们将讨论创建可靠的机器学习模型所需的各种数据准备任务。