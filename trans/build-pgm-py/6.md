# 第六章。使用图形模型的精确推理

到目前为止，我们已经了解了构建图形模型的方法。在本章中，我们将使用各种推理机的完全指定的图形模型来获得问题的答案。

在我们开始推理之前，我们将了解推理问题的复杂性。在特定的环境中使用不同类型的推理是合适的，我们将学习哪种类型适用于哪里，然后用这两种方法进行实验。

# 推理的复杂性

图形模型可用于回答概率查询和地图查询。使用该模型最简单的方法是生成联合分布，并将所有变量相加，除了我们感兴趣的变量。然而，我们需要确定和指定指数爆炸发生的联合分布。

在最坏的情况下，我们需要确定 NP-hard 中的确切推断。精确这个词的意思是以一定的精度指定概率值(比如，小数点后五位数)。假设我们降低精度要求(例如，小数点后最多两位数)。现在，(近似)推理任务变得容易了吗？不幸的是没有——即使是近似推理也是 NP 难的，也就是说，获得值远比随机猜测(50%或 0.5 的概率)好，后者需要指数时间。

看起来推理是一项无望的任务，但这只是最坏的情况。在一般情况下，我们可以使用精确推理来解决某些类别的现实世界问题(例如具有少量离散随机变量的贝叶斯网络)。当然，对于更大的问题，我们不得不求助于近似推理。

由于这本书是关于 Python 中的图形模型的，让我们离题到我们可以用来运行推理的工具的选择。

## 现实世界的问题

由于推理是一个 NP 难的任务，所以推理机是用尽可能接近裸机的语言编写的；通常用 C 或 C++。由于这是一本关于 Python 中 PGM 的书，我们有几个选择:

*   使用推理算法的 Python 实现。这些完整和成熟的软件包并不常见。
*   使用有 Python 接口的推理引擎，比如斯坦([mc-stan.org](http://mc-stan.org))。这个选择很好地平衡了运行 Python 代码和快速推理实现之间的关系。
*   使用没有 Python 接口的推理引擎，这对于大多数推理引擎来说都是正确的。一份相当全面的名单可以在[http://en.wikipedia.org/wiki/Bayesian_network#Software](http://en.wikipedia.org/wiki/Bayesian_network#Software)找到。这里 Python 的使用仅限于创建一个文件，该文件以推理引擎可以使用的格式描述模型。

在关于推理的章节中，我们将坚持列表中的前两个选择。我们将在运行玩具大小的问题时使用(推理算法的)本机 Python 实现来窥视推理算法的内部，然后使用带有 Python 接口的外部推理引擎来尝试一个更真实的问题。

# 使用变量消除算法

在部分，我们将学习变量消去算法。在`VarElim_asia.ipynb` IPython 笔记本中，我们将使用亚洲网络来了解该算法的细节。

亚洲网络([http://www.bnlearn.com/bnrepository/#asia](http://www.bnlearn.com/bnrepository/#asia))是一个用于患者诊断的玩具贝叶斯网络。该网络试图根据去亚洲旅行、吸烟史和 x 光结果等因素来引出肺癌或肺结核的可能性。有关亚洲网络的更多详情，请访问[http://www.norsys.com/tutorials/netica/secA/tut_A1.htm](http://www.norsys.com/tutorials/netica/secA/tut_A1.htm)。

以下为亚洲网示意图(由[http://www.bnlearn.com/bnrepository/asia/asia.png](http://www.bnlearn.com/bnrepository/asia/asia.png)提供):

![Using the Variable Elimination algorithm](images/9004OS_06_01.jpg)

这个代码片段的目标是比较两种推理方法。第一种方法是蛮力，我们首先列出整个联合分布，然后询问我们的概率。第二种方法使用变量消除，我们将研究它如何改进蛮力方法。

假设我们希望推断患者患有支气管炎![Using the Variable Elimination algorithm](images/9004OS_06_02.jpg)的概率；从今以后，这将被写成![Using the Variable Elimination algorithm](images/9004OS_06_03.jpg)。

我们先来了解一下我们是如何利用积法则![Using the Variable Elimination algorithm](images/9004OS_06_04.jpg)得出![Using the Variable Elimination algorithm](images/9004OS_06_03.jpg)的值的，如果我们知道![Using the Variable Elimination algorithm](images/9004OS_06_05.jpg)，就可以计算![Using the Variable Elimination algorithm](images/9004OS_06_06.jpg)。此外，![Using the Variable Elimination algorithm](images/9004OS_06_07.jpg)可以用类似的方式计算，然后概率必须归一化。

我们将从存储网络定义的文件中加载网络，如下所示:

```
from libpgm.graphskeleton import GraphSkeleton
from libpgm.nodedata import NodeData
from libpgm.discretebayesiannetwork import DiscreteBayesianNetwork
from libpgm.tablecpdfactor import TableCPDFactor
import itertools
import pandas as pd
from libpgm.tablecpdfactorization import TableCPDFactorization

def loadbn(jsonpath):
    nd = NodeData()
    skel = GraphSkeleton()
    nd.load(jsonpath)
    skel.load(jsonpath)
    skel.toporder()

    bn = DiscreteBayesianNetwork(skel, nd)
    return bn

bn=loadbn("asia1.txt")
```

我们第一次尝试计算边际概率将使用所有变量的联合分布，然后将我们不需要的变量相加。为了列出联合分布，我们将所有因素相乘。根据因子乘积计算的联合分布列在以下代码中，其完整规范有 255 个条目:

```
#a method that prints the distribution as a table.
def printdist(jd,bn,normalize=False):
    x=[bn.Vdata[i]["vals"] for i in jd.scope]
    zipover=[i/sum(jd.vals) for i in jd.vals] if normalize else jd.vals
    #creates the cartesian product
    k=[a + [b] for a,b in zip([list(i) for i in itertools.product(*x[::-1])],zipover)]
    df=pd.DataFrame.from_records(k,columns=[i for i in reversed(jd.scope)]+['probability'])
    return df

#instantiate TableCPDs for all the nodes and multiply them. The result is in the factor that calls the methods.
jc=TableCPDFactor("asia",bn)    
[jc.multiplyfactor(TableCPDFactor(i,bn)) for i in bn.V if i != "asia"]
df=printdist(jc,bn,normalize=True)
print "values in joint distribution ",len(jc.vals)
#print the first few values in the table
df.head()
values in joint distribution  256
```

前面代码的输出如下:

```
Rows	 dysp	 xray	either	tub	lung	bronc	smoke	asia	probability
0	 yes	 yes	 yes	 yes	 yes	 yes	 yes	 yes	 0.000013
1	 yes	 yes	 yes	 yes	 yes	 yes	 yes	 no	 0.000262
2	 yes	 yes	 yes	 yes	 yes	 yes	 no	 yes	 0.000001
3	 yes	 yes	 yes	 yes	 yes	 yes	 no	 no	 0.000013
4	 yes	 yes	 yes	 yes	 yes	 no	 yes	 yes	 0.000007

```

上表列出了联合分布中 255 行中的前五行。

在整个讨论中，我们还会稍微增加符号。由于所有变量都是二元的，![Using the Variable Elimination algorithm](images/9004OS_06_05.jpg)表示证据变量`smoke`为真，除非另有说明，即![Using the Variable Elimination algorithm](images/9004OS_06_05.jpg)与![Using the Variable Elimination algorithm](images/9004OS_06_08.jpg)相同

## 边缘化不相关的因素

从联合分布中，我们可以通过执行以下操作获得期望的边际概率。我们只对`bronc`栏的数据感兴趣。因此，我们首先折叠其他列(以及其中的数据)，这是一个称为因子边缘化的操作。

我们将使用`Pandas`数据分析库来操作类似于表格的 CPD。将联合分布加载到`Pandas`数据框后，我们将只对我们想要的列进行分组，并将`probability`列中的值相加，如下代码所示:

```
t2=df.groupby(['bronc','smoke'],as_index=False)
t3=t2['probability'].sum()
t3
```

前面代码的输出如下:

```
Rows	bronc	smoke	probability
0	 no	 no	 0.35
1	 no	 yes	 0.20
2	 yes	 no	 0.15
3	 yes	 yes	 0.30

```

这个操作是怎么发生的？让我们以上表中的第 0 行(bronc == no，smoke == no)为例，看看该值是如何获得的。`0.35`的值是将其他列的所有值相加得到的。我们将选择表格中所有`bronc`和`smoke`值都为`no`的行，如下代码所示:

```
df.loc[(df["smoke"] == 'no') & (df["bronc"] == 'no'), :].head()
```

前面代码的输出如下:

```
 dysp	xray	either	tub	lung	bronc	smoke	asia	probability
6	 yes	 yes	 yes	 yes	 yes	 no	 no	 yes	 0.000001
7	 yes	 yes	 yes	 yes	 yes	 no	 no	 no	 0.000024
14	 yes	 yes	 yes	 yes	 no	 no	 no	 yes	 0.000119
15	 yes	 yes	 yes	 yes	 no	 no	 no	 no	 0.002353
22	 yes	 yes	 yes	 no	 yes	 no	 no	 yes	 0.000023

```

我们将从上表的中选择所有行，只选择`probability`列。然后，我们将总结这些值，如下面的代码所示:

```
df.loc[(df["bronc"] == 'no') & (df["smoke"] == 'no'), "probability"].sum()
```

前面代码的输出如下:

```
0.34999999999999998
```

我们可以看到因子边缘化操作如何给我们赋值的概率`0.35`(*bronc = = no，smoke ==no* )。

## 过滤证据的因子约简

让我们稍微离题一下来观察查询观察到变量如![Factor reduction to filter evidence](images/9004OS_06_09.jpg)的情况。然后，我们只需要前一个表的第一行和第三行(带有列`bronc`、`smoke`和`probability`)。这些行对应![Factor reduction to filter evidence](images/9004OS_06_10.jpg)

该操作称为**因子减少**，因为其他行没有`smoke`的具体赋值(即值不是`yes`)，所以被删除，如下代码所示:

```
t4=t3.loc[ (t3["smoke"] == 'yes'), :]
t4
```

前面代码的输出如下:

```
Rows	bronc	smoke	probability
1	 no	 yes	 0.2
3	 yes	 yes	 0.3

```

这几乎是我们需要的概率集，只是概率加起来不等于 1。因此，我们将通过将每个概率除以所有概率的总和来进行归一化，如以下代码所示:

```
psum=t4['probability'].sum()
t4['probability']=t4['probability']/psum
t4
```

前面代码的输出如下:

```
 bronc	smoke	probability
1	 no	 yes	 0.4
3	 yes	 yes	 0.6

```

让我们回到最初的查询——![Factor reduction to filter evidence](images/9004OS_06_12.jpg)。我们已经看到上表表示了`smoke`存在时`bronc`的条件概率分布。现在，我们简单地将`smoke`的值相加，得到![Factor reduction to filter evidence](images/9004OS_06_12.jpg)的值，如下代码所示:

```
t5=t3.groupby(['bronc'],as_index=False)
t5['probability'].sum()
```

前面代码的输出如下:

```
 bronc	probability
0	 no	 0.55
1	 yes	 0.45

```

上表给出了推理查询的期望结果![Factor reduction to filter evidence](images/9004OS_06_12.jpg)。

### 蛮力方法的缺点

我们之前讨论的是得出概率查询值的蛮力方法。这种方法效率低下，原因如下:

*   它不使用模型中存在的任何条件独立性。假设我们想知道![Shortcomings of the brute-force approach](images/9004OS_06_13.jpg)的概率，以及节点 X 和 Y 是否有条件独立于 A 和 B，那么计算联合分布就没有意义了，联合分布包括 X 和 Y，然后求和出值。
*   我们已经知道，获取联合分布中的概率值并不是一件容易的事情，存储和操作大的联合分布也不是一件容易的事情。对于报警示例中的仅 8 个二进制变量，联合分布有 255 行(或 2 个 <sup>8 个</sup>，对于更大的网络，学习、存储和操作联合分布变得不可能。即使有了联合分布，将其他值相加也有一个时间复杂度![Shortcomings of the brute-force approach](images/9004OS_06_14.jpg)。

### 使用变量消去法

我们现在来看看变量消去算法，这是一种计算概率的有效方法。效率源于避免重复操作以及避免计算条件独立的概率。即使存在任何证据/观察变量，该算法也可用于计算边际概率。

我们将使用**因子**，类似于我们之前看到的表格。**因子积**(是两组元素之间的笛卡儿积)**因子边缘化****因子约简**的运算是的演算，通过我们可以操纵**因子**。

每个因素都有一个范围。如果我们把一个因子看作一个表，那么作用域中的变量就是表的列名。`asia.txt`定义的八个 CPD 为初始因子。例如![Using the Variable Elimination approach](images/9004OS_06_15.jpg)因子范围内有变量`tub`和`asia`，对应![Using the Variable Elimination approach](images/9004OS_06_16.jpg) CPD。类似地，![Using the Variable Elimination approach](images/9004OS_06_18.jpg)因子对应于![Using the Variable Elimination approach](images/9004OS_06_19.jpg) CPD。有多个父节点的 CPD，如![Using the Variable Elimination approach](images/9004OS_06_20.jpg)，转换为![Using the Variable Elimination approach](images/9004OS_06_21.jpg)因子，该因子的范围内有所有的父节点和子节点。

我们在下面的列表中列出了`asia`网络中的因素，以及它们所源自的相应的 CPD。包含所有变量的联合分布将是列表中各因素的乘积。

*   ![Using the Variable Elimination approach](images/9004OS_06_22.jpg)
*   ![Using the Variable Elimination approach](images/9004OS_06_23.jpg)
*   ![Using the Variable Elimination approach](images/9004OS_06_24.jpg)
*   ![Using the Variable Elimination approach](images/9004OS_06_25.jpg)
*   ![Using the Variable Elimination approach](images/9004OS_06_26.jpg)
*   ![Using the Variable Elimination approach](images/9004OS_06_27.jpg)
*   ![Using the Variable Elimination approach](images/9004OS_06_28.jpg)
*   ![Using the Variable Elimination approach](images/9004OS_06_29.jpg)

我们将使用变量消去算法来推断![Using the Variable Elimination approach](images/9004OS_06_30.jpg)的条件概率，它对应于一个在其范围内具有相同变量的因子:![Using the Variable Elimination approach](images/9004OS_06_31.jpg)

在扫描前面的因素列表时，我们会看到没有一个因素在其范围内有![Using the Variable Elimination approach](images/9004OS_06_32.jpg)。因此，为了创建![Using the Variable Elimination approach](images/9004OS_06_33.jpg)，我们必须使用因子乘积运算，其中结果因子范围内的变量应该出现在被相乘的因子之一中。

以为例，这组因素![Using the Variable Elimination approach](images/9004OS_06_34.jpg)在其组合范围内有三个变量:![Using the Variable Elimination approach](images/9004OS_06_35.jpg)。因此，因子的乘积![Using the Variable Elimination approach](images/9004OS_06_36.jpg)将给出一个![Using the Variable Elimination approach](images/9004OS_06_37.jpg)因子，其中概率值是通过将因子中与赋值匹配的行相乘得到的。

顾名思义，在变量消去法中，我们一次消去一个变量。每个步骤包括以下操作:

*   倍增因子
*   边缘化一个变量(它在所有倍增因素的范围内)
*   产生新的因素

我们将通过以下方式得出![Using the Variable Elimination approach](images/9004OS_06_38.jpg)因子。下面表的每一行都详细说明了算法的一个步骤，即**因子积**、**淘汰变量**、**新因子创建**，追求![Using the Variable Elimination approach](images/9004OS_06_38.jpg)。

<colgroup><col style="text-align: left"> <col style="text-align: left"> <col style="text-align: left"> <col style="text-align: left"></colgroup> 
| 

第一步

 | 

要素产品

 | 

消除变量

 | 

新因子已创建

 |
| --- | --- | --- | --- |
| one | ![Using the Variable Elimination approach](images/9004OS_06_39.jpg) | `asia` | ![Using the Variable Elimination approach](images/9004OS_06_40.jpg) |
| Two | ![Using the Variable Elimination approach](images/9004OS_06_41.jpg) | `smoke` | ![Using the Variable Elimination approach](images/9004OS_06_42.jpg) |
| three | ![Using the Variable Elimination approach](images/9004OS_06_43.jpg) | `tub` | ![Using the Variable Elimination approach](images/9004OS_06_44.jpg) |
| four | ![Using the Variable Elimination approach](images/9004OS_06_45.jpg) | `lung` | ![Using the Variable Elimination approach](images/9004OS_06_46.jpg) |
| five | ![Using the Variable Elimination approach](images/9004OS_06_47.jpg) | `either` | ![Using the Variable Elimination approach](images/9004OS_06_48.jpg) |
| six | ![Using the Variable Elimination approach](images/9004OS_06_49.jpg) | `dysp` | ![Using the Variable Elimination approach](images/9004OS_06_50.jpg) |

在图中的八个节点中，每一步都有六个被淘汰，这就给我们留下了我们想要的因子![Using the Variable Elimination approach](images/9004OS_06_50.jpg)。

我们将使用`libpgm`库来完成算法的每一步。`libpgm`库为我们提供了`multiplyfactor`和`sumout`方法来在每一步创建一个新的因子。我们已经介绍了以下代码的第一步:

```
asia=TableCPDFactor("asia",bn)
phi_1=TableCPDFactor("tub",bn)

phi_1.multiplyfactor(asia)
printdist(phi_1,bn)
```

前面代码的输出如下:

```
Rows	asia	tub	probability
0	 yes	 yes	 0.0005
1	 yes	 no	 0.0095
2	 no	 yes	 0.0099
3	 no	 no	 0.9801

```

由于起始因子只是 CPD，所以我们在创建`TableCPDFactor("tub",bn)`时，是同时涉及`tub`和`asia`的因子，这是从![Using the Variable Elimination approach](images/9004OS_06_51.jpg)的 CPD 计算出来的。我们现在将使用以下代码消除`asia`:

```
phi_1.sumout("asia")
printdist(phi_1,bn)
```

前面代码的输出如下:

```
 tub	probability
0	 yes	 0.0104
1	 no	 0.9896

```

第二步，我们将因子![Using the Variable Elimination approach](images/9004OS_06_41.jpg)相乘，消去`smoke`产生![Using the Variable Elimination approach](images/9004OS_06_42.jpg)，如下代码所示:

```
phi_2=TableCPDFactor("smoke",bn)
[phi_2.multiplyfactor(TableCPDFactor(i,bn)) for i in ["lung","bronc"]]
phi_2.sumout("smoke")
printdist(phi_2,bn)
```

前面代码的输出如下:

```
 bronc	lung	probability
0	 yes	 yes	 0.0315
1	 yes	 no	 0.4185
2	 no	 yes	 0.0235
3	 no	 no	 0.5265

```

在第三步中，乘以因子![Using the Variable Elimination approach](images/9004OS_06_52.jpg)并消除`tub`产生![Using the Variable Elimination approach](images/9004OS_06_44.jpg)，如下代码所示:

```
phi_3=TableCPDFactor("either",bn)
phi_3.multiplyfactor(phi_1)
phi_3.sumout("tub")
printdist(phi_3,bn)
```

前面代码的输出如下:

```
Rows	lung	either	probability
0	 yes	 yes	 1.0000
1	 yes	 no	 0.0000
2	 no	 yes	 0.0104
3	 no	 no	 0.9896

```

在第四步中，乘以因子![Using the Variable Elimination approach](images/9004OS_06_45.jpg)以消除`lung`并产生![Using the Variable Elimination approach](images/9004OS_06_46.jpg)因子，如下代码所示![Using the Variable Elimination approach](images/9004OS_06_46.jpg)我们没有打印 CPD，因为我们已经看到了它的样子。

```
phi_4=phi_3
phi_4.multiplyfactor(phi_2)
phi_4.sumout("lung")
print "variables in scope ",phi_4.scope
```

前面代码的输出如下:

```
variables in scope  ['either', 'bronc']

```

在第五步，将因子![Using the Variable Elimination approach](images/9004OS_06_47.jpg)相乘至消除`either`，产生![Using the Variable Elimination approach](images/9004OS_06_48.jpg)因子，如下代码所示:

```
phi_5=TableCPDFactor("xray",bn)
phi_5.multiplyfactor(phi_4)
phi_5.multiplyfactor(TableCPDFactor("dysp",bn))
phi_5.sumout("either")
print "variables in scope ",phi_5.scope
```

前面代码的输出如下:

```
variables in scope  ['xray', 'bronc', 'dysp']

```

在最后一步中，从![Using the Variable Elimination approach](images/9004OS_06_48.jpg)因子中，我们将消除`dysp`以产生![Using the Variable Elimination approach](images/9004OS_06_50.jpg)因子，如下代码所示:

```
phi_6=phi_5
phi_6.sumout("dysp")
printdist(phi_6,bn)
```

前面代码的输出如下:

```
Rows	bronc	xray	probability
0	 yes	 yes	 0.055843
1	 yes	 no	 0.394157
2	 no	 yes	 0.054447
3	 no	 no	 0.495553

```

前面的因子在其范围内有`xray`和`bronc`变量，这是我们需要的，由于我们只需要`xray=yes`的一个具体赋值，我们可以通过给定的证据来减少因子，如下代码所示:

```
phi_6.reducefactor("xray",'yes')
printdist(phi_6,bn)
```

前面代码的输出如下:

```
Rows	bronc	probability
0	 yes	 0.055843
1	 no	 0.054447

```

由于这是不是一个有效的概率分布，我们必须通过除以和来归一化概率，如下代码所示:

```
summ = sum(phi_6.vals)
phi_6.vals=[i/float(summ) for i in phi_6.vals]
printdist(phi_6,bn)
```

前面代码的输出如下:

```
 bronc	probability
0	 yes	 0.506326
1	 no	 0.493674

```

前面的代码片段详细描述了获得所需 CPD 的步骤。

使用`libpgm`库时，所有算法步骤都包含在`condprobve`方法中；因此，我们只需加载网络并使用该方法，如以下代码所示:

```
bn = loadbn("asia1.txt")
evidence = {"xray":'yes'}
query = {"bronc":'yes'}
fn = TableCPDFactorization(bn)
result = fn.condprobve(query, evidence)
printdist(result,bn)
```

前面代码的输出如下(注意，我们得到的值与逐步过程中得到的值相同):

```
Rows	bronc	probability
0	 yes	 0.506326
1	 no	 0.493674

```

变量消除算法可以总结如下:

*   每个节点上的 CPD 的起始因素
*   从因素中消除非查询变量 *Z*
*   将其余因素相乘
*   对所有非查询变量重复相同的步骤，直到只剩下查询变量(和证据/观察变量，如果有的话)

变量消除算法适用于贝叶斯网络和马尔可夫网络。一旦 *Z* 中的非查询变量集从所有因素的范围中消除，则算法完成。虽然 *Z* 中的变量可以以任何顺序消除(它产生相同的最终 CPD)，但在下一节中，我们将了解优化的消除顺序可以帮助算法快速终止。

## 变量消去的复杂性

在前一节的开始，我们声称对于推理查询，使用变量消除算法是对查询联合分布的改进。为了更好地理解为什么变量推理是一种改进，我们需要了解算法的算法复杂度。

我们将从 *m* 因子开始，在每个消除步骤中，我们将生成一个因子(通过消除一个非查询变量)。如果我们有 *n* 个变量，我们最多有 *n* 轮淘汰。生成的因子总数，即 *m** ，将少于 *m + n* (我们从因子开始，除了淘汰的 *n* 变量)。

让 *N* 代表最大因子(其范围内变量数最多的因子)的大小。

算法中的每一步都包括推导出一个 Factor 乘积，然后求和出一个变量，这叫做和积运算。

因此，复杂度与和积运算的次数成正比。乘积运算小于*N×m **，和运算为*N×N*。因此，就 *N* 和 *m** 而言，复杂度是线性的，最大因子的大小和生成因子的总数也是线性的。

虽然出现了线性项，但事实上，计算最大因子 *N* 需要指数时间。如果一个因子有四个变量，并且都是二进制值，那么它的复杂性就是![Complexity of Variable Elimination](images/9004OS_06_55.jpg)。一般情况下，![Complexity of Variable Elimination](images/9004OS_06_56.jpg)是计算一个因子的计算成本，如果 *v* 是一个变量在其范围内的最大值个数(称为其基数) *k* 是变量个数。

### 为什么消去顺序很重要？

尽管变量消除算法没有指定消除变量(非查询、非证据)的顺序，但消除顺序在复杂性中起着一定作用。我们来看看下图中的马尔可夫网络:

![Why does elimination ordering matter?](images/9004OS_06_57.jpg)

在前面的网络中，假设我们选择求和或消除变量 **A** 。我们首先需要有一个因子的乘积![Why does elimination ordering matter?](images/9004OS_06_58.jpg)，然后将变量求和。创建的因子有一个范围![Why does elimination ordering matter?](images/9004OS_06_58.jpg)，在 *n* 中是指数的。

相反，如果我们选择先将变量![Why does elimination ordering matter?](images/9004OS_06_59.jpg)相加，因子乘积会产生一个范围为![Why does elimination ordering matter?](images/9004OS_06_60.jpg)的因子，它只有三个变量。我们在前面的部分已经了解到复杂性包含术语 *N* (最大因子的大小)。假设所有变量都是二进制的，消去顺序的不同导致第一种情况下的![Why does elimination ordering matter?](images/9004OS_06_61.jpg)和第二种情况下的![Why does elimination ordering matter?](images/9004OS_06_62.jpg)的复杂性。

由于变量消除算法的复杂度很大程度上取决于生成的最大因子的大小(其范围是指数级的)，因此由消除排序来生成小的中间因子以改善变量消除算法的运行时间。本示例摘自 Coursera PGM 课程，可在[https://www.coursera.org/course/pgm.](https://www.coursera.org/course/pgm.)访问

## 图形透视

当我们忙于执行因子操作时，图形结构也随着每次因子相乘和边缘化而改变。我们知道，因素和图形只是同一信息的不同表示；那么，新因素的消除和创造是如何影响图形的呢？

由于有向图和无向图在变量消除算法中的工作方式相同，我们可以通过假设图是无向图(即使对于贝叶斯网络)来继续分析。

我们将从上一节看一个亚洲网络中图形变化的例子。当我们通过乘法来消除一个变量，也就是作用域中节点的父节点时，结果因子在一个叫做道德化的过程中增加了子节点之间的联系。

例如，在运行亚洲网络的变量消除算法的第二步中，我们将乘以因子![Graph perspective](images/9004OS_06_41.jpg)并消除`smoke`以产生一个新的因子:![Graph perspective](images/9004OS_06_42.jpg)。这个新因子表示两个节点之间增加了一个新的链接，如下图所示(左侧和右侧表示第二步完成前后的变量):

![Graph perspective](images/9004OS_06_63.jpg)

为什么要创建一个新的因子需要我们连接之前没有连接的节点？因为一个因子编码一些独立，同样的独立也必须存在于图中。因此，随着因子消除和(新的)因子创建的过程在变量消除算法中继续，我们向图中添加新的边来编码相同的独立性。由于道德化而产生的马尔可夫网络被称为**诱导马尔可夫网络**。

对于在变量消除算法中生成的每个因子，因子范围内的变量通过称为填充边的边连接(即，如果不存在边，则添加边)。对应于每个因子的全连通子图是该因子中变量分布的最小 I 图。你可以回想一下，如果满足以下条件，图 G 是分布 P 的最小 I 图:

*   g 是 P 的一个 I 图
*   如果![Graph perspective](images/9004OS_06_64.jpg)和![Graph perspective](images/9004OS_06_65.jpg)不是 P 的 I 图

换句话说，一个最小的 I 图是一组独立的图，从 G 中移除任何边都会导致它不再是 I 图。

添加的边取决于变量消除的顺序(这也决定了创建的因素)。

### 从图结构中学习诱导宽度

在我们继续讨论诱导宽度之前，让我们离题提醒自己一些用来描述图形结构的术语。团是一个极大的、完全连通的子图。让我们看看下面这个有四个节点的马尔可夫网络:

![Learning the induced width from the graph structure](images/9004OS_06_66.jpg)

节点 **B** 、 **C** 和 **D** 由于都是相互连接的，所以形成了一个小团体。团是最大连接的，因为它不能添加更多的节点。将 **A** 添加到小团体将使完全连接的属性失效。

#### 诱导宽度为什么重要？

诱导宽度是最大团的节点数减 1。最小诱导宽度是在所有 VE 排序上获得的最小诱导宽度，这将是最佳性能的下限。

事实证明，在价值工程算法运行过程中产生的每个新因素都是诱导马尔可夫网络中的一个小团体。因此，诱导图的小团体给了我们一个 ve 算法运行时间的快速近似。即使我们确实找到了最佳的 VE 排序(这本身就是一个 NP 难问题)，推理仍然需要指数时间，即使有最佳排序。因此，如果我们使用最优排序，并发现诱导图中的团在其范围内有许多(这是基于您的硬件的相对数量)变量，那么可能是时候抛弃精确推理方法，转而使用近似方法了。

#### 寻找价值工程订单

贪婪算法是找到最佳 ve 排序的相当有效的机制。可以使用几个成本函数，例如首先选择最小的因子(邻居数量最少的节点)。

# 树算法

我们现在来看另一类基于消息传递的精确推理算法。

**消息传递** 是一种通用机制，消息传递算法存在多种变体。我们将看一小段团树消息传递算法(有时也称为连接树算法)。消息传递算法的其他版本也被用于近似推理。

我们通过澄清使用的一些术语开始讨论。

聚类图是一种网络排列，其中变量组被放置在聚类中。它类似于一个因子，其中每个聚类在其范围内有一组变量。

消息传递算法就是在集群之间传递消息。举个例子，想象一下聚会上的流言蜚语，雪莉和克莱尔正在谈话。如果 Shelly 认识 B、C 和 D，并且她正在和认识 D、E 和 F 的 Clair 聊天(注意，他们唯一共同认识的人是 D)，他们可以分享关于他们共同朋友 D 的信息(或传递消息)。

在消息传递算法中，两个集群通过一个**分离集** ( **分离集**)连接，该分离集包含两个集群共有的变量。使用前面的例子，两个集群![The tree algorithm](images/9004OS_06_67.jpg)和![The tree algorithm](images/9004OS_06_68.jpg)通过分离集![The tree algorithm](images/9004OS_06_69.jpg)连接，分离集包含两个集群共有的唯一变量。

在下一节中，我们将了解连接树算法的实现细节。我们将首先了解算法的四个阶段，然后使用代码片段从实现的角度了解它。

## 连接树算法的四个阶段

在本节中，我们将讨论连接树算法的四个阶段。

在第一个阶段，贝叶斯网络被转换成称为连接树的二级结构(文献中这种结构的替代名称是连接树、聚类树或团树)。从贝叶斯网络到连接树的转换按照以下步骤进行:

*   我们将通过把所有有向边变为无向边来构造一个道德图。进入所述节点的具有 V 型结构的所有节点的父节点都与一条边相连。我们已经看到了这个过程的一个例子(在 ve 算法中)叫做道德化，这是一个可能的参考来连接(显然未婚)有一个孩子(节点)的父母。
*   然后，我们将有选择地为道德图添加边，以创建一个三角图。三角图是一个无向图，其中节点之间的最大循环长度是 3。
*   从三角化的图中，我们将识别节点的子集(称为团)。
*   从团作为簇开始，我们将排列簇以形成一个称为连接树的无向树，它满足运行交集属性。该属性声明，如果一个节点出现在两个群中，它也应该出现在连接这两个群的路径上的所有节点中。

在第二阶段，初始化每个簇的电势。电位类似于 CPD 或表格。它们有一个值列表，对应于其范围内变量的每个赋值。团簇和分离集都包含一组势。势这一术语与概率相对使用，因为在马尔可夫网络中，与概率不同，势的值不必加起来等于 1。

这个阶段包括相邻集群之间的消息传递或信任传播。每条消息都包含一个信念，即集群拥有一个特定的变量。

每个消息都可以异步传递，但是它必须等待来自其他集群的信息，然后整理这些信息并将其传递给下一个集群。考虑一个树形结构的集群图会很有用，其中消息传递分为两个阶段:向上传递阶段和向下传递阶段。只有在节点从叶节点接收到消息后，它才会将消息发送给其父节点(在“向上传递”中)，只有在节点从其父节点接收到消息后，它才会将消息发送给其子节点(在“向下传递”中)。

当每个集群分离集具有一致的信念时，消息传递阶段就完成了。回想一下，连接到 sepset 的集群有公共变量。例如，聚类 C 和 sepset S 在其范围内有![The four stages of the junction tree algorithm](images/9004OS_06_70.jpg)和![The four stages of the junction tree algorithm](images/9004OS_06_71.jpg)变量。然后，从聚类或分离集获得的对![The four stages of the junction tree algorithm](images/9004OS_06_72.jpg)的势具有相同的值，这就是为什么说聚类图具有一致的信念或集团被校准。

一旦整个聚类图具有一致的信念，第四个阶段是边缘化，在这里我们可以查询图中任何变量的边际分布。

我们现在将继续研究连接树算法的实现。

## 使用连接树算法进行推理

在`JunctionTreeAlgorithm.ipynb` IPython 笔记本中，我们将使用**贝叶斯信念网络** ( **BBN** )库，使用连接树算法运行精确推理。在 Github([https://github.com/eBay/bayesian-belief-networks](https://github.com/eBay/bayesian-belief-networks))上有库，在 Github 页面上提到了安装库的文档。

BBN 具有加载存储在**贝叶斯交换格式** ( **bif** 中的网络的功能，该格式由贝叶斯社区开发，以促进不同推理工具之间更容易的数据共享。

我们将再次使用我们在本章前面看到的`asia`网络。

强制导入后，我们用`bif_parser`模块解析`.bif`格式文件，返回一个贝叶斯网络对象，如下代码所示:

```
import bif_parser
import prettytable
import pydot
from IPython.core.display import Image 
from bayesian.bbn import *

name = 'asia'

module_name = bif_parser.parse(name)
module = __import__(module_name)
bg = module.create_bbn()
```

我们可以使用 BBN 提供的`graphviz`功能查看贝叶斯网络(`graphviz`是一个图形可视化工具)，如下面的代码所示:

```
def show_graphgiz_image(graphviz_data):
    graph = pydot.graph_from_dot_data(graphviz_data)
    graph.write_png('temp.png')
    return 'temp.png'

sf=bg.get_graphviz_source()
Image(filename=show_graphgiz_image(sf))
```

![Using the junction tree algorithm for inference](images/9004OS_06_73.jpg)

上图为我们展示了`.bif`文件中编码的网络结构。就是我们在本章前面看到的同一个`asia`网络。

### 阶段 1.1–道德化

在下面的片段中，我们将查看道德化阶段。请注意，V 型结构，例如![Stage 1.1 – moralization](images/9004OS_06_74.jpg)，让他们的父母说教或加入了一个新的环节。

```
gu=make_undirected_copy(bg)
m1=make_moralized_copy(gu,bg)
s2=m1.get_graphviz_source()
Image(filename=show_graphgiz_image(s2)) 
```

![Stage 1.1 – moralization](images/9004OS_06_75.jpg)

### 阶段 1.2–三角测量

在三角化阶段，无向图是三角化的，如果有一个周期长度大于 4，则是节点间边的相加。注意，在上图中，![Stage 1.2 – triangulation](images/9004OS_06_76.jpg)形成了一个有四个节点的循环。在下面的片段中，在![Stage 1.2 – triangulation](images/9004OS_06_77.jpg)和![Stage 1.2 – triangulation](images/9004OS_06_78.jpg)之间添加了一个链接来对其进行三角测量，这将循环的最大长度减少到 3:

```
cliques, elimination_ordering = triangulate(m1, priority_func)
s2=m1.get_graphviz_source()
Image(filename=show_graphgiz_image(s2))
```

前面代码的输出如下:

![Stage 1.2 – triangulation](images/9004OS_06_79.jpg)

### 阶段 1.3–构建连接树

接下来，我们将创建团和分离集，这完全在`build_join_tree`方法中完成。该方法根据前面的图创建小团体，并创建每对小团体之间的交叉点 sepsets。

请注意，生成的图中节点的命名约定并不十分美观。`Clique_EITHERLUNGTUB`团有变数`either`、`lung`、`tub`都砸在一起了。

```
jt=bg.build_join_tree()
sf=jt.get_graphviz_source()

Image(filename=show_graphgiz_image(sf)) 
```

![Stage 1.3 – building the join tree](images/9004OS_06_80.jpg)

我们可以从变量名中看出，分离集包含了它们所连接的小集团的变量的交点。例如，派系![Stage 1.3 – building the join tree](images/9004OS_06_81.jpg)和![Stage 1.3 – building the join tree](images/9004OS_06_82.jpg)中的变量就是![Stage 1.3 – building the join tree](images/9004OS_06_83.jpg)分离集中的变量。

### 阶段 2–初始化电位

下一步是创建初始集群(通常是小集团)和初始化电势，如下代码所示:

```
assignments = jt.assign_clusters(bg)
jt.initialize_potentials(assignments,bg)
```

### 第 3 阶段——信息传递

在建立了运行消息传递所需的结构后，我们进入下一阶段。在这个阶段，每对拉帮结派之间会发送两条消息:一条是正向传递，另一条是反向传递。

消息传递的细节包含在连接树对象中的`propagate()`方法中。下图显示了网络中的消息序列:

```
Image(filename="../book/chapter6/images/9004OS_06_05.png")

```

![Stage 3 – message passing](images/9004OS_06_84.jpg)

那么，当一条消息被传递时，到底会发生什么呢？一个消息传递有三个参与者:源集群、中间的分离集和目标集群。每一个都有一组势(非常类似于 CPD 以及相关的概率，除了它不需要是有效的概率分布)。

例如，从![Stage 3 – message passing](images/9004OS_06_85.jpg)到![Stage 3 – message passing](images/9004OS_06_86.jpg)的消息将传递![Stage 3 – message passing](images/9004OS_06_87.jpg)分离集，其中变量在每个集群或分离集的范围内。

消息将修改![Stage 3 – message passing](images/9004OS_06_88.jpg)和![Stage 3 – message passing](images/9004OS_06_89.jpg)的电位，即分离电位和目的簇电位。

对分离集 R 的第一个赋值是源中的电位，分离集中没有的变量被边缘化，如下式所示:

![Stage 3 – message passing](images/9004OS_06_90.jpg)

对集群 Y 的第二次分配如下:

![Stage 3 – message passing](images/9004OS_06_91.jpg)

让我们使用`propagate`()方法运行消息传递位，如下面的代码所示。这将运行所有变量之间的消息传递，并在信念收敛时停止。

```
jt.propagate()
```

一旦所有的信息传递完成，我们就剩下一棵树，它的所有集群都有一致的信念。

在查询特定变量(例如`bronc`)时，我们只需要找到一个范围内有`bronc`的聚类(或一个分离集)，并边缘化其他变量。这里打印的集群范围内有`bronc`、`dysp`和`either`变量。我们打印了所有与这个集群相关的潜力。我们可以观察到每一行都列出了对`bronc`、`dysp`和`either`的具体分配。

```
bronc_clust=[i for i in jt.clique_nodes for v in i.variable_names if v =='bronc']
bronc_clust[0].potential_tt
```

前面代码的输出如下:

```
{(('bronc', 'no'), ('dysp', 'no'), ('either', 'no')): 0.4689219599,
 (('bronc', 'no'), ('dysp', 'no'), ('either', 'yes')): 0.008692680,
 (('bronc', 'no'), ('dysp', 'yes'), ('either', 'no')): 0.05210244,
 (('bronc', 'no'), ('dysp', 'yes'), ('either', 'yes')): 0.020282,
 (('bronc', 'yes'), ('dysp', 'no'), ('either', 'no')): 0.08282951999,
 (('bronc', 'yes'), ('dysp', 'no'), ('either', 'yes')): 0.003585240,
 (('bronc', 'yes'), ('dysp', 'yes'), ('either', 'no')): 0.3313180799,
 (('bronc', 'yes'), ('dysp', 'yes'), ('either', 'yes')): 0.032267160}

```

让我们通过使用以下代码边缘化集群中的`dysp`和`either`变量来尝试找到`bronc`的边际:

```
pot=bronc_clust[0].potential_tt

#a function to return the sum for a specific assignment, such as 'bronc,yes'
sum_assignments=lambda imap,tup:sum([v for k,v in imap.iteritems() for i in k if i == tup])

#get the sum for bronc=yes and bronc=no
yes,no=[sum_assignments(pot,('bronc',i)) for i in ['yes','no']]

print 'bronc: yes ', yes/float(yes+no)," no ", no/float(yes+no)

bronc: yes  0.45  no  0.55
```

既然我们声称连接树是一致的，那么包含`bronc`的其他聚类会返回相同的边际值吗？让我们使用第二个范围为`bronc`的集群，并忽略它的潜力，如下面的代码所示:

```
pot2=bronc_clust[1].potential_tt
yes,no=[sum_assignments(pot2,('bronc',i)) for i in ['yes','no']]

print 'bronc: yes ', yes/float(yes+no)," no ", no/float(yes+no)
bronc: yes  0.45  no  0.55
```

我们可以看到，我们可以从两个不同的聚类中获得相同的`bronc`的边际值，这表明所有变量的信念在所有聚类和分离集中是一致的。

### 注

关于连接树算法的详尽介绍，请参考黄和达维切在的程序指南。

在总结中，本章中给出的两种算法在原理上是相似的，它们使用相同的因子乘积和因子边缘化运算。团树中的小团体类似于变量消除中使用的因子。

连接树算法有一些优点，例如能够在校准树的单次计算中回答几个边缘查询。由于消息传递是异步的，所以算法可以并行化，并且可以更快地返回结果。

该算法的缺点是在空间方面更昂贵。在变量消去法中，中间因子不被存储，但是它们被存储在连接树中。

# 总结

我们首先探讨了推理问题，在这里我们研究了推理的类型。然后，我们了解到推理是 NP 难的，并了解到，对于大型网络，精确的推理是不可行的。

然后，我们探索了一些精确的推理算法，例如变量消除和消息传递，以及解释其基本原理的代码示例。我们研究了变量消除的复杂性和降低其复杂性的方法。

在下一章中，我们将研究运行近似推理算法的方法。