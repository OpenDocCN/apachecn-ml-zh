# 四、层次聚类实战

在本章中，我们将讨论层次聚类的概念，这是一种强大而广泛的技术，用于生成完整的聚类配置层次结构，从相当于数据集的单个聚类(除法方法)或等于样本数的多个聚类(凝聚方法)开始。当需要立刻分析整个分组过程以便理解例如如何将较小的簇合并成较大的簇时，这种方法特别有用。

特别是，我们将讨论以下主题:

*   分层聚类策略(分裂和聚集)
*   距离度量和链接方法
*   树木图及其解释
*   聚集聚类
*   作为绩效衡量标准的共病相关性
*   连通性限制

# 技术要求

本章中的代码要求如下:

*   Python 3.5+(强烈推荐 Anaconda 发行版([https://www.anaconda.com/distribution/](https://www.anaconda.com/distribution/))
*   库:
    *   SciPy 0.19+
    *   NumPy 1.10+
    *   学习 0.20+
    *   熊猫 0.22+
    *   Matplotlib 2.0+
    *   seaborn 0.9+

数据集可以从 UCI 机器学习存储库中获得。CSV 文件可以从[https://archive . ics . UCI . edu/ml/datasets/water+processing+plant](https://archive.ics.uci.edu/ml/datasets/water+treatment+plant)下载，除了添加列名外，不需要任何预处理，这将在加载阶段发生。

示例可在 GitHub 资源库上获得:[https://GitHub . com/packktpublishing/HandsOn-Unsupervised-Learning-with-Python/chapter 04](https://github.com/PacktPublishing/HandsOn-Unsupervised-Learning-with-Python/tree/master/Chapter04)。

# 集群层次结构

在前面的章节中，我们分析了聚类算法，其中输出是基于预定义数量的聚类或参数集和精确基础几何的结果的单个分割。另一方面，**分层聚类**生成一系列聚类配置，这些配置可以排列在树的结构中。特别是，让我们假设我们有一个数据集， *X* ，包含 *n* 个样本:

![](img/04851041-5aa5-40f4-871e-41cee0b1eb59.png)

一种**凝聚**方法开始于将每个样本分配给一个聚类，*C<sub class="calibre20">I</sub>T5】，并通过在每一步合并两个聚类直到产生单个最终聚类(对应于 *X* ):*

![](img/543602f0-e890-481e-9ebd-28453d15217b.png)

在上例中，集群*C<sub class="calibre20">I</sub>T3】和*C<sub class="calibre20">j</sub>T7】合并为*C<sub class="calibre20">k</sub>T11】；因此，我们在第二步中获得 *n-1* 簇。该过程继续进行，直到剩余的两个聚类被合并成包含整个数据集的单个块。相反地，**分裂**方法(最初由考夫曼和鲁索提出，有一种称为 DIANA 的算法)的操作方向相反，从 *X* 开始，以每个聚类包含一个样本的分割结束:***

![](img/ed6383c7-bc83-46ba-9f2c-9282e9b7a8bd.png)

在这两种情况下，结果都是层次结构的形式，其中每一层都是通过在前一层上执行合并或拆分操作而获得的。复杂性是这两种方法的主要区别，因为它对于分裂聚类来说更高。事实上，合并/拆分决策是通过考虑所有可能的组合并选择最合适的组合(根据特定标准)做出的。例如，在比较第一步时，很明显，考虑到所有可能的组合(在分裂情况下)，找到最合适的样本对(在凝聚情况下)比找到 *X* 的最佳分裂更容易，这需要指数复杂度。

由于最终的结果几乎相同，虽然除法算法的计算复杂度要高得多，但一般来说，没有特别的理由倾向于这种方法。因此，在本书中，我们将只讨论凝聚聚类(假设所有概念都立即适用于分裂算法)。我鼓励您始终考虑整个层次结构，即使大多数实现(例如 scikit-learn)都需要指定期望的集群数量。实际上，在实际应用程序中，一旦达到目标，最好停止该过程，而不是计算整个树。然而，这一步是分析阶段必不可少的一部分(特别是当集群的数量没有很好地定义时)，我们将演示如何可视化树，并为每个特定的问题做出最合理的决策。

# 聚集聚类

正如在其他算法中看到的，为了执行聚合，我们需要首先定义一个距离度量，它表示样本之间的不同。我们已经分析了其中的许多，但是在这种情况下，开始考虑通用的**闵可夫斯基距离**(用 *p* 参数化)是有帮助的:

![](img/07f5f198-bcf4-4f91-811d-a5bc867600af.png)

两种特殊情况对应于 *p=2* 和 *p=1* 。在前一种情况下，当 *p=2* 时，我们得到标准的**欧氏距离**(相当于 *L <sub class="calibre20">2</sub>* 范数):

![](img/be525080-5382-4146-9d15-6f26830f8970.png)

当 *p=1* 时，我们获得**曼哈顿**或**城市街区**距离(相当于*L<sub class="calibre20">1</sub>T9】范数):*

![](img/d851736e-2e48-4de6-b798-6fd828d24c42.png)

这些距离之间的主要差异在[第 2 章](02.html)、*聚类基础*中讨论。在本章中，引入**余弦**距离是有用的，这不是一个合适的距离度量(从数学的角度来看)，但是当样本之间的区分必须仅取决于它们形成的角度时，这是非常有用的:

![](img/f6f3bba1-aaea-4d25-85a7-a5c056ec4320.png)

余弦距离的应用非常特殊(例如**自然语言处理** ( **自然语言处理**)，因此不是一个常见的选择。但是，我鼓励大家用一些样本向量来检查它的属性(例如，( *0，1* )、( *1，0* )和( *0.5，0.5* )，因为它可以解决很多现实生活中的问题(例如，在 word2vec 中，可以通过检查两个单词的余弦相似度来轻松评估它们的相似度)。一旦定义了距离度量，定义一个**接近矩阵**、 *P* 是有帮助的:

![](img/3e160312-31e8-4484-bd90-4697761fa04f.png)

*P* 对称，所有对角元素都为空。由于这个原因，一些应用程序(如 SciPy 的`pdist`函数)产生一个压缩矩阵， *P <sub class="calibre20">c</sub>* ，它是一个只包含矩阵上三角部分的向量，因此 *P <sub class="calibre20">c</sub>* 的*ij*T9】第元素对应于 *d(x <sub class="calibre20">i</sub> ，x <sub class="calibre20">j</sub> )* 。

下一步是定义合并策略，在这种情况下，称为**链接**。链接方法的目标是找出必须在层次结构的每一层合并成一个的集群。因此，它必须使用代表集群的通用样本集。在这种情况下，假设我们正在分析几个集群( *C <sub class="calibre20">a</sub> ，C<sub class="calibre20">b</sub>T7)，我们需要找到哪个索引 *a* 或 *b* 对应于将要合并的集群。*

# 单一和完整的联系

最简单的方法称为**单**和**完全** **联动**，定义如下:

![](img/468320c4-2db4-4977-8a96-3eb224ae5111.png)

单一链接方法选择包含最接近的样本对的对(每个样本属于不同的聚类)。该流程如下图所示，选择 **C1** 和 **C2** 进行合并:

![](img/ba15026b-5a5e-445b-b800-b80c546f59e8.png)

Example of single linkage. C<sub class="calibre26">1</sub> and C<sub class="calibre26">2</sub> are selected to be merged

这种方法的主要缺点是可能有小簇和非常大的簇。正如我们将在下一节中看到的，单个链接可以将*异常值*隔离，直到存在非常高的相异度。为了避免或减轻这个问题，可以使用平均法和沃德法。

相反，完整链接定义为:

![](img/08986d3c-687c-4832-b1c2-fc5d8442702a.png)

这种链接方法的目标是最小化属于合并聚类的最远样本之间的距离。下图中有一个完整联动的例子，选择了**C<sub class="calibre20">1</sub>T3**C<sub class="calibre20">3</sub>T7】:****

![](img/5630c46a-e57c-4099-bcf2-bc30a9ed942e.png)

Example of complete linkage. C<sub class="calibre26">1</sub> and C<sub class="calibre26">3</sub> are selected for merging

算法选择 **C <sub class="calibre20">1</sub>** 和 **C** **<sub class="calibre20">3</sub>** 以增加内部凝聚力。事实上，考虑到所有可能的组合，很容易理解完全连锁导致集群密度最大化。在上图所示的示例中，如果期望的集群数量是两个，合并 **C <sub class="calibre20">1</sub>** 和 **C <sub class="calibre20">2</sub>** 或 **C <sub class="calibre20">2</sub>** 和 **C <sub class="calibre20">3</sub>** 将产生凝聚力较小的最终配置，这通常是不期望的结果。

# 平均联系

另一种常用的方法叫做**平均联动**(或**未加权对组法，算术平均** ( **UPGMA** ))。其定义如下:

![](img/4fc15787-cd87-42b0-85ce-5ca51d3e66f9.png)

这个想法与完全联动非常相似，但是，在这种情况下，考虑到每个集群的平均值，目标是最小化平均集群间距离，考虑所有可能的对( *C <sub class="calibre20">a</sub> ，C<sub class="calibre20">b</sub>T5】)。下图显示了平均链接的示例:*

![](img/c960034e-9433-4aa2-9418-3442cd67dac3.png)

Example of average linkage. C<sub class="calibre26">1</sub> and C<sub class="calibre26">2</sub> are selected for merging. The highlighted points are the averages.

平均连锁在生物信息学应用中特别有用(这是层次聚类被定义的主要背景)。对其性质的数学解释并不简单，我鼓励你查看原始论文(*评估系统关系的统计方法，索卡尔 r，麦切纳 c，堪萨斯大学科学通报，1958 年第 38 期*)以获得更多细节。

# 沃德的联系

我们要讨论的最后一种方法叫做**沃德联系**(以其作者命名，最初是在*优化目标函数的层次分组中提出的，美国统计协会杂志。58(301)，1963* 。它基于欧几里得距离，形式定义如下:

![](img/5680453f-6f08-4ff1-ac72-e1fe210b77f6.png)

在每一个级别，所有的集群都被考虑在内，其中两个集群的选择目标是最小化平方距离的总和。该过程本身与平均链接没有很大不同，并且有可能证明合并过程导致集群方差的减少(即增加它们的内部凝聚力)。此外，沃德的链接倾向于产生包含大约相同数量样本的聚类(也就是说，与单个链接相比，沃德的方法避免了小聚类和非常大的聚类的存在，这将在下一节中讨论)。沃德链接是一个流行的默认选择，但是，为了在每个特定的上下文中做出正确的选择，有必要引入树图的概念。

# 分析树木图

一个**树形图**是一个树形数据结构，允许我们表示由凝聚或分裂算法产生的整个聚类层次。想法是将样品放在 *x* 轴上，将相异度放在 *y* 轴上。每当两个聚类被合并时，树图显示对应于它出现时的不同级别的联系。因此，在聚集场景中，树图总是从所有被视为聚类的样本开始，并向上移动(方向纯粹是常规的)，直到定义了单个聚类。

出于教学目的，最好显示与非常小的数据集 *X* 相对应的树形图，但是我们将要讨论的所有概念都可以应用于任何情况。但是，对于较大的数据集，通常需要应用一些截断，以便以更紧凑的形式可视化整个结构。

让我们考虑一个小数据集， *X* ，由`4`高斯分布生成的`12`二维样本组成，其平均向量在( *-1，1* ) × ( *-1，1* ):

```py
from sklearn.datasets import make_blobs

nb_samples = 12
nb_centers = 4

X, Y = make_blobs(n_samples=nb_samples, n_features=2, center_box=[-1, 1], centers=nb_centers, random_state=1000)
```

数据集(带有标签)如下图所示:

![](img/9c8ab7a5-cafc-4e28-9f39-0407503eb6c3.png)

Dataset employed for dendrogram analysis

为了生成树图(使用 SciPy)，我们首先需要创建一个链接矩阵。在这种情况下，我们选择了带有沃德链接的欧几里德度量(但是，像往常一样，我鼓励您使用不同的配置来执行分析):

```py
from scipy.spatial.distance import pdist
from scipy.cluster.hierarchy import linkage

dm = pdist(X, metric='euclidean')
Z = linkage(dm, method='ward')
```

`dm`数组是一个压缩的成对距离矩阵，而`Z`是由沃德方法产生的链接矩阵(`linkage()`函数需要`method`参数，该参数接受值`single`、`complete`、`average`和`ward`)。此时，我们可以生成并绘制树形图(使用默认或提供的 Matplotlib `axis`对象，`dendrogram()`功能可以自动绘制图表):

```py
import matplotlib.pyplot as plt

from scipy.cluster.hierarchy import dendrogram

fig, ax = plt.subplots(figsize=(12, 8))

d = dendrogram(Z, show_leaf_counts=True, leaf_font_size=14, ax=ax)

ax.set_xlabel('Samples', fontsize=14)
ax.set_yticks(np.arange(0, 6, 0.25))

plt.show()
```

下图显示了该图:

![](img/0881b636-fc88-46f7-a273-3812b775e55c.png)

Dendrogram corresponding to Ward's linkage applied to the dataset

如前一张截图中所述， *x* 轴代表旨在最小化交叉连接风险的样本，而 *y* 轴则显示不同程度。现在让我们从底部分析图表。初始状态对应于被视为独立聚类的所有样本(因此相异度为空)。向上移动，我们开始观察第一次合并。特别地，当相异度约为 0.35 时，样本 1 和 3 被合并。

当样本 0 和 9 也合并时，第二步的相异度略低于 0.5。这个过程一直持续到创建单个集群时，相异度约为 5.25。现在让我们水平解剖树图，当相异度等于 1.25 时。查看底层连接，我们发现集群结构为:{6}、{7，5，8}、{0，9，4，10}、{11}、{2，1，3}。

因此，我们有五个集群，其中两个由单个样本组成。观察到样本 6 和样本 11 是最后合并的样本也就不足为奇了。事实上，它们之间的距离比其他所有的都远。在下面的屏幕截图中，显示了四个不同的级别(只有包含多个样本的集群用圆圈标记):

![](img/3f3dda45-7893-433b-b406-faab6e39afaf.png)

Clusters generated by cutting the dendrogram at different levels (Ward's linkage)

很容易理解，聚集从选择最相似的集群/样本开始，并通过添加*最近的邻居*进行，直到到达树根。在我们的例子中，在相异度等于 2.0 的情况下，已经检测到三个定义明确的聚类。左边的一个也保留在下一个切割中，而右边的两个(显然更接近)被选择合并，以便产生单个簇。这个过程本身很简单，不需要特别的解释；然而，有两个重要的考虑。

第一个是固有的树图结构本身。与其他方法相反，层次聚类允许观察整个聚类树，并且当需要通过增加相异度来显示过程如何发展时，这样的特性非常有用。例如，一个产品推荐应用程序不能提供任何关于代表用户的期望集群数量的信息，但是执行管理层可能有兴趣理解合并过程是如何构造和发展的。

事实上，观察集群是如何合并的，可以深入了解底层的几何结构，也有可能发现哪些集群可能被认为是更大集群的一部分。在我们的示例中，在 0.5 级，我们有一个小集群{1，3}。“通过增加相异度可以将什么样的样本添加到这个聚类中？”可以立即用{2}回答。当然，在这种情况下，这是一个微不足道的问题，可以通过查看数据图来解决，但是对于高维数据集，如果没有树图的支持，这可能会变得更加困难。

谱系图的第二个优点是可以比较不同连锁方法的行为。使用沃德的方法，第一次合并发生在相当低的相异度水平上，但是五个集群和三个集群之间有很大的差距。这是几何和合并策略的结果。例如，如果我们使用一个单一的链接(本质上非常不同)，会发生什么？相应的树形图如下图所示:

![](img/259e95d8-59c2-40dc-bb7b-4188f5ffd702.png)

Dendrogram corresponding to single linkage applied to the dataset

结论是树图是不对称的，簇通常与单个样品或小团块合并。从右侧开始，我们可以看到样本{11}和{6}很晚才合并。此外，样本{6}(可能是异常值)在最高相异度时被合并，此时必须产生最终的单个聚类。通过下面的截图可以更好地理解这个过程:

![](img/fcddb815-9420-4fc1-b74e-59dff373faf9.png)

Clusters generated by cutting the dendrogram at different levels (single linkage)

从截图中可以看到，虽然沃德的方法生成了两个包含所有样本的聚类，但单个链接通过将潜在的异常值保持在外部来聚合 1.0 级的最大块。因此，谱系图也允许定义聚合语义，这在心理测量学和社会学的背景下非常有帮助。虽然沃德的链接方式与其他对称算法非常相似，但单个链接有一种逐步的方式，显示出对增量构建的集群的潜在偏好，从而避免了相异度的巨大差距。

最后，有趣的是，虽然沃德的链接通过在 3.0 级切割树形图产生了潜在的最佳聚类数(三个)，但单个链接从未达到这样的配置(因为聚类{6}仅在最后一步被合并)。这种效果与最大分离和最大凝聚的双重原则严格相关。沃德的联系倾向于很快找到最有凝聚力和分离的集群。当相异度差距超过预定阈值时(当然，当达到期望的聚类数时)，它允许切割树形图，而其他链接需要不同的方法，有时会产生不期望的最终配置。

考虑到问题的性质，我总是鼓励大家去检验所有联动方法的行为，找出最适合一些样本场景的方法(例如，根据教育水平、入住率和收入对一个国家的人口进行细分)。这是提高意识和提高提供过程语义解释能力的最佳方法(这是任何聚类过程的基本目标)。

# 作为性能度量的共亨相关性

可以使用前面章节中介绍的任何方法来评估分层聚类性能。然而，在这种特殊情况下，可以采用特定的措施(不需要基本事实)。给定一个邻近矩阵、 *P* 和一个链接、 *L* 、几个样本、 *x <sub class="calibre20">i</sub>* 和 *x <sub class="calibre20">j</sub> ∈ X* 总是被分配给某个层次级别的同一聚类。当然，重要的是要记住，在聚集场景中，我们从 *n* 个不同的集群开始，最终得到一个相当于 *X* 的集群。此外，当两个合并的聚类成为单个聚类时，属于一个聚类的两个样本将总是继续属于同一*扩大的*聚类，直到该过程结束。

考虑到上一节中显示的第一个树形图，样本{1}和{3}会立即合并；然后添加样本{2}，接着是{11}。此时，整个集群与另一个块(包含样本{0}、{9}、{4}和{10})合并。在最后一级，合并剩余的样本以形成单个最终聚类。因此，命名不同等级 *DL <sub class="calibre20">0</sub>* 、 *DL <sub class="calibre20">1</sub>* ，...，和 *DL <sub class="calibre20">k</sub>* ，样品{1}和{3}在 *DL <sub class="calibre20">1</sub>* 开始属于同一个聚类，而例如在 *DL <sub class="calibre20">6</sub>* 发现{2}和{1}在同一个聚类中。

此时，我们可以将 *DL <sub class="calibre20">ij</sub>* 定义为 *x <sub class="calibre20">i</sub>* 和 *x <sub class="calibre20">j</sub>* 第一次属于同一个聚类的相异度，以下( *n × n* )矩阵中**共亨** **矩阵** 表示为 *CP* :

![](img/5a3b29f5-fbf7-4acd-abfb-849ff081295e.png)

换句话说， *CP <sub class="calibre20">ij</sub>* 元素是观察同一簇中的*x<sub class="calibre20">I</sub>T7】和*x<sub class="calibre20">j</sub>T11】所需的最小相异度。有可能证明 *CP <sub class="calibre20">ij</sub>* 是 *x <sub class="calibre20">i</sub>* 和 *x <sub class="calibre20">j</sub>* 之间的距离度量，；因此， *CP* 与 *P* 类似，具有相似矩阵的相同性质(例如所有对角元素都为空)。特别是，我们对它们的相关性感兴趣(在 *-1* 和 *1* 范围内归一化)。这个值(**共病相关系数** ( **CPC** )表示 *P* 和 *CP* 之间的一致程度，可以很容易地计算出来，如下式所示。**

由于 *P* 和 *CP* 都是( *n × n* )对角元素为零的对称矩阵，因此可以只考虑包含 *n(n-1)/2* 值的下三角部分(不包括对角线，表示为*Tril()*)。因此，平均值如下:

![](img/d7e30ca9-4bbe-45de-a1ee-1f98624ceced.png)

标准化的平方和值如下:

![](img/5c392bef-a64c-47a1-83dd-b377adabd52c.png)

因此，标准化的共病相关性简单地等于如下:

![](img/2ff0529a-5715-44fc-8b52-347801de1a6b.png)

前面的等式是基于这样的假设，如果三个样本, *x <sub class="calibre20">i</sub>* 、 *x <sub class="calibre20">j</sub>* 和 *x <sub class="calibre20">p</sub>* 具有诸如 *d(x <sub class="calibre20">i</sub> 、x<sub class="calibre20">j</sub>)<d(x<sub class="calibre20">I</sub>，x <sub class="calibre20">p</sub> )* ， 有理由期待 *x <sub class="calibre20">i</sub>* 和 *x <sub class="calibre20">j</sub>* 会在*x<sub class="calibre20">I</sub>T33】和*x*<sub xmlns:epub="http://www.idpf.org/2007/ops" class="calibre20">T37】p</sub>*之前合并到同一个集群中(即 *x <sub class="calibre20">i</sub>* 和*x 合并对应的相异程度 因此， *CPC → 1* 表示联动生成了一个最优的层次，反映了底层的几何。另一方面， *CPC* *→ -1* 表示完全不一致和与几何不一致的潜在聚类结果。不言而喻，给定一个问题，我们的目标是找到一个最大化 *CPC* 的指标和联系。*

考虑到[第三章](03.html)、*高级聚类*中描述的例子，我们可以使用 SciPy 函数`cophenet`计算共亨矩阵和对应于不同连杆(假设欧几里德距离)的共亨矩阵。该函数要求链接矩阵作为第一个参数，邻近矩阵作为第二个参数，并返回共亨矩阵和 CPC(变量`dm`是先前计算的浓缩邻近矩阵):

```py
from scipy.cluster.hierarchy import linkage, cophenet

cpc, cp = cophenet(linkage(dm, method='ward'), dm)
print('CPC Ward\'s linkage: {:.3f}'.format(cpc))

cpc, cp = cophenet(linkage(dm, method='single'), dm)
print('CPC Single linkage: {:.3f}'.format(cpc))

cpc, cp = cophenet(linkage(dm, method='complete'), dm)
print('CPC Complete linkage: {:.3f}'.format(cpc))

cpc, cp = cophenet(linkage(dm, method='average'), dm)
print('CPC Average linkage: {:.3f}'.format(cpc))
```

这个片段的输出如下所示:

```py
CPC Ward's linkage: 0.775
CPC Single linkage: 0.771
CPC Complete linkage: 0.779
CPC Average linkage: 0.794
```

这些值非常接近，表明所有的联系都产生了相当好的结果(即使它们不是最佳的，因为存在两个异常值)。然而，如果我们需要选择一种方法，平均联系是最准确的，如果没有跳过它的具体原因，应该优先于其他联系。

共亨相关性是层次聚类特有的评估指标，通常提供可靠的结果。然而，当几何形状更复杂时，CPC 值可能会产生误导，并导致次优配置。出于这个原因，我总是建议使用其他指标(例如，轮廓分数或调整后的兰德分数)，以便再次检查性能并做出最合适的选择。

# 水处理厂数据集上的凝聚聚类

现在让我们在一个更大的数据集上考虑一个更详细的问题(下载说明在本章开头的*技术要求*部分提供)，该数据集包含 527 个样本，38 个描述水处理厂状态的化学和物理变量。正如同一批作者(贝哈尔、科尔特斯和波奇)所言，该领域结构不佳，需要仔细分析。同时，我们的目标是用不可知的方法找到最优聚类；换句话说，我们不会考虑语义标注过程(需要领域专家)，而只考虑数据集的几何结构和凝聚算法发现的关系。

一旦下载，CSV 文件(称为`water-treatment.data`)就可以使用熊猫加载(当然，术语`<DATA_PATH>`必须更改，以便指向文件的确切位置)。第一列是与特定植物相关的索引，而所有其他值都是数字，可以转换为`float64`。缺少的值用`'?'`字符表示，由于我们没有任何其他信息，它们用每个属性的平均值来设置:

```py
import pandas as pd

data_path = '<DATA_PATH>/water-treatment.data'

df = pd.read_csv(data_path, header=None, index_col=0, na_values='?').astype(np.float64)
df.fillna(df.mean(), inplace=True)
```

由于单个变量具有非常不同的大小(我邀请读者使用数据框上的`describe`函数检查这一陈述)，最好在范围( *-1，1* )内对它们进行归一化，保留原始方差:

```py
from sklearn.preprocessing import StandardScaler

ss = StandardScaler(with_std=False)
sdf = ss.fit_transform(df)
```

此时，像往常一样，我们可以使用 t-SNE 算法将数据集投影到二维空间:

```py
from sklearn.manifold import TSNE

tsne = TSNE(n_components=2, perplexity=10, random_state=1000)
data_tsne = tsne.fit_transform(sdf)

df_tsne = pd.DataFrame(data_tsne, columns=['x', 'y'], index=df.index)
dff = pd.concat([df, df_tsne], axis=1)
```

下面的截图显示了结果图:

![](img/68392175-8413-4141-b51b-1f1808f7a85e.png)

t-SNE plot of the Water Treatment Plant dataset

该图显示了一个潜在的非凸几何形状，其中许多小*岛*(密集区域)由空白空间分隔。然而，如果没有任何领域信息，就不太容易决定哪些 blobs 可以被认为是同一个集群的一部分。我们可以决定强加的唯一*伪约束*(考虑到所有工厂都以类似的方式运行)是拥有中等或较小的最终集群数量。因此，假设欧几里德距离并使用 scikit-learn `AgglomerativeClustering`类，我们可以计算所有连杆机构的共亨相关性和轮廓得分以及集群的`4`、`6`、`8`和`10`数量:

```py
import numpy as np

from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_score

from scipy.spatial.distance import pdist
from scipy.cluster.hierarchy import linkage, cophenet

nb_clusters = [4, 6, 8, 10]
linkages = ['single', 'complete', 'ward', 'average']

cpcs = np.zeros(shape=(len(linkages), len(nb_clusters)))
silhouette_scores = np.zeros(shape=(len(linkages), len(nb_clusters)))

for i, l in enumerate(linkages):
    for j, nbc in enumerate(nb_clusters): 
        dm = pdist(sdf, metric='minkowski', p=2)
        Z = linkage(dm, method=l)
        cpc, _ = cophenet(Z, dm)
        cpcs[i, j] = cpc

        ag = AgglomerativeClustering(n_clusters=nbc, affinity='euclidean', linkage=l)
        Y_pred = ag.fit_predict(sdf)
        sls = silhouette_score(sdf, Y_pred, random_state=1000)
        silhouette_scores[i, j] = sls
```

相应的图如下图所示:

![](img/22e87346-d2f3-4423-b75b-c700146d7550.png)

Cophenetic correlation (left) and silhouette score (right) for a different number of clusters and four linkage methods

需要考虑的第一个因素是，共基因相关性对于完全和平均连锁来说是合理可接受的，而对于单一连锁来说太低了。考虑到剪影分数，最大值(约 0.6)是通过单个链接和四个集群实现的。该结果表明，即使分层算法产生次优配置，四个区域也可以用中等或高水平的内部内聚性来分离。

如前一节所述，共基因关联有时可能会产生误导，在这种情况下，我们可以得出结论，如果潜在集群的理论数量为四个，则使用单链是最佳选择。然而，所有其他图表显示了对应于完整链接的最大值(以及单个图表的最小值)。因此，要回答的第一个问题是:我们需要偶数簇吗？在这个例子中，让我们假设许多工厂以非常标准的方式运行(许多样本共享差异)，但是也可能有一些特殊的情况(不适当的*异常值*)会表现出非常不同的行为。

这种假设在许多情况下是现实的，可能是由于创新或实验过程、缺乏资源、测量过程中的内部问题等。领域专家可以确认或拒绝我们的假设，但是，由于这是一个通用的例子，我们可以决定保留八个具有完全链接的集群(轮廓分数约为 0.5)。该值表示存在重叠，但考虑到数据集的维度和非凸性，它在许多实际情况下是可以接受的。

此时，我们还可以分析截断到 80 片叶子的树形图(这可以通过设置`trucate_mode='lastp'`参数和`p=80`来实现)，以避免间隔过小和难以区分(不过，可以去掉这个约束，提高分辨率):

![](img/ce8a15a9-6a97-430f-a742-ed2add760a19.png)

Dendrogram of the Water Treatment Plant dataset with the Euclidean metric and complete linkage

我们可以看到，凝聚过程是不均匀的。在该过程的开始，相异度增加得相当慢，但是在大约对应于 10，000 的值之后，跳跃变得更大。看一下 t-SNE 图，可以理解非凸性的影响对非常大的星系团有更强的影响，因为密度降低了，隐含地，相异度增加了。很简单，极少数集群(例如 1、2 或 3)的特点是内部差异非常大，内聚性非常低。

此外，树图显示，在大约 17，000 的水平上有两个主要的不均匀聚集，因此我们可以推断粗粒度分析突出了一个主要行为(从顶部看图)和一个次要行为的存在，由较少数量的植物使用。特别是，较小的组非常稳定，因为它将以大约 50，000 的相异度合并到最终的单个集群中。因此，我们应该预期伪异常值的存在，这些伪异常值被分组到更孤立的区域中(这也由 t-SNE 图所证实)。

在 4，000 ÷ 6，000 的范围内切割(对应于大约八个簇)，较大的块比较小的块更密集。换句话说，离群点聚类包含的样本将比其他聚类少得多。这并不奇怪，因为正如*分析树状图*一节所讨论的，最远的聚类通常在完全连锁中合并得很晚。

此时，我们最终可以执行聚类并检查结果。Scikit-learn 的实现并不计算整个树形图，而是在达到期望的聚类数时停止该过程(除非`compute_full_tree`参数不是`True`):

```py
import pandas as pd

from sklearn.cluster import AgglomerativeClustering

ag = AgglomerativeClustering(n_clusters=8, affinity='euclidean', linkage='complete')
Y_pred = ag.fit_predict(sdf)

df_pred = pd.Series(Y_pred, name='Cluster', index=df.index)
pdff = pd.concat([dff, df_pred], axis=1)
```

最后的图显示在下面的截图中:

![](img/98f4f451-1bb3-45a7-b415-72132df4ab00.png)

Clustering result of the Water Treatment Plant dataset (eight clusters)

不出所料，这些簇是不均匀的，但它们与几何形状相当一致。此外，孤立的聚类(例如，在区域*x∑(-40，-20)* 和 *y > 60* 中)非常小，并且它们非常可能包含真正的异常值，这些异常值的行为与大多数其他样本非常不同。我们不打算分析语义，因为问题非常具体。然而，有理由认为*x∞(-40，40)* 和*y∞(-40，-10)* 区域的大星系团，尽管是非凸的，但代表了一个合适的基线。相反，其他大块(在这个集群的极端)对应于具有特定属性或行为的植物，这些属性或行为足够分散，可以被认为是标准的替代实践。当然，正如开头提到的，这是一个不可知论的分析，应该有助于理解如何使用分层聚类。

作为最后一步，我们希望在大约 35，000(对应于两个聚类)的相异度水平上切割树图。结果如下图所示:

![](img/a886363f-4119-45eb-b45c-a28bfbf0e0a7.png)

Clustering result of the Water Treatment Plant dataset (two clusters)

在这个级别上，树图显示了属于一个集群和剩余较小区块的大部分样本。现在我们知道这样的二级区域对应*x∑(-40，10)* 和 *y > 20* 。同样，结果并不令人惊讶，因为 t-SNE 图显示，这些样本是唯一具有 *y > 20 ÷ 25* 的样本(而更大的星系团，即使有巨大的空白区域，也几乎覆盖了所有的范围)。

因此，我们可以说这样的样本代表了具有*极端*行为的非常不同的植物，并且，如果一个新的样本被分配到那个集群，它可能是一个非标准植物(假设一个标准植物具有与其大多数同类相似的行为)。作为练习，我鼓励你测试其他数量的集群和不同的链接(特别是单个链接，这是非常特殊的)，并尝试验证或拒绝一些样本，先前的假设(它们没有必要在物理上是可接受的)。

# 连通性限制

凝聚层次聚类的一个重要特征是可以包含连接性约束来强制合并特定的样本。这种先验知识在邻居之间存在强关系的上下文中或者当我们知道某些样本由于其内在属性而必须属于同一聚类时非常常见。为了实现这个目标，我们需要使用一个**连接矩阵，***A∑{ 0，1} <sup class="calibre27">n × n</sup>* :

![](img/132f7bce-0b7b-4715-bc63-c055335c291c.png)

一般来说， *A* 是数据集的一个图诱导的邻接矩阵；然而，唯一重要的要求是没有孤立的样本(没有连接)，因为它们不能以任何方式合并。连通性矩阵在初始合并阶段应用，并强制算法聚集指定的样本。由于以下聚集不会影响连通性(两个合并的样本或聚类将保持合并状态，直到过程结束)，因此约束总是被强制执行。

为了理解这个过程，让我们考虑一个包含从`8`二元高斯分布提取的`50`二维点的样本数据集:

```py
from sklearn.datasets import make_blobs

nb_samples = 50
nb_centers = 8

X, Y = make_blobs(n_samples=nb_samples, n_features=2, center_box=[-1, 1], centers=nb_centers, random_state=1000)
```

下图显示了标记的数据集:

![](img/2babcaa4-c3d5-4faf-8e30-a7371de35be1.png)

Dataset for connectivity constraints example

看剧情，我们看到样本 18 和 31(*x<sub class="calibre20">0</sub>∑(-2，-1)* 和*x<sub class="calibre20">1</sub>∑(1，2)* )相当接近；然而，我们不希望它们被合并，因为样本 18 在大的中心斑点中有更多的邻居，而点 31 是部分隔离的，应该被认为是一个自治集群。我们还希望样本 33 形成单个集群。这些要求将迫使算法合并不再考虑底层几何的聚类(根据高斯分布)，而是考虑先验知识。

为了检查聚类是如何工作的，现在让我们使用欧几里德距离和平均链接来计算树图(被截断为 20 片叶子):

```py
from scipy.spatial.distance import pdist
from scipy.cluster.hierarchy import linkage, dendrogram

dm = pdist(X, metric='euclidean')
Z = linkage(dm, method='average')

fig, ax = plt.subplots(figsize=(20, 10))

d = dendrogram(Z, orientation='right', truncate_mode='lastp', p=20, ax=ax)

ax.set_xlabel('Dissimilarity', fontsize=18)
ax.set_ylabel('Samples', fontsize=18)
```

树形图(从右到左)如下图所示:

![](img/603d9203-3dd1-4123-b027-ffdd7a1f4cc9.png)

Dendrogram for the connectivity constraints example with the Euclidean distance and average linkage

如预期的那样，样本 18 和 31 被立即合并，然后与包含 2 个样本的另一个集群聚合(当数字在括号之间时，这意味着它是包含更多样本的复合块)，这可能是 44 和 13。样本 33 也被合并，所以它不会留在一个孤立的集群中。作为确认，我们用`n_clusters=8`进行聚类:

```py
from sklearn.cluster import AgglomerativeClustering

ag = AgglomerativeClustering(n_clusters=8, affinity='euclidean', linkage='average')
Y_pred = ag.fit_predict(X)
```

以下屏幕截图显示了群集数据集的图:

![](img/77db8173-7f8c-460b-b7f5-b24274dff263.png)

Dataset clustered using the Euclidean distance and average linkage

结果证实了前面的分析。在没有约束的情况下，平均关联产生了与基本事实(八个高斯分布)兼容的合理划分。为了分割大的中心斑点并保持所需数量的聚类，算法还必须合并孤立的样本，即使树形图确认它们在末端以最高的相异度被合并。

为了施加我们的约束，我们可以观察到，基于前两个最近邻居的连通性矩阵很可能会强制聚合属于更密集区域的所有样本(考虑到邻居更近)，并最终将孤立点保留在自治集群中。这种假设行为的原因是基于平均链接的目标(最小化集群间的平均距离)。因此，在施加约束之后，该算法更倾向于聚集具有其他邻居的接近的聚类(记住 *A* 具有空值，但是在对应于两个最近邻居的位置)并且不聚集最远的点，直到相异度足够大(产生非常不均匀的聚类)。

为了检查我们的假设是否正确，让我们使用 scikit-learn、`kneighbors_graph()`函数和`n_neighbors=2`生成一个连通性矩阵，并重新聚集数据集，设置`connectivity`约束:

```py
from sklearn.cluster import AgglomerativeClustering
from sklearn.neighbors import kneighbors_graph

cma = kneighbors_graph(X, n_neighbors=2)

ag = AgglomerativeClustering(n_clusters=8, affinity='euclidean', linkage='average', connectivity=cma)
Y_pred = ag.fit_predict(X)
```

上一个片段的图形输出显示在下面的屏幕截图中:

![](img/a8e4c902-45bd-4510-b43b-c879a8625acb.png)

Dataset clustered using the Euclidean distance and average linkage using connectivity constraints

不出所料，样本 18 已被分配到大的中央集群，而点 31 和 33 现在被隔离。当然，由于过程是分层的，所以施加连接性约束比分离性约束更容易。事实上，虽然单个样本在初始阶段可以很容易地合并，但在最终合并之前，不能很容易地保证使用所有链接来排除它们。

当需要复杂的约束时(给定一个距离和一个链接)，通常需要调整连接矩阵和期望的集群数量。当然，如果期望的结果是用特定数量的聚类实现的，那么它也将用更大的值实现，直到相异度下限(即，合并过程减少了聚类的数量；因此，如果相异度足够大，所有现有约束将保持有效)。例如，如果三个样本被约束为属于同一个聚类，我们通常不能期望在初始合并阶段之后出现这种结果。

但是，如果所有三个样本的合并发生在某个不同的级别(例如，2.0 对应于 30 个集群)，它也将对 *n < 30* 集群和所有具有 *DL > 2.0* 的配置保持有效。因此，如果我们从 5 个集群开始，我们可以很容易地增加这个数量，同时注意具有比约束所施加的最后一次合并所对应的不同级别更大的不同级别。我鼓励您用其他数据集测试这种方法，并尝试定义可以在聚类过程后轻松验证的先验约束。

# 摘要

在这一章中，我们介绍了层次聚类方法，重点是可以使用的不同策略(分裂和聚集策略)。我们还讨论了用于发现哪些集群可以合并或拆分(链接)的方法。特别是，给定一个距离度量，我们分析了四种链接方法的行为:单一、完整、平均和沃德方法。

我们已经展示了如何构建一个树形图，以及如何分析它，以便使用不同的链接方法来理解整个分层过程。引入了一种称为共亨蒂奇相关的特定性能度量，以在不了解基本事实的情况下评估分层算法的性能。

我们分析了一个更大的数据集(水处理厂数据集)，定义了一些假设，并使用前面讨论的所有工具验证了它们。在这一章的最后，我们讨论了连通性约束的概念，它允许使用连通性矩阵将先验知识引入到过程中。

在下一章中，我们将介绍软聚类的概念，重点介绍一种模糊算法和两种非常重要的高斯混合模型。

# 问题

1.  凝聚方法和分裂方法有什么区别？
2.  给定两个簇 *a: [(-1，-1)，(0，0)]* 和 *b: [(1，1)，(1，0)]* ，考虑欧氏距离的单个完整的连杆机构有哪些？
3.  树形图表示给定数据集的不同链接结果。这是正确的吗？
4.  在聚集聚类中，树形图的底部(初始部分)包含单个聚类。这是正确的吗？
5.  聚类分析中树形图的 *y* 轴是什么意思？
6.  当合并更小的簇时，相异度减小。这是正确的吗？
7.  共亨矩阵的元素 *C(i，j)* 报告了两个对应元素*x<sub class="calibre20">I</sub>T5】和*x<sub class="calibre20">j</sub>T9】首次出现在同一簇中的不同程度。这是正确的吗？**
8.  连接限制的主要目的是什么？

# 进一步阅读

*   *评价系统关系的统计方法*，*索卡尔 R.* ，*麦切纳 C.* ，*堪萨斯大学科学通报*，38，1958
*   *优化目标函数的分层分组*、*沃德 Jr* 、 *J. H.* 、*美国统计协会杂志*。58(301), 1963
*   *LINNEO+:一种针对不良结构域的分类方法*、*贝哈尔 J.* 、*科尔特斯 U.* 、*波奇 M.* 、*研究报告 RT-93-10-R .科学信息学部*、*巴塞罗纳*，1993
*   *机器学习算法，第二版*，*博纳科尔索集团*，*帕克特出版*，2018