# 三、高级聚类

在这一章中，我们将继续探索更复杂的聚类算法，这些算法可以用于非凸任务(也就是说，例如，K-means 无法同时获得内聚性和分离性)。一个经典的例子由交错的几何图形表示)。我们还将展示如何将基于密度的算法应用于复杂数据集，以及如何根据所需结果正确选择超参数并评估性能。这样，数据科学家可以准备好面对不同类型的问题，排除价值较低的解决方案，只关注最有希望的解决方案。

特别是，我们将讨论以下主题:

*   光谱聚类
*   均值漂移
*   带噪声应用的基于密度的空间聚类 ( **数据库扫描**)
*   其他评估指标:卡林斯基-哈拉巴斯指数和集群不稳定性
*   k-水母
*   在线聚类(小批量 K-means 和**使用层次结构的平衡迭代约简和聚类** ( **BIRCH** ))

# 技术要求

本章中的代码要求:

*   Python 3.5+(蟒蛇分布:[https://www.anaconda.com/distribution/](https://www.anaconda.com/distribution/)强烈推荐)
*   库:
    *   SciPy 0.19+
    *   NumPy 1.10+
    *   学习 0.20+
    *   熊猫 0.22+
    *   Matplotlib 2.0+
    *   seaborn 0.9+

数据集可以通过 UCI 获得。CSV 文件可以从[https://archive.ics.uci.edu/ml/datasets/Absenteeism+at+work](https://archive.ics.uci.edu/ml/datasets/Absenteeism+at+work)下载，除了添加加载阶段会出现的列名外，不需要任何预处理。

Github 存储库中提供了一些示例:

[https://github . com/PacktPublishing/HandsOn-Unsupervised-Learning-with-Python/chapter 03](https://github.com/PacktPublishing/HandsOn-Unsupervised-Learning-with-Python/tree/master/Chapter03)。

# 光谱聚类

能够管理非凸聚类的最常见算法家族之一是**谱聚类**。其主要思想是将数据集 *X* 投影到一个空间上，在这个空间中，集群可以被超球体捕获(例如，使用 K-means)。这个结果可以通过不同的方式来实现，但是，由于算法的目标是去除一般形状区域的凹陷，所以第一步总是将 *X* 表示为图形 *G={V，E}，*，其中顶点 *V ≡ X* 和加权边缘表示每对样本的接近度 *x <sub class="calibre20">i</sub>* ， *x <sub class="calibre20">j</sub> ∈ X 生成的图可以是完整的(完全连通的)，也可以只有一些样本对之间的边(也就是说，不存在的权重的权重设置为零)。在下图中，有一个局部图的示例:*

![](img/85a481c5-78cf-4e85-85b2-f8a8faa6cc13.png)

Example of a graph: Point x<sub class="calibre26">0</sub> is the only one that is connected to x<sub class="calibre26">1</sub>

有两种主要策略可以用来确定权重*w<sub class="calibre20">ij</sub>T3:【KNN】和**径向基函数** ( **径向基函数**)。第一个是基于上一章讨论的相同算法。考虑到邻居的数量 *k* ，数据集被表示为球树或 kd 树，并且对于每个样本 *x <sub class="calibre20">i</sub>* ，计算集合 *kNN(x <sub class="calibre20">i</sub> )* 。此时，给定另一个样本 *x <sub class="calibre20">j</sub>* ，重量计算如下:*

![](img/4db86673-2afb-402b-a4b5-024582b58949.png)

在这种情况下，该图不包含任何关于实际距离的信息，因此，考虑到在 KNN 使用的相同距离函数*d()*，最好将*w*<sub xmlns:epub="http://www.idpf.org/2007/ops" class="calibre20">T5</sub>表示为:

![](img/ac69d18f-0a9d-4ea7-9837-d658e8d1b89c.png)

这种方法简单而且相当可靠，但是得到的图并不是完全连通的。这种情况可以通过使用如下定义的径向基函数来轻松实现:

![](img/4f5ad428-6f15-48c6-9fdf-28243aa857ed.png)

这样，所有夫妇都会根据他们的距离自动加权。由于径向基函数是高斯曲线，当*x<sub class="calibre20">I</sub>= x<sub class="calibre20">j</sub>T7】时，它等于 *1* ，并与平方距离 *d(x <sub class="calibre20">i</sub> ，x <sub class="calibre20">j</sub> )* 成比例减小(表示为差值的范数)。参数 *γ* 决定半钟曲线的振幅(一般默认值为 *γ=1* )。当 *γ < 1* 时，振幅增大，反之亦然。因此， *γ < 1* 表示对距离的敏感度较低，而 *γ > 1* 表示径向基函数下降较快，如下图截图所示:*

![](img/48336cf2-f566-4d38-bb63-ba3c12267595.png)

Bidimensional RBFs as functions of the distance between x and 0 computed for γ = 0.1, 1.0, and 5.0

*γ = 0.1* ， *x = 1* (相对于 0.0)加权约 0.9。对于 *γ = 1.0* 该值约为 0.5，对于*γ = 5.0* 该值几乎为零。因此，在调整谱聚类模型时，考虑 *γ* 的不同值并选择产生最佳性能的值(例如，使用[第 2 章](02.html)、*聚类基础*中讨论的标准进行评估)是极其重要的。一旦创建了图形，就可以使用对称的**亲和矩阵** *W = {w <sub class="calibre20">ij</sub> }* 来表示它。对于 KNN 来说 *W* 一般比较稀疏，可以用专门的库进行高效的存储和操作。相反，对于 RBF，它总是密集的，如果 *X ∈ ℜ <sup class="calibre27">N × M</sup>* ，它需要存储 *N <sup class="calibre27">2</sup>* 值。

不难证明，我们到目前为止所分析的过程相当于将 *X* 分割成多个内聚区域。事实上，让我们考虑一个例子，一个图 *G* 与 KNN 获得的亲和矩阵。连接的组件 *C <sub class="calibre20">i</sub>* 是一个子图，其中每对顶点 *x <sub class="calibre20">a</sub>* 、*x<sub class="calibre20">b</sub>∈C<sub class="calibre20">I</sub>*通过属于*C*<sub xmlns:epub="http://www.idpf.org/2007/ops" class="calibre20">T21】I</sub>的顶点路径连接，并且不存在连接 *C <sub class="calibre20">i</sub> 的任何顶点的边换句话说，连接的组件是内聚子集 *C <sub class="calibre20">i</sub> ![](img/055b1708-309e-409f-ab58-e4017eae690d.png) G* ，其代表聚类选择的最佳候选。在下图中，有一个从图中提取的连接组件的示例:*

![](img/48005b95-043f-414c-b315-0e673fbc01c5.png)

Example of a connected component extracted from a graph

在原空间中，点 **x <sub class="calibre20">0</sub>** 、 **x <sub class="calibre20">2</sub>** 、 **x <sub class="calibre20">3</sub>** 连接到 **x <sub class="calibre20">n</sub>** 、 **x** **<sub class="calibre20">m</sub>** ，以及**x****T25】q**至**这可以表示非常简单的非凸几何形状，例如半月形。事实上，在这种情况下，凸性假设对于最佳分离不再是必要的，因为正如我们将要看到的，这些分量被提取并投影到具有平坦几何形状的子空间上(通过诸如 K-means 的算法容易管理)。**

当使用 KNN 时，这个过程更加明显，但是，一般来说，当区域间距离(例如，两个最近点之间的距离)与平均区域内距离相当时，我们可以说两个区域可以合并。解决这个问题最常见的方法之一是由石和马利克提出的(在*规范化切割和图像分割中，石和马利克，IEEE 模式分析和机器智能学报，第 22 卷，* 08/2000)，它被称为规范化切割。整个证明超出了本书的范围，但我们可以讨论主要概念。给定一个图，可以构建规范化的拉普拉斯图，定义为:

![](img/64e77204-f962-41bb-b3d1-076147f11b63.png)

对角矩阵 *D* 称为**度矩阵**，每个元素 *d <sub class="calibre20">i</sub> <sub class="calibre20">i</sub>* 是对应行的权重之和。有可能证明以下陈述:

*   在特征分解 *L* (考虑非正规图拉普拉斯 *L <sub class="calibre20">u</sub> = D - W* 并求解方程 *L <sub class="calibre20">u</sub> v = λDv* 后，很容易计算出特征值和特征向量，零特征值总是以多重性 *p* 存在。
*   如果 *G* 是无向图(所以 *w <sub class="calibre20">ij</sub> ≥ 0 ∀ i，j* ，则连接的分量数等于 *p* (零特征值的重数)。
*   如果 *A ![](img/055b1708-309e-409f-ab58-e4017eae690d.png) ℜ <sup class="calibre27"> N </sup>* 和*θ*是 *A* 的可数子集(即 *X* 是可数子集，因为样本数总是有限的)，则给定 *θ <sub class="calibre20"> i，向量*v∈ℜ<sup class="calibre27">n</sup>t14】被称为**指示向量**,用于*θ*if 例如，如果我们有两个向量 *a = (1，0)* 和 *b = (0，0)* (所以，*θ= { a，b}* )并且我们考虑 *A = {(1，n)其中 n∈1，10】}*，则向量 *v = (1，0)* 是一个指示向量，因为 *a ∈ A**</sub>*
*   *L* 的第一 *p* 特征向量(对应于零特征值)是每个连通分量 *C <sub class="calibre20">1</sub> ，C <sub class="calibre20">2</sub> 所跨越的特征空间的指示向量，...，C* <sub class="calibre20">*p* 。</sub>

因此，如果数据集由 *M* 样本 *x <sub class="calibre20">i</sub> ∈ ℜ <sup class="calibre27">N</sup>* 组成，并且图 *G* 与亲和矩阵*w<sup class="calibre27">m×m</sup>t13】相关联，Shi 和 Malik 提议构建包含第一个 *p* 特征向量的矩阵*b∈ℜ<sup class="calibre27">m×p</sup>t17】事实上，每一行都代表了样本在 *p* 维子空间上的投影，其中非凸性由可以包含在规则球中的子区域表示。**

现在，让我们应用光谱聚类来分离由以下片段生成的二维正弦数据集:

```py
import numpy as np

nb_samples = 2000

X0 = np.expand_dims(np.linspace(-2 * np.pi, 2 * np.pi, nb_samples), axis=1)
Y0 = -2.0 - np.cos(2.0 * X0) + np.random.uniform(0.0, 2.0, size=(nb_samples, 1))

X1 = np.expand_dims(np.linspace(-2 * np.pi, 2 * np.pi, nb_samples), axis=1)
Y1 = 2.0 - np.cos(2.0 * X0) + np.random.uniform(0.0, 2.0, size=(nb_samples, 1))

data_0 = np.concatenate([X0, Y0], axis=1)
data_1 = np.concatenate([X1, Y1], axis=1)
data = np.concatenate([data_0, data_1], axis=0)
```

数据集如下图所示:

![](img/004e9580-699d-4995-b7ec-1ec7119c6e6e.png)

A sinusoidal dataset for the spectral clustering example

我们没有具体说明任何基本事实；然而，目标是分离两个正弦曲线(它们是非凸的)。很容易检查捕捉正弦曲线的球是否还包括属于其他正弦子集的许多样本。为了显示纯 K-means 和谱聚类(scikit-learn 实现了 Shi-Malik 算法，然后是 K-means 聚类)之间的区别，我们将训练两个模型，为后者使用带有 *γ = 2.0* ( `gamma`参数)的径向基函数(`affinity`参数)。当然，我邀请读者也来测试其他价值观和 KNN 亲和力。基于径向基函数的解决方案如下所示:

```py
from sklearn.cluster import SpectralClustering, KMeans

km = KMeans(n_clusters=2, random_state=1000)
sc = SpectralClustering(n_clusters=2, affinity='rbf', gamma=2.0, random_state=1000)

Y_pred_km = km.fit_predict(data)
Y_pred_sc = sc.fit_predict(data)
```

结果显示在下面的截图中:

![](img/12b75552-ecb4-4d0c-9600-18f9c56e6040.png)

Original dataset (left). Spectral clustering result (center). K-means result (right)

如您所见，K-means 沿着 x 轴用两个球分割数据集，而谱聚类成功地正确分离了两个正弦曲线。每当聚类的数量和 *X* 的维数都不太大时(在这种情况下，拉普拉斯算子的特征分解会变得非常昂贵)，该算法就非常强大。此外，由于该算法基于图形*切割*过程，因此它非常适合集群数量为偶数的情况。

# 均值漂移

让我们考虑让数据集*x∈ℜ<sup class="calibre27">m×n</sup>*(*m n*)维样本从多元数据生成过程 *p <sub class="calibre20">数据</sub>* 中提取。应用于聚类问题的**均值漂移**算法的目标是找到 *p <sub class="calibre20">数据</sub>* 最大的区域，并将周围子区域中包含的样本关联到同一聚类。由于 *p <sub class="calibre20">数据</sub>* 是一个 P **概率密度函数** ( **PDF** )，将其表示为以均值和方差等参数的小子集为特征的正则 PDF(例如高斯)的和是合理的。这样，样本就可以假设是由概率最高的 PDF 生成的。我们也将在[第 5 章](05.html)、*软聚类和高斯混合模型、*和[第 6 章](06.html)、*异常检测*中讨论这个过程。出于我们的目的，将问题重构为更新平均向量(质心)位置直到它们达到最大值的迭代过程是有帮助的。当质心到达它们的最终位置时，使用标准邻域函数将样本分配给每个聚类。

这个算法的第一步是确定一个合适的方法来近似 p <sub class="calibre20">数据</sub>。经典方法(将在[第 6 章](https://cdp.packtpub.com/hands_on_unsupervised_learning_with_python/wp-admin/post.php?post=26&action=edit#post_29)、*异常检测*中讨论)基于 **Parzen 窗口**的使用。目前来说，帕尔岑窗是一个非负核函数*f()*的特征是一个名为**带宽**的参数(更多细节，请查看《数理统计年鉴》帕尔岑 e .关于概率密度函数和模式的估计的原始论文*)。33、* 1962)。顾名思义，此类参数的作用是加宽或限制 Parzen 窗口接近其最大值的区域。考虑与高斯分布的类比，带宽具有与方差相同的作用。因此，小带宽将产生在平均值附近非常峰值的函数，而较大的值与更平坦的函数相关。不难理解，在这种特殊情况下，集群的数量隐含地由带宽决定，反之亦然。因此，大多数实现(如 scikit-learn)只使用一个参数，并计算另一个参数。考虑到这种算法被设计用于概率分布，自然的选择是指定期望的带宽或者让实现检测最优的带宽。这个过程可能看起来比强加特定数量的集群更复杂，但是，在许多实际情况下，特别是当基本事实至少部分已知时，测试不同带宽的结果更容易。

均值漂移最常见的选择是用 *n* 个平核的和( *n* 为质心数)来近似数据生成过程:

![](img/9a04b232-2baf-4182-93e4-2589ad0863d0.png)

因此，收敛后，每个样本由最近的质心表示。不幸的是，这种近似导致了不可能代表真实过程的分段函数。因此，最好使用基于相同底层内核的平滑 Parzen 窗口*K()*:

![](img/6d8ae609-2ad9-48b4-ab6a-58226436d1a1.png)

*K()*是平方距离(例如标准球)和带宽 *h* 的函数。有许多可能的候选函数可以使用，但是，当然，最明显的是高斯核(RBF)，其中 *h <sup class="calibre27">2</sup>* 扮演方差的角色。得到的近似现在非常平滑，其中 *n* 个峰值对应于质心(即平均值)。一旦定义了函数，就可以计算质心 *x <sub class="calibre20">1</sub> ，x <sub class="calibre20">2</sub> 的最佳位置，...，x <sub class="calibre20">n</sub>* 。

给定一个质心和一个邻域函数(为简单起见，我们假设使用半径为 *h* 和 *K(x) ≠ 0 ∀ x ∈ B <sub class="calibre20">r</sub>* 的标准球*b<sub class="calibre20">h</sub>T3】，相应的均值偏移向量定义为:*

![](img/56b8adef-5b58-4051-9ead-7105566eef4f.png)

可以看到，*m()*是用*K()*加权的所有邻域样本的平均值。显然，由于*K()*是对称的，并且与距离有关，当 *x <sub class="calibre20">i</sub>* 达到实际平均值时，*m()*将趋于稳定。带宽的作用是限制 *x <sub class="calibre20">i</sub>* 周围的区域。现在应该更清楚了，小值迫使算法引入更多的质心，以便将所有样本分配给一个聚类。相反，大带宽可能导致单个集群的*最终*配置。迭代过程从初始质心猜测 *x <sub class="calibre20">1</sub> <sup class="calibre27">(0)</sup> ，x <sub class="calibre20">2</sub> <sup class="calibre27">(0)</sup> 开始，...，x <sub class="calibre20">n</sub> <sup class="calibre27">(0)</sup>* 并用以下规则修正向量:

![](img/191cd488-c515-4840-b56f-2138d7929be0.png)

前面的公式很简单；在每一步，质心都移动(移动)到更靠近*m()*的位置。这样，由于*m**()*与相对于 *x <sub class="calibre20">i</sub>* 计算的邻域密度成正比，当 *x <sub class="calibre20">i</sub>* 到达概率最大的位置时，*m()→m<sub class="calibre20">最终</sub>* 不再需要更新。当然，收敛速度受到样本数量的强烈影响。对于非常大的数据集，该过程会变得非常慢，因为每个均值偏移向量的计算需要邻域的预先计算。另一方面，当聚类标准由数据密度定义时，该算法非常有用。

例如，现在让我们考虑一个合成数据集，其二维样本由三个具有对角协方差矩阵的多元高斯生成，如下所示:

```py
import numpy as np

nb_samples = 500

data_1 = np.random.multivariate_normal([-2.0, 0.0], np.diag([1.0, 0.5]), size=(nb_samples,))
data_2 = np.random.multivariate_normal([0.0, 2.0], np.diag([1.5, 1.5]), size=(nb_samples,))
data_3 = np.random.multivariate_normal([2.0, 0.0], np.diag([0.5, 1.0]), size=(nb_samples,))

data = np.concatenate([data_1, data_2, data_3], axis=0)
```

数据集如下图所示:

![](img/e081689d-97bc-4ef5-833e-17ddcc48e51d.png)

Sample dataset for the mean shift algorithm example

在这种情况下，我们知道基本事实，但我们想测试不同的带宽并比较结果。由于生成的高斯区域彼此非常接近，一些*外部*区域可以被识别为集群。为了集中研究最优参数，我们可以观察到平均方差(考虑不对称)为 1，因此，我们可以考虑值`h` = `0.9`、`1.0`、`1.2`和`1.5`。此时，我们可以实例化 scikit-learn 类`MeanShift`，通过参数`bandwidth`传递`h`值，如下所示:

```py
from sklearn.cluster import MeanShift

mss = []
Y_preds = []
bandwidths = [0.9, 1.0, 1.2, 1.5]

for b in bandwidths:
   ms = MeanShift(bandwidth=b)
    Y_preds.append(ms.fit_predict(data))
    mss.append(ms)
```

在密度分析之后，训练过程自动选择质心的数量和初始位置。不幸的是，这个数字通常比最后一个数字大(因为局部密度差异)；因此，该算法将优化所有质心，但是在完成之前，执行合并过程以消除所有那些与其他质心过于接近的质心(即，重复的质心)。Scikit-learn 提供了参数`bin_seeding`，通过根据带宽对样本空间进行离散化(宁滨)，可以加快研究速度。这样，就有可能在合理损失精度的情况下减少候选项的数量。

下图显示了四个培训过程结束时的结果:

![](img/9fddf7e1-0071-485e-bd33-c60a17b29d14.png)

Mean shift clustering results for different bandwidths

如您所见，带宽的微小差异会导致不同数量的集群。在我们的例子中，最佳值是`h=1.2`，这产生了一个结果，其中确定了三个不同的区域(加上一个包含潜在异常值的额外聚类)。最大簇的质心大致对应于实际平均值，但簇的形状不像任何高斯分布。这是一个缺点，可以通过使用其他方法解决(在[第 5 章](https://cdp.packtpub.com/hands_on_unsupervised_learning_with_python/wp-admin/post.php?post=26&action=edit#post_28)、*软聚类和高斯混合模型*中讨论)。实际上，均值漂移适用于局部邻域，并且 *p <sub class="calibre20">数据</sub>* 不被认为属于特定分布。因此，最终结果是将数据集非常精确地分割成高度密集的区域(请注意，最大分离不再是要求)，这也可以从多个标准分布的叠加中得出。没有任何预先假设，我们不能期望结果非常规则，但是，将这个算法与 VQ 进行比较，很容易注意到分配是基于找到每个密集斑点的最佳代表的想法。为此，由概率较低的高斯 *N(μ，σ)*生成的一些点被分配给质心比 *μ* 更具代表性的不同聚类。

# DBSCAN

**DBSCAN** 是另一种基于数据集密度估计的聚类算法。然而，与均值漂移相反，没有直接引用数据生成过程。在这种情况下，事实上，过程*从一般假设 *X* 由高密度区域(斑点)和低密度区域(斑点)分开开始，通过自下而上的分析建立样本之间的关系*。因此，DBSCAN 不仅需要最大分离约束，而且为了确定集群的边界，它还强制执行这样的条件。此外，该算法不允许指定期望的聚类数，这是 *X* 结构的结果，但是，类似于均值漂移，可以控制过程的粒度。

具体来说，DBSCAN 基于两个基本参数: *ε* ，其代表以样品 *x <sub class="calibre20">i</sub>* 为中心的球 *B <sub class="calibre20">ε</sub> (x <sub class="calibre20">i</sub> )* 和 *n <sub class="calibre20">min</sub>* ， 这是必须包含在*B<sub class="calibre20">ε</sub>(x<sub class="calibre20">I</sub>**)*中的最小样本数，以便将 *x <sub class="calibre20">i</sub>* 视为一个**核心点**(即一个可以作为一个聚类的实际成员的点)。 形式上，给定一个对一个集合中包含的样本数进行计数的函数*N()*，在以下情况下，样本 *x <sub class="calibre20">i</sub> ∈ X* 称为核心点:

![](img/20cd8208-d768-4e45-b2b4-b5943b8908eb.png)

所有点*x<sub class="calibre20">j</sub>∈B<sub class="calibre20">ε</sub>(x<sub class="calibre20">I</sub>**)*定义为**从 *x <sub class="calibre20">i</sub>* 直接密度可达**。这样的条件是点之间最强的关系，因为它们都属于以 *x* <sub xmlns:epub="http://www.idpf.org/2007/ops" class="calibre20">*i*</sub> 为中心的同一个球，并且*B<sub class="calibre20">ε</sub>(x<sub class="calibre20">I</sub>)*中包含的样本总数足够大，可以将邻域视为一个密集的子区域。而且，如果有顺序*x<sub class="calibre20">I</sub>→x<sub class="calibre20">I+1</sub>→...→ x* <sub xmlns:epub="http://www.idpf.org/2007/ops" class="calibre20">*j*</sub> 其中 *x <sub class="calibre20">i+1</sub>* 是从 *x <sub class="calibre20">i</sub>* 直接密度可达的(对于所有顺序配对)，*x*<sub xmlns:epub="http://www.idpf.org/2007/ops" class="calibre20">j</sub>被定义为**密度-从 *x <sub class="calibre20">i</sub>* 可达的**。这个概念非常直观，可以通过考虑下图立即理解:

![](img/01b82f74-374a-45b3-b789-3cb5d7d9e745.png)

The point x<sub class="calibre26">2</sub> is density-reachable from x<sub class="calibre26">0</sub> if n<sub class="calibre26">min</sub> = 4

如果我们将最小样本数设为 4，则**x<sub class="calibre20">0</sub>T3、**x<sub class="calibre20">1</sub>T7、**x<sub class="calibre20">2</sub>T11】为核心点，**x<sub class="calibre20">1</sub>T15】为从**x<sub class="calibre20">0</sub>T19】直接密度可达，**x<sub class="calibre20">2</sub>T23】为直接密度可达因此， **x <sub class="calibre20">2</sub>** 是从 **x <sub class="calibre20">0</sub>** 密度可达的。换句话说，这意味着可以定义一系列重叠的密集球(*N(≥N<sub class="calibre20">min</sub>*)从 **x <sub class="calibre20">0</sub>** 开始，到**x<sub class="calibre20">2</sub>T47】结束。这个概念可以通过增加一个进一步的定义扩展到属于球的所有其他点:给定一个点 *x <sub class="calibre20">k</sub>* ，点*x<sub class="calibre20">I</sub>T55】和*x<sub class="calibre20">j</sub>T59】是**密度连接的**，如果两者*x<sub class="calibre20">I</sub>T65】和 *x******************

理解这样的条件弱于密度可达性是很重要的，因为为了保证一个*密集链*，需要考虑第三个点，代表两个密集子区域之间的*连接器*。事实上，可以有两个密度连接点 *a* 和 *b* ，其中 *a* 不是从 *b* 密度可达的(反之亦然)。只要满足最小数量的样本条件，只在一个方向上移动，就会出现这种情况(也就是说，属于一个球的样本不是均匀分布的，而是倾向于聚集在一个小的超体积中)。

因此，例如，如果 *N(a) > > n <sub class="calibre20">min</sub> 和 N(a <sub class="calibre20">1</sub> ) < < N(a)* ，则过渡 *a → a <sub class="calibre20">1</sub>* 可以允许构建一个球 *B <sub class="calibre20">ε</sub> (a)* 也包含 *a <sub class="calibre20">1</sub>* (连同许多其他然而，在逆转换中 *a <sub class="calibre20">1</sub> → a、B<sub class="calibre20">ε</sub>**(a<sub class="calibre20">1</sub>)*不能足够密集以建立直接密度可达性条件。

因此，当在两个方向中的一个方向上移动时，更长的序列可能被*打破*，从而导致密度可达性的损失。现在应该更清楚了，两点*x<sub class="calibre20">I</sub>T5】和 *x <sub class="calibre20">j</sub>* 之间的密度连接允许我们避免这个问题，前提是有另一个点可以同时到达*x<sub class="calibre20">I</sub>T13】和*x<sub class="calibre20">j</sub>T17】。***

所有密度连通点对( *x <sub class="calibre20">i</sub> 、x <sub class="calibre20">j</sub>* )与 *x <sub class="calibre20">i</sub>* 、 *x <sub class="calibre20">j</sub> ∈ X* 将被分配到同一个集群 *C <sub class="calibre20">t</sub>* 。而且，如果*X<sub class="calibre20">k</sub>∈C<sub class="calibre20">t</sub>*，则从 *x <sub class="calibre20">k</sub>* 密度可达的所有点 *x <sub class="calibre20">p</sub> ∈ X* 也将属于同一簇。不能从任何其他点 *x <sub class="calibre20">i</sub> ∈ X* 密度可达的点 *x <sub class="calibre20">n</sub>* 被定义为**噪声点**。因此，与其他算法相反，DBSCAN 输出 *n* 聚类加上一个包含所有噪声点的附加集合(这些噪声点不一定被视为异常值，而是不属于任何密集子区域的点)。当然，由于噪音点没有标签，它们的数量应该相当低；因此，调整参数 *ε* 和*n<sub class="calibre20">min</sub>T49】很重要，有双重目标:最大化衔接和分离，避免太多的点被标记为嘈杂。没有标准规则来实现这样的目标，因此我建议在做出最终决定之前测试不同的值。*

最后，重要的是要记住，DBSCAN 可以处理非凸几何图形，与均值漂移相反，它假设存在被低密度区域包围的高密度区域。此外，它的复杂性与所采用的 KNN 方法(蛮力、球树或 kd 树)密切相关。一般来说，在数据集不太大的情况下，平均性能在 *O(N log N)* 左右，但在 *N* 很大的情况下，可以倾向于 *O(N <sup class="calibre27">2</sup> )* 。另一个需要记住的重要因素是样本的维度。正如我们已经讨论过的，高维度量可以减少两点的可区分性，从而对 KNN 方法的性能产生负面影响。因此，当维度非常高时，应该避免(或者至少仔细分析)数据库扫描，因为生成的聚类不能有效地表示实际的密集区域。

在展示一个具体的例子之前，有必要介绍一种在不明真相的情况下可以使用的评估方法。

# 卡林斯基-哈拉巴斯茨评分

让我们假设已经对包含 *M* 样本的数据集 *X* 应用了聚类算法，以便将其分割成*n<sub class="calibre20">c</sub>T7】聚类*c<sub class="calibre20">I</sub>t11 】,由质心 *μ <sub class="calibre20">i</sub> ∀ i = 1 表示..n <sub class="calibre20">c</sub>* 。我们可以将**W****I-Cluster distribution**(**WCD**)**定义如下:****

 **![](img/741ff380-397d-4cc0-b3e3-e2386ba5783e.png)

如果 *x <sub class="calibre20">i</sub>* 是一个 *N* 维列向量，*x<sub class="calibre20">k</sub>∈ℜ<sup class="calibre27">n×n</sup>*。不难理解 *WCD(k)* 编码了关于聚类的*伪方差*的全局信息。如果满足最大内聚力条件，我们期望质心周围的分散有限。另一方面， *WCD(k)* 甚至可能受到包含异常值的单个聚类的负面影响。因此，我们的目标是在每种情况下最小化 *WCD(k)* 。以类似的方式，我们可以定义一个 **B** **团簇间分散** ( **BCD** )为:

![](img/35bb2ca3-fcae-4e7b-9943-107c6e61c69d.png)

在前面的公式中， *N(C <sub class="calibre20">i</sub> )* 是分配给聚类*C<sub class="calibre20">I</sub>T7】的元素个数， *μ* 是整个数据集的全局质心。考虑到最大分离的原则，我们希望密集的区域远离全局质心。 *BCD(k)* 精确表达了这一原理，因此我们需要将其最大化以获得更好的性能。*

**卡林斯基-哈拉巴斯兹评分**定义为:

![](img/3c40be02-03d1-4795-a6e8-ecba3faac6cb.png)

引入了对预测标签的显式依赖，因为质心的计算不被认为是聚类算法的一部分。分数没有绝对的意义，而是有必要比较不同的值，以了解哪种解决方案更好。显然，*CH<sub class="calibre20">k</sub>()*越高，聚类性能越好，因为这样的条件意味着更大的分离和更大的内部凝聚力。

# 使用数据库扫描分析工作缺勤数据集

“工作缺勤”数据集(按照本章开头的说明下载)由 740 条记录组成，这些记录包含休假员工的信息。共有 20 个属性，代表年龄、服务时间、教育程度、习惯、疾病、违纪行为、交通费用、从家到办公室的距离等(字段的完整描述可在[https://archive.ics.uci.edu/ml/datasets/Absenteeism+at+work](https://archive.ics.uci.edu/ml/datasets/Absenteeism+at+work)获得)。我们的目标是对数据进行预处理并应用数据库扫描，以便发现具有特定语义内容的密集区域。

第一步是如下加载 CSV 文件(必须更改占位符`<data_path>`以指向文件的实际位置):

```py
import pandas as pd

data_path = '<data_path>\Absenteeism_at_work.csv'

df = pd.read_csv(data_path, sep=';', header=0, index_col=0).fillna(0.0)
print(df.count())
```

前一个命令的输出如下:

```py
Reason for absence                 740
Month of absence                   740
Day of the week                    740
Seasons                            740
Transportation expense             740
Distance from Residence to Work    740
Service time                       740
Age                                740
Work load Average/day              740
Hit target                         740
Disciplinary failure               740
Education                          740
Son                                740
Social drinker                     740
Social smoker                      740
Pet                                740
Weight                             740
Height                             740
Body mass index                    740
Absenteeism time in hours          740
dtype: int64
```

其中一些特征是分类的，并用连续整数编码(例如，`Reason for absence`、`Month of absence`，等等)。由于这些值可能会影响距离，但没有精确的语义原因(例如，`Month=12`比`Month=10`大，但两个月在距离方面是相等的)，我们需要在继续下一步之前对所有这些特征进行一次热编码(新特征将被附加在列表的末尾)。在下面的片段中，我们使用`get_dummies()`熊猫函数来执行编码；然后删除原始列:

```py
import pandas as pd

cdf = pd.get_dummies(df, columns=['Reason for absence', 'Month of absence', 'Day of the week', 'Seasons', 'Disciplinary failure', 'Education', 'Social drinker', 'Social smoker'])

cdf = cdf.drop(labels=['Reason for absence', 'Month of absence', 'Day of the week', 'Seasons', 'Disciplinary failure', 'Education', 'Social drinker', 'Social smoker']).astype(np.float64)
```

一次热编码的结果通常会产生平均值之间的差异，因为许多特征将被限制为 0 或 1，而其他特征(例如年龄)可以具有更大的范围。因此，最好将平均值标准化(在不影响标准偏差的情况下，保持不变是有帮助的，因为它们与现有的信息内容成比例)。这一步可以通过`StandardScaler`类设置参数`with_std=False`来实现，如下所示:

```py
from sklearn.preprocessing import StandardScaler

ss = StandardScaler(with_std=False)
sdf = ss.fit_transform(cdf)
```

此时，像往常一样，我们可以使用 t-SNE 算法来降低数据集的维度(使用`n_components=2`)并可视化结构。数据框`dff`将包含原始数据集和 t-SNE 坐标，如下所示:

```py
from sklearn.manifold import TSNE

tsne = TSNE(n_components=2, perplexity=15, random_state=1000)
data_tsne = tsne.fit_transform(sdf)

df_tsne = pd.DataFrame(data_tsne, columns=['x', 'y'], index=cdf.index) 
dff = pd.concat([cdf, df_tsne], axis=1)
```

下面的截图显示了结果图:

![](img/dbcdbb62-2632-4b58-b8ab-fcecec940d5d.png)

t-SNE bidimensional representation of the Absenteeism at Work dataset

在任何考虑之前，重要的是要重复 t-SNE 产生一个最佳的低维表示，但是总是有必要在原始数据集上测试算法，以便检查由 t-SNE 识别的邻居是否对应于实际的团块。特别地，考虑到 DBSCAN 的结构，考虑到 t-SNE 表示，ε值可以是合理的，但是当移动到更高维空间时，球不能再捕获相同的样本。然而，前面的图显示了被空白空间包围的密集区域的存在。不幸的是，密度不太可能是均匀的(这是 DBSCAN 的建议要求之一，因为 *ε* 和*n<sub class="calibre20">min</sub>T5】的值都不能改变)，但是，在这种情况下，我们假设所有斑点的密度都是恒定的。*

为了找到最适合我们的配置，我们使用闵可夫斯基度量将簇的数量、噪声点的数量、轮廓分数和卡林斯基-哈拉巴斯分数绘制为ε的函数，其中 *p=2* 、 *p=4* 、 *p=8* 和 *p=12、*如下图所示:

![](img/2e0b2920-1c6a-414b-b922-8b19c762b6c6.png)

Evaluation metrics as functions of ε

剪影和 Calinski-Harabasz 都基于凸簇的假设(例如，色散显然是一种假设样本围绕质心径向分布的度量)，因此它们在非凸情况下的期望值通常较小。但是，我们希望最大化这两个分数(剪影→ 1 和 Calinski-Harabasz → ∞)，同时避免大量的聚类。考虑到我们的初始目标(寻找以一组特定特征为特征的内聚集群)，我们选择了 *ε=25* 和一个具有 *p=12* 的闵可夫斯基度量，这产生了合理数量的集群(13)和 22 个噪声点。在[第二章](02.html)、*聚类基本面、*中我们已经表明，当 *p → ∞* (但对于 *p > 2* 效果已经可见)时，距离趋于最大特征差。

因此，这种选择应该总是通过上下文分析来证明。在这种情况下，我们可以假设每个(非)凸斑点代表一个由特定特征支配的类别(所有其他特征的次要贡献)，因此 *p=12* (导致 17 个聚类)对于中粗粒度分析(考虑到有 20 个属性)来说是一个很好的权衡。此外， *ε=22.5* 与最高的卡林斯基-哈拉巴斯分数之一 129.3 以及大约等于 0.2 的轮廓分数相关联。特别是，后一个值表示总体聚类是合理正确的，但是可能有重叠。由于底层几何很可能是非凸的，这样的结果是可以接受的(通常在凸的情况下是不可接受的)，同时考虑到具有相应峰值的 Calinski-Harabasz 分数。ε的较大值产生略高的轮廓分数(小于 0.23)，但所得的簇数和卡林斯基-哈巴斯兹分数都不受所得构型的影响。必须明确的是，这一选择没有得到任何外部证据的证实，必须通过对结果的语义分析进行验证。如果需要精细的分析，具有更多簇和更多噪声点的配置也是可以接受的(因此，读者可以使用值来玩*，并且还可以提供结果的解释)。然而，这个例子的最终目标保持不变:分割数据集，以便每个聚类包含特定的(可能是唯一的)属性。*

 *我们现在可以实例化一个`DBSCAN`模型，并使用包含规范化特征的数组`sdf`对其进行训练。配置为 *ε=25* (参数`eps`)和 *n <sub class="calibre20">min</sub> =3* (参数`min_samples`)，Minkowski 公制(`metric='minkowski'`)带`p=12`。

我们现在可以执行以下集群:

```py
from sklearn.cluster import DBSCAN
from sklearn.metrics import silhouette_score, calinski_harabaz_score

ds = DBSCAN(eps=25, min_samples=3, metric='minkowski', p=12)
Y_pred = ds.fit_predict(sdf)

print('Number of clusters: {}'.format(np.max(Y_pred) + 1))
print('Number of noise points: {}'.format(np.sum(Y_pred==-1)))

print('Silhouette score: {:.3f}'.format(silhouette_score(dff, Y_pred, metric='minkowski', p=12)))
print('Calinski-Harabaz score: {:.3f}'.format(calinski_harabaz_score(dff, Y_pred)))
```

由于`DBSCAN`用标签`-1`标记了有噪声的点，所以上一个片段的输出如下:

```py
Number of clusters: 13
Number of noise points: 22

Silhouette score: 0.2
Calinski-Harabaz score: 129.860
```

下面的截图显示了结果图:

![](img/bb182b6f-ada9-4a14-a2a9-6183d443158b.png)

Clustering result for the Absenteeism at Work dataset

如您所见(我建议运行代码，以便获得更好的视觉确认)，大多数孤立的(即使在 t-SNE 图中没有内聚性)区域已被成功检测到，并且样本已被分配到同一聚类。我们还可以观察到两个基本结果:噪声点(用十字标记)在 t-SNE 表示中不是孤立的，一些簇是部分分裂的。这不是算法的失败，而是降维的直接后果。在原始空间中，所有噪声点实际上都没有与任何其他样本进行密度连接，但它们可能会在 t-SNE 图中出现重叠或靠近某些斑点。然而，我们对高密度和准内聚的非凸区域感兴趣，幸运的是，它们在二维图中看起来也是连接的。

现在让我们考虑两个不同的区域(为简单起见，将分析限制在一次热编码后的前 10 个属性)。第一个是二维区域`x < -45`，如下:

```py
sdff = dff[(dff.x < -45.0)]
print(sdff[sdff.columns[0:10]].describe())
```

输出的漂亮打印版本如下图所示:

![](img/df4d9058-6795-4cb7-a86a-d2a4cc46aa79.png)

Statistical measures corresponding to the subdataset x < -45

有两个因素可以立即引起我们的注意:运输费用(似乎标准化为 179 的值)和儿子的数量(考虑到平均值和标准偏差，大多数样本对应于 0)。让我们也考虑服务时间和从住所到工作地点的距离，这可以帮助我们找到集群的语义标签。所有其他参数都不太具有歧视性，我们在这一简要分析中将它们排除在外。因此，我们可以假设这样一个亚集群包含 40 岁左右没有孩子的人，服务时间大，住的离办公室相当远(我请读者查看整体统计数据来证实这一点)，交通费用标准化(例如，一次付清的车费)。

现在让我们将此结果与区域 *-20 < x < 20* 和 *y < 20* 进行比较，如下所示:

```py
sdff = dff[(dff.x > 20.0) & (dff.y > -20.0) & (dff.y < 20.0)]
print(sdff[sdff.columns[0:10]].describe())
```

相应的输出如下:

![](img/21f51b34-043b-4238-8312-2a42582de788.png)

Statistical measures corresponding to the subdataset -20 < x < -20 and y < 20

在这种情况下，交通费用较大，而从住所到工作地点的距离大约是前面例子的一半(也考虑了标准差)。而且儿子平均数量为 1 个，有两个孩子的员工比例适中，服务时间约为 12 个，标准差为 3.6。我们可以推断，这个聚类包含了年龄范围内(28–58 岁)有家庭的(已婚)人的所有样本，他们住得离办公室相对较近，但旅行费用较高(例如，由于使用出租车服务)。这类员工倾向于避免加班，但他们的平均工作量几乎与前面例子中观察到的相同。即使没有正式确认，我们也可以假设这样的员工通常更有效率，而第一组包含有生产力的员工，然而，他们需要更多的时间来实现他们的目标(例如，因为旅行时间更长)。

这显然不是详尽的分析，也不是一套客观的陈述。目标是展示如何通过观察样本的统计特征来找到聚类的语义内容。在现实生活中，所有的观察都必须由专家(例如，人力资源经理)验证，以了解分析的最后部分(特别是语义上下文的定义)是否正确，或者是否有必要使用大量的集群、不同的指标或其他算法。作为练习，我邀请读者分析包含单个聚类的所有区域，以便完成一幅大图，并测试不同类别(例如，非常年轻的人、有三个孩子的员工等)对应的人工样本的预测。

# 作为性能指标的集群不稳定性

聚类不稳定性是冯·卢克斯堡(在*聚类稳定性:概述，冯·卢克斯堡 u，arXiv 1007:1075v1，2010* 中)提出的一种方法，可以衡量算法相对于特定数据集的优良性。它可以用于不同的目的(例如，调整超参数或找到最佳的集群数量)，并且相对容易计算。该方法基于这样的思想，即满足最大内聚和分离要求的聚类结果也应该对数据集的噪声扰动具有鲁棒性。换句话说，如果数据集 *X* 已经被分割成聚类集 *C* ，则导出的数据集 *X <sub class="calibre20">n</sub>* (基于特征的小扰动)应该被映射到相同的聚类集。如果不满足这个条件，通常有两种可能:噪声扰动太强或者算法对小的变化太敏感，因此不是很稳定。因此，我们定义了原始数据集 *X* 的一组 *k* 扰动(或二次采样)版本:

![](img/9c11b1f1-7355-49a9-b593-74696be63e29.png)

如果我们应用一个算法 *A* 产生相同数量的簇*n<sub class="calibre20">c</sub>T5】，我们可以定义一个距离度量*d()*在 *A(X <sub class="calibre20">i</sub> )* 和 *A(X <sub class="calibre20">j</sub> )* 之间，它测量不一致分配的数量(即 *A(X <sub class="calibre20">i</sub> )* 因此，*d()*可以简单地计算不同标签的数量，假设算法(如有必要)以相同的方式播种，并且数据集显然没有被打乱。算法的不稳定性(相对于 *X* 的 *k* 噪声变化)定义为:*

![](img/f7d41112-b38c-4121-b764-1979a72dd4e4.png)

因此，不稳定性是噪声变化对的聚类结果之间的平均距离。当然，这个值不是绝对的，所以有可能导出的规则是:选择产生最小不稳定性的配置。同样重要的是，这种方法不能与前面讨论的其他方法相比，因为它基于其他超参数(噪声变化的数量、噪声均值和方差、子采样比等)，因此当 *A* 和 *X* 固定时，它也可以产生不同的结果。特别是噪声的大小会极大地改变不稳定性，因此在确定例如高斯噪声的 *μ* 和*σ*之前，需要评估 *X* 的均值和协方差矩阵。在我们的示例中(基于工作缺勤数据集中的 DBSCAN 聚类)，我们创建了 20 个扰动版本，从一个加性噪声项*N<sub class="calibre20">I</sub>∞N(E[X]*， *Cov(X)/4* )开始，并应用从均匀分布 *U(0，1)* 采样的乘法掩码。这样，一些噪声项会被随机抵消或减少，如以下代码所示:

```py
import numpy as np

data = sdf.copy()

n_perturbed = 20
n_data = []

data_mean = np.mean(data, axis=0)
data_cov = np.cov(data.T) / 4.0

for i in range(n_perturbed):
    gaussian_noise = np.random.multivariate_normal(data_mean, data_cov, size=(data.shape[0], ))
    noise = gaussian_noise * np.random.uniform(0.0, 1.0, size=(data.shape[0], data.shape[1]))
    n_data.append(data.copy() + noise)
```

在这种情况下，我们希望将不稳定性计算为 *ε* 的函数，但是可以用任何其他算法和超参数重复该示例。此外，我们使用归一化汉明距离，它与两个聚类结果之间不一致分配的数量成比例，如下所示:

```py
from sklearn.cluster import DBSCAN
from sklearn.metrics.pairwise import pairwise_distances

instabilities = []

for eps in np.arange(5.0, 31.0, 1.5):
    Yn = []

    for nd in n_data:
        ds = DBSCAN(eps=eps, min_samples=3, metric='minkowski', p=12)
        Yn.append(ds.fit_predict(nd))

    distances = []

    for i in range(len(Yn)-1):
        for j in range(i, len(Yn)):
            d = pairwise_distances(Yn[i].reshape(-1, 1), Yn[j].reshape(-1, 1), 'hamming')
            distances.append(d[0, 0])

    instability = (2.0 * np.sum(distances)) / float(n_perturbed ** 2)
    instabilities.append(instability)
```

结果如下图所示:

![](img/6d7e3937-f8a3-42e5-b908-fdead50ad71c.png)

Cluster instability of DBSCAN applied to the Absenteeism at Work dataset as a function of ε

大约 *ε < 7* 的值为空。这样的结果是由于算法产生了大量的聚类和噪声样本。由于样本分布在不同的区域，小的扰动不能改变分配。对于 *7 <* *ε < 17* ，我们观察到正斜率达到最大值对应约 *ε = 12.5* ，随后负斜率达到最终值 0。在这种情况下，集群变得越来越大，包括越来越多的样本；然而，当 *ε* 仍然太小时，*密度可达性链*很容易被小扰动打破(也就是说，样本可以克服球的边界，因此它被排除在簇之外)。因此，在应用加性噪声之后，样本通常被分配给不同的聚类。这种现象在 *ε = 12.5* 时达到最大，然后开始变得不太显著。

事实上，当 *ε* 足够大时，球的结合能够*包裹*整个簇，为小扰动留下*足够的自由空间*。当然，在取决于数据集的阈值之后，将只产生单个聚类，并且如果噪声不是太强，任何扰动版本将产生相同的分配。在我们的具体情况下， *ε = 25* 保证了高稳定性，这也是 t-SNE 图所证实的。一般来说，这种方法可以用于所有算法和几何图形，但我建议在决定如何创建扰动版本之前，对 *X* 进行彻底分析。事实上，一个错误的决定可能会损害结果，产生大的/小的不稳定性，这并不表示表现不好/好。特别是，当聚类具有不同的方差时(例如，在高斯混合中)，加性噪声项对某些样本的影响可以忽略不计，而它可以完全改变其余样本的结构。在这些情况下，这种方法比其他方法弱，应该使用例如二次采样和具有非常小方差(通常小于最小聚类(共)方差)的高斯噪声。另一方面，在基于密度的算法中，二次采样显然是非常危险的，在这种算法中，由于可达性的丧失，一个小的聚类可能变成一组孤立的噪声点。我邀请读者也用 K 均值来测试这个方法，以便找到最佳的聚类数(这通常与最小的不稳定性有关)。

# k-水母

在前一章中，我们已经表明，当簇的几何形状是凸的时，K 均值通常是一个很好的选择。然而，这种算法有两个主要的局限性:度量总是欧几里德的，并且对异常值不太鲁棒。第一个要素是显而易见的，而第二个要素是质心性质的直接结果。事实上，K-means 选择形心作为不能成为数据集一部分的实际均值。因此，当一个聚类有一些异常值时，平均值会受到影响，并成比例地向它们移动。下图显示了一个示例，其中一些异常值的存在迫使质心到达密集区域之外的位置:

![](img/cb3a0724-e734-4782-a2dc-aa7ecfe5abda.png)

Example of centroid selection (left) and medoid selection (right)

**K-medoids** (在*借助 medoids 进行聚类，Kaufman L .，rousseuw p . j .，在基于 L1-诺姆和相关方法的统计数据分析中，North-Holland，* 1987)最初是为了缓解对异常值缺乏鲁棒性的问题(在原始论文中，该算法被设计为仅适用于 Manhattan 度量)，但后来不同的版本被设计为允许使用任何度量(特别是任意 Minkowski 度量)。与 K 均值的主要区别在于质心的选择，在这种情况下，质心是始终属于数据集的示例性样本(称为**水母**)。该算法本身非常类似于标准的 K-means，并且通过将样本重新分配到具有最接近的 medoid 的聚类中，来替换 med oid 的定义*μ<sub class="calibre20">I</sub>= X<sub class="calibre20">I</sub>∈X*(作为最小化与分配给聚类的所有其他样本的平均或总距离的元素 *C <sub class="calibre20">i</sub>* )。

很容易理解，离群值不再有很高的权重，因为与标准质心相反，它们被选择为水母的概率接近于零。另一方面，当一个聚类由不能被归类为异常值的*远*样本包围的密集斑点组成时，K-水母的表现较差。在这种情况下，算法可能会错误地分配这些样本，因为它无法生成能够捕获它们的*虚拟球*(请记住，半径是由质心/水母的相互位置隐式定义的)。因此，虽然 K-means 可以将质心移动到非密集区域以便也捕获远点，但是当密集斑点包含许多点时，K-med oid 不太可能以这种方式表现。

此外，K-medoids 倾向于聚集密度具有两个峰值的非常重叠的斑点，而 K-means 通常根据平均值的位置将整个区域分成两部分。如果凸几何的假设成立，这种行为通常会被接受，但在其他情况下，这可能是一种限制(我们将在示例中展示这种效果)。

最后一个基本区别是公制距离。由于没有限制，K-甲状旁腺素或多或少可以具有攻击性。正如我们在[第 2 章](02.html)、*聚类基本原理的开头所讨论的，*最长的距离是由 Manhattan 度量提供的(它以相同的方式评估每个不同的组件)，而当 *p* 增加时(在通用的 Minkowski 度量中)，组件之间的最大差异变得占主导地位。K-means 是基于最常见的折衷(欧几里德距离)，但是当更大的 *p* 可以导致更好的性能时(当比较 *p=1* 和 *p > 1* 时，效果更明显)，也有一些特殊的情况。例如，如果 *c <sub class="calibre20">1</sub> =(0，0)* ， *c <sub class="calibre20">2</sub> =(2，1)* ，以及 *x=(0.55，1.25)* ，曼哈顿距离 *d <sub class="calibre20">1</sub> (x，c <sub class="calibre20">1</sub> )* 和 *d <sub class="calibre20">1</sub> 因此，当 *p=1* 时，该点被分配给第二个簇，而当 *p=2* 时，该点被分配给第一个簇。*

一般来说，预测正确的值 *p* 并不容易，但是总是有可能使用轮廓和调整后的兰德分数等方法测试几种配置，并选择产生更好分割的配置(即最大内聚和分离或更高的调整后兰德分数)。在我们的示例中，我们将生成一个包含基本事实的数据集，因此我们可以使用后一个选项轻松评估性能。因此，我们将使用函数`make_blobs`生成`1000`样本，这些样本在由`[-5.0, 5.0]`限定的框中被分割成`8`斑点，如下所示:

```py
from sklearn.datasets import make_blobs

nb_samples = 1000
nb_clusters = 8

X, Y = make_blobs(n_samples=nb_samples, n_features=2, centers=nb_clusters, 
                  cluster_std=1.2, center_box=[-5.0, 5.0], random_state=1000)
```

生成的数据集呈现出一些强烈的重叠(如最终图所示)，因此我们不期望使用对称方法获得高水平的结果，但我们对比较 K-means 和 K-med oid 所做的分配感兴趣。

让我们开始评估通过 K-means 达到的调整后的 Rand 分数，如下所示:

```py
from sklearn.cluster import KMeans
from sklearn.metrics import adjusted_rand_score

km = KMeans(n_clusters=nb_clusters, random_state=1000)
C_km = km.fit_predict(X)

print('Adjusted Rand score K-Means: {}'.format(adjusted_rand_score(Y, C_km)))
```

上一个块的输出如下:

```py
Adjusted Rand score K-Means: 0.4589907163792297
```

这个值足以理解 K-means 正在进行许多错误的赋值，尤其是在重叠区域。由于使用这种方法很难对数据集进行聚类，因此我们不将这一结果视为真正的指标，而仅将其视为可与 K-medoids 评分进行比较的衡量标准。现在让我们使用带有`p = 7`的闵可夫斯基度量来实现该算法(请读者更改该值并检查结果)，如下所示:

```py
import numpy as np

C = np.random.randint(0, nb_clusters, size=(X.shape[0], ), dtype=np.int32)
mu_idxs = np.zeros(shape=(nb_clusters, X.shape[1]))

metric = 'minkowski'
p = 7
tolerance = 0.001

mu_copy = np.ones_like(mu_idxs)
```

数组`C`包含赋值，`mu_idxs`将包含水母。由于存储整个 medoids 所需的空间通常很小，我们更喜欢这种方法，而不是只存储索引。优化算法如下:

```py
import numpy as np

from scipy.spatial.distance import pdist, cdist, squareform
from sklearn.metrics import adjusted_rand_score

while np.linalg.norm(mu_idxs - mu_copy) > tolerance:
    for i in range(nb_clusters):
        Di = squareform(pdist(X[C==i], metric=metric, p=p))
        SDi = np.sum(Di, axis=1)

        mu_copy[i] = mu_idxs[i].copy()
        idx = np.argmin(SDi)
        mu_idxs[i] = X[C==i][idx].copy()

    C = np.argmin(cdist(X, mu_idxs, metric=metric, p=p), axis=1)

print('Adjusted Rand score K-Medoids: {}'.format(adjusted_rand_score(Y, C)))
```

行为很简单。在每次迭代中，我们计算属于一个聚类的所有元素之间的成对距离(这实际上是最昂贵的部分)，然后我们选择最小化总和的 med oid`SDi`。一个周期后，我们通过最小化样本与水母的距离来分配样本。重复这些操作，直到水母的范数变化变得小于预定阈值。调整后的兰德分数如下:

```py
Adjusted Rand score K-Medoids: 0.4761670824763849
```

The final adjusted Rand score is influenced by the random initialization of the algorithm (hence, the previous result can slightly change when running the code). In real applications, I suggest employing a double stopping criterion based on the maximum number of iterations together with a small tolerance.

因此，即使重叠没有被解决，性能也比 K-means 稍好。地面实况、K-均值和 K-med oid 结果显示在下面的截图中:

![](img/4f1d33c8-b0db-47c7-aa13-07185fdef096.png)

Ground truth (left), K-means (center), and K-medoids (right)

正如你所看到的，地面真相包含两个重叠的区域，极难聚集。在这个特殊的例子中，我们对解决这个问题不感兴趣，而是想展示这两种方法的不同行为。如果我们考虑前两个斑点(左上角)，K-means 将整个区域分成两部分，而 K-medoids 将所有元素分配给同一个聚类。在不知道基本事实的情况下，后一个结果可能比第一个结果更连贯。事实上，观察第一个图，可能会注意到密度差并没有强到完全证明分裂是合理的(然而，在某些情况下，这可能是合理的)。由于该区域相当密集，并且与其邻居分隔开，因此单个集群可能是预期的结果。此外，几乎不可能根据相异度来区分样本(大多数靠近分隔线的样本被错误分配)，因此 K-med oid 比 K-means 的攻击性更小*，显示出更好的折衷。相反，第二个重叠区域(右下角)由两种算法以几乎相同的方式管理。这是因为 K-means 将质心放置在非常靠近一些实际样本的位置。在这两种情况下，算法需要在 0 和 4 之间创建几乎水平的间隔，因为否则不可能分割区域。这种行为在所有基于标准球的方法中都很常见，在这种特殊情况下，这是极其复杂的几何图形的正常结果(许多相邻的点有不同的标签)。因此，我们可以得出结论说，K-med oid 对异常值更稳健，有时通过避免不希望的分离比 K-means 表现得更好。另一方面，当在没有异常值的非常密集的区域中工作时，这两种算法(特别是当采用相同的度量时)是等效的。作为练习，我邀请读者使用其他指标(包括余弦距离)并比较结果。*

 *# 在线聚类

有时，数据集太大，无法放入内存，或者样本通过通道以不同的时间步长传输和接收。在这种情况下，前面讨论的算法都不能使用，因为它们假设从第一步开始就可以访问整个数据集。由于这个原因，已经提出了一些在线替代方案，它们目前正在许多现实生活的过程中实施。

# 小批量 K 均值

该算法是标准 K-means 的扩展，但是，由于质心不能用所有样本计算，当现有聚类不再有效时，有必要包括负责重新分配样本的附加步骤。特别是，**小批量 K 均值**不是计算全局均值，而是处理流均值。一旦接收到批次，算法计算部分平均值并确定质心的位置。然而，不是所有的聚类都有相同数量的分配，因此算法必须决定是等待还是重新分配样本。

这个概念可以通过考虑一个非常低效的流过程来立即理解，该流过程开始发送属于一个半空间的所有样本，并且只包括属于互补半空间的几个点。由于簇的数量是固定的，算法将开始优化质心，同时只考虑一个子区域。让我们假设质心已经被放置在球的中心，球围绕着属于互补子空间的几个样本。如果越来越多的批次继续向密集区域添加点，算法可以合理地决定丢弃孤立质心并重新分配样本。然而，如果该过程开始发送属于互补半空间的点，算法必须准备好将它们分配给最合适的簇(即，它必须将其他质心放置在空区域中)。

该方法通常基于名为**再分配比** *α* 的参数。当 *α* 较小时，算法会等待较长的时间再重新分配样本，而较大的值会加速这一过程。当然，我们希望避免这两种极端情况。换句话说，我们需要避免一个过于静态的算法，它在做出决定之前需要很多样本，同时也需要避免一个过于快速变化的算法，它在每批之后重新分配样本。一般来说，第一种情况以较低的计算成本产生次优解，而后者可能变得非常类似于在每次批处理后重新应用于流式数据集的标准 K 均值。考虑到这种通常与实时过程相关的场景，我们通常对需要高计算成本的极其精确的解决方案不感兴趣，而是对在收集新数据时得到改进的良好近似感感兴趣。

然而，重新分配比率的选择必须考虑到每一个单独的上下文，包括流过程的合理预测(例如，它纯粹是随机的吗？样本是独立的吗？某些样本在某个时间段内更频繁吗？).此外，必须群集的数据量(即批处理大小，这是一个极其重要的因素)，当然还有可以调配的硬件。一般来说，当批量不是太小(但这通常不是一个可控的超参数，因为它依赖于外部资源)并且相应地选择重新分配比率时，可以证明小批量 K-means 产生的结果与标准 K-means 相当，具有更低的内存需求和更高的计算复杂度。

相反，当从真实数据生成过程中对批次进行统一采样时，重新分配比率成为不太重要的参数，并且其影响较低。事实上，在这些情况下，批次大小通常是获得良好结果的主要影响因素。如果它足够大，算法能够立即确定质心的最可能位置，并且后续批次不能显著改变这种配置(因此减少了连续重新分配的需要)。当然，在在线场景中，很难确定数据生成过程的结构，因此通常只能假设一个批次(如果不是太小的话)包含足够的每个独特区域的代表。数据科学家的主要任务是通过收集足够的样本来执行完整的 K 均值，并将性能与小批量版本进行比较，从而验证这一假设。观察到较小批量产生更好的最终结果的场景(具有相同的重新分配比率)并不奇怪。这种现象可以通过考虑这种算法不会立即重新分配样本来理解；因此，有时，较大的批次可能会导致错误的配置，然而，具有更多的代表，因此重新分配的概率较低(也就是说，算法更快，但准确性较低)。相反，在相同的情况下，由于频繁的重新分配，较小的批次可以迫使算法执行更多的迭代，从而得到(更精确的)最终配置。由于定义一个通用的经验法则并不容易，一般的建议是在做出决定之前检查不同的值。

# 桦树

该算法(其名称代表**使用层次结构的平衡迭代约简和聚类**)比小批量 K-means 具有稍微复杂的动态性，并且最后一部分采用了我们将在[第 4 章](04.html)、*行动中的层次聚类*中介绍的方法(**层次聚类**)。然而，就我们的目的而言，最重要的部分涉及数据准备阶段，该阶段基于称为**聚类**或**特征-特征树** ( **CF-Tree** )的特定树结构。给定一个数据集 *X* ，树的每个节点都由三个元素的元组组成:

![](img/704067e3-6d6d-4dc0-89b4-a834109a4585.png)

特征元素分别是属于一个节点的样本数、所有样本的和以及平方范数的和。这一选择背后的原因将立即清楚，但现在让我们将注意力集中在树的结构上，以及如何在试图平衡高度的同时插入新元素。在下图中，有一个通用的 CF-Tree 表示，其中所有终端节点都是实际的子集群，为了获得所需数量的集群，必须对其进行合并:

![](img/3a61d3c9-22b9-482c-a79f-95b35685736e.png)

Example of a simple CF-Tree with a binary repartition

在上图中，点代表指向子节点的指针。因此，每个非终端节点都与指向其所有子节点的指针存储在一起( *CF <sub class="calibre20">i</sub> ，p <sub class="calibre20">i</sub>* )，而终端节点是纯 CFs。为了讨论插入策略，必须考虑另外两个因素。第一个叫做**分支因子** *B* ，第二个叫做**阈值** *T* 。而且，每个非终端节点最多可以包含 *B* 元组。该策略旨在通过减少存储的数据量和计算次数，最大限度地提高仅依赖主内存的流处理的性能。

现在让我们考虑一个需要插入的新样本 *x <sub class="calibre20">i</sub>* 。很容易理解，a *CF <sub class="calibre20">j</sub> = (n <sub class="calibre20">j</sub> 、a <sub class="calibre20">j</sub> 、b <sub class="calibre20">j</sub> )* 的质心简单来说就是*μ<sub class="calibre20">j</sub>= a<sub class="calibre20">j</sub>/n<sub class="calibre20">j</sub>*；因此， *x <sub class="calibre20">i</sub>* 由于到达末端 CF(子集群)而沿着树传播，其中距离 *d(x <sub class="calibre20">i</sub> ，μ <sub class="calibre20">j</sub> )* 最小。此时，CF 会逐步更新:

![](img/3a24c4e7-7463-4f9f-a215-c21bf165d098.png)

但是，如果没有控制，树很容易变得不平衡，从而导致性能损失。因此，该算法执行额外的步骤。一旦确定了 CF，就计算更新的半径*r<sub class="calibre20">j</sub>T3 】,并且无论*r<sub class="calibre20">j</sub>T30*和 CFs 的数量是否大于分支因子，都分配新的块并且保持原始 CF 不变。由于这个新的区块几乎完全是空的(除了 *x <sub class="calibre20">i</sub>* )，BIRCH 执行一个额外的步骤来检查所有子集群之间的差异(这个概念将在[第 4 章](https://cdp.packtpub.com/hands_on_unsupervised_learning_with_python/wp-admin/post.php?post=26&action=edit#post_27)、*行动中的层次聚类*中更清楚；然而，读者可以考虑属于两个不同子聚类的点之间的平均距离)。最不相似的一对被分成两部分，其中一个被移到新的区块。这样的选择保证了子集群的高度紧凑性，并加快了最后一步。实际上，实际聚类阶段涉及的算法需要合并子聚类，直到总数减少到期望值；因此，如果总相异度先前已经被最小化，则更容易执行该操作，因为*段*可以被立即识别为连续的并被合并。本章不会详细讨论这个阶段，但不难想象。所有终端 CFs 被顺序合并到更大的块中，直到确定了单个集群(即使当数量与期望的集群数量匹配时有可能停止该过程)。因此，与小批量 K-means 相反，这种方法可以轻松管理大量集群 *n <sub class="calibre20">c</sub>* ，而当 *n <sub class="calibre20">c</sub>* 很小时，这种方法不是很有效。事实上，正如我们将在示例中看到的，它的精度通常低于小批量 k-means 所能达到的精度，并且它的最佳使用需要准确选择分支因子和阈值。由于该算法的主要目的是在在线场景下工作，因此 *B* 和 *T* 在处理一些批次后可能会变得无效(而小批次 K 均值通常能够在几次迭代后校正聚类)，从而产生次优结果。因此，BIRCH 的主要用例是一个在线过程，需要非常细粒度的分割，而在所有其他情况下，通常最好选择小批量 K 均值作为初始选项。*

# 小批量 K 均值与 BIRCH 的比较

在本例中，我们希望将两种算法的性能与包含 2000 个样本的二维数据集进行比较，这些样本被分割成`8`个斑点(因为目的是分析性的，所以我们也使用基本事实)，如下所示:

```py
from sklearn.datasets import make_blobs

nb_clusters = 8
nb_samples = 2000

X, Y = make_blobs(n_samples=nb_samples, n_features=2, centers=nb_clusters,
                  cluster_std=0.25, center_box=[-1.5, 1.5], shuffle=True, random_state=100)
```

数据集(已经被混洗以消除流过程中的任何相互关联)显示在下面的截图中:

![](img/c79cce09-3214-4f88-90af-bd1485460604.png)

Bidimensional dataset for a comparison between mini-batch K-means and BIRCH

在执行在线聚类之前，评估标准 K 均值的调整后的 Rand 分数是有帮助的，如下所示:

```py
from sklearn.cluster import KMeans

km = KMeans(n_clusters=nb_clusters, random_state=1000)
Y_pred_km = km.fit_predict(X)

print('Adjusted Rand score: {}'.format(adjusted_rand_score(Y, Y_pred_km)))
```

上一个块的输出如下:

```py
Adjusted Rand score: 0.8232109771787882
```

考虑到数据集的结构(没有凹陷)，我们可以合理地假设该值代表在线流程的基准。我们现在可以实例化类`MiniBatchKMeans`和`Birch`，参数分别等于`reassignment_ratio=0.001`、`threshold=0.2`和`branching_factor=350`。这些值是在研究后选择的，但我邀请读者用不同的配置重复这个例子，比较结果。在这两种情况下，我们假设批次大小等于`50`样品，如下所示:

```py
from sklearn.cluster import MiniBatchKMeans, Birch

batch_size = 50

mbkm = MiniBatchKMeans(n_clusters=nb_clusters, batch_size=batch_size, reassignment_ratio=0.001, random_state=1000)
birch = Birch(n_clusters=nb_clusters, threshold=0.2, branching_factor=350)
```

该示例的目标是现在使用方法`partial_fit()`递增地训练两个模型，并评估调整后的兰德分数，考虑到直到每一步处理的全部数据量，如下所示:

```py
from sklearn.metrics import adjusted_rand_score

scores_mbkm = []
scores_birch = []

for i in range(0, nb_samples, batch_size):
    X_batch, Y_batch = X[i:i+batch_size], Y[i:i+batch_size]

    mbkm.partial_fit(X_batch)
    birch.partial_fit(X_batch)

    scores_mbkm.append(adjusted_rand_score(Y[:i+batch_size], mbkm.predict(X[:i+batch_size])))
    scores_birch.append(adjusted_rand_score(Y[:i+batch_size], birch.predict(X[:i+batch_size])))

print('Adjusted Rand score Mini-Batch K-Means: {}'.format(adjusted_rand_score(Y, Y_pred_mbkm)))
print('Adjusted Rand score BIRCH: {}'.format(adjusted_rand_score(Y, Y_pred_birch)))
```

前一个片段的输出包含整个数据集的调整后兰德分数:

```py
Adjusted Rand score Mini-Batch K-Means: 0.814244790452388
Adjusted Rand score BIRCH: 0.767304858161472
```

不出所料，小批量 K-means 在所有样本处理完毕后几乎达到基准，而 BIRCH 的性能稍差。为了更好地理解这种行为，让我们考虑一个增量分数作为批次函数的图，如下图所示:

![](img/54ea0ef1-87a0-4c51-b761-14700f6c18b4.png)

Incremental adjusted Rand scores as functions of the batches (number of samples)

如你所见，小批量 K 均值很快达到最大值，所有后续振荡都是由于重新分配。相反，随着负面趋势的出现，BIRCH 的表现总是更差。造成这种差异的主要原因是不同的策略。事实上，小批量 K-means 可以在几个批次后纠正质心的初始猜测，并且重新分配不会显著改变配置。另一方面，BIRCH 执行的合并数量受样本数量的影响。

在开始时，由于 CF-Tree 中的子集群数量不是很大(因此，聚合更*连贯*，但是在几个批次之后，BIRCH 不得不聚合越来越多的子集群，以便获得期望的最终集群数量，因此性能没有很大的不同。这种情况，加上流样本数量的增加，驱动算法重新排列树，通常会导致稳定性的损失。此外，数据集有一些重叠，可以通过对称方法更容易地管理(在这种情况下，质心实际上可以到达它们的最终位置，即使分配是错误的)，而分层方法(例如 BIRCH 使用的方法)更能够找到所有子区域，但是在以最小的间隔合并子聚类时更容易出错，或者更糟糕的是，重叠。然而，这个例子证实了小批量 K-means 作为第一选项通常是优选的，并且只有当性能没有达到预期时才应该选择 BIRCH(仔细选择其参数)。我邀请读者用更大数量的期望簇(例如`nb_clusters=20`和`center_box=[-10.5, 10.5]`)重复该示例。有可能看到在这种情况下(保持所有其他参数不变)，由小批量 K-means 执行的重新分配如何以更差的最终调整兰德分数减缓收敛，而 BIRCH 立即达到最佳值(几乎等于标准 K-means 实现的值)，并且它不再受样本数量的影响。

# 摘要

在这一章中，我们介绍了一些最重要的聚类算法，可以用来解决非凸问题。光谱聚类是一种非常流行的技术，它将数据集投影到一个新的空间中，在这个空间中，凹的几何形状变成凸的，标准算法(如 K-means)可以轻松地分割数据。

相反，mean shift 和 DBSCAN 分析数据集的密度，并尝试将其拆分，以便将所有密集和连接的区域合并在一起，组成聚类。特别是，DBSCAN 在非常不规则的上下文中非常有效，因为它基于连接在一起的本地最近邻集，直到间隔超过预定义的阈值。以这种方式，该算法可以解决许多特定的聚类问题，唯一的缺点是它还产生一组噪声点，这些噪声点不能自动分配给现有的聚类。在基于工作缺勤数据集的示例中，我们展示了如何选择超参数，以便获得所需数量的聚类，同时具有最小数量的噪声点和可接受的轮廓或 Calinski-Harabasz 分数。

在最后一部分中，我们分析了 K-med oid 作为 K-means 的一种替代方法，它对异常值的鲁棒性也更强。该算法不能用于解决非凸问题，但它有时比 K 均值更有效，因为它不选择实际均值作为质心，而是仅依赖数据集，聚类中心(称为 medoids)是示例性样本。此外，该算法不受欧几里德度量的严格限制，因此可以充分挖掘替代距离函数的潜力。最后一个主题涉及两种在线聚类算法(小批量 K-means 和 BIRCH)，当数据集太大而无法放入内存时，或者当数据在长时间帧内流动时，可以使用这两种算法。

在下一章中，我们将分析一个非常重要的聚类算法家族，它可以输出完整的层次结构，使我们能够观察完整的聚合过程，并选择最有帮助和最一致的最终配置。

# 问题

1.  半月形数据集是凸簇吗？
2.  二维数据集由两个半月组成。第二个完全包含在第一个的凹度中。哪种核可以很容易地允许两个聚类的分离(使用谱聚类)？
3.  应用 *ε=1.0* 的 DBSCAN 算法后，我们发现噪声点太多。 *ε=0.1* 我们应该期待什么？
4.  k-med oid 基于欧几里得度量。这是正确的吗？
5.  DBSCAN 对数据集的几何非常敏感。这是正确的吗？
6.  一个数据集包含 10，000，000 个样本，可以使用大型机器使用 K-means 轻松进行聚类。相反，我们可以使用更小的机器和小批量 K-means 吗？
7.  聚类的标准偏差等于 1.0。施加一个噪声 *N* (0，0.005)后，80%的原始赋值被改变。我们能说这样的集群配置总体稳定吗？

# 进一步阅读

*   *归一化切割和图像分割*，*史和马立克*， *IEEE 模式分析和机器智能交易*，*第 22 卷*，08/2000
*   *光谱聚类教程*，*冯卢克斯堡大学*，2007
*   *函数与图形第 2 卷*、*盖尔范德 I. M.* 、*格拉戈列娃 E. G.* 、*施诺尔 E.* *E.* 、*麻省理工学院出版社，* 1969
*   *关于概率密度函数和模式的估计*，*帕尔森 E.* ，*数理统计年鉴，* 33，1962
*   *神经模糊网络在预测工作缺勤中的应用，马蒂亚诺，费雷拉，萨希，阿方索，信息系统与技术(CISTI)* ，*第七届伊比利亚会议(第 1-4 页)。IEEE* ，2012 年
*   *一种基于密度的算法，用于在有噪声的大型空间数据库中发现聚类*，*埃斯特 M.* ，*克里格尔 H. P.* ，*桑德 J.* ，*徐 X.* ，*第二届知识发现和数据挖掘国际会议论文集，美国俄勒冈州波特兰市，出版社*，1996
*   *Mac* *hine 学习算法**第二版**Bonaccorso g .**Packt 出版*，2018
*   *集群稳定性:概述*，*冯卢克斯堡大学*， *arXiv 1007:1075v1* ，2010
*   *通过水母* *，考夫曼 l，罗塞夫 P.J .，在基于 L1**-规范和相关方法**的统计数据分析中，北荷兰*，1987****